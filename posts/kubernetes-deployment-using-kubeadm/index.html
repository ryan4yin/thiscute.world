<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>部署一个 Kubernetes 集群 - This Cute World</title><meta name=Description content="This Cute World"><meta property="og:title" content="部署一个 Kubernetes 集群"><meta property="og:description" content="本文完成于 2022-01-25，其中部分内容已经过时，仅供参考。 本文由个人笔记 ryan4yin/knowledge 整理而来，不保证正确 本地 Kubernetes 集群安装工具 云上的 Kubernetes 集群，基本上各"><meta property="og:type" content="article"><meta property="og:url" content="https://thiscute.world/posts/kubernetes-deployment-using-kubeadm/"><meta property="og:image" content="https://thiscute.world/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-25T01:37:00+08:00"><meta property="article:modified_time" content="2022-01-25T01:37:00+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://thiscute.world/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp"><meta name=twitter:title content="部署一个 Kubernetes 集群"><meta name=twitter:description content="本文完成于 2022-01-25，其中部分内容已经过时，仅供参考。 本文由个人笔记 ryan4yin/knowledge 整理而来，不保证正确 本地 Kubernetes 集群安装工具 云上的 Kubernetes 集群，基本上各"><meta name=application-name content="This Cute World"><meta name=apple-mobile-web-app-title content="This Cute World"><meta name=theme-color content="#f8f8f8"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=canonical href=https://thiscute.world/posts/kubernetes-deployment-using-kubeadm/><link rel=prev href=https://thiscute.world/posts/kubernetes-best-practices/><link rel=next href=https://thiscute.world/posts/python-tips-and-tricks/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><meta name=google-site-verification content="E8bpp1lVVlb9YnSJcUzPL1dLAG17Nl_sp5Ru9a8tUDQ"><meta name=baidu-site-verification content="code-ZZtDruAnX1"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"部署一个 Kubernetes 集群","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/thiscute.world\/posts\/kubernetes-deployment-using-kubeadm\/"},"image":[{"@type":"ImageObject","url":"https:\/\/thiscute.world\/posts\/kubernetes-deployment-using-kubeadm\/adopt-kubernetes.webp","width":1920,"height":960}],"genre":"posts","keywords":"Kubernetes, 云原生","wordcount":6277,"url":"https:\/\/thiscute.world\/posts\/kubernetes-deployment-using-kubeadm\/","datePublished":"2022-01-25T01:37:00+08:00","dateModified":"2022-01-25T01:37:00+08:00","publisher":{"@type":"Organization","name":"ryan4yin","logo":"https:\/\/thiscute.world\/avatar\/myself.png"},"author":{"@type":"Person","name":"ryan4yin"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="This Cute World"><span id=desktop-header-typeit class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/statistics/>阅读排行 </a><a class=menu-item href=/categories/tech/>技术 </a><a class=menu-item href=/categories/life/>生活 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/friends/>朋友们 </a><a class=menu-item href=/now/>此刻 </a><a class=menu-item href=/about/>关于 </a><span class="menu-item delimiter"></span><a href=# onclick=return!1 class="menu-item language" title=选择语言>Simplified Chinese<i class="fas fa-chevron-right fa-fw"></i>
<select class=language-select title="Select Language" id=language-select-desktop onchange="location=this.value"><option value=/posts/kubernetes-deployment-using-kubeadm/ selected>Simplified Chinese</option></select>
</a><span class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span>
</span><a href=# onclick=return!1 class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="This Cute World"><span id=mobile-header-typeit class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=# onclick=return!1 class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/statistics/ title>阅读排行</a><a class=menu-item href=/categories/tech/ title>技术</a><a class=menu-item href=/categories/life/ title>生活</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/friends/ title>朋友们</a><a class=menu-item href=/now/ title>此刻</a><a class=menu-item href=/about/ title>关于</a><a href=# onclick=return!1 class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
</a><a href=# onclick=return!1 class=menu-item title=选择语言>Simplified Chinese<i class="fas fa-chevron-right fa-fw"></i>
<select class=language-select title="Select Language" onchange="location=this.value"><option value=/posts/kubernetes-deployment-using-kubeadm/ selected>Simplified Chinese</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#本地-kubernetes-集群安装工具>本地 Kubernetes 集群安装工具</a></li><li><a href=#0-网络环境的准备>0. 网络环境的准备</a></li><li><a href=#1-节点的环境准备>1. 节点的环境准备</a><ul><li><a href=#11-iptables-设置>1.1 iptables 设置</a></li><li><a href=#12-开放节点端口>1.2 开放节点端口</a></li></ul></li><li><a href=#2-安装容器运行时-containerd>2. 安装容器运行时 containerd</a></li><li><a href=#3-安装-kubeletkubeadmkubectl>3. 安装 kubelet/kubeadm/kubectl</a></li><li><a href=#4-为-master-的-kube-apiserver-创建负载均衡实现高可用>4. 为 master 的 kube-apiserver 创建负载均衡实现高可用</a></li><li><a href=#5-使用-kubeadm-创建集群>5. 使用 kubeadm 创建集群</a><ul><li><a href=#51-常见问题>5.1 常见问题</a><ul><li><a href=#511-重置集群配置>5.1.1 重置集群配置</a></li></ul></li></ul></li><li><a href=#6-验证集群的高可用性>6. 验证集群的高可用性</a></li><li><a href=#7-安装网络插件>7. 安装网络插件</a><ul><li><a href=#71-安装-cilium>7.1 安装 Cilium</a></li><li><a href=#72-安装-calico>7.2 安装 Calico</a></li></ul></li><li><a href=#8-查看集群状态>8. 查看集群状态</a></li><li><a href=#9-安装-metrics-server>9. 安装 metrics-server</a></li><li><a href=#10-为-etcd-添加定期备份能力>10. 为 etcd 添加定期备份能力</a></li><li><a href=#11-安装-volume-provisioner>11. 安装 Volume Provisioner</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">部署一个 Kubernetes 集群</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=https://thiscute.world/ title=Author target=_blank rel="noopener noreferrer author" class=author>ryan4yin</a>
</span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/tech/><i class="far fa-folder fa-fw"></i>tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-01-25>2022-01-25</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-01-25>2022-01-25</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 6277 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 13 分钟&nbsp;</div></div><div class=featured-image><img loading=eager src=/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp srcset="/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp, /posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp 1.5x, /posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp 2x" sizes=auto alt=/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp title=/posts/kubernetes-deployment-using-kubeadm/adopt-kubernetes.webp height=960 width=1920></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#本地-kubernetes-集群安装工具>本地 Kubernetes 集群安装工具</a></li><li><a href=#0-网络环境的准备>0. 网络环境的准备</a></li><li><a href=#1-节点的环境准备>1. 节点的环境准备</a><ul><li><a href=#11-iptables-设置>1.1 iptables 设置</a></li><li><a href=#12-开放节点端口>1.2 开放节点端口</a></li></ul></li><li><a href=#2-安装容器运行时-containerd>2. 安装容器运行时 containerd</a></li><li><a href=#3-安装-kubeletkubeadmkubectl>3. 安装 kubelet/kubeadm/kubectl</a></li><li><a href=#4-为-master-的-kube-apiserver-创建负载均衡实现高可用>4. 为 master 的 kube-apiserver 创建负载均衡实现高可用</a></li><li><a href=#5-使用-kubeadm-创建集群>5. 使用 kubeadm 创建集群</a><ul><li><a href=#51-常见问题>5.1 常见问题</a><ul><li><a href=#511-重置集群配置>5.1.1 重置集群配置</a></li></ul></li></ul></li><li><a href=#6-验证集群的高可用性>6. 验证集群的高可用性</a></li><li><a href=#7-安装网络插件>7. 安装网络插件</a><ul><li><a href=#71-安装-cilium>7.1 安装 Cilium</a></li><li><a href=#72-安装-calico>7.2 安装 Calico</a></li></ul></li><li><a href=#8-查看集群状态>8. 查看集群状态</a></li><li><a href=#9-安装-metrics-server>9. 安装 metrics-server</a></li><li><a href=#10-为-etcd-添加定期备份能力>10. 为 etcd 添加定期备份能力</a></li><li><a href=#11-安装-volume-provisioner>11. 安装 Volume Provisioner</a></li></ul></nav></div></div><div class=content id=content><blockquote><p>本文完成于 2022-01-25，其中部分内容已经过时，仅供参考。</p></blockquote><blockquote><p>本文由个人笔记 <a href=https://github.com/ryan4yin/knowledge/tree/master/kubernetes target=_blank rel="noopener noreferrer">ryan4yin/knowledge</a> 整理而来，不保证正确</p></blockquote><h2 id=本地-kubernetes-集群安装工具 class=headerLink><a href=#%e6%9c%ac%e5%9c%b0-kubernetes-%e9%9b%86%e7%be%a4%e5%ae%89%e8%a3%85%e5%b7%a5%e5%85%b7 class=header-mark></a>本地 Kubernetes 集群安装工具</h2><blockquote><p>云上的 Kubernetes 集群，基本上各云厂商都支持一键部署。这里主要关注本地部署，或者叫做裸机(baremetal)部署</p></blockquote><blockquote><p>本文介绍的方法适合开发测试使用，安全性、稳定性、长期可用性等方案都可能还有问题。</p></blockquote><p>kubernetes 是一个组件化的系统，安装过程有很大的灵活性，很多组件都有多种实现，这些实现各有特点，让初学者眼花缭乱。</p><p>而且要把这些组件一个个安装配置好并且能协同工作，也是很不容易的。</p><p>因此社区出现了各种各样的安装方案，下面介绍下几种支持裸机（Baremetal）部署的工具：</p><ol><li><a href=https://github.com/kubernetes/kubeadm target=_blank rel="noopener noreferrer">kubeadm</a>: 社区的集群安装工具，目前已经很成熟了。<ol><li>使用难度：简单</li></ol></li><li><a href=https://github.com/k3s-io/k3s target=_blank rel="noopener noreferrer">k3s</a>: 轻量级 kubernetes，资源需求小，部署非常简单，适合开发测试用或者边缘环境<ol><li>支持 airgap 离线部署</li><li>使用难度：超级简单</li></ol></li><li><a href=https://github.com/alibaba/sealer target=_blank rel="noopener noreferrer">alibaba/sealer</a>: 支持将整个 kubernetes 打包成一个镜像进行交付，而且部署也非常简单。<ol><li>使用难度：超级简单</li><li>这个项目目前还在发展中，不过貌似已经有很多 toB 的公司在使用它进行 k8s 应用的交付了。</li></ol></li><li><a href=https://github.com/kubernetes-sigs/kubespray target=_blank rel="noopener noreferrer">kubespray</a>: 适合自建生产级别的集群，是一个大而全的 kubernetes 安装方案，自动安装容器运行时、k8s、网络插件等组件，而且各组件都有很多方案可选，但是感觉有点复杂。<ol><li>使用难度：中等</li><li>支持 airgap 离线部署，但是以前我试用过是有坑，现在不知道咋样了</li><li>底层使用了 kubeadm 部署集群</li></ol></li><li><a href=https://github.com/labring/sealos target=_blank rel="noopener noreferrer">sealos</a>: 也很方便，一行命令部署</li><li>其他社区部署方案</li><li>自己写脚本，使用各组件的二进制文件进行部署。</li></ol><p>笔者为了学习 Kubernetes，下面采用官方的 kubeadm 进行部署，容器运行时使用 containerd，网络插件则使用目前最潮的基于 eBPF 的 Cilium.</p><p>kubernetes 官方介绍了两种高可用集群的拓扑结构：「堆叠 Etcd 拓扑（Stacked Etcd Topology）」和「外部 Etcd 拓扑（External Etcd Topology）」。
「堆叠 Etcd 拓扑」是指 Etcd 跟 Kubernetes Master 的其他组件部署在同一节点上，而「外部 Etcd 拓扑（External Etcd Topology）」则是指 Etcd 单独部署，与 Kubernetes Master 分开。</p><p>简单起见，本文使用「堆叠 Etcd 拓扑」结构，创建一个 3 master 的高可用集群。</p><p>参考：</p><ul><li><a href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ target=_blank rel="noopener noreferrer">Kubernetes Docs - Installing kubeadm</a></li><li><a href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/ target=_blank rel="noopener noreferrer">Kubernetes Docs - Creating Highly Available clusters with kubeadm</a></li></ul><h2 id=0-网络环境的准备 class=headerLink><a href=#0-%e7%bd%91%e7%bb%9c%e7%8e%af%e5%a2%83%e7%9a%84%e5%87%86%e5%a4%87 class=header-mark></a>0. 网络环境的准备</h2><p>本文行文未考虑国内网络环境，但是 Kubernetes 用到的很多镜像都在 gcr.io 上，在国内访问会有困难，这里提供三个解决办法：</p><ul><li>在家庭路由器上整个科学代理，实现全局科学上网。（我就是这么干的）</li><li>使用 <a href=https://github.com/liangyuanpeng target=_blank rel="noopener noreferrer">liangyuanpeng</a> 大佬在评论区提供的 gcr 国内镜像地址，这需要进行如下替换：<ul><li>k8s.gcr.io&mdash;> lank8s.cn</li></ul></li><li>自己维护一个国内镜像仓库（或私有镜像仓库如 harbor），使用 <code>skopeo</code> 等工具或脚本将上述镜像列表拷贝到你的私有仓库</li></ul><p>如果对可靠性要求高，最好是选择第三个方案——自建私有镜像仓库，把镜像推送到私有仓库。可以通过如下命令列出所有 kubeadm 需要用到的镜像地址（需要先安装好 kubeadm，建议读完本篇文章全文后再尝试）：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>❯ kubeadm config images list --kubernetes-version v1.22.1
</span></span><span class=line><span class=cl>k8s.gcr.io/kube-apiserver:v1.22.1
</span></span><span class=line><span class=cl>k8s.gcr.io/kube-controller-manager:v1.22.1
</span></span><span class=line><span class=cl>k8s.gcr.io/kube-scheduler:v1.22.1
</span></span><span class=line><span class=cl>k8s.gcr.io/kube-proxy:v1.22.1
</span></span><span class=line><span class=cl>k8s.gcr.io/pause:3.5
</span></span><span class=line><span class=cl>k8s.gcr.io/etcd:3.5.0-0
</span></span><span class=line><span class=cl>k8s.gcr.io/coredns/coredns:v1.8.4
</span></span></code></pre></td></tr></table></div></div><h2 id=1-节点的环境准备 class=headerLink><a href=#1-%e8%8a%82%e7%82%b9%e7%9a%84%e7%8e%af%e5%a2%83%e5%87%86%e5%a4%87 class=header-mark></a>1. 节点的环境准备</h2><p>首先准备三台 Linux 虚拟机，系统按需选择，然后调整这三台机器的设置：</p><ul><li>节点配置：<ul><li>master：不低于 2c/3g，硬盘 20G<ul><li>主节点性能也受集群 Pods 个数的影响，上述配置应该可以支撑到每个 Worker 节点跑 100 个 Pod.</li></ul></li><li>worker：看需求，建议不低于 2c/4g，硬盘不小于 20G，资源充分的话建议 40G 以上。</li></ul></li><li>处于同一网络内并可互通（通常是同一局域网）</li><li>各主机的 hostname 和 mac/ip 地址以及 <code>/sys/class/dmi/id/product_uuid</code>，都必须唯一<ul><li>这里新手最容易遇到的问题，是 hostname 冲突</li></ul></li><li><strong>必须</strong>关闭 swap 交换内存，kubelet 才能正常工作</li></ul><p>方便起见，我直接使用 <a href=https://github.com/ryan4yin/pulumi-libvirt#examples target=_blank rel="noopener noreferrer">ryan4yin/pulumi-libvirt</a> 自动创建了五个 opensuse leap 15.3 虚拟机，并设置好了 ip/hostname.</p><h3 id=11-iptables-设置 class=headerLink><a href=#11-iptables-%e8%ae%be%e7%bd%ae class=header-mark></a>1.1 iptables 设置</h3><p>目前 kubernetes 的容器网络，默认使用的是 bridge 模式，这种模式下，需要使 <code>iptables</code> 能够接管 bridge 上的流量。</p><p>配置如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>sudo modprobe br_netfilter
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span></span></span><span class=line><span class=cl><span class=s>br_netfilter
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span></span></span><span class=line><span class=cl><span class=s>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span class=line><span class=cl><span class=s>net.bridge.bridge-nf-call-iptables = 1
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>sudo sysctl --system
</span></span></code></pre></td></tr></table></div></div><h3 id=12-开放节点端口 class=headerLink><a href=#12-%e5%bc%80%e6%94%be%e8%8a%82%e7%82%b9%e7%ab%af%e5%8f%a3 class=header-mark></a>1.2 开放节点端口</h3><blockquote><p>局域网环境的话，建议直接关闭防火墙。这样所有端口都可用，方便快捷。</p></blockquote><blockquote><p>通常我们的云上集群，也是关闭防火墙的，只是会通过云服务提供的「安全组」来限制客户端 ip</p></blockquote><p>Control-plane 节点，也就是 master，需要开放如下端口：</p><table><thead><tr><th>Protocol</th><th>Direction</th><th>Port Range</th><th>Purpose</th><th>Used By</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>6443*</td><td>Kubernetes API server</td><td>All</td></tr><tr><td>TCP</td><td>Inbound</td><td>2379-2380</td><td>etcd server client API</td><td>kube-apiserver, etcd</td></tr><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>kubelet API</td><td>Self, Control plane</td></tr><tr><td>TCP</td><td>Inbound</td><td>10251</td><td>kube-scheduler</td><td>Self</td></tr><tr><td>TCP</td><td>Inbound</td><td>10252</td><td>kube-controller-manager</td><td>Self</td></tr></tbody></table><p>Worker 节点需要开发如下端口：</p><table><thead><tr><th>Protocol</th><th>Direction</th><th>Port Range</th><th>Purpose</th><th>Used By</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>kubelet API</td><td>Self, Control plane</td></tr><tr><td>TCP</td><td>Inbound</td><td>30000-32767</td><td>NodePort Services†</td><td>All</td></tr></tbody></table><p>另外通常我们本地测试的时候，可能更想直接在 <code>80</code> <code>443</code> <code>8080</code> 等端口上使用 <code>NodePort</code>，
就需要修改 kube-apiserver 的 <code>--service-node-port-range</code> 参数来自定义 NodePort 的端口范围，相应的 Worker 节点也得开放这些端口。</p><h2 id=2-安装容器运行时-containerd class=headerLink><a href=#2-%e5%ae%89%e8%a3%85%e5%ae%b9%e5%99%a8%e8%bf%90%e8%a1%8c%e6%97%b6-containerd class=header-mark></a>2. 安装容器运行时 containerd</h2><p>首先是环境配置：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span></span></span><span class=line><span class=cl><span class=s>overlay
</span></span></span><span class=line><span class=cl><span class=s>br_netfilter
</span></span></span><span class=line><span class=cl><span class=s>nf_conntrack
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>sudo modprobe overlay
</span></span><span class=line><span class=cl>sudo modprobe br_netfilter
</span></span><span class=line><span class=cl>sudo modprobe nf_conntrack
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Setup required sysctl params, these persist across reboots.</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span></span></span><span class=line><span class=cl><span class=s>net.bridge.bridge-nf-call-iptables  = 1
</span></span></span><span class=line><span class=cl><span class=s>net.ipv4.ip_forward                 = 1
</span></span></span><span class=line><span class=cl><span class=s>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply sysctl params without reboot</span>
</span></span><span class=line><span class=cl>sudo sysctl --system
</span></span></code></pre></td></tr></table></div></div><p>安装 containerd+nerdctl:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>wget https://github.com/containerd/nerdctl/releases/download/v0.11.1/nerdctl-full-0.11.1-linux-amd64.tar.gz
</span></span><span class=line><span class=cl>tar -axvf nerdctl-full-0.11.1-linux-amd64.tar.gz
</span></span><span class=line><span class=cl><span class=c1># 这里简单起见，rootless 相关的东西也一起装进去了，测试嘛就无所谓了...</span>
</span></span><span class=line><span class=cl>mv bin/* /usr/local/bin/
</span></span><span class=line><span class=cl>mv lib/systemd/system/containerd.service /usr/lib/systemd/system/
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>systemctl <span class=nb>enable</span> containerd
</span></span><span class=line><span class=cl>systemctl start containerd
</span></span></code></pre></td></tr></table></div></div><p><code>nerdctl</code> 是一个 containerd 的命令行工具，但是它的容器、镜像与 Kubernetes 的容器、镜像是完全隔离的，不能互通！</p><p>目前只能通过 <code>crictl</code> 来查看、拉取 Kubernetes 的容器、镜像，下一节会介绍 crictl 的安装。</p><h2 id=3-安装-kubeletkubeadmkubectl class=headerLink><a href=#3-%e5%ae%89%e8%a3%85-kubeletkubeadmkubectl class=header-mark></a>3. 安装 kubelet/kubeadm/kubectl</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># 一些全局都需要用的变量</span>
</span></span><span class=line><span class=cl><span class=nv>CNI_VERSION</span><span class=o>=</span><span class=s2>&#34;v0.8.2&#34;</span>
</span></span><span class=line><span class=cl><span class=nv>CRICTL_VERSION</span><span class=o>=</span><span class=s2>&#34;v1.17.0&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># kubernetes 的版本号</span>
</span></span><span class=line><span class=cl><span class=c1># RELEASE=&#34;$(curl -sSL https://dl.k8s.io/release/stable.txt)&#34;</span>
</span></span><span class=line><span class=cl><span class=nv>RELEASE</span><span class=o>=</span><span class=s2>&#34;1.22.1&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># kubelet 配置文件的版本号</span>
</span></span><span class=line><span class=cl><span class=nv>RELEASE_VERSION</span><span class=o>=</span><span class=s2>&#34;v0.4.0&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 架构</span>
</span></span><span class=line><span class=cl><span class=nv>ARCH</span><span class=o>=</span><span class=s2>&#34;amd64&#34;</span>
</span></span><span class=line><span class=cl><span class=c1>#　安装目录</span>
</span></span><span class=line><span class=cl><span class=nv>DOWNLOAD_DIR</span><span class=o>=</span>/usr/local/bin
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># CNI 插件</span>
</span></span><span class=line><span class=cl>sudo mkdir -p /opt/cni/bin
</span></span><span class=line><span class=cl>curl -L <span class=s2>&#34;https://github.com/containernetworking/plugins/releases/download/</span><span class=si>${</span><span class=nv>CNI_VERSION</span><span class=si>}</span><span class=s2>/cni-plugins-linux-</span><span class=si>${</span><span class=nv>ARCH</span><span class=si>}</span><span class=s2>-</span><span class=si>${</span><span class=nv>CNI_VERSION</span><span class=si>}</span><span class=s2>.tgz&#34;</span> <span class=p>|</span> sudo tar -C /opt/cni/bin -xz
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># crictl 相关工具</span>
</span></span><span class=line><span class=cl>curl -L <span class=s2>&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span class=si>${</span><span class=nv>CRICTL_VERSION</span><span class=si>}</span><span class=s2>/crictl-</span><span class=si>${</span><span class=nv>CRICTL_VERSION</span><span class=si>}</span><span class=s2>-linux-</span><span class=si>${</span><span class=nv>ARCH</span><span class=si>}</span><span class=s2>.tar.gz&#34;</span> <span class=p>|</span> sudo tar -C <span class=nv>$DOWNLOAD_DIR</span> -xz
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># kubelet/kubeadm/kubectl</span>
</span></span><span class=line><span class=cl><span class=nb>cd</span> <span class=nv>$DOWNLOAD_DIR</span>
</span></span><span class=line><span class=cl>sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span class=si>${</span><span class=nv>RELEASE</span><span class=si>}</span>/bin/linux/<span class=si>${</span><span class=nv>ARCH</span><span class=si>}</span>/<span class=o>{</span>kubeadm,kubelet,kubectl<span class=o>}</span>
</span></span><span class=line><span class=cl>sudo chmod +x <span class=o>{</span>kubeadm,kubelet,kubectl<span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># kubelet/kubeadm 配置</span>
</span></span><span class=line><span class=cl>curl -sSL <span class=s2>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span class=si>${</span><span class=nv>RELEASE_VERSION</span><span class=si>}</span><span class=s2>/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> <span class=p>|</span> sed <span class=s2>&#34;s:/usr/bin:</span><span class=si>${</span><span class=nv>DOWNLOAD_DIR</span><span class=si>}</span><span class=s2>:g&#34;</span> <span class=p>|</span> sudo tee /etc/systemd/system/kubelet.service
</span></span><span class=line><span class=cl>sudo mkdir -p /etc/systemd/system/kubelet.service.d
</span></span><span class=line><span class=cl>curl -sSL <span class=s2>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span class=si>${</span><span class=nv>RELEASE_VERSION</span><span class=si>}</span><span class=s2>/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> <span class=p>|</span> sed <span class=s2>&#34;s:/usr/bin:</span><span class=si>${</span><span class=nv>DOWNLOAD_DIR</span><span class=si>}</span><span class=s2>:g&#34;</span> <span class=p>|</span> sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>systemctl <span class=nb>enable</span> --now kubelet
</span></span><span class=line><span class=cl><span class=c1># 验证 kubelet 启动起来了，但是目前还没有初始化配置，过一阵就会重启一次</span>
</span></span><span class=line><span class=cl>systemctl status kubelet
</span></span></code></pre></td></tr></table></div></div><p>试用 crictl:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nb>export</span> <span class=nv>CONTAINER_RUNTIME_ENDPOINT</span><span class=o>=</span><span class=s1>&#39;unix:///var/run/containerd/containerd.sock&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># 列出所有 pods，现在应该啥也没</span>
</span></span><span class=line><span class=cl>crictl  pods
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 列出所有镜像</span>
</span></span><span class=line><span class=cl>crictl images
</span></span></code></pre></td></tr></table></div></div><h2 id=4-为-master-的-kube-apiserver-创建负载均衡实现高可用 class=headerLink><a href=#4-%e4%b8%ba-master-%e7%9a%84-kube-apiserver-%e5%88%9b%e5%bb%ba%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1%e5%ae%9e%e7%8e%b0%e9%ab%98%e5%8f%af%e7%94%a8 class=header-mark></a>4. 为 master 的 kube-apiserver 创建负载均衡实现高可用</h2><p>根据 kubeadm 官方文档 <a href=https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#kube-vip target=_blank rel="noopener noreferrer">Kubeadm Docs - High Availability Considerations</a> 介绍，要实现 kube-apiserver 的高可用，目前最知名的负载均衡方式是 keepalived+haproxy，另外也可以考虑使用 kube-vip 等更简单的工具。</p><p>简单起见，我们直接用 kube-vip 吧，参考了 kube-vip 的官方文档：<a href=https://kube-vip.io/install_static/ target=_blank rel="noopener noreferrer">Kube-vip as a Static Pod with Kubelet</a>.</p><blockquote><p>P.S. 我也见过有的安装工具会直接抛弃 keepalived，直接在每个节点上跑一个 nginx 做负载均衡，配置里写死了所有 master 的地址&mldr;</p></blockquote><p>首先使用如下命令生成 kube-vip 的配置文件，以 ARP 为例（生产环境建议换成 BGP）：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF | sudo tee add-kube-vip.sh
</span></span></span><span class=line><span class=cl><span class=s># 你的虚拟机网卡，opensuse/centos 等都是 eth0，但是 ubuntu 可能是 ens3
</span></span></span><span class=line><span class=cl><span class=s>export INTERFACE=eth0
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s># 用于实现高可用的 vip，需要和前面的网络接口在同一网段内，否则就无法路由了。
</span></span></span><span class=line><span class=cl><span class=s>export VIP=192.168.122.200
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s># 生成 static-pod 的配置文件
</span></span></span><span class=line><span class=cl><span class=s>mkdir -p /etc/kubernetes/manifests
</span></span></span><span class=line><span class=cl><span class=s>nerdctl run --rm --network=host --entrypoint=/kube-vip ghcr.io/kube-vip/kube-vip:v0.3.8 \
</span></span></span><span class=line><span class=cl><span class=s>  manifest pod \
</span></span></span><span class=line><span class=cl><span class=s>  --interface $INTERFACE \
</span></span></span><span class=line><span class=cl><span class=s>  --vip $VIP \
</span></span></span><span class=line><span class=cl><span class=s>  --controlplane \
</span></span></span><span class=line><span class=cl><span class=s>  --services \
</span></span></span><span class=line><span class=cl><span class=s>  --arp \
</span></span></span><span class=line><span class=cl><span class=s>  --leaderElection | tee  /etc/kubernetes/manifests/kube-vip.yaml
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>bash add-kube-vip.sh
</span></span></code></pre></td></tr></table></div></div><p>三个 master 节点都需要跑下上面的命令（worker 不需要），创建好 kube-vip 的 static-pod 配置文件。
在完成 kubeadm 初始化后，kubelet 会自动把它们拉起为 static pod.</p><h2 id=5-使用-kubeadm-创建集群 class=headerLink><a href=#5-%e4%bd%bf%e7%94%a8-kubeadm-%e5%88%9b%e5%bb%ba%e9%9b%86%e7%be%a4 class=header-mark></a>5. 使用 kubeadm 创建集群</h2><p>其实需要运行的就是这条命令：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># 极简配置：</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF | sudo tee kubeadm-config.yaml
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: kubeadm.k8s.io/v1beta3
</span></span></span><span class=line><span class=cl><span class=s>kind: InitConfiguration
</span></span></span><span class=line><span class=cl><span class=s>nodeRegistration:
</span></span></span><span class=line><span class=cl><span class=s>  criSocket: &#34;/var/run/containerd/containerd.sock&#34;
</span></span></span><span class=line><span class=cl><span class=s>  imagePullPolicy: IfNotPresent
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>kind: ClusterConfiguration
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: kubeadm.k8s.io/v1beta3
</span></span></span><span class=line><span class=cl><span class=s>kubernetesVersion: v1.22.1
</span></span></span><span class=line><span class=cl><span class=s>clusterName: kubernetes
</span></span></span><span class=line><span class=cl><span class=s>certificatesDir: /etc/kubernetes/pki
</span></span></span><span class=line><span class=cl><span class=s>imageRepository: k8s.gcr.io
</span></span></span><span class=line><span class=cl><span class=s>controlPlaneEndpoint: &#34;192.168.122.200:6443&#34;  # 填 apiserver 的 vip 地址，或者整个域名也行，但是就得加 /etc/hosts 或者内网 DNS 解析
</span></span></span><span class=line><span class=cl><span class=s>networking:
</span></span></span><span class=line><span class=cl><span class=s>  serviceSubnet: &#34;10.96.0.0/16&#34;
</span></span></span><span class=line><span class=cl><span class=s>  podSubnet: &#34;10.244.0.0/16&#34;
</span></span></span><span class=line><span class=cl><span class=s>etcd:
</span></span></span><span class=line><span class=cl><span class=s>  local:
</span></span></span><span class=line><span class=cl><span class=s>    dataDir: /var/lib/etcd
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: kubelet.config.k8s.io/v1beta1
</span></span></span><span class=line><span class=cl><span class=s>kind: KubeletConfiguration
</span></span></span><span class=line><span class=cl><span class=s>cgroupDriver: systemd
</span></span></span><span class=line><span class=cl><span class=s># 让 kubelet 从 certificates.k8s.io 申请由集群 CA Root 签名的 tls 证书，而非直接使用自签名证书
</span></span></span><span class=line><span class=cl><span class=s># 如果不启用这个， 安装 metrics-server 时就会遇到证书报错，后面会详细介绍。
</span></span></span><span class=line><span class=cl><span class=s>serverTLSBootstrap: true
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看 kubeadm 默认的完整配置，供参考</span>
</span></span><span class=line><span class=cl>kubeadm config print init-defaults &gt; init.default.yaml
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 执行集群的初始化，这会直接将当前节点创建为 master</span>
</span></span><span class=line><span class=cl><span class=c1># 成功运行的前提：前面该装的东西都装好了，而且 kubelet 已经在后台运行了</span>
</span></span><span class=line><span class=cl><span class=c1># `--upload-certs` 会将生成的集群证书上传到 kubeadm 服务器，在两小时内加入集群的 master 节点会自动拉证书，主要是方便集群创建。</span>
</span></span><span class=line><span class=cl>kubeadm init --config kubeadm-config.yaml --upload-certs
</span></span></code></pre></td></tr></table></div></div><p>kubeadm 应该会报错，提示你有些依赖不存在，下面先安装好依赖项。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>sudo zypper in -y socat ebtables conntrack-tools
</span></span></code></pre></td></tr></table></div></div><p>再重新运行前面的 kubeadm 命令，应该就能正常执行了，它做的操作有：</p><ul><li>拉取控制面的容器镜像</li><li>生成 ca 根证书</li><li>使用根证书为 etcd/apiserver 等一票工具生成 tls 证书</li><li>为控制面的各个组件生成 kubeconfig 配置</li><li>生成 static pod 配置，kubelet 会根据这些配置自动拉起 kube-proxy 以及其他所有的 k8s master 组件</li></ul><p>运行完会给出三部分命令：</p><ul><li>将 <code>kubeconfig</code> 放到 <code>$HOME/.kube/config</code> 下，<code>kubectl</code> 需要使用该配置文件连接 kube-apiserver</li><li>control-plane 节点加入集群的命令:<ul><li>这里由于我们提前添加了 kube-vip 的 static-pod 配置，这里的 preflight-check 会报错，需要添加此参数忽略该报错 - <code>--ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests</code></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>kubeadm join 192.168.122.200:6443 --token &lt;token&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --discovery-token-ca-cert-hash sha256:&lt;hash&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --control-plane --certificate-key &lt;key&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --ignore-preflight-errors<span class=o>=</span>DirAvailable--etc-kubernetes-manifests
</span></span></code></pre></td></tr></table></div></div></li><li>worker 节点加入集群的命令:<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>kubeadm join 192.168.122.200:6443 --token &lt;token&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>      --discovery-token-ca-cert-hash sha256:&lt;hash&gt; 
</span></span></code></pre></td></tr></table></div></div></li></ul><p>跑完第一部分 <code>kubeconfig</code> 的处理命令后，就可以使用 kubectl 查看集群状况了：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>k8s-master-0:~/kubeadm <span class=c1># kubectl get no</span>
</span></span><span class=line><span class=cl>NAME           STATUS     ROLES                  AGE   VERSION
</span></span><span class=line><span class=cl>k8s-master-0   NotReady   control-plane,master   79s   v1.22.1
</span></span><span class=line><span class=cl>k8s-master-0:~/kubeadm <span class=c1># kubectl get po --all-namespaces</span>
</span></span><span class=line><span class=cl>NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE
</span></span><span class=line><span class=cl>kube-system   coredns-78fcd69978-6tlnw               0/1     Pending   <span class=m>0</span>          83s
</span></span><span class=line><span class=cl>kube-system   coredns-78fcd69978-hxtvs               0/1     Pending   <span class=m>0</span>          83s
</span></span><span class=line><span class=cl>kube-system   etcd-k8s-master-0                      1/1     Running   <span class=m>6</span>          90s
</span></span><span class=line><span class=cl>kube-system   kube-apiserver-k8s-master-0            1/1     Running   <span class=m>4</span>          90s
</span></span><span class=line><span class=cl>kube-system   kube-controller-manager-k8s-master-0   1/1     Running   <span class=m>4</span>          90s
</span></span><span class=line><span class=cl>kube-system   kube-proxy-6w2bx                       1/1     Running   <span class=m>0</span>          83s
</span></span><span class=line><span class=cl>kube-system   kube-scheduler-k8s-master-0            1/1     Running   <span class=m>7</span>          97s
</span></span></code></pre></td></tr></table></div></div><p>现在在其他节点运行前面打印出的加入集群的命令，就可以搭建好一个高可用的集群了。</p><p>所有节点都加入集群后，通过 kubectl 查看，应该是三个控制面 master，两个 worker：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>k8s-master-0:~/kubeadm <span class=c1># kubectl get node</span>
</span></span><span class=line><span class=cl>NAME           STATUS     ROLES                  AGE     VERSION
</span></span><span class=line><span class=cl>k8s-master-0   NotReady   control-plane,master   26m     v1.22.1
</span></span><span class=line><span class=cl>k8s-master-1   NotReady   control-plane,master   7m2s    v1.22.1
</span></span><span class=line><span class=cl>k8s-master-2   NotReady   control-plane,master   2m10s   v1.22.1
</span></span><span class=line><span class=cl>k8s-worker-0   NotReady   &lt;none&gt;                 97s     v1.22.1
</span></span><span class=line><span class=cl>k8s-worker-1   NotReady   &lt;none&gt;                 86s     v1.22.1
</span></span></code></pre></td></tr></table></div></div><p>现在它们都还处于 NotReady 状态，需要等到我们把网络插件安装好，才会 Ready.</p><p>现在再看下集群的证书签发状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>❯ kubectl get csr --sort-by<span class=o>=</span><span class=s1>&#39;{.spec.username}&#39;</span>
</span></span><span class=line><span class=cl>NAME        AGE     SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION
</span></span><span class=line><span class=cl>csr-95hll   6m58s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
</span></span><span class=line><span class=cl>csr-tklnr   7m5s    kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
</span></span><span class=line><span class=cl>csr-w92jv   9m15s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
</span></span><span class=line><span class=cl>csr-rv7sj   8m11s   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:q8ivnz    &lt;none&gt;              Approved,Issued
</span></span><span class=line><span class=cl>csr-nxkgx   10m     kubernetes.io/kube-apiserver-client-kubelet   system:node:k8s-master-0   &lt;none&gt;              Approved,Issued
</span></span><span class=line><span class=cl>csr-cd22c   10m     kubernetes.io/kubelet-serving                 system:node:k8s-master-0   &lt;none&gt;              Pending
</span></span><span class=line><span class=cl>csr-wjrnr   9m53s   kubernetes.io/kubelet-serving                 system:node:k8s-master-0   &lt;none&gt;              Pending
</span></span><span class=line><span class=cl>csr-sjq42   9m8s    kubernetes.io/kubelet-serving                 system:node:k8s-master-1   &lt;none&gt;              Pending
</span></span><span class=line><span class=cl>csr-xtv8f   8m56s   kubernetes.io/kubelet-serving                 system:node:k8s-master-1   &lt;none&gt;              Pending
</span></span><span class=line><span class=cl>csr-f2dsf   8m3s    kubernetes.io/kubelet-serving                 system:node:k8s-master-2   &lt;none&gt;              Pending
</span></span><span class=line><span class=cl>csr-xl8dg   6m58s   kubernetes.io/kubelet-serving                 system:node:k8s-worker-0   &lt;none&gt;              Pending
</span></span><span class=line><span class=cl>csr-p9g24   6m52s   kubernetes.io/kubelet-serving                 system:node:k8s-worker-1   &lt;none&gt;              Pending
</span></span></code></pre></td></tr></table></div></div><p>能看到有好几个 <code>kubernetes.io/kubelet-serving</code> 的证书还处于 pending 状态，
这是因为我们在 kubeadm 配置文件中，设置了 <code>serverTLSBootstrap: true</code>，让 Kubelet 从集群中申请 CA 签名证书，而不是自签名导致的。</p><p>设置这个参数的主要目的，是为了让 metrics-server 等组件能使用 https 协议与 kubelet 通信，避免为 metrics-server 添加参数 <code>--kubelet-insecure-tls</code>.</p><p>目前 kubeadm 不支持自动批准 kubelet 申请的证书，需要我们手动批准一下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># 批准 Kubelet 申请的所有证书</span>
</span></span><span class=line><span class=cl>kubectl certificate approve csr-cd22c csr-wjrnr csr-sjq42 csr-xtv8f csr-f2dsf csr-xl8dg csr-p9g24
</span></span></code></pre></td></tr></table></div></div><p>在未批准这些证书之前，所有需要调用 kubelet api 的功能都将无法使用，比如：</p><ul><li>查看 pod 日志</li><li>获取节点 metrics</li><li>等等</li></ul><h3 id=51-常见问题 class=headerLink><a href=#51-%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98 class=header-mark></a>5.1 常见问题</h3><h4 id=511-重置集群配置 class=headerLink><a href=#511-%e9%87%8d%e7%bd%ae%e9%9b%86%e7%be%a4%e9%85%8d%e7%bd%ae class=header-mark></a>5.1.1 重置集群配置</h4><p>创建集群的过程中出现任何问题，都可以通过在所有节点上运行 <code>kubeadm reset</code> 来还原配置，然后重新走 kubeadm 的集群创建流程。</p><p>但是要注意几点：</p><ul><li><code>kubeadm reset</code> 会清除包含 kube-vip 配置在内的所有 static-pod 配置文件，所以 master 节点需要重新跑下前面给的 kube-vip 命令，生成下 kube-vip 配置。</li><li><code>kubeadm reset</code> 不会重置网络接口的配置，master 节点需要手动清理下 kube-vip 添加的 vip: <code>ip addr del 192.168.122.200/32 dev eth0</code>.</li><li>如果你在安装了网络插件之后希望重装集群，顺序如下：<ul><li>通过 <code>kubectl delete -f xxx.yaml</code>/<code>helm uninstall</code> 删除所有除网络之外的其他应用配置</li><li>删除网络插件</li><li>先重启一遍所有节点，或者手动重置所有节点的网络配置<ul><li>建议重启，因为我不知道该怎么手动重置&mldr; 试了 <code>systemctl restart network</code> 并不会清理所有虚拟网络接口。</li></ul></li></ul></li></ul><p>如此操作后，再重新执行集群安装，应该就没啥毛病了。</p><h2 id=6-验证集群的高可用性 class=headerLink><a href=#6-%e9%aa%8c%e8%af%81%e9%9b%86%e7%be%a4%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8%e6%80%a7 class=header-mark></a>6. 验证集群的高可用性</h2><p>虽然网络插件还没装导致集群所有节点都还没 ready，但是我们已经可以通过 kubectl 命令来简单验证集群的高可用性了。</p><p>首先，我们将前面放置在 k8s-master-0 的认证文件 <code>$HOME/.kube/config</code> 以及 kunbectl 安装在另一台机器上，比如我直接放我的宿主机。</p><p>然后在宿主机上跑 <code>kubectl get node</code> 命令验证集群的高可用性：</p><ul><li>三个主节点都正常运行时，kubectl 命令也正常</li><li>pause 或者 stop 其中一个 master，kubectl 命令仍然能正常运行</li><li>再 pause 第二个 master，kubectl 命令应该就会卡住，并且超时，无法使用了</li><li>resume 恢复停掉的两个 master 之一，会发现 kubectl 命令又能正常运行了</li></ul><p>到这里 kubeadm 的工作就完成了，接下来再安装网络插件，集群就可用了。</p><h2 id=7-安装网络插件 class=headerLink><a href=#7-%e5%ae%89%e8%a3%85%e7%bd%91%e7%bb%9c%e6%8f%92%e4%bb%b6 class=header-mark></a>7. 安装网络插件</h2><p>社区有很多种网络插件可选，比较知名且性能也不错的，应该是 Calico 和 Cilium，其中 Cilium 主打基于 eBPF 的高性能与高可观测性。</p><p>下面分别介绍这两个插件的安装方法。（注意只能安装其中一个网络插件，不能重复安装。）</p><p>需要提前在本机安装好 helm，我这里使用宿主机，因此只需要在宿主机安装:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># 一行命令安装，也可以自己手动下载安装包，都行</span>
</span></span><span class=line><span class=cl>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 <span class=p>|</span> bash
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 或者 opensuse 直接用包管理器安装</span>
</span></span><span class=line><span class=cl>sudo zypper in helm
</span></span></code></pre></td></tr></table></div></div><h3 id=71-安装-cilium class=headerLink><a href=#71-%e5%ae%89%e8%a3%85-cilium class=header-mark></a>7.1 安装 Cilium</h3><blockquote><p>官方文档：https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-kubeadm/</p></blockquote><p>cilium 通过 eBPF 提供了高性能与高可观测的 k8s 集群网络，
另外 cilium 还提供了比 kube-proxy 更高效的实现，可以完全替代 kube-proxy.</p><p>这里我们还是先使用 kube-proxy 模式，先熟悉下 cilium 的使用：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>helm repo add cilium https://helm.cilium.io/
</span></span><span class=line><span class=cl>helm search repo cilium/cilium -l <span class=p>|</span> head
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm install cilium cilium/cilium --version 1.10.4 --namespace kube-system
</span></span></code></pre></td></tr></table></div></div><p>可以通过 <code>kubectl get pod -A</code> 查看 cilium 的安装进度，当所有 pod 都 ready 后，集群就 ready 了~</p><p>cilium 也提供了专用的客户端：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz<span class=o>{</span>,.sha256sum<span class=o>}</span>
</span></span><span class=line><span class=cl>sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
</span></span><span class=line><span class=cl>sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
</span></span><span class=line><span class=cl>rm cilium-linux-amd64.tar.gz<span class=o>{</span>,.sha256sum<span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><p>然后使用 cilium 客户端检查网络插件的状态：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl> $ cilium status --wait
</span></span><span class=line><span class=cl>    /¯¯<span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span> /¯¯<span class=se>\_</span>_/¯¯<span class=se>\ </span>   Cilium:         OK
</span></span><span class=line><span class=cl> <span class=se>\_</span>_/¯¯<span class=se>\_</span>_/    Operator:       OK
</span></span><span class=line><span class=cl> /¯¯<span class=se>\_</span>_/¯¯<span class=se>\ </span>   Hubble:         disabled
</span></span><span class=line><span class=cl> <span class=se>\_</span>_/¯¯<span class=se>\_</span>_/    ClusterMesh:    disabled
</span></span><span class=line><span class=cl>    <span class=se>\_</span>_/
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>DaemonSet         cilium             Desired: 5, Ready: 5/5, Available: 5/5
</span></span><span class=line><span class=cl>Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
</span></span><span class=line><span class=cl>Containers:       cilium             Running: <span class=m>5</span>
</span></span><span class=line><span class=cl>                  cilium-operator    Running: <span class=m>2</span>
</span></span><span class=line><span class=cl>Cluster Pods:     2/2 managed by Cilium
</span></span><span class=line><span class=cl>Image versions    cilium             quay.io/cilium/cilium:v1.10.4@sha256:7d354052ccf2a7445101d78cebd14444c7c40129ce7889f2f04b89374dbf8a1d: <span class=m>5</span>
</span></span><span class=line><span class=cl>                  cilium-operator    quay.io/cilium/operator-generic:v1.10.4@sha256:c49a14e34634ff1a494c84b718641f27267fb3a0291ce3d74352b44f8a8d2f93: <span class=m>2</span>
</span></span></code></pre></td></tr></table></div></div><p>cilium 还提供了命令，自动创建 pod 进行集群网络的连接性测试：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>❯ cilium connectivity <span class=nb>test</span>
</span></span><span class=line><span class=cl>ℹ️  Monitor aggregation detected, will skip some flow validation steps
</span></span><span class=line><span class=cl>✨ <span class=o>[</span>kubernetes<span class=o>]</span> Creating namespace <span class=k>for</span> connectivity check...
</span></span><span class=line><span class=cl>✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying echo-same-node service...
</span></span><span class=line><span class=cl>✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying same-node deployment...
</span></span><span class=line><span class=cl>✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying client deployment...
</span></span><span class=line><span class=cl>✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying client2 deployment...
</span></span><span class=line><span class=cl>✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying echo-other-node service...
</span></span><span class=line><span class=cl>✨ <span class=o>[</span>kubernetes<span class=o>]</span> Deploying other-node deployment...
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>ℹ️  Expose Relay locally with:
</span></span><span class=line><span class=cl>   cilium hubble <span class=nb>enable</span>
</span></span><span class=line><span class=cl>   cilium status --wait
</span></span><span class=line><span class=cl>   cilium hubble port-forward<span class=p>&amp;</span>
</span></span><span class=line><span class=cl>🏃 Running tests...
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>---------------------------------------------------------------------------------------------------------------------
</span></span><span class=line><span class=cl>✅ All <span class=m>11</span> tests <span class=o>(</span><span class=m>134</span> actions<span class=o>)</span> successful, <span class=m>0</span> tests skipped, <span class=m>0</span> scenarios skipped.
</span></span></code></pre></td></tr></table></div></div><p>通过 <code>kubectl get po -A</code> 能观察到，这个测试命令会自动创建一个 <code>cilium-test</code> 名字空间，并在启动创建若干 pod 进行详细的测试。</p><p>整个测试流程大概会持续 5 分多钟，测试完成后，相关 Pod 不会自动删除，使用如下命令手动删除：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>kubectl delete namespace cilium-test
</span></span></code></pre></td></tr></table></div></div><h3 id=72-安装-calico class=headerLink><a href=#72-%e5%ae%89%e8%a3%85-calico class=header-mark></a>7.2 安装 Calico</h3><blockquote><p>官方文档：https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises</p></blockquote><p>也就两三行命令。安装确实特别简单，懒得介绍了，看官方文档吧。</p><p>但是实际上 calico 的细节还蛮多的，建议通读下它的官方文档，了解下 calico 的架构。</p><h2 id=8-查看集群状态 class=headerLink><a href=#8-%e6%9f%a5%e7%9c%8b%e9%9b%86%e7%be%a4%e7%8a%b6%e6%80%81 class=header-mark></a>8. 查看集群状态</h2><p>官方的 dashboard 个人感觉不太好用，建议直接在本地装个 k9s 用，特别爽。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>sudo zypper in k9s
</span></span></code></pre></td></tr></table></div></div><p>然后就可以愉快地玩耍了。</p><h2 id=9-安装-metrics-server class=headerLink><a href=#9-%e5%ae%89%e8%a3%85-metrics-server class=header-mark></a>9. 安装 metrics-server</h2><blockquote><p>这一步可能遇到的问题：<a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs target=_blank rel="noopener noreferrer">Enabling signed kubelet serving certificates</a></p></blockquote><p>如果需要使用 HPA 以及简单的集群监控，那么 metrics-server 是必须安装的，现在我们安装一下它。</p><p>首先，跑 kubectl 的监控命令应该会报错：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>❯ kubectl top node
</span></span><span class=line><span class=cl>error: Metrics API not available
</span></span></code></pre></td></tr></table></div></div><p>k9s 里面应该也看不到任何监控指标。</p><p>现在通过 helm 安装它：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
</span></span><span class=line><span class=cl>helm search repo metrics-server/metrics-server -l <span class=p>|</span> head
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install metrics-server metrics-server/metrics-server --version 3.5.0 --namespace kube-system
</span></span></code></pre></td></tr></table></div></div><blockquote><p>metrics-server 默认只会部署一个实例，如果希望高可用，请参考官方配置：<a href=https://github.com/kubernetes-sigs/metrics-server/tree/master/manifests/high-availability target=_blank rel="noopener noreferrer">metrics-server - high-availability manifests</a></p></blockquote><p>等 metrics-server 启动好后，就可以使用 <code>kubectl top</code> 命令啦：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>❯ kubectl top node
</span></span><span class=line><span class=cl>NAME           CPU<span class=o>(</span>cores<span class=o>)</span>   CPU%   MEMORY<span class=o>(</span>bytes<span class=o>)</span>   MEMORY%   
</span></span><span class=line><span class=cl>k8s-master-0   327m         16%    1465Mi          50%       
</span></span><span class=line><span class=cl>k8s-master-1   263m         13%    1279Mi          44%       
</span></span><span class=line><span class=cl>k8s-master-2   289m         14%    1282Mi          44%       
</span></span><span class=line><span class=cl>k8s-worker-0   62m          3%     518Mi           13%       
</span></span><span class=line><span class=cl>k8s-worker-1   115m         2%     659Mi           8%        
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>❯ kubectl top pod
</span></span><span class=line><span class=cl>No resources found in default namespace.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>❯ kubectl top pod -A
</span></span><span class=line><span class=cl>NAMESPACE     NAME                                   CPU<span class=o>(</span>cores<span class=o>)</span>   MEMORY<span class=o>(</span>bytes<span class=o>)</span>   
</span></span><span class=line><span class=cl>kube-system   cilium-45nw4                           9m           135Mi           
</span></span><span class=line><span class=cl>kube-system   cilium-5x7jf                           6m           154Mi           
</span></span><span class=line><span class=cl>kube-system   cilium-84sr2                           7m           160Mi           
</span></span><span class=line><span class=cl>kube-system   cilium-operator-78f45675-dp4b6         2m           30Mi            
</span></span><span class=line><span class=cl>kube-system   cilium-operator-78f45675-fpm5g         1m           30Mi            
</span></span><span class=line><span class=cl>kube-system   cilium-tkhl4                           6m           141Mi           
</span></span><span class=line><span class=cl>kube-system   cilium-zxbvm                           5m           138Mi           
</span></span><span class=line><span class=cl>kube-system   coredns-78fcd69978-dpxxk               3m           16Mi            
</span></span><span class=line><span class=cl>kube-system   coredns-78fcd69978-ptd9p               1m           18Mi            
</span></span><span class=line><span class=cl>kube-system   etcd-k8s-master-0                      61m          88Mi            
</span></span><span class=line><span class=cl>kube-system   etcd-k8s-master-1                      50m          85Mi            
</span></span><span class=line><span class=cl>kube-system   etcd-k8s-master-2                      55m          83Mi            
</span></span><span class=line><span class=cl>kube-system   kube-apiserver-k8s-master-0            98m          462Mi           
</span></span><span class=line><span class=cl>kube-system   kube-apiserver-k8s-master-1            85m          468Mi           
</span></span><span class=line><span class=cl>kube-system   kube-apiserver-k8s-master-2            85m          423Mi           
</span></span><span class=line><span class=cl>kube-system   kube-controller-manager-k8s-master-0   22m          57Mi            
</span></span><span class=line><span class=cl>kube-system   kube-controller-manager-k8s-master-1   2m           23Mi            
</span></span><span class=line><span class=cl>kube-system   kube-controller-manager-k8s-master-2   2m           23Mi            
</span></span><span class=line><span class=cl>kube-system   kube-proxy-j2s76                       1m           24Mi            
</span></span><span class=line><span class=cl>kube-system   kube-proxy-k6d6z                       1m           18Mi            
</span></span><span class=line><span class=cl>kube-system   kube-proxy-k85rx                       1m           23Mi            
</span></span><span class=line><span class=cl>kube-system   kube-proxy-pknsc                       1m           20Mi            
</span></span><span class=line><span class=cl>kube-system   kube-proxy-xsq4m                       1m           15Mi            
</span></span><span class=line><span class=cl>kube-system   kube-scheduler-k8s-master-0            3m           25Mi            
</span></span><span class=line><span class=cl>kube-system   kube-scheduler-k8s-master-1            4m           21Mi            
</span></span><span class=line><span class=cl>kube-system   kube-scheduler-k8s-master-2            5m           21Mi            
</span></span><span class=line><span class=cl>kube-system   kube-vip-k8s-master-0                  4m           17Mi            
</span></span><span class=line><span class=cl>kube-system   kube-vip-k8s-master-1                  2m           16Mi            
</span></span><span class=line><span class=cl>kube-system   kube-vip-k8s-master-2                  2m           17Mi            
</span></span><span class=line><span class=cl>kube-system   metrics-server-559f85484-5b6xf         7m           27Mi    
</span></span></code></pre></td></tr></table></div></div><h2 id=10-为-etcd-添加定期备份能力 class=headerLink><a href=#10-%e4%b8%ba-etcd-%e6%b7%bb%e5%8a%a0%e5%ae%9a%e6%9c%9f%e5%a4%87%e4%bb%bd%e8%83%bd%e5%8a%9b class=header-mark></a>10. 为 etcd 添加定期备份能力</h2><p>请移步 <a href=https://github.com/ryan4yin/knowledge/blob/master/datastore/etcd/etcd%20%E7%9A%84%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D.md target=_blank rel="noopener noreferrer">etcd 的备份与恢复</a></p><h2 id=11-安装-volume-provisioner class=headerLink><a href=#11-%e5%ae%89%e8%a3%85-volume-provisioner class=header-mark></a>11. 安装 Volume Provisioner</h2><p>在我们学习使用 Prometheus/MinIO/Tekton 等有状态应用时，它们默认情况下会通过 PVC 声明需要的数据卷。</p><p>为了支持这个能力，我们需要在集群中部署一个 Volume Provisioner.</p><p>对于云上环境，直接接入云服务商提供的 Volume Provisioner 就 OK 了，方便省事而且足够可靠。</p><p>而对于 bare-metal 环境，比较有名的应该是 rook-ceph，但是这个玩意部署复杂，维护难度又高，不适合用来测试学习。</p><p>对于开发、测试环境，或者个人集群，建议使用：</p><ul><li>local 数据卷，适合数据可丢失，且不要求分布式的场景，如开发测试环境<ul><li><a href=https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner target=_blank rel="noopener noreferrer">https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner</a></li><li><a href=https://github.com/rancher/local-path-provisioner target=_blank rel="noopener noreferrer">https://github.com/rancher/local-path-provisioner</a></li></ul></li><li>NFS 数据卷，适合数据可丢失，对性能要求不高，并且要求分布式的场景。比如开发测试环境、或者线上没啥压力的应用<ul><li><a href=https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner target=_blank rel="noopener noreferrer">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner</a></li><li><a href=https://github.com/kubernetes-csi/csi-driver-nfs target=_blank rel="noopener noreferrer">https://github.com/kubernetes-csi/csi-driver-nfs</a></li><li>NFS 数据的可靠性依赖于外部 NFS 服务器，企业通常使用群晖等 NAS 来做 NFS 服务器</li><li>如果外部 NFS 服务器出问题，应用就会崩。</li></ul></li><li>直接使用云上的对象存储，适合希望数据不丢失、对性能要求不高的场景。<ul><li>直接使用 <a href=https://github.com/rclone/rclone target=_blank rel="noopener noreferrer">https://github.com/rclone/rclone</a> mount 模式来保存数据，或者直接同步文件夹数据到云端（可能会有一定数据丢失）。</li></ul></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2022-01-25</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=# onclick=return!1 title="分享到 Twitter" data-sharer=twitter data-url=https://thiscute.world/posts/kubernetes-deployment-using-kubeadm/ data-title="部署一个 Kubernetes 集群" data-via=ryan4yin data-hashtags=Kubernetes,云原生><i class="fab fa-twitter fa-fw"></i></a><a href=# onclick=return!1 title="分享到 Facebook" data-sharer=facebook data-url=https://thiscute.world/posts/kubernetes-deployment-using-kubeadm/ data-hashtag=Kubernetes><i class="fab fa-facebook-square fa-fw"></i></a><a href=# onclick=return!1 title="分享到 Reddit" data-sharer=reddit data-url=https://thiscute.world/posts/kubernetes-deployment-using-kubeadm/><i class="fab fa-reddit fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/kubernetes/>Kubernetes</a>,&nbsp;<a href=/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/>云原生</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/kubernetes-best-practices/ class=prev rel=prev title="Kubernetes 微服务最佳实践"><i class="fas fa-angle-left fa-fw"></i>Kubernetes 微服务最佳实践</a>
<a href=/posts/python-tips-and-tricks/ class=next rel=next title="Python 实用技巧与常见错误集锦">Python 实用技巧与常见错误集锦<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments><div id=utterances></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://utteranc.es/>Utterances</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><a href=https://www.foreverblog.cn/ target=_blank><img src=https://img.foreverblog.cn/logo_en_default.png alt style=width:auto;height:15px;margin-bottom:5px></a> <a href=https://www.foreverblog.cn/go.html target=_blank><img src=https://img.foreverblog.cn/wormhole_3.gif alt style=width:auto;height:24px title=穿梭虫洞-随机访问十年之约友链博客></a></div><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreferrer" title="Hugo 0.110.0">Hugo</a> 强力驱动&nbsp;|&nbsp;主题 - <a href=https://github.com/HEIGE-PCloud/DoIt target=_blank rel="noopener noreferrer" title="DoIt 0.2.13"><i class="far fa-edit fa-fw"></i> DoIt</a></div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://thiscute.world/ target=_blank rel="noopener noreferrer">ryan4yin</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:30},comment:{utterances:{darkTheme:"github-dark",issueTerm:"pathname",label:"",lightTheme:"github-light",repo:"ryan4yin/thiscute.world"}},data:{"desktop-header-typeit":"This Cute World","mobile-header-typeit":"This Cute World"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"747LJ10EI7",algoliaIndex:"ryan-space",algoliaSearchKey:"658db5f2bf056f83458cacf5dd58ec80",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},sharerjs:!0,typeit:{cursorChar:null,cursorSpeed:null,data:{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},duration:null,speed:null}}</script><script type=text/javascript src=/js/utterances.min.js defer></script><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/algoliasearch/algoliasearch-lite.umd.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><script type=text/javascript src=/js/theme.min.js defer></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-4V93QVSNFW",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-4V93QVSNFW" async></script></div></body></html>