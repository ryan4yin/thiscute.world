[{"categories":["life","tech"],"content":" 前言相比跌宕起伏的 2023 年, 2024 年我少了一些焦虑与内耗, 花在技术上的时间也少了不少. 我将大量的精力转移到了徒步旅行上, 享受了诸多旅行的快乐. 可能因为 23 年写的太多，24 年少了些创作的热情，也因此这份年终一直拖着。本来想效仿去年的风格过一遍一整年中比较有意义的事情，但是不太能下手。 不过，总得写点什么给这一年画上一个句号，今天总算交差了. ","date":"2025-01-07","objectID":"/posts/2024-summary/:1:0","series":["年终总结"],"tags":["总结"],"title":"我的 2024 - 稳中求进、热爱生活","uri":"/posts/2024-summary/#前言"},{"categories":["life","tech"],"content":" 2024 年 Highlights","date":"2025-01-07","objectID":"/posts/2024-summary/:2:0","series":["年终总结"],"tags":["总结"],"title":"我的 2024 - 稳中求进、热爱生活","uri":"/posts/2024-summary/#2024-年-highlights"},{"categories":["life","tech"],"content":" 1. 旅行与徒步2024 年跟 2023 年最大的变化, 是我 3 月份抽时间办了港澳通行证跟护照, 在香港跟国内完成了多次徒步旅行, 今年的最后一天也是在香港维多利亚港的烟花中度过的. 这篇文章的封面图就是香港维多利亚港的跨年烟花(因为自己拍摄的角度不太好, 所以网上找了这张图). 我在 2024 年的徒步旅行与 City Walk 记录如下: 3/30 - SRE 小组第一次以户外运动的形式进行团建，一起爬了凤凰山（鲲鹏径） 4/4 - 跟我妹一起逛了仙湖植物园，很多奇花异草，另外回程意外爬上了梧桐山，给我俩都累坏了， 当然也很开心 4/14 - 第一次去香港玩，从维多利亚港沿着海岸线一路徒步到坚尼地城，然后坐地铁回家，海岸线很美，香港也有独特的风土人情在 解锁成就 - 第一次出境中国大陆 5/2 ~ 5/3 - 单人刷了一遍香港麦里浩径二段，从北潭凹管理站下车开始徒步，沿着麦理浩径二段又走回到北潭凹站，算是环线，大概 20 到 30 公里的样子，中间在西湾村租帐篷露营了一晚上 解锁成就 - 第一次露营、第一次在山林里孤身赶夜路 不知道是谁，在牛粪上插鲜花 emmm 这一段风景绝赞，全程最佳！ 5/18 - 5/19 - 单人背着 17 公斤的背包重装徒步麦理浩径三段，中间还解锁了一些支线，全程走了 14 公里，走走停停近 8 个小时（体力不够所以走得很慢），夜间在水浪窝营地露营了一晚上 解锁成就 - 第一次重装徒步 山顶继续前行，远方城市灯火通明 清晨 7 点多，解决卫生问题，顺便随处走走，发现营地标牌 5/25 - 继续单人徒步麦径四段，坐九巴 299X 路到大浪窝站下车开始徒步，从四段起点出发的时间为 13:20，到达大老山隧道站时已经是 22:20, 全程差不多 9 个小时，超过 21 公里，背的还是 16kg 的重装背包 解锁成就 - 第一次重装徒步超过 20 公里，到目前为止的人生巅峰了 这应该是四段风景最好的一段, 可惜雾太大 6/22 - 6/23 - 徒步至铅矿坳营地露营 解锁成就 - 这次带了卡式炉气罐跟炊具，第一次户外做饭，很香 6/29 - 与同事四人组团麦理浩径一二段徒步 从北潭凹反穿，一路走到万宜水库东坝，因为计划单日徒步，这次只背了 30 升小包，运动量相比之前几次并不大 解锁成就 - 第一次与同事组团长距离徒步、第一次大雨中徒步（有风险，不建议冒雨上山） 08/03 - 跟老妹一起在香港维多利亚港沿海漫步，人比之前五一假期少了太多，体验非常好！可以悠闲地慢慢走，拍照，聊天 8/21 - 8/23 - 在香港参加为期三天的 KubeCon China 2024, 顺便跟着朋友逛香港 主会场过厅，海景不错的 冰镇饮料也可以随便喝，好哇 好多的 CNCF 贴纸，可以随便拿，我给同事也带了一些 香港夜景，相当繁华哪 Linus 咱的合影 10/18 - 10/19 - 公司团建，在惠州东江玩皮划艇，18 公里，挺愉快 跟同事划皮划艇 江边放点烟花，不得不说公司是会玩的 10/26 - 10/27 - 跟同事武功山徒步，10/25 提前下班坐高铁到长沙休息，10/26 早上坐高铁到萍乡再叫车送到武功山下开始徒步。我们是反穿，第一天徒步到云中峰客栈住宿，第二天上午继续徒步到武功山大门口，中间乘了两段下山索道。两天武功山都起大雾，没看到日出，视野也差了许多，但云海也还算不错，在山脊线上走，两边都是悬崖，而且还好大的风，还是有点刺激的 10/27 凌晨, 喝着热水欣赏早晨四五点的山景 10/27 快清晨六点了，对面山上的早餐叫卖声隔这么远都听得到 10/27 从云中峰客栈再次出发 11/23 - SRE 小组深圳梅林登山徒步, 路程大概 14 公里, 早上 9 点 30 从梅林水库大坝出发，下午 14 点 50 到终点, 全程 5 个多小时 11/24 - 到香港招商局码头看海南舰(075 两栖攻击舰), 不过没拿到门票上船参观 11/30 - 陪朋友香港办银行卡, 顺便在皇后大道跟维多利亚港一直 City Walk 到晚上九点 沿着皇后大道走到了一条市集小街，节日氛围浓厚 K11 商场海边的圣诞树布景，好多人拍照 维港渡轮上回头，能看到标志性的摩天轮 12/31 - 2024 年最后一天, 在香港 City Walk, 晚上到维多利亚港看跨年烟花，人山人海，很有氛围 我拍的烟花，位置不好效果差挺多 我拍的烟花 - 2 我拍的烟花 - 3 关于香港徒步旅行的细节, 我之前专门写过篇文章, 有兴趣的可以看看: 2024 年香港徒步旅行记录（一） - This Cute World 总的来说, 我 2024 年的运动量远超 2023 年, 这是一个很好的开始. ","date":"2025-01-07","objectID":"/posts/2024-summary/:2:1","series":["年终总结"],"tags":["总结"],"title":"我的 2024 - 稳中求进、热爱生活","uri":"/posts/2024-summary/#1-旅行与徒步"},{"categories":["life","tech"],"content":" 2. 业余技术今年业余技术上的进展比较符合去年底的期望. 首先是在我 Homelab 上更深入地使用了 NixOS 系统, 其次也发表了一些不错的 NixOS 文章, 还给 Nixpkgs 提了一些 PR, 另外去年做的几个 Nix 相关开源项目的 stars 也持续增长. 其次是在 Linux 系统编程跟 Rust 语言方面取得了不错的进展, 学习这些技术的过程中, 对过去遇到的许多 Linux 系统故障也有了更深的理解. 算是年底两个月最有价值的技术突破. 2024 年我写过的一些技术文章: 个人数据安全不完全指南 这是我 2023 年 5 月开始的一个长期计划，到 2024 年中时这份计划基本落地，写了这篇博客总结我当前的方案。 Kubernetes 集群伸缩组件 - Karpenter 这篇文章来自我过去几年工作中对 Karpenter 的研究与改造经验. KubeCon China 2024 之旅 参加 KubeCon China 2024 的一些感受, 技术跟旅行结合的一篇文章. OS as Code - 我的 NixOS 使用体会 在知乎上回答了一个关于 NixOS 的问题写的文章, 英文版还得到了 NixOS 官方的推特转发. NixOS 在 Lichee Pi 4A 上是如何启动的 这篇实际是 23 年的存货. 24 年我写的文章相较 23 年少了不少, 不过整体质量是有所提高的. 考虑到 24 年我在旅行徒步以及关心家人上花了许多时间, 这个成绩也可以接受. 最后再对比下从 2024 年 12 月 31 日到现在，我的 GitHub Metrics 统计数据： 2023/12/31 GitHub 统计数据 2025/01/01 GitHub 统计数据 2024 年我没有开什么新的项目, 上述成绩基本都是 2023 年的旧项目 Stars 稳步增长带来的. ","date":"2025-01-07","objectID":"/posts/2024-summary/:2:2","series":["年终总结"],"tags":["总结"],"title":"我的 2024 - 稳中求进、热爱生活","uri":"/posts/2024-summary/#2-业余技术"},{"categories":["life","tech"],"content":" 3. 工作工作上, 2024 也仍然是按部就班的一年, 我有做一些新技术的尝试, 但总体来说变化不多. 与往年不同的是, 今年在工作上遇到的更多是技术之外的问题. 一些团队协作、沟通、管理等问题, 让我认识到了公司与各个团队的另一面, 以及人的复杂性. 单纯从工作内容的角度看, 工作越来越得心应手, 相对的也就越来越难以激发我的兴趣与动力, 对 ADHDer 而言要按部就班地把这类工作做好, 挑战很大. 总之多方因素影响下, 我在 24 年底不想干了, 遂向 leader 提出了辞职, 目前已经确定 last day 是 2025 年 1 月底, 正在交接工作中. 我 2021 年入职这家公司, 到离开大概是 3 年零 10 个月, 一段说长不长说短不短的时光. 这是我从业生涯的第二份工作, 回过头看, 21 年刚入职时我还是萌新一个, 做事情都很小心翼翼, 当时我对公司的评价是 梦幻般的待遇，不限量的三餐供应，窗明几净的落地窗工位，这一切都像是在做梦 还有 22 年初发过的推文也是相当正面的: 新办公区真好呐～ 值此良辰美景，好想整个榻榻米坐垫，坐在角落的落地窗边工作🤣 那种使用公共设施工（mo）作（yu）的乐趣，以及平常工位见不到的景色交相辉映，是不太好表述的奇妙体验 pic.twitter.com/FASffzw8N3 — ryan4yin | 於清樂 (@ryan4yin) January 17, 2022 从入职一直到 24 年上半年, 我在这里的工作体验都是很不错的. 只能说很感慨吧, 三年多的时间, 我在这里学到了很多. 我很感谢我的两任 leader, 他们都给我了很多机会, 让我能够在工作中不断成长. 也很感谢 SRE 的其他同事, 在我遇到困难时给予了我很多帮助. 后会有期! ","date":"2025-01-07","objectID":"/posts/2024-summary/:2:3","series":["年终总结"],"tags":["总结"],"title":"我的 2024 - 稳中求进、热爱生活","uri":"/posts/2024-summary/#3-工作"},{"categories":["life","tech"],"content":" 4. 阅读2024 年我在阅读方面, 最大的亮点应该是终于读完了《Linux/Unix 系统编程手册（上册）》, 并且使用 Rust 做了不少习题. 2024 年完整的已读书目: 《户外旅行终极指南：基础装备、露营技能、交通方式、饮食、环境和急救》：内容很多，但都比较入门级，好处是图很多，读着很轻松，几个小时就能走马观花全过一遍。 Programming Kubernetes - Developing Cloud Native Applications: 2022 年开始读的书，但当时没啥兴趣。最近在照猫画虎写 karpenter provider，有了些编程经验后又对它产生了兴趣。书不厚，花了三个小时走马观花全读了一遍，代码内容大都跳过了（不少也过时了，譬如还在介绍 dep），做了些笔记。挺有帮助，帮我系统地梳理了最近折腾 karpenter 学到的 operator 编程相关知识。 走出荒野 2021 年 2 月读此书的评价：「没读书的内容前，我完全没预料到作者的人生曾如此不堪。 最近刚离职，毕业后的第一份工作就这样结束了。心里好多想法，也好想多看看这个世界。 嗯，有点想来上一次徒步旅行了哈哈。」 2024 年 7 月重读评价：「今年我爱上了徒步，重读此书，又有新知。我想徒步也类似跑步，也存在村上春树所言的跑者蓝调。今年已经在香港麦理浩径上完成了 5 次徒步，越发上瘾。我想我也该带老妹体验下，见山见水见自我。」 Linux/Unix 系统编程手册（上册） 未读完书目: 《Educated - A Memoir（中文名：你当像鸟飞往你的山）》 Linux/Unix 系统编程手册（下册） 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 年初定的目标是每月一本书, 但实际上只读完了 4 本, 25 年再接再厉吧! ","date":"2025-01-07","objectID":"/posts/2024-summary/:2:4","series":["年终总结"],"tags":["总结"],"title":"我的 2024 - 稳中求进、热爱生活","uri":"/posts/2024-summary/#4-阅读"},{"categories":["life","tech"],"content":" 2025 年展望我在去年年终总结的文末写了, 我对自己 2024 年的期许是「工作与业余爱好上稳中求进，生活上锻炼好身体、多关心家人」，感觉确实应验了。由衷地喜欢与感谢这一年来乐观、开朗、积极的自己， 也感谢身边的亲人、朋友、同事. 人的一生, 尤其是 ADHDer 的一生要怎么过才能拥有鲜活、快乐且充实的人生? 我们天生只有在自己喜欢的事情上才能摆脱拖延症并获得足够的专注力, 这就注定了我们无法适应「稳定、枯燥」的工作与生活. 2025 年, 我不急着找下一份工作, 计划先 gap 几个月, 调整下自己的心态, 重新审视自己的职业生涯, 以及未来的规划. 世界那么大, 我想去看看, 也许在旅行中能找到一些答案. 因此, 我给自己定的 2025 年目标是: 深入浅出 Linux, 徒步中国、徒步世界 作为一名从未出过国的 IT 农民工, 我对世界上其他国家的认识仅仅停留在书本与各种网络资料上. 为了能够亲眼见识下中国以外的世界, 我计划在 2025 年开始走出国门, 亲身体验不同国家的文化与风景. 我已经办好了日本签证, 正在办韩国签证, 打算先去这两个国家徒步旅行. 如果签证顺利的话, 我在日韩之后还想去尼泊尔、马来西亚、澳洲跟欧洲旅行. 但这个并没有那么急, 如果 2025 年 gap 的这几个月不够用的话, 未来还有很多机会. 除了去国外旅行满足我对「外国」的好奇心, 我也很想在 2025 年去亲眼见证 960 万平方公里的中国大地, 亲眼看看这片土地上的鬼斧神工. 不过暂时还没有很明确的计划, 中国的风景太多太美, 也许我会先去青海, 又或者是广西? 路还很长, 2025 年, 让我用双脚去丈量这个世界～ Carpe Diem. Seize The Day, Boys. Make Your Lives Extraordinary. – 《死亡诗社》 ","date":"2025-01-07","objectID":"/posts/2024-summary/:3:0","series":["年终总结"],"tags":["总结"],"title":"我的 2024 - 稳中求进、热爱生活","uri":"/posts/2024-summary/#2025-年展望"},{"categories":["tech"],"content":" 前言很早就有了解到今年的 KubeCon China 会在香港举办，虽然有些兴趣，但我最初是有被 KubeCon 高昂的门票价格劝退了的。 有时候不得不相信运气的魔力，机缘巧合之下，我从朋友 @Kev 处得知了 KubeCon 的「最终用户门票计划」并借此 0 元购了门票，又邀上了 0xFFFF 社区 的@Chever-John @0xdeadbeef @茗洛 三位朋友一起参加，在香港租了个 airbnb 住宿，期间也逛了香港城市中的不少地方，收货颇丰。 其实本来也尝试过邀请其他认识的朋友同事，但都因为种种原因无法参加，略感遗憾。 ","date":"2024-08-27","objectID":"/posts/kubecon-china-2024/:1:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","服务网格","Istio"],"title":"KubeCon China 2024 之旅","uri":"/posts/kubecon-china-2024/#前言"},{"categories":["tech"],"content":" TL;DR本文多图，也挺多技术无关的内容，为了方便想了解技术的朋友，我先做个大致的总结。 从 KubeCon 回来后我又听了些 CNCF 其他的会议视频，其中比较有印象的是这几个： Keynote: Cloud Native in its Next Decade - KubeCon Europe 2024: 聊了 CloudNative 的未来，结论跟这次在 KubeCon China 现场听到的内容类似。 Another Choice for Istio Multi-Cluster \u0026 Multi-Network Deployment Model - KubeCon Europe 2024: 提到了 Istio 多集群方案的痛点，介绍了中国移动的解决方案。我一直有想尝试多集群方案，但一直担心 hold 不住而不敢下手，这个视频给了我一些启发。 DRA in KubeVirt: Overcoming Challenges \u0026 Implementing Changes - KubeCon Europe 2024: DRA 也是 K8s 中的新 API，这里介绍如何在 kubevirt 中使用 DRA 解决一些问题。从这里能感觉到 K8s 这几年还是弄了不少新东西的。 结合 KubeCon China 三天的经历，以及上面这些视频的内容，我大概的感觉是： （几乎）所有聊网络的人都在聊 eBPF, Envoy, Gateway API. Istio 的 Ambient Mode 吸引了很多曾经因为 sidecar 性能问题而放弃使用服务网格的公司。 Karmada 多集群管理方案在许多公司得到了实际应用，挺多讲这个的。 AI 与 WASM 方面的演讲也有不少，但感觉有些无趣，可能是我对这方面不太感兴趣。 蔚来汽车、中国移动等公司正在尝试将 K8s 应用在边缘计算场景（智能汽车、通信基站），但这些离普通互联网公司有点远。 云原生的未来十年会变成什么样？ Kubernetes, Service Mesh 等过去十年的新兴技术，现在已经成为了「Boring but useful infrastructrue」，它们将是其他云原生技术潮流的基石，被广泛应用，但自身不会再有太多的变化。 AI, eBPF, WASM, Rust 等技术也将在未来十年走向成熟，取代 Kubernetes 当前的地位。 KubeCon China 2024 的会议视频将会陆续被添加到如下这个 Youtube Playlist 中，有兴趣的朋友可以一观： KubeCon + CloudNativeCon + Open Source Summit + AI_dev China 2024 - Youtube 视频相关的 PPT 可以在这里下载： KubeCon China 2024 - Schedule ","date":"2024-08-27","objectID":"/posts/kubecon-china-2024/:2:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","服务网格","Istio"],"title":"KubeCon China 2024 之旅","uri":"/posts/kubecon-china-2024/#tldr"},{"categories":["tech"],"content":" 技术这次我主要关注的是 Istio、Gateway API 相关的议题，最近在研究 Istio 的 Ambient Mode，因此希望能够从会议中了解到更多的实现细节与其中的权衡。 三天下来听到的内容也很好的满足了我的期待，Istio / Envoy Gateway / Ingress Controller 的几位核心贡献者分享了很多这些项目的最新进展，实现细节，以及未来的发展方向。 Ambient Mode 在最近 beta 了，是我关注的重点，总结下目前了解到的几个关键点： istio/ztunnel: 一个 userspace 的 l4 proxy，仅支持处理 L4 流量。 ztunnel 会分别与上游和下游建立连接，导致 A \u003c=\u003e B 之间的一个连接会变成 A \u003c=\u003e ztunnel \u003c=\u003e ztunnel \u003c=\u003e B 三个连接，这也会带来性能开销。 因为所有流量都经由 ztunnel 转发，更新 ztunnel 会导致短暂的流量中断。感觉比较好的解决方案是采用 recreate 的更新策略 + 滚动更新节点组下的所有节点来更新 ztunnel. ztunnel 使用的 HBONE 协议强制启用 mTLS，无法关闭，对于不要求安全性的场景会带来额外的性能开销。 istio/proxy: 基于 envoy 的 l7 proxy，在 ambient mode 下它被单独部署为一个 waypoint, 用于处理 L7 流量。 waypoint 架构下 proxy 与上下游 Pod 很可能在不同的节点上，这会导致比 sidecar 模式多一次网络跳转，可能带来性能损耗，以及跨 Zone 流量上涨。 waypoint 与 sidecar 都是 envoy，它是通过减少 envoy 容器的数量来达到减少资源消耗的目的。 以及一些其他方案： kmesh: 架构类似 Ambient Mode，特点是完全使用 eBPF 来实现 L4 proxy，好处是 eBPF 直接在内核空间修改网络包，不需要与上下游分别建立连接。因此性能更好，而且 eBPF 程序更新不会中断流量。 cilium service mesh: 特点是 per-node proxy，l7 的 envoy proxy 运行在每个节点上，而不是像 waypoint 一样单独通过 deployment 部署。但也存在一些问题： per-node proxy 无法灵活地调整资源占用，可能会导致资源浪费。 同一节点上的所有流量都由同一 envoy proxy 处理，无法实现 waypoint 那样的 namespace 级别的流量隔离。 与 cilium cni 强绑定，必须使用 cilium cni 才能使用 cilium service mesh. 据说使用起来较为复杂？ 总体而言，KubeCon 是一次了解行业前沿技术动态、跟项目开发者及其他行业内的技术人面对面认识交流的好机会，可以帮助自己提升技术视野，维持技术热情与动力，不至于局限在公司业务中闭门造车。 ","date":"2024-08-27","objectID":"/posts/kubecon-china-2024/:3:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","服务网格","Istio"],"title":"KubeCon China 2024 之旅","uri":"/posts/kubecon-china-2024/#技术"},{"categories":["tech"],"content":" 行程","date":"2024-08-27","objectID":"/posts/kubecon-china-2024/:4:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","服务网格","Istio"],"title":"KubeCon China 2024 之旅","uri":"/posts/kubecon-china-2024/#行程"},{"categories":["tech"],"content":" 住宿因为要在香港呆三天，衣食住行是必须要考虑的事情。这方面我拉上的几位朋友都比较有旅行住宿的经验，我们最后在香港找了个离会场不远的 airbnb 住宿，最终的体验也是相当不错。房间干净整洁有格调，虽然我觉得稍微有点小了，但朋友说这个空间在香港都是一家三四口住的标准，已经吊打同价位的酒店了。 ","date":"2024-08-27","objectID":"/posts/kubecon-china-2024/:4:1","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","服务网格","Istio"],"title":"KubeCon China 2024 之旅","uri":"/posts/kubecon-china-2024/#住宿"},{"categories":["tech"],"content":" Day 1虽然提早定了住宿，做了点功课，但第一天就出了问题——深圳这边一直下雨导致 @Chever-John 的飞机直接被取消，改订了另一趟航班也晚点。虽然正点到达了会场，但他一晚上就睡了俩小时，在深圳定的前一晚的酒店也没住成，第一天看他整个人都听得迷迷糊糊的。不过没事，至少我听了个爽 说回正题，到了会场领完胸牌，我们就开始了为期三天的 KubeCon China 之旅。 具体的技术内容已经在前文总结过了，这里主要就贴些照片吧。 主会场过厅，海景不错的 去各会议室的过道，酒店的服务很到位 午休茶歇，吃饱喝足 冰镇饮料也可以随便喝，好哇 几位大佬聊 Istio 与 Gateway API 的未来 然后晚上@茗洛带着我们逛了香港的诚品书店，书店好几层，但感兴趣的书不多。后面又逛了好多电子商城、二次元周边店，我算是开了眼界。 《我推的孩子》 另一本 好多二次元钢琴谱，有《四月是我的谎言》 不知道逛到了哪，到处都是二次元周边 轻小说书店 1 轻小说书店 2 轻小说书店 3 第一天就差不多是这样，听了点技术，晚上逛了逛香港，回去休息。 ","date":"2024-08-27","objectID":"/posts/kubecon-china-2024/:4:2","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","服务网格","Istio"],"title":"KubeCon China 2024 之旅","uri":"/posts/kubecon-china-2024/#day-1"},{"categories":["tech"],"content":" Day 2 好多的 CNCF 贴纸，可以随便拿，我给同事也带了一些 我收获的 CNCF 贴纸 首先是听了华为的演讲，介绍了 Kmesh 的方案创新，技术细节讲得很赞。想看 PPT 与视频请移步用内核原生无边车架构彻底改变服务网格 - Xin Liu, Huawei Technologies Co., Ltd. 华为介绍 Kmesh 介绍 Kmesh 如何借助 eBPF 实现热更新不中断连接 还听了晋涛老师的十年云原生之旅：容器技术和Kubernetes生态系统的演变 - Jintao Zhang, Kong Inc. 晋涛不愧是行业老将，这么早就开始玩 Docker 了 然后晚上我们随便走了走逛了逛，看了看香港海边夜景。 香港夜景，相当繁华哪 灯红酒绿，游人如织 路上碰到了《商务印书馆》 ","date":"2024-08-27","objectID":"/posts/kubecon-china-2024/:4:3","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","服务网格","Istio"],"title":"KubeCon China 2024 之旅","uri":"/posts/kubecon-china-2024/#day-2"},{"categories":["tech"],"content":" Day 3第三天早上是我这次最期待的，Linus 的访谈，见到了本人，这次行程也圆满了。 Linus 第三天没什么我特别感兴趣的话题，听完 Linus 的访谈后随便逛了逛，跟几位 朋友合了个影，就搭地铁回家了。 咱的合影 咱的 PC 与鲨鲨合影 另外朋友听了个 TiDB 的演讲，看 PPT 是有点意思的哈哈。 TiDB 以及在项目展厅三天逛下来，我帆布袋领了四个，T恤领了三件，还有别的小礼品一堆，吃的喝的都不用说了，管够。另外看网上不少信息说香港的服务业态度很差，但这家酒店可能星级比较高，体验还是相当到位的。 总之，体验相当不错的，有机会的话明年还来！Love you, KubeCon China \u0026 Hong Kong！ ","date":"2024-08-27","objectID":"/posts/kubecon-china-2024/:4:4","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","服务网格","Istio"],"title":"KubeCon China 2024 之旅","uri":"/posts/kubecon-china-2024/#day-3"},{"categories":["tech"],"content":" 前言Kubernetes 具有非常丰富的动态伸缩能力，这体现在多个层面： Workloads 的伸缩：通过 Horizontal Pod Autoscaler（HPA）和 Vertical Pod Autoscaler（VPA）等资源，可以根据资源使用情况自动调整 Pod 的数量和资源配置。 相关项目： metrics-server: 采集指标数据供 HPA 使用 KEDA: 用于支持更多的指标数据源与触发方式 kubernetes/autoscaler: 提供 VPA 功能 Nodes 的伸缩：根据集群的负载情况，可以自动增加或减少 Nodes 的数量，以适应负载的变化。 相关项目： kubernetes/autoscaler: 目前最流行的 Node 伸缩方案，支持绝大多数云厂商。 karpenter: AWS 捐给 CNCF 的一个新兴 Node 伸缩方案，目前仅支持 AWS/Azure，但基于其核心库可以很容易地扩展支持其他云厂商。 本文主要介绍新兴 Node 伸缩与管理方案 Karpenter 的优势、应用场景及使用方法。 ","date":"2024-07-10","objectID":"/posts/kubernetes-cluster-autoscaling-1-karpenter/:1:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","自动扩缩容","Karpenter","Cluster-Autoscaler"],"title":"Kubernetes 集群伸缩组件 - Karpenter","uri":"/posts/kubernetes-cluster-autoscaling-1-karpenter/#前言"},{"categories":["tech"],"content":" Karpenter 简介Karpenter 项目由 AWS 于 2020 年创建，其目标是解决 AWS 用户在 EKS 上使用 Cluster Autoscaler 做集群伸缩时遇到的一些问题。在经历了几年发展后，Karnepnter 于 2023 年底被捐献给 CNCF（kubernetes/org#4258），成为目前 （2024/07/10）唯二的官方 Node 伸缩方案之一。 我于 2022 年 4 月在做 Spark 离线计算平台改造的时候尝试了 Karpenter v0.8.2，发现它的确比 Cluster Autoscaler 更好用，并在随后的两年中逐渐将它推广到了更多的项目中。目前我司在 AWS 云平台上所有的离线计算任务与大部分在线服务都是使用 Karpenter 进行的集群伸缩。另外我还为 karpenter 适配了 K3s 与 DigitalOcean 云平台用于一些特殊业务，体验良好。 Karpenter 官方目前只有 AWS 与 Azure 两个云平台的实现，也就是说只有在这两个平台上 karpenter 才能开箱即用。但考虑到它在易用性与成本方面的优势以及在可拓展性、标准化方面的努力，我对它的未来发展持乐观态度。 ","date":"2024-07-10","objectID":"/posts/kubernetes-cluster-autoscaling-1-karpenter/:2:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","自动扩缩容","Karpenter","Cluster-Autoscaler"],"title":"Kubernetes 集群伸缩组件 - Karpenter","uri":"/posts/kubernetes-cluster-autoscaling-1-karpenter/#karpenter-简介"},{"categories":["tech"],"content":" Karpenter 与 Cluster Autoscaler 的对比Cluster Autoscaler 是目前社区最流行的 Node 伸缩方案，基本所有云厂商的 Kubernetes 服务默认都会集成它。 Karpenter 与 Cluster Autoscaler 的设计理念与实现方式有很大的不同。 Cluster Autoscaler 是 Kubernetes 平台上早期的集群伸缩方案，也是目前最流行的方案。但它做的事情比较有限，最大的问题是它本身并不直接管理集群的节点，而是借助云厂商的伸缩组 （AutoScaling Group）或节点池（Node Pool）来间接地控制节点（云服务器）的数量。这样的设计导致了一些问题： 部署与维护比较繁琐：需要先在云厂商的控制台上创建好伸缩组或节点池，然后再在 Kubernetes 集群上部署 Cluster Autoscaler，并将伸缩组或节点池的名称等信息填写到 Cluster Autoscaler 的配置文件中。增删节点池时也需要走一遍这个流程。 能力受限于云厂商的伸缩组或节点池服务：如果云厂商的伸缩组或节点池服务不支持某些功能，那么 Cluster Autoscaler 也无法使用这些功能。 举例来说，AWS EKS 的 Node Group 功能非常难用，毛病一大堆。但如果要用 Cluster Autoscaler，你就没得选，Node Group 再难用也只能忍着。 而 Karpenter 则完全从零开始实现了一套节点管理系统，它直接管理所有节点（云服务器，如 AWS EC2），负责节点的创建、删除、修改等操作。 相较 Cluster Autoscaler, Karpenter 的优势主要体现在以下几个方面： 声明式地定义节点池: Karpenter 提供了一套 CRD 来定义节点池，用户只需要编写好 Yaml 配置部署到集群中，Karpenter 就会根据配置自动申请与管理节点。这比 Cluster Autoscaler 的配置要方便得多。 以 AWS 为例，你简单地改几行 Yaml 配置，就可以修改掉节点池的实例类型、AMI 镜像、数量上下限、磁盘大小、节点 Labels 跟 Taints、EC2 Tags 等信息。 借助 Flux 或 ArgoCD 等 GitOps 工具，你还可以实现自动化的节点池管理以及配置的版本控制。 成本感知的节点管理：Karpenter 不仅负责节点数量的伸缩，它还能根据节点的规格、负载情况、成本等因素来选择最优的节点类型，以达成成本、性能、稳定性之间的平衡。 - 具体而言，Karpenter 在成本优化方面具有这些 Cluster Autoscaler 不具备的功能： Spot/On-Demand 实例调整: 在 AWS 上，Karpenter 可以设置为优先使用 Spot 实例，并在申请不到 Spot 实例时自动切换到 On-Demand 实例，从而大大降低成本。 多节点类型支持: Karpenter 支持在同一个集群中使用多种不同规格的节点，并且支持控制不同实例类型的优先级、数量或占比，以满足不同的业务需求。 节点替换策略：Karpenter 支持灵活的节点替换策略，可以通过 Yaml 控制每个节点池的节点替换条件、频率、比例等参数，以避免因节点替换导致的服务不可用。 节点的生命周期管理：Karpenter 支持定义节点的生命周期策略，可以根据节点的年龄、负载、成本等因素来决定节点的续租、下线、销毁等操作。而 Cluster Autoscaler 只能控制节点的数量，它不直接管理节点，也就做不到此类节点的精细管理。 主动优化：Karpenter 支持主动根据负载情况使用不同实例类型的节点替换高风险节点，或合并低负载节点，以节省成本。 Pod 精细化调度：Karpeneter 本身也是一个调度器，它能根据 Pod 的资源需求、优先级、Node Affinity、Topology Spread Constraints 等因素来申请节点并主动将 Pod 调度到该节点上。而 Cluster Autoscaler 只能控制节点的数量，并无调度能力。 快速、高效：因为 Karpenter 直接创建、删除节点，并且主动调度 Pod，所以它的伸缩速度与效率要比 Cluster Autoscaler 高很多。这是因为 Karpneter 能快速获知节点创建、删除、加入集群是否成功，而 Cluster Autoscaler 只能被动地等待云厂商的伸缩组或节点池服务完成这些操作，它无法主动感知节点的状态。 总之，个人的使用体验上，Karpenter 吊打了 Cluster Autoscaler. ","date":"2024-07-10","objectID":"/posts/kubernetes-cluster-autoscaling-1-karpenter/:3:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","自动扩缩容","Karpenter","Cluster-Autoscaler"],"title":"Kubernetes 集群伸缩组件 - Karpenter","uri":"/posts/kubernetes-cluster-autoscaling-1-karpenter/#karpenter-与-cluster-autoscaler-的对比"},{"categories":["tech"],"content":" Karpenter 的使用这部分建议直接阅读官方文档Karpenter - Just-in-time Nodes for Any Kubernetes Cluster. ","date":"2024-07-10","objectID":"/posts/kubernetes-cluster-autoscaling-1-karpenter/:4:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","自动扩缩容","Karpenter","Cluster-Autoscaler"],"title":"Kubernetes 集群伸缩组件 - Karpenter","uri":"/posts/kubernetes-cluster-autoscaling-1-karpenter/#karpenter-的使用"},{"categories":["tech"],"content":" 适配其他 Kubernetes 发行版与云服务商如果你使用的是 Proxmox VE, Aliyun 等其他云平台，或者使用的是 K3s, Kubeadm 等非托管 Kubernetes 发行版，那么你就需要自己适配 Karpenter 了。 Karpenter 官方目前并未提供详细的适配文档，社区建议以用于测试的Kwok Provider 为参考，自行实现。Kwok 是一个极简的 Karpenter Provider 实现，更复杂的功能也可以参考 AWS 与 Azure 的实现。 国内云服务方面, 目前已经有人做了 Aliyun 的适配，项目地址如下： karpenter-provider-aliyun 对于个人 Homelab 玩家来说，使用 Proxmox VE + K3s 这个组合的用户应该会比较多。我个人目前正在尝试为这个组合适配 Karpenter，希望能够在未来的文章中分享一些经验。项目地址如下： ryan4yin/karpenter-provider-proxmox: 因为我已经换了 KubeVirt, 这个项目缺乏开发动力,暂时搁置,并改为私有仓库了… ","date":"2024-07-10","objectID":"/posts/kubernetes-cluster-autoscaling-1-karpenter/:5:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","自动扩缩容","Karpenter","Cluster-Autoscaler"],"title":"Kubernetes 集群伸缩组件 - Karpenter","uri":"/posts/kubernetes-cluster-autoscaling-1-karpenter/#适配其他-kubernetes-发行版与云服务商"},{"categories":["tech"],"content":" Karpenter 与 Cluster APICluster API (CAPI) 是 Kubernetes 社区提供的一个用于管理多集群的项目，从介绍上看，它跟 Karpenter 好像没啥交集。但如果你有真正了解使用过 CAPI 的话，你会发现 Karpenter 与 CAPI 有一些功能上的重叠： CAPI 的 Infrastructure Provider 专门负责处理云厂商相关逻辑的组件。Karpenter 的标准实现内也包含了 cloud provider 相关代码，还提供了 NodeClass 这个 CRD 用于设定云服务器相关的参数。 Cluster API Bootstrap Provider (CABP) 负责将云服务器初始化为 Kubernetes Node，实际上就是生成对应的 cloud-init user data. Karpenter 的 NodeClass 实现中同样也包含了 user data 的生成逻辑。 Cluster API 的目标是多集群管理，并且它的设计上将 Bootstrap, ControlPlane 跟 Infrastructure 三个部分分离出来了，好处是方便各云厂商、各 Kubernetes 发行版的接入，但也导致了它的架构比较复杂、出问题排查起来会比较麻烦。 历史案例：Istio 曾经就采用了微服务架构，结果因为性能差、维护难度高被不少人喷，后来才改成了单体结构。 而 Karpenter 则是一个单体应用，它的核心功能被以 Go Library 的形式发布，用户需要基于这个库来实现自己的云平台适配。这样的设计使得 Karpenter 的架构简单、易于维护。但这也意味着 Karpenter 的可扩展性、通用性不如 Cluster API. 从结果来看，现在 Cluster API 的生态相当丰富，从Provider Implementations - Cluster API Docs 能看到已经有了很多云厂商、发行版的适配. 而 Karpenter 2023 年底才捐给 CNCF，目前只有 AWS 与 Azure 的实现，未来发展还有待观察。 那么有没有可能结合两者的优势呢？Kubernetes 社区其实就有类似的尝试： Cluster API Karpenter Feature Group Notes Karpenter Provider Cluster API Open Questions elmiko/karpenter-provider-cluster-api 上面这个实验性质的项目尝试使用 Karpenter 作为 Cluster API 的 Node Autoscaler，取代掉现在的 Cluster Autoscaler. 我目前对 Cluster API 有些兴趣，但感觉它还是复杂了点。我更想试试在 Karpenter 的实现中复用 Cluster API 各个 Provider 的代码，快速适配其他云厂商与 Kubernetes 发行版。 ","date":"2024-07-10","objectID":"/posts/kubernetes-cluster-autoscaling-1-karpenter/:6:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","自动扩缩容","Karpenter","Cluster-Autoscaler"],"title":"Kubernetes 集群伸缩组件 - Karpenter","uri":"/posts/kubernetes-cluster-autoscaling-1-karpenter/#karpenter-与-cluster-api"},{"categories":["tech"],"content":" 参考资料 Cluster Autoscaling - Kubernetes Official Docs ","date":"2024-07-10","objectID":"/posts/kubernetes-cluster-autoscaling-1-karpenter/:7:0","series":["云原生相关"],"tags":["云原生","Cloud-Native","Kubernetes","MultiCloud","多云","自动扩缩容","Karpenter","Cluster-Autoscaler"],"title":"Kubernetes 集群伸缩组件 - Karpenter","uri":"/posts/kubernetes-cluster-autoscaling-1-karpenter/#参考资料"},{"categories":["life"],"content":" 缘起在 2023 年年度总结中，我给 2024 年定下的目标是「工作与业余爱好上稳中求进，生活上锻炼好身体、多关心家人」，为此今年我做了许多旅行的准备。 在深圳呆了快五年，挺长时间里一直将自己局限在技术与工作中，少有时间去关注周围的世界——生存已属不易，没有多余的精力去关心其他事情。2023 年是一个分界点，工作上越来越得心应手，手头也不再紧张，个人精力跟业余时间得以解放，我自然开始追求更多的生活体验。 说是要锻炼身体，多旅游，但也没啥明确的计划，只是想着该出国走一走开开眼界，于是 3 月份到出入境管理局搞定了护照跟港澳通行证。 香港离深圳近得很，自然就想着先去香港多走走，这是缘起。 而这件事情后来的发展，现在回看起来，跟我当初折腾 NixOS 或电子电路的故事如出一辙，大概性格使然吧哈哈。 简单总结下呢，就是 4 月份去香港海边徒步了一天，从维多利亚港沿着海岸线一路走到坚尼地城，然后就爱上了这种在海边徒步的感觉，回来后也一直念念不忘，查了许多香港徒步路线的资料。 五一假期的时候跟着小红书跟 Bilibili 上的攻略，徒步了香港麦理浩径第一段的部分以及第二段全程。因为准备不足，举着手机走了一个小时夜路，并且在西湾村营地租帐篷露营了一晚上。这次体验又让我迷上了夜间徒步跟露营。一回家就查各种徒步露营装备，疯狂下单，收了一大堆快递，花掉了一万多 RMB… 接着就是 5 月 18 日，背上我 65L 的重装背包，总重量大概有 17 kg，从深圳坐地铁 + 大巴直达北潭凹，爬了一遍麦理浩径三段，夜间在水浪窝营地露营一晚，第二天一早直接打道回府。本来计划是周末两天刷完三四段，甚至又余力再走完第五段的。但是我显然高估了自己的体力，而且背着 17 kg 的重装背包，这也不是一件轻松的事情。 休息了一周后，在 5 月 25 日，我又完成了麦理浩径第四段的徒步。这次出发前根据上次的经验精简了装备，总总量应该轻了大概 1kg，但仍旧有 16kg。四段是麦径难度最高的一段，而且终点基维尔营地不接受个人预约，又导航徒步到大老山隧道站乘九巴 74X 路至广福邨下车，再步行到地铁站乘车到罗湖口岸回家，到达车站时已经是 22:20。全程差不多 9 个小时，步行超过 21 公里，是我目前的人生巅峰。 以上就是我到目前为止的香港徒步经历，目前对重装徒步兴趣浓厚，计划今年先把麦理浩径全程走完， 积累经验，后续再看看香港跟国内外的其他徒步路线。 ","date":"2024-05-21","objectID":"/posts/hong-kong-travel-notes-in-2024/:1:0","series":["徒步旅行"],"tags":["Travel","旅游","Backpacking","重装徒步","Hiking","徒步","Hong Kong","香港","MacLehose Trail","麦理浩径"],"title":"2024 年香港徒步旅行记录（一）","uri":"/posts/hong-kong-travel-notes-in-2024/#缘起"},{"categories":["life"],"content":" 一、从维多利亚港到坚尼地城 时间：2024-04-14 这是我第一次出境游，整个行程都是临时起意，没有做任何准备，即使是过了罗湖口岸，站在东铁线地铁上的时候，我对整个行程也并无太多期待，心里想着无非是不同的人与物，无甚特殊的。但总得出去走走，看看大陆之外究竟是个什么样子，所以就这么随意地来到了香港。 具体有多随意呢？就这么说，过了口岸没多久，手机就没信号了，我这才意识到香港终归不是大陆，慌得我又出站再往回坐，在罗湖口岸连上网络开了个漫游跟流量包，这才算是正式开始了我的香港之旅。 最初期待甚少，但现实常常超出我的想象。在香港徒步的一日，我切实感受到了更宽广更多元化的世界，这是在大陆境内无法体会到的。 刚上东铁线甚至刚出地铁站的时候，我对自己已经出境这一点尚无实感，因为香港的地铁跟深圳实在太像了，不论是地铁车厢的设计还是车站的建筑风格、干净整洁的程度，都跟深圳非常相似。 后面跟朋友聊起来，才知道国内深圳地铁跟香港地铁是有合作关系的，很多技术最初都来自香港，算是一脉相承了。 在去往维多利亚港的路上，看到香港多的双层巴士、随处的繁体字跟英文标识，才逐渐感觉到这里确实是个不同的地方。 出了会展地铁站，往维多利亚港去的路上 路上遇到的敞篷观光大巴 接着就到了维港的海边步道，海景很美，两岸都是高楼大厦，海上有很多游船，还有很多钓鱼的人。 海边 两岸都挺繁华 大海与高耸的大楼 钓鱼佬 从另一个角度看钓鱼佬 接着我意识到，从我住处可全程地铁直达维港，坐个地铁再浅浅走几步路就能看到这么漂亮的海景，这不比去深圳大鹏半岛方便多了！我为此兴奋起来。这风景吊打深圳湾，但深圳湾步道上的人甚至比维港还多，只能说深圳人真的太多了… 补充：听说深圳湾以前真的是海边，但填海造路使它成了现在的烂泥滩 沿着海岸线继续走，渐渐发现人群里外国游客相当的多，粗略估算超过一半是外国人，这让我感受到了香港的国际化氛围。 裹头巾的女游客在自拍，有点异域风情 外国游客们音箱歌一放，就地跳起了舞来 海边还有很多细节值得一看。 海岸上的微观建筑，挺有意思 天星小轮的港口，休息、舞蹈的人挺多 天星小轮 摩天轮旁的小摊贩，这小摩托车比较有年代感了... 维多利亚港的免费 WiFi 在维港逗留玩耍了一个小时，买了点面包当午餐吃了，然后就沿着海边步道继续往前走，也没啥明确的目的地，反正累了就回家。维港的人很多，但出了维港后步道上人就少了很多，走在这样的路上看着风景，相当闲适。 海与城市 沿着海岸线继续走，阳光正好 波光潋滟 充满涂鸦的转角，有人在拍照 似是废弃的小码头 被改造成游玩地点的码头 海边步道上的小店，小女孩在选冰棒 香港的叮叮车 悠闲、轻快地随走随停，大概一个半小时后，到达了步道的终点——坚尼地城，我还有点失望，这步道居然就这么没了。只好掉转方向往城市内走，市内倒也别有一番风景。 天快黑了，沿街商店亮起了灯牌 这家店门口人挺多，或许味道不错？ 在市内逛了逛，确认了香港各种店铺的特点是小而美，各种店铺都非常小，但弄得比较精致，能看得出花了心思。相对而言深圳的商场店铺就大气很多了，但也缺了香港这些店铺的种种细节。 玩得尽兴，又找了家店吃饱喝足，打道回府。 坚尼地城地铁站，回家啦 这一趟下来，我的感觉是，香港确实是国际化大都市，跟深圳的气息很不一样，海边随处可见的酒吧， 比例非常高的外国游客跟在香港工作的外国人，让我感觉到了一种国际化的氛围。至于城市的繁华程度，跟深圳各有千秋。 ","date":"2024-05-21","objectID":"/posts/hong-kong-travel-notes-in-2024/:2:0","series":["徒步旅行"],"tags":["Travel","旅游","Backpacking","重装徒步","Hiking","徒步","Hong Kong","香港","MacLehose Trail","麦理浩径"],"title":"2024 年香港徒步旅行记录（一）","uri":"/posts/hong-kong-travel-notes-in-2024/#一从维多利亚港到坚尼地城"},{"categories":["life"],"content":" 二、麦理浩径第一二段 我的路线（抄了近路） - Google Earth Map 食髓知味，在香港海边徒步一日后，我对香港徒步来了兴趣，一直瞅着机会再来一次。 到了五一假期，我终于有了机会，随便在小红书上查了些资料，发现麦理浩径一二段挺有名，还能露营，于是就决定是它了。 看天气预报最近几天都有雨，五一那天下得比较大没出门，顺便就买了迪卡侬冲锋衣、冲锋裤、速干 T 恤、大号水杯，想着明天就算有雨也要去。 幸而天公作美，第二天天气转阴，上午东西到货，又做了大半天准备才出门。因为计划是连续步行两天，所以跟着小红书上的香港旅游攻略，吃的穿的用的都带了挺多。 下午出境，首先买了 OPPO 9.9 元的香港流量包（五一特惠），然后坐地铁到深水埗（bù）地铁站整了张八达通，200 港币，其中 50 块押金。另外考虑到以后会常去香港，又多充了 100 块。 香港地铁 深水埗（同埠）办八达通实体卡，因为计划常来香港玩 接着就是各种地图导航，首先坐小巴到西贡码头，然后查半天导航，坐九巴 94 路到麦理浩径起点。 小巴要叫停车才停，真的 I 人（内向）天敌，决定以后尽量选九巴，下一站前按个 Stop （香港人称掀钟）这种才适合 I 人。 乘小巴到达西贡 乘上九巴 94 路去往麦理浩径起点 路边海湾 导航本来是说坐到麦理浩夫人度假村站，但我一下没注意就坐过了，然后下车时第一次「掀钟」不熟悉，又过了一站才按到 Stop，结果就是在「北潭凹管理站」才下车（16:34），然后往回走了半个小时才到「麦理浩径起点」（17:09）。 坐过了站，在北潭凹管理处下车 路上几乎没人，风景很好，很安静 麦理浩夫人渡假村 多走了半个小时公路才到达麦径起点，路上体验很好，主要是远离城市，很安静 从「麦理浩径起点」往万宜水库的路上（大网仔路）看到很多出租车来来往往，联想到之前查的攻略， 就知道这些出租车是为了接送徒步的人。我是单人徒步，为了省 150 的打车费，就选择了坐九巴到起点然后步行。 路上标牌 路上很多烧烤点，貌似是政府修建的，可以免费用，但没遇到过人在用这些烧烤点，可能季节或天气原因？ 到达万宜水库岔路口时已经是 17:26 了，走第一段晚上肯定到不了我的目标地址「咸田湾」，而且我了解到的是二段风景最好。一番心理斗争后，我直接走了左边的路线，绕过麦径一段，直接走到西湾亭走二段的路线。 到达万宜水库岔路口，往右是正经麦径，不过比较晚了，我抄近路走了左边。 万宜水库 一路上依旧遇不到人，远离城市跟人群的感觉很奇妙 风景很不错 瞅一眼路边地图看看我在哪 抄近路到达西湾亭，从麦径起点算用了一个半小时 沿着大路走了整整一个小时才到西湾亭，为了省个出租车费我也是拼了。不过倒也不算亏，这两个小时的路上风景相当不错，而且没遇到几个人，体验非常棒！有种我很喜欢的孤独感。 这时已经是 18:34，天色已经快黑了，我有点慌，刚好有辆出租送人到这，我向司机问路，他好心地帮我指了路。从这里开始才是山路，前面两个小时一直走在大马路上，而且我的脚在走了两个小时后已经很痛了。 接着就手机打着灯，一直走到 19:10 才到西湾村。 走小路下山到西湾村 走到 M30 标距柱（每 500 米一个）时天已经黑了 快到西湾村了，路边开始出现路灯 看到第一个租帐篷的店，立马就整了套帐篷。押金 300 租一晚 200，跟国内比可能很贵，但跟香港劳动节期间 1300+ 的旅馆比已经性价比爆棚了！ 我之前的目标是到咸水湾租帐露营篷，但到西湾村的时候天已经完全黑了，而且一路心惊胆战地连续赶了三个小时路后我也累得不行了，在往西湾村的路上我就一直憧憬着西湾村总该能找到地方住，万幸它没让我失望。 终于到了，租了个帐篷露营，200 港币一晚，因为港币不够付了人民币 我的帐篷 本来想吃口热乎的，但看到店里 55 港币起步的面食，我还是决定吃点自己的能量棒当晚餐。 洗澡收费 30，但人太多了，我就没洗。晚上海浪声、旁边帐篷里人的说话声等，环境变化太大，睡不着，小说看到凌晨三点，然后跑去海滩上听了听海浪声，回来的路上一路被村民家的狗狂吠，惊出我一声冷汗，万幸我还懂点这种场面的处理方式，盯着狗看，慢慢后退。即使走得远了，我还是一步三回头，置到回到营地把门关上才松了一口气。并且明白一个道理 —— 晚上还是不要这么跳脱的好… 西湾村手机信号挺好，但用的人太多带宽不够，很多站点怎么刷都刷不出来… 想起 21 年看过的《走出荒野》，讲的是通过徒步自我救赎的故事，打算翻出来读一读但网络问题根本刷不出来。 然后迷糊睡到了早上 7 点，出帐篷发现我这个营地人都走差不多了。想洗漱但意识到我带了牙刷却没带牙膏…因为最初是打算住旅馆的，谁 TM 知道居然没找到个便宜旅馆，香港的物价太 TM 离谱了， 逼得我连夜赶路到这里来住帐篷。 厕所也没上，早餐看太贵了也没吃，买了瓶 0.6L 的水（20 港币），07:24 直接出发了，早餐仍然是能量棒配水。 第二天一早从西湾村出发。 西湾村出发点 - M31 标距柱 没走几步就看到了西湾营地，原来西湾村过来没几步就是西湾营地，垃圾好多… 西湾营地 西湾营地这有黄色标牌，写着野猪出没。朋友也提醒过我，说它们不咬人，但如果你包里有吃的，它们会弄坏你的包。 过了西湾营地又开始爬山。 过了西湾营地开始上山，这张图漂亮 回望西湾营地 山上海景独好 07:58 到达我印象中全程风景最赞的地方 —— 接近咸田湾沙滩的一段步道。 这一段风景绝赞，全程最佳！ 这段山路也很有味道 08:15 到达咸田湾沙滩，这里人很多，帐篷也很多。从这里开始接着是往赤径去，钻的就是山路了，这一段不是海景，但也别有韵味。 看到了咸田湾 咸田湾的独木桥 10:00 到达赤径，这一处水湾风景也很美！在这里玩了很久。 在赤径休息 10:30 到达赤径公厕解决了下个人卫生问题。意外发现赤径公厕个别涂鸦很可爱！随手一拍。 赤径公厕的简笔画 接着是从赤径往北潭凹的路，同样是起起伏伏。 继续往北潭凹走，左侧指路牌 路上遇到一泡插着鲜花的牛粪，很有意思，前面的女生蹲下拍照，搞得我差点以为花是她插的… 一路上再怎么陡峭的步道上都能看得到牛粪，挺原生态的哈哈。甚至回去路上在西贡码头汽车站都看到了两头牛在绿化带上吃草。 不知道是谁，在牛粪上插鲜花 emmm 接着一路上攀，脚踝已经酸痛得不行，非常痛苦。有一段累到直接坐在地上看了 20 分钟《走出荒野》。 到达 M044 柱子上的简笔画 11:57 到达麦径二段的终点，走不动了。 到达终点的公厕 终点的指示牌 又坐九巴 94 路回西贡市区，来去都是这趟车，从「北潭凹管理站」下车又从「北潭凹」上车，几乎是围着这一片走了刚好一圈。 又坐九巴 94 路回西贡市区 12:30 到西贡市区，饿得不行，找一圈吃饭的地儿发现，麦当劳居然是最实惠的——因为它价格跟国内差不多。于是搞了一顿麦当劳，发现公司有点事，因为今天我 Oncall，顺便拿出 MacBook 处理了下公司的事。是的没错，我背包里还有一台 1.4kg 的 MacBook Pro，真是要了老命。 到达市区，饿得不行到处找吃的 发现麦当劳是最实惠的，跟深圳差不多价。顺便处理个 Oncall... 累得不行，昨晚又没洗澡，一路上疯狂出汗，浑身气味比较感人。吃完饭随便逛了逛，就打道回府了。从西贡坐九巴 299x 路到沙田站，然后转乘东铁线到罗湖口岸，再坐深圳地铁回家。 这家店装修有点意思 理发店，香港物价... 这几家店面比较有年代感 西贡码头 总的来说，香港西贡这块开发得更好，步道很多，原生态的同时路线也足够成熟，而且过来比深圳大鹏半岛更方便，期待下次再来。 查攻略最有用的几个 APP： Bilibili -香港麦理浩径 一二段最全攻略。支线破边洲，蚺蛇尖一并献上 - 发现的最详细的攻略，相当用心 小红书 - 许多有用的信息 ","date":"2024-05-21","objectID":"/posts/hong-kong-travel-notes-in-2024/:3:0","series":["徒步旅行"],"tags":["Travel","旅游","Backpacking","重装徒步","Hiking","徒步","Hong Kong","香港","MacLehose Trail","麦理浩径"],"title":"2024 年香港徒步旅行记录（一）","uri":"/posts/hong-kong-travel-notes-in-2024/#二麦理浩径第一二段"},{"categories":["life"],"content":" 三、麦理浩径第三段上次徒步走完麦理浩径二段后就有点上头了，刚回到家累得不行，脚都要废了，但隔天还想找个步道走走。另外就是这两次徒步都太随意，缺乏登山杖、登山鞋、背包等专业装备，而且露营还是租的人家帐篷，接着就是看各种徒步教程攻略，疯狂买买买。 买的一堆装备陆续到货，在家试用了好几天，比如说穿登山鞋上班、空调开 16 度在室内搭帐篷露营、床上铺蛋巢垫盖睡袋、晚餐吃自热米饭，等等不一而足 emmm 试用露营锅具、炉子以及食物 试用了好几天帐篷（室内露营 emmm） 目前的所有装备，算上没拍进来的衣物，总重量大概 14kg 接着 5 月 18 跟 19 两日又是个空闲的周末，这次做足了准备，计划两天走完麦理浩径三四段。 整理背包时，意识到气罐很难处理，带着上地铁、过海关，感觉都不太行，在香港我也不知道是否好买，所以把锅具跟炉子都踢出了背包，只带了两盒海底捞自热米饭跟一些能量胶、压缩饼干以及零食。另外水带得相当充足，3L 水袋 + 600ml 小水杯，光水就有 3.6kg. 大约 9 点多出发，首先是乘东铁线到沙田站，转程九巴 299x 路到麦边站，再转乘 94 路到麦径三段的起点。 麦边站等九巴 94 路 在二段终点解决完个人卫生问题，热了个身，顺便帮一批反穿麦径二段的大陆人合了个影，接着就开始上山。 三段一开始就是急攀，路面很陡，而且透露着一股年久失修的味道，路况比一二段差远了。 麦径三段-开头的急攀路段 三段的人流量相比二段那是断崖式下降，几乎依山遇不到人，有一种远离人世的孤独感。有的人可能会喜欢热闹，但我恰恰相反，相当享受这种孤独感。 上升太快，我又负重 17 公斤，没爬几步就累到要休息，甚至有点怀疑今天能不能走完三段。但辛苦带来的收获也挺大，越往上爬，风景越美，山景与海景交相辉映，让人心旷神怡。 风景很不错 山路 山路上美美自拍 继续急攀 在山顶还解锁了一些隐藏支线，因为走的人少，灌木丛茂盛，登山杖几乎没法用，这时候就很庆幸我穿了长袖紧身运动打底衣裤，不然走这种路小腿难免挂彩。 接着就是下降。 开始下降 山水共长天一色 Me - 发现眼镜确实变色了欸 下降路段走完，到达嶂上营地跟士多店。 这里再往前就是障上士多店 麦径路标 - M61 又开始上升，不过跟三段起始的那段比起来，这段路还算平缓。接着我高兴没多久，就到了一段相当陡峭，几乎没有台阶的路段。 上升，没有台阶的陡坡 一小段平路 坐下休息一会儿，顺便自拍一个 又二十多分钟后，累得不行，寻个地坐下，顺便回头看看。 回望 继续走，没多久就到了 17 点，天开始黑了，我也提前翻出了头灯准备着随时打开。 继续走 晚上 7 点，天开始黑了 到达山顶，已经打开了头灯 山顶继续前行，远方城市灯火通明 遇到的轻装夜爬团，应该是香港学生，而重装徒步的我已经精疲力竭，一阶阶楼梯地蹒跚下降，被很快超越 到达 M68，离三段终点近了，全身无一处不疼，真真是行百里者半九十，最后这一公里路相当折磨人 到达三段终点，接水、自热米饭当晚餐，休息了一个多小时。 很想直接回家，但导航发现水浪窝营地只差 500 米了，内心天人交战后继续往营地进发 继续夜行 路标 500 米走了我 25 分钟，中间还有一段陡坡 到达营地开始搭帐篷，营地很空旷，只有我跟另一伙人露营。 18 号这一天下来一共走了 33582 步，而且背着十七八公斤爬上爬下，最后两三公里完全是咬着牙拼命爬的。 慢吞吞搭帐篷，花了半小时才搞定 第二日早上 6 点醒来 7 点多，在流动厕所解决完卫生问题，在营地逛了一圈没找到水源，只好拿折叠水桶在流动厕所的洗手池接了点水洗脸顺便擦身体。 随处走走，发现营地标牌 早餐随便吃了点东西，接着就收拾帐篷打算回家，发现收拾起来还挺费劲，慢慢吞吞弄了也大概 40 分钟，而且发现蛋巢垫下面有跟毛毛虫，帐篷内还有好几只蚂蚁，还发现一只跳蚤，另外内帐外面也爬了根看着就很毒的毛毛虫… 蛋巢垫下的毛毛虫、帐篷内的跳蚤大概是昨晚搭帐篷时，把东西放在一个石墩上，从石墩上爬上来的， 蚂蚁可能是从内帐的孔洞爬进来的。总之它们吓得我收拾东西的时候检查了一遍又一遍，生怕碰到虫子或者把虫子收拾进了行李中。 回家路上，取水点 在车站等车 总的来说，计划两日麦理浩径徒步，实际只 18 号走完麦径三段就精疲力尽了，第二日早上直接打道回府。从二段起点走到终点，用时 12:45 - 20:30，在交界处吃晚饭、休息了 1 个小时。之后从 21:50 - 22:15 走到水浪窝营地，到 22:50 才搭好帐篷。这是我第一次重装徒步，积累了宝贵的经验，也发现了许多问题，下次再徒步麦理浩径肯定能更得心应手了~ 只爬完第三段就精疲力尽，我分析了主要有这几个原因： 体力不够，还需要多加锻炼 第一次重装徒步，装备不够精炼，一共得有十七八公斤，其中有许多东西完全没用上。 带了过多食物，可以更精简。 ","date":"2024-05-21","objectID":"/posts/hong-kong-travel-notes-in-2024/:4:0","series":["徒步旅行"],"tags":["Travel","旅游","Backpacking","重装徒步","Hiking","徒步","Hong Kong","香港","MacLehose Trail","麦理浩径"],"title":"2024 年香港徒步旅行记录（一）","uri":"/posts/hong-kong-travel-notes-in-2024/#三麦理浩径第三段"},{"categories":["life"],"content":" 四、麦理浩径第四段徒步完第三段后，休息没一周，又是周末，周五下班后赶紧跑去续签了香港签证，精简了一番装备，周六上午就再次出发徒步第四段了。 这次出发前根据上次的经验精简了装备，总总量应该轻了大概 1kg，但仍旧有 16kg。主要变化： 去掉了折叠桌跟折叠凳：上次带了没用上，吃饭都是在营地或者烧烤点，路上补充能量都是直接拿出零食或能量胶随便啃两口，都用不上它们。 自热米饭也从两盒改成了一盒：因为计划就徒步一天然后露营，第二天回家。只晚上休息的时候来顿热乎的就 OK 了。 四段是麦径难度最高的一段，而且终点基维尔营地不接受个人预约，只接受团体预约。但我还是抱着侥幸心理背着重装背包去了，想着路上总不会只有这一个营地吧（后面的经历证明我有点鲁莽了）。 早饭吃得饱饱的，又洗了澡、休息了会儿消消食，然后 10 点左右就从家里出发了。 深圳地铁一号线上，我滴背包 12:13，香港沙田汽车站，排队上九巴 299X 路 到达大浪窝站，风景很好 去往四段起点路上，美美自拍 烧烤点与海湾，以及远方的城市 走了约十分钟到达三段终点，在这里用直饮水机将 3L 水袋灌满，然后取出登山杖就出发了。 首先是到达上次露营过的水浪窝营地，然后沿着大路继续上山，路上没人。 开始登山不久，发现起雾了 沿大路到达山顶后，是沿着黄泥小路下山。这两天下雨，路面泥泞湿滑，我又背着个 16kg 的重装背包，走起来有点难度。还好有登山杖，倒不用怕滑倒。 黄泥路，这两天下雨，路面湿滑，还好有登山杖 细竹林 岔路口 走黄泥路下完山，接着又是沿着石阶路开始登山，石阶也有点湿滑。 石阶，路面湿润 登山没多久，就遇到了雾气，接着雾就越来越浓。 登山路上也开始有雾了 跟山路上的雾来个合照 雾气加重 因为雾气跟汗水，头发已经湿了 下完一座小山又开始登另一座，路边好多棕叶 又到达一座山顶，在这里休息了一阵子，吃了点东西，接着突然想到我出发时没拉伸，想着补救一下， 就在这里做了个拉伸。 中间还遇到位外国女士背着个很小的跑步背包爬上来，也休息了一会儿喝了口水，往另一边去了，很快消失在了雾中。 到达山顶，雾相当的重 在山顶休息了一会儿，吃了点东西 好重的雾啊，啥都看不清 休息好了开始下山，没多久就到了四段风景最好的一段，因为浓雾没远景看没，如果没雾这里视野会是很开阔的。 这应该是四段风景最好的一段 跟浓雾来个合照 继续前行，到达昂平营地，这里是一块山顶平原草地，浓雾下也有点意境。走着走着旁边雾中出来一只狗跟一个人，说起来这路线上挺多人遛狗的，今天遇到两三波了。 昂平，山顶平原草地 M88 标距柱 因为阴天，下细雨，又这么大雾，天黑得很快，17 点后天就有点看不清路面了，开始需要灯光。头灯在包里懒得拿出来，就把背包背带上的手电夹到腰间别着的便携坐垫上照亮路面，还别说，效果不错。 17:12 了，走树林路已经需要借助灯光 17:40，天黑差不多了，先休息一会儿 可能因为下雨，路上挺多癞蛤蟆 到 20:10 左右，终于到达基维尔营地，听到了有人声，也看到了灯光。一番调查确认了跟之前了解到的一样，这里不接受个人露营。地图上往前看也没露营点，我有点慌了，但总之先到四段终点瞧瞧吧。走到终点发现就是基维尔营地下山的大水泥路面，既然有大路，那就能沿着它回到城市，这样想着终于有了一点安全感。 20:27，沿着大马路下山 下了个山坡后实在累得不行，把便携坐垫一铺就坐下休息了，尝试用高德地图导航，但信号有点差，一直转不出来。 然后一辆车从山下开上来，看到我瘫坐在路边，停下来问我要去哪，了解清楚情况后又给我指路，还说这里徒步下去要一个多小时，路上没路灯，问我行不行，要不要他送我去车站。 手机一直没信号，我一开始是有点心动的，但不想麻烦别人，刚好手机终于加载出了导航，我对照了下跟他指的路是同一个方向，就婉拒了他，并给他展示我的手电筒表示我不担心走夜路。 高德地图给的教我先徒步到大老山隧道站乘九巴 74X 路至广福邨下车，再步行到地铁站乘车到罗湖口岸。 但它给的徒步路线有坑，我跟着导航越走就越荒凉，公路路面开叉，长满荒草，接着就直接没公路了。我慌了，仔细确认才意识到它教我往草丛里钻，仔细看草丛里还真有条路… 但这条小路显然已经半荒废，草木林立，不仔细看几乎分辨不出路线，让人忍不住怀疑这条路真的能走吗？不会走到一半成断头路吧。 还好这条灌木丛近期有人走过，沿途草木有明显被人趟过，沿途灌木上偶尔还扎了很干净显眼的飘带， 明显才扎上去没多久，这给我增加了一点对它的信心。 钻灌木丛，中间还过了条溪流，接着又是上山，好走的山路没走多远，接着又是在山上钻更深的灌木丛，我越来越慌——这真的是下山的路吗？同时我也有点担心被灌木遮挡的路面会有蛇，但现在已经很晚了，下山心切的我没时间顾虑这些，一路急行。 灌木钻了没多久就开始下山路，而且能看到山下明亮的高速公路跟城市夜景了，这让我放心了一点——至少确实是在下山朝城市里去。但这下山路可不好走，几乎是钻着灌木丛林走直线下降，而且是原生地形，非常的陡，即使有登山杖的辅助，也摔了好多跤，还好草木灌木比较密，有效减缓了摔倒的冲击， 也避免我滚下山。 下山路没走多久，我突然发现腰包里的水杯跟夹着的折叠坐垫都不见了，显然是在前面几次摔跤的时候掉了，不过也就三四十块钱，不管了，继续向下。 下山路仿佛没有尽头，万幸途中发现底下的山坳里有好几片亮光，一开始怀疑是山里废弃的房子，前面趟这条路的旅游队在房子里露营，这也给了我希望——或许能有人给点帮助跟一口吃的，一起露营也不错。 到九点半左右终于下到山脚的时候，发现是个电站之类的建筑，周围还有监控警示，挺失望的。不过到这里又是大路了，也能很明显听到不远处车辆来往的声音，悬着的心总算放了下来。 考虑到手表快没电了，先把登山记录停了，显示今天徒步了接近 21 公里，真是累到够呛。 OPPO 健康 - 日行四万步 登山 21 公里 在路边找了个地坐着休息，想到因为钻灌木林、摔跤、一路剧烈运动疯狂出汗，加之今天又下小雨，身上都是各种小叶子、木棍、汗水、沾了叶子上的雨水，这个样子可不好上车见人。见周围也没人，我直接把衣服都脱掉，换了身干净的。 登山鞋里也湿透了，刚刚钻林子导致石子叶子小木棍雨水也进去了不少，也换了备用的沙滩鞋。换鞋时不知道哪跑出来只蚂蟥在我脚面上爬，赶紧给拔掉丢了（也很庆幸我一直穿运动紧身内衣裤，不然这一趟灌木丛徒步下来，小腿刮伤不说，还可能被蚂蟥等各种虫子叮咬）。 衣服鞋子换好后又休息了挺久，然后沿着大路走了可能十多二十分钟，才终于到了山脚，远远看到不远处就有个公交站，再次导航一下，确认它就是大老山隧道站。 到达车站已经是 22:20 了，乘九巴 74X 路到广福邨下车，再步行到地铁站乘车到罗湖口岸、过关、再乘一号线时已经 23:40，这个点居然还有一趟末班车。最后到家已经过了 0 点，饿得不行搞完夜宵、休息、再洗澡，搞到两三点才睡觉。 22:37，在乘九巴 74X 路往广福邨的路上，香港城市夜景 下车，步行前往地铁站 22:52，到达大埔墟地铁站 22:57，等地铁中 总的来说这次徒步距离真的是到目前为止的人生巅峰了，另外这次下山路线也是我走过最险的一次，有点刺激跟后怕，高德地图坑我啊。 ","date":"2024-05-21","objectID":"/posts/hong-kong-travel-notes-in-2024/:5:0","series":["徒步旅行"],"tags":["Travel","旅游","Backpacking","重装徒步","Hiking","徒步","Hong Kong","香港","MacLehose Trail","麦理浩径"],"title":"2024 年香港徒步旅行记录（一）","uri":"/posts/hong-kong-travel-notes-in-2024/#四麦理浩径第四段"},{"categories":["life"],"content":" 装备总结 不应该带的装备： 折叠桌跟折叠凳： 沐浴露：许多营地没洗澡的地方，擦身体也用不上沐浴露，带这个不如带点湿巾。 洗洁精：同理，就几天徒步，用纸巾擦一擦就行，回家后再用洗洁精清理一遍不迟。 太阳能电板：三天以内，一个 30000 mah 的电源完全够用。如果 3-6 天，还不如带两个移动电源。目前根本没有徒步超过一周的计划，所以这个也没必要带。 保温杯、餐具、炉子：现在是夏季，香港挺温暖，我全程靠能量胶跟压缩饼干维持体力，晚饭吃自热米饭，用不上这些装备。其实主要是气罐不好带，担心被海关查扣。 起了重大作用的装备： 曦途第三代 7075 登山杖：铝合金本身够轻，将部分腿部负重转移到上半身，减轻了腿部的负担， 在重装徒步时能起到保护膝盖、提升稳定性、延长行走时间的作用。 目前发现的缺点： 使用时登山杖中部会共振，声音稍微有点吵，不过问题不大。 一次下雨徒步后放了一天多，就发现杖尖周围生锈了，看来是用的材料不够好（毕竟是不到 60 块钱一根，还要啥自行车）。 挪客云径 65L 徒步背包：够大，带背负系统，非常好用。 目前发现的缺点： 腰封的带子会滑动，行走久了就会松掉，需要重新手动调整。这个很烦，走着走着肩部受力就越来越大，而且行走的时候调整带子松紧也很不方便。 挪客 3L 水袋、600ml 水杯：喝水，必备 迪卡侬 MH100 登山鞋：好的硬底登山鞋，防滑、防水、防磨损，能保护脚部，提供足够的支撑， 减少脚部疲劳。 挪客夏季信封睡袋：即使是现在夏季，山上的营地（水浪窝）晚上还是有点冷的，盖一个夏季睡袋刚刚好。 能量胶、盐丸、压缩饼干、零食：帮助保持体力。能量胶恢复体力但饿得快，压缩饼干能维持饱腹感，零食能提供口感。 能量胶跟盐丸非常有用！ 折叠水桶、速干毛巾：打水洗头洗澡洗衣必备！（营地没洗澡的地方，用湿毛巾擦擦身体，也OK， 擦完舒服太多了） 运动速干套装（紧身速干打底衣裤长袖 + 短袖短裤 + 薄外套）：防止蚊虫叮咬、速干、防晒、防风，钻个灌木丛也不惧，我愿称之为徒步必备！ 速干汗带：吸汗，防止汗水流入眼睛，而且速干款比棉制品强多了，蒸发很快。 变色防风眼镜：防风防晒、防雨、防风沙蚊虫。不管是白天也是夜间徒步，都很实用！ 保鲜袋：一时半会儿没吃完的食物，用这个装非常方便。 一次性内裤、袜子：用完即抛，不用洗，方便得很。 神火 A20 手电筒：一开始觉得好像没啥用，毕竟已经有头灯了，结果发现晚上搭帐篷、在营地逛逛，这个很好用也很有必要。 30000 mah 移动电源（111Wh）：显然这个起了大作用，用完一天到回家的时候，还有 70% 电量， 它大概能顶三天，650g 的重量物有所值。 大概两年前买的，最近查了下，有个别能量密度更高的电源，但市面上大部分电源的能量密度都跟这差不多，没有太大的提升。 曦途折叠坐垫：行走路上累了，随时坐下休息，这个很有用。 本来是打算用折叠凳的，但折叠凳收纳还是麻烦许多，而且它建议承重 75Kg，我背着个接近 20 公斤的包，很怕一屁股给坐坏了。 不是很满意的装备 Warsun W81s 头灯：用了两个半小时就没电了，而且还不能换电池！后面去露营地点的路上只能手举手电筒同时还要用登山杖，很不方便。必须得换个电池可更换的头灯，最好是能用 18650 电池！ 下单了耐朗 B71 转角手电筒、不知名的万向手电筒 U 型夹（当手电腰夹用）、倍量的最高密度 18650 电池跟 21700 电池。这样转角手电 + 直角手电 + 手电腰夹 + 备用电池组合，能应对任何多日长线徒步的夜行情况了 防晒袖套脖套：穿了长袖紧身衣的情况下袖套感觉就没啥必要了，脖套可以保留一个。 迪卡侬 MH500 冲锋衣 + 普通雨裤：占地方，而且也挺重，这次没派上用场。但考虑到万一下大雨刮大风，这俩还是得带上。大风天气下雨披用处不大，只能靠冲锋衣。 急救包：有 400g，感觉太重了，下次可以精简一下。 牧高笛雨披天幕：至少在香港用这东西，有点太热了，而且我买的这个没有腰带，背部也没有加长，现在看应该买三峰出的那个会好很多。 这次没带，但发觉应该补充的装备 能放水杯、证件、手机的腰包：背包腰封会挡住裤子口袋，腰封的口袋也没法放手机，这个时候腰包就非常有用了。 驱蚊水：驱蚊驱虫，一是防止叮咬，二是防止蚊虫进入帐篷，我还挺怕小虫子的。 湿巾：一些营地淡水不太好获得（比如水浪窝营地，我这次是用了水袋里的饮水擦身体…），这时候湿巾就非常有用了，可以用来简单清理下身体。 短款雪套：防止各种碎石、树枝、草叶进入鞋子，保护脚部。用短款是因为紧身运动裤已经提供了保护腿部的功能，长款雪套就显得多余了（下雨另当别论）。 TODOs: 需要练习下雨披 + 背包的使用方式，这次路上下雨点，手忙脚乱地穿上，搞半天都盖不住背包。 学习下雨天如何快速收拾帐篷，这次收拾都花了半个多小时，要是下大雨大概帐篷跟东西得全搞湿了。 如何选择扎营地点？这次选在了树下的草地，结果恰好是少有人扎营的一块，虫子特别多（毛毛虫、跳蚤、蚂蚁、蜘蛛，等等…）。 登山杖的手柄系带如何拆卸清洗，以及如何调整长短。走完一趟系带都被汗水浸湿了，而且有点长了。 再复习一遍登山杖使用方法，这次发现登山杖起了大用，而且经过一天的高强度使用，经验也丰富了，现在再回头补充下理论知识，应该会有更深的体会。 学习伞绳、急救包的使用方法，经常徒步的话，出了意外不会用就尴尬了。 现在已经爱上了徒步这种运动方式，花钱折磨自己毫不手软（ ","date":"2024-05-21","objectID":"/posts/hong-kong-travel-notes-in-2024/:6:0","series":["徒步旅行"],"tags":["Travel","旅游","Backpacking","重装徒步","Hiking","徒步","Hong Kong","香港","MacLehose Trail","麦理浩径"],"title":"2024 年香港徒步旅行记录（一）","uri":"/posts/hong-kong-travel-notes-in-2024/#装备总结"},{"categories":["life"],"content":" 参考资料如下是我近期发现的一些高质量徒步相关资料，对我有挺大帮助： 比装备推荐更重要的户外知识：重装徒步入坑指南，学到挺多新 户外装备超级指南 - 知乎专栏: 一位户外高玩的专栏，深入浅出地讲解了许多重装徒步装备（包括用药）的设计原理、使用方法、选购策略等等，非常有用！ 户外这 7 种常见的地形，登山杖的使用方法也各不相同 - Bilibili: 这位 UP 主出了许多简短的视频讲解各种户外技巧，非常实用，尤其是这个讲解登山杖用法的视频。 35位医生投票：家中常备什么药？竟然有一个德不配位！- 哔哩哔哩： 常用药指南，结合户外背包打包与装备检查清单 - 1.22 医疗救援与防护用品 的内容选择药品，非常实用。 一個人的麥理浩徑: 包含详细的麦理浩徑全程路线、政府营地、水源、交通等信息，非常实用。 露營地點 - 香港渔农自然护理署: 包含香港全部 41 个政府营地的介绍。 ","date":"2024-05-21","objectID":"/posts/hong-kong-travel-notes-in-2024/:7:0","series":["徒步旅行"],"tags":["Travel","旅游","Backpacking","重装徒步","Hiking","徒步","Hong Kong","香港","MacLehose Trail","麦理浩径"],"title":"2024 年香港徒步旅行记录（一）","uri":"/posts/hong-kong-travel-notes-in-2024/#参考资料"},{"categories":["tech"],"content":" 本文最初发表于 如何评价NixOS? - 知乎，觉得比较有价值所以再搬运到我的博客。 我 23 年 4 月开始用 NixOS 之前看过（如何评价NixOS? - 知乎） 这个问答，几个高赞回答都从不同方面给出了很有意义的评价，也是吸引我入坑的原因之一。 现在是 2024 年 2 月，距离我入坑 NixOS 刚好 10 个月，我当初写的新手笔记已经获得了大量好评与不少的赞助，并成为了整个社区最受欢迎的入门教程之一。自 2023 年 6 月我为它专门创建一个 GitHub 仓库与单独的文档站点以来，它已经获得了 1189 个 stars，除我之外还有 37 位读者给它提了 PR： NixOS 与 Flakes - 一份非官方的新手指南 NixOS \u0026 Flakes Book 那么作为一名已经深度使用 NixOS 作为主力桌面系统接近 10 个月的熟手，我在这里也从另一个角度来分享下我的入坑体会。 注意，这篇文章不是 NixOS 入门教程，想看教程请移步上面给的链接。 ","date":"2024-02-21","objectID":"/posts/my-experience-of-nixos/:0:0","series":["NixOS 与 Nix Flakes"],"tags":["NixOS","Nix","Flakes","Linux","DevOps"],"title":"OS as Code - 我的 NixOS 使用体会","uri":"/posts/my-experience-of-nixos/#"},{"categories":["tech"],"content":" Nixpkgs 中的包太少？先澄清下一点，NixOS 的包非常多，Repository statistics 的包仓库统计数据如下： 上面这个 Nixpkgs 的包数量确实有挺多水分——Nixpkgs 还打包了许多编程语言的 Libraries（貌似挺多 Haskell 人用 nix 当语言包管理器用），比如Haskell Packages(18000+),R Packages(27000+),Emacs Packages(6000+) ，但即使把它们去掉后 Nixpkgs 的包数量也有大约 40000+，虽然逊色于 AUR，但这个数量再怎么算也跟「包太少」这个描述扯不上关系。 包仓库这里也是 NixOS 跟 Arch 不太同的地方，Arch 的官方包仓库收录很严格，相对的 AUR 生态相当繁荣。但任何人都能往 AUR 上传内容，虽然有一个投票机制起到一定审核作用，但个人感觉这个限制太松散了。 而 NixOS 就很不一样了，它的官方包仓库 Nixpkgs 很乐于接受新包，想为 Nixpkgs 提个 PR 加包或功能相对其他发行版而言要简单许多，这是 Nixpkgs 的包数量这么多的重要原因（GitHub 显示 Nixpkgs 有 5000+ 历史贡献者，这很夸张了）。 Nixpkgs 仓库的更新流程相对 AUR 也严格许多，PR 通常都需要通过一系列的 GitHub Actions 测试 + Maintainer Review + Ofborg 检查与自动构建测试后才能被合并，Nixpkgs 也鼓励维护者为自己的包添加测试（包的 doCheck 默认为 true），这些举措都提升了 Nixpkgs 的包质量。 NixOS 其实也有个与 AUR(Arch User Repository) 类似的 NUR（Nix User Repository），但因为 Nixpkgs 的宽松，NUR 反而没啥内容。 举例来说，QQ 能直接从 Nixpkgs 官方包仓库下载使用，而在 Arch 上你得用 AUR 或者 archlinux-cn. 这算是各有优势吧。NixOS 被人喷包少，主要是因为它不遵循 FHS 标准，导致大部分网上下载的 Linux 程序都不能直接在 NixOS 上运行。这当然有解决方案，我建议是首先看看 Nixpkgs 中是否已经有这个包了，有的话直接用就行。如果没有，再尝试一些社区的解决方案，或者自己给打个包。 用 NixOS 的话自己打包程序是不可避免的，因为即使 Nixpkgs 中已经有了这么多包，但它仍然不可能永远 100% 匹配你的需求，总有你想用但 Nixpkgs 跟 NUR 里边都没有的包，在 NixOS 上你常常必须要给你的包写个打包脚本，才能使它在 NixOS 上正常运行。 另外即使有些程序本身确实能在 NixOS 上无痛运行，但为了做到可复现，NixOS 用户通常也会选择自己手动给它打个包。 OK，闲话说完，下面进入正题。 首先，NixOS 比传统发行版复杂很多，也存在非常多的历史遗留问题。 举例来说，它的官方文档烂到逼得我一个刚学 NixOS 的新手自己边学边写入门文档。在我用自己的渣渣英语把笔记翻译了一遍发到 reddit （NixOS \u0026 Nix Flakes - A Guide for Beginners） 后，居然还获得了许多老外的大量好评（经过这么长时间的持续迭代，现在甚至已经变成了社区最受欢迎的新手教程之一），这侧面也说明官方文档到底有多烂。 ","date":"2024-02-21","objectID":"/posts/my-experience-of-nixos/:1:0","series":["NixOS 与 Nix Flakes"],"tags":["NixOS","Nix","Flakes","Linux","DevOps"],"title":"OS as Code - 我的 NixOS 使用体会","uri":"/posts/my-experience-of-nixos/#is-nixpkgs-lacking-packages"},{"categories":["tech"],"content":" NixOS 值不值得学？NixOS 值不值得学或者说投入产出比是否够高？在我看来，这归根结底是个规模问题。 这里的规模，一是指你对 Linux 系统所做的自定义内容的规模，二是指你系统更新的频繁程度，三是指你 Linux 机器的数量。 下面我从个人经历的角度来讲下我以前用 Arch Linux、Ubuntu 等传统发行版的体验，以及我为什么选择了 NixOS，NixOS 又为我带来了什么样的改变。 举个例子，以前我用 Deepin Ubuntu 时我基本没对系统做过什么深入定制，一是担心把系统弄出问题修复起来头疼，二是如果不额外写一份文档或脚本记录下步骤的话，我做的所有定制都是黑盒且不可迁移的，一个月后我就全忘了，只能战战兢兢地持续维护这个随着我的使用而越来越黑盒、状态越来越混沌的系统。 如果用的是 Arch 这种滚动发行版还好，系统一点点增量更新，遇到的一般都是小问题。而对 Ubuntu Deepin 这种，原地升级只出小问题是很少见的，这基本就意味着我必须在某个时间点，在新版本的 Ubuntu 上把我以前做过的定制再全部重做一遍，更关键的是，我非常有可能已经忘了我以前做了什么，这就意味着我得花更多的时间去研究我的系统环境里到底都有些啥东西，是怎么安装配置的，这种重复劳动非常痛苦。 总之很显然的一点是，我对系统做的定制越多越复杂，迁移到新版本的难度就越大。 我想也正是因为这一点，Arch、Gentoo、Fedora 这种滚动发行版才在 Linux 爱好者圈子中如此受欢迎，喜欢定制自己系统的 Linux 用户也大都使用这类滚动发行版。 那么 Arch、Fedora 就能彻底解决问题了么？显然并不是。首先它们的更新频率比较高，这代表着你会更容易把你的系统搞出点毛病来。当然这其实是个小问题，现在 Linux 社区谁还没整上个 btrfs / zfs 文件系统快照啊，出问题回滚快照就行。它们最根本的问题是： 你的 Arch 系统环境、文件系统快照、或者虚拟机快照，它们仍然是个黑盒，仍然会随着你的持续使用而越来越混沌，也并不包含如何从零构建这个环境的「知识」，是不可解释的。 我在工作中就见到过一些「祖传虚拟机快照」或「祖传云服务器快照」，没人知道这个环境是怎么搭建的，每一任接手的人都只能继续往上叠 Buff，然后再把这个定时炸弹传给下一任。这就像那个轮流往一个水杯里加水的游戏，最后在谁加水的时候溢出来了，那就算他倒霉。 Arch 实质要求你持续跟着它的更新走，这意味着你必须要持续更新维护它。 如果你把机器放了一年半载跑得很稳定，然后你想要更新一下，那出问题的风险会相当高。如果你因此而决定弄台最新版本的 Arch 机器再把旧环境还原出来，那就又回到了之前的问题——你得想办法从旧环境中还原出你的定制流程，这也不是个好差事。 快照与当前硬件环境强相关，直接在不同硬件的机器上使用很容易遇到各种奇怪的问题，也就是说这东西不可迁移。 快照是一堆庞大的二进制文件，它的体积非常大，这使得备份与分享它的成本高昂。 Docker 能解决上述问题中的一部分。首先它的容器镜像可由 Dockerfile 完全描述，也就是说它是可解释的，此外容器镜像能在不同环境中复现出完全一致的环境，这表明它是可迁移的。对于服务器环境，将应用程序全都跑在容器中，宿主机只负责跑容器，这种架构使得你只需要维护最基础的系统环境，以及一些 Dockerfile 跟 yaml 文件，这极大地降低了系统的维护成本，从而成为了 DevOps 的首选。 但 Docker 容器技术是专为应用程序提供一致的运行环境而设计的，在虚拟机、桌面环境等场景下它并不适用（当然你非要这么弄也不是不行，很麻烦就是了）。此外 Dockerfile 仍旧依赖你所编写的各种脚本、命令来构建镜像，这些脚本、命令都需要你自己维护，其运行结果的可复现能力也完全看你自己的水平。 如果你因为这些维护难题而选择极简策略——尽可能少地定制任何桌面系统与虚拟机环境，能用默认的就用默认——这就是换到 NixOS 之前的我。为了降低系统维护难度，我以前使用 Deepin Manjaro EndeavourOS 的过程中，基本没对系统配置做任何大变动。作为一名 SRE/DevOps，我在工作中就已经踩了够多的环境问题的坑，写腻写烦各种安装脚本、Ansible 配置了，业余完全不想搞这些幺蛾子。 但如果你是个喜欢定制与深入研究系统细节的极客，随着你对系统所做的定制越来越多，越来越复杂， 或者你 Homelab 与云上的 Linux 机器越来越多，你一定会在某个时间点开始编写各种部署流程的文档、部署脚本或使用一些自动化工具帮自己完成一些繁琐的工作。 文档就不用说了，这个显然很容易过时，没啥大用。如果你选择自己写自动化脚本或选用自动化工具， 它的配置会越来越复杂，而且系统更新经常会破坏掉其中一些功能，需要你手动修复。此外它还高度依赖你当前的系统环境，当你某天装了台新机器然后信心满满地用它部署环境时，大概率会遇到各种环境不一致导致的错误需要手动解决。还有一点是，你写的脚本大概率并没有仔细考量抽象、模块化、错误处理等内容，这也会导致随着规模的扩大，维护它变得越来越痛苦。 然后你发现了 NixOS，它有什么声明式的配置，你仔细看了下它的实现，哦这声明式的配置，不就是把一堆 bash 脚本封装了下，对用户只提供了一套简洁干净的 api 么，它实际干的活不跟我自己这几年写的一堆脚本一模一样？好像没啥新鲜的。 嗯接着你试用了一下，发现 NixOS 的这套系统定制脚本都存在一个叫 Nixpkgs 的仓库中，有数千人在持续维护它，几十年积累下来已经拥有了一套非常丰富、也比较稳定的声明式抽象、模块系统、类型系统、专为这套超大型的软件包仓库与 NixOS 系统配置而开发的大规模 CI 系统 Hydra、以及逐渐形成的能满足数千人协作更新这套复杂配置的社区运营模式。 你立马学习 nix 语言，然后动手把这套维护了 N 年的脚本改写成 NixOS 配置。 越写就对它越满意，改造后的配置缩水了相当多，维护难度直线下降。 很大部分以前自己用各种脚本跟工具实现的功能，都被 Nixpkgs 封装好了，只需要 enable 一下再传几个关键参数，就能无痛运行。nixpkgs 中的脚本都有专门的 maintainer 维护更新，任何发现了问题的用户也可以提个 PR 修下问题，在没经过 CI 与 staging unstable 等好几个阶段的广泛验证前，更新也不会进入 stable. 上面所说的你，嗯就是我自己。 现在回想下我当初就为了用 systemd 跑个简单的小工具而跟 systemd 疯狂搏斗的场景，泪目… 要是我当初就懂 NixOS… ","date":"2024-02-21","objectID":"/posts/my-experience-of-nixos/:2:0","series":["NixOS 与 Nix Flakes"],"tags":["NixOS","Nix","Flakes","Linux","DevOps"],"title":"OS as Code - 我的 NixOS 使用体会","uri":"/posts/my-experience-of-nixos/#is-nixos-worth-learning"},{"categories":["tech"],"content":" NixOS 的声明式配置 - OS as Code有过一定编程经验的人都应该知道抽象与模块化的重要性，复杂程度越高的场景，抽象与模块化带来的收益就越高。Terraform、Kubernetes 甚至 Spring Boot 的流行都体现了这一点。NixOS 的声明式配置也是如此，它将底层的实现细节都封装起来了，并且这些底层封装大都有社区负责更新维护，还有 PR Review、CI 与多阶段的测试验证确保其可靠性，这极大地降低了我的心智负担，从而解放了我的生产力。它的可复现能力则免除了我的后顾之忧，让我不再担心搞坏系统。 NixOS 构建在 Nix 函数式包管理器这上，它的设计理念来自 Eelco Dolstra 的论文 The Purely Functional Software Deployment Model（纯函数式软件部署模型），纯函数式是指它没有副作用， 就类似数学函数 $y = f(x)$，同样的 NixOS 配置文件（即输入参数 $x$ ）总是能得到同样的 NixOS 系统环境（即输出 $y$）。 这也就是说 NixOS 的配置声明了整个系统完整的状态，OS as Code！ 只要你 NixOS 系统的这份源代码没丢，对它进行修改、审查，将源代码分享给别人，或者从别人的源代码中借鉴一些自己想要的功能，都是非常容易的。你简单的抄点其他 NixOS 用户的系统配置就能很确定自己将得到同样的环境。相比之下，你抄其他 Arch/Ubuntu 等传统发行版用户的配置就要麻烦的多，要考虑各种版本区别、环境区别，不确定性很高。 ","date":"2024-02-21","objectID":"/posts/my-experience-of-nixos/:3:0","series":["NixOS 与 Nix Flakes"],"tags":["NixOS","Nix","Flakes","Linux","DevOps"],"title":"OS as Code - 我的 NixOS 使用体会","uri":"/posts/my-experience-of-nixos/#nixos-declarative-configuration"},{"categories":["tech"],"content":" NixOS 的学习成本NixOS 的入门门槛相对较高，也不适合从来没接触过 Linux 与编程的小白，这是因为它的设计理念与传统 Linux 发行版有很大不同。但这也是它的优势所在，跨过那道门槛，你会发现一片新天地。 举例来说，NixOS 用户翻 Nixpkgs 中的实现源码实际是每个用户的基本技能，给 Nixpkgs 提 PR 加功能、加包或者修 Bug 的 NixOS 用户也相当常见。 这既是使新用户望而却步的拦路之虎，同时也是给选择了 NixOS 的 Linux 用户提供的进阶之梯。 想象下大部分 Arch 用户（比如以前的我）可能用了好几年 Arch，但根本不了解 Arch 底层的实现细节，没打过自己的包。而 NixOS 能让翻源码成为常态，实际也说明理解它的实现细节并不难。我从两个方面来说明这一点。 第一，Nix 是一门相当简单的语言，语法规则相当少，比 Java Python 这种通用语言简单了太多。因此有一定编程经验的工程师能花两三个小时就完整过一遍它的语法。再多花一点时间，读些常见 Nix 代码就没啥难度了。 第二，NixOS 良好的声明式抽象与模块化系统，将 OS 分成了许多层来实现，使用户在使用过程中， 既可以只关注当前这一层抽象接口，也可以选择再深入到下一层抽象来更自由地实现自己想要的功能（这种选择的权利，实际也给了用户机会去渐进式地理解 NixOS 本身）。举例来说，新手用户只要懂最上层的抽象就正常使用 NixOS。当你有了一点使用经验，想实现些自定义需求，挖下深挖一层抽象（比如说直接通过 systemd 的声明式参数自定义一些操作）通常就足够了。如果你已经是个 NixOS 熟手，想更极客一点，就可以再继续往下挖。 总之因为上面这两点，理解 Nixpkgs 中的源码或者使用 Nix 语言自己打几个包并不难，可以说每个有一定经验的 NixOS 用户同时也会是 NixOS 打包人。 ","date":"2024-02-21","objectID":"/posts/my-experience-of-nixos/:4:0","series":["NixOS 与 Nix Flakes"],"tags":["NixOS","Nix","Flakes","Linux","DevOps"],"title":"OS as Code - 我的 NixOS 使用体会","uri":"/posts/my-experience-of-nixos/#nixos-learning-curve"},{"categories":["tech"],"content":" NixOS 的卖点？我们看了许多人提到 NixOS 的优点，上面我也提到了不少。Nix 的圈外人听得比较多的可能主要是它解决了依赖冲突问题，能随时回滚，强大的可复现能力。如果你有实际使用过 NixOS，那你也应该知道 NixOS 的这些优势： NixOS 的 Flakes 特性使你能将系统锁定在一个特定的状态，你可以在任何想更新的时候才更新它，即使有个一年半载不更新也完全没毛病。NixOS 不会强迫你频繁更新系统，你可以选择是否这么做。因为系统的状态可以完全从你的 NixOS 配置中推断出来，所以从旧版本升级到最新版本也容易很多。 有的选总是好的，我不喜欢被强迫频繁更新（即使我实际更新还挺频繁的），公司里的系统管理员或者 DevOps 就更是如此了。 系统更新具有类似数据库事务的原子化特性，这意味着你的系统更新要么成功要么失败，（一般） 不会出现中间状态。 NixOS 的声明式配置实际实现了 OS as Code，这使得这些配置非常便于分享。直接在 GitHub 上从其他 NixOS 用户那里 Copy 需要的代码到你的系统配置中，你就能得到一个一模一样的功能。新手用户也能很容易地从别人的配置中学到很多东西。 这也是近几年 GitHub 与 reddit r/unixporn 上使用 NixOS 做桌面 ricing 的用户越来越多的原因。 声明式配置为用户提供了高度便捷的系统自定义能力，通过改几行配置，就可以快速更换系统的各种组件。 等等 这些都是 NixOS 的卖点，其中一些特性现在在传统发行版上也能实现，Fedora Silverblue 等新兴的不可变发行版也在这些方面有些不错的创新。但能解决所有这些问题的系统，目前只有 NixOS（以及更小众的 Guix. 据Guix 的 README 所言，它同样基于 Nix 包管理器）。 ","date":"2024-02-21","objectID":"/posts/my-experience-of-nixos/:5:0","series":["NixOS 与 Nix Flakes"],"tags":["NixOS","Nix","Flakes","Linux","DevOps"],"title":"OS as Code - 我的 NixOS 使用体会","uri":"/posts/my-experience-of-nixos/#nixos-advantages"},{"categories":["tech"],"content":" NixOS 的缺点与历史债务自 NixOS 项目创建至今二十多年来，Nix 包管理器与 NixOS 操作系统一直是非常小众的技术，尤其是在国内，知道它们存在的人都是少数 Linux 极客，更别说使用它们了。 NixOS 很特殊，很强大，但另一方面它也有着相当多的历史债务，比如说： 文档混乱不说人话 Flakes 特性使 NixOS 真正满足了它一直宣称的可复现能力，但从 2021 年正式发布到现在 2024 年，它仍旧处在实验状态。 Nix 的 CLI 处在换代期，新版本的 CLI 优雅很多，但其实现目前与 Flakes 特性强绑定，导致两项功能都难以 stable，甚至还阻碍了许多其他特性的开发工作。 模块系统的缺陷与 Nix 错误处理方面的不足，导致长期以来它的报错信息相当隐晦，令人抓狂 Nix 语言太过简单导致 Nixpkgs 中大量使用 Bash 脚本，以及 Nix 语言的大多数特性都完全是使用 C++ 实现的，从 Nix 语言的角度看这很黑盒。 NixOS 的大量实现细节隐藏在 Nixpkgs 源码中，比如说软件包的分类、derivation 有哪些属性可被 override。 Nixpkgs 长期一直使用文件夹来对软件包进行分类，没有任何查看源码之外的手段来分类查询其中的软件包。 Nixpkgs 中的所有 derivation 相关信息，目前也只能通过查看源码来了解。 https://nixos.wiki 站点维护者跑路，官方又长期未提供替代品，导致 NixOS 的文档在本来就很烂的基础上又雪上加霜。 Nix/NixOS 近来快速增长的用户群体，使得它的社区运营模式也面临着挑战 … 这一堆历史债是 NixOS 一直没能得到更广泛使用的主要原因。但这些问题也是 NixOS 未来的机会，社区目前正在积极解决这些问题，我很期待看到这些问题被解决后， NixOS 将会有怎样的发展。 ","date":"2024-02-21","objectID":"/posts/my-experience-of-nixos/:6:0","series":["NixOS 与 Nix Flakes"],"tags":["NixOS","Nix","Flakes","Linux","DevOps"],"title":"OS as Code - 我的 NixOS 使用体会","uri":"/posts/my-experience-of-nixos/#nixos-disadvantages"},{"categories":["tech"],"content":" NixOS 的未来谁也不会对一项没前途的技术感兴趣，那么 NixOS 的未来如何呢？我是否看好它？这里我尝试使用一些数据来说明我对 NixOS 的未来的看法。 首先看 Nixpkgs 项目，它存储了 NixOS 所有的软件包及 NixOS 自身的实现代码： 上图能看到从 2021 年开始 Nixpkgs 项目的活跃度开始持续上升，Top 6 贡献者中有 3 位都是 2021 年之后开始大量提交代码，你点进 GitHub 看，能看到 Top 10 贡献者中有 5 位都是 2021 年之后加入社区的（新增的 @NickCao 与 @figsoda 都是 NixOS 中文社区资深用户）。 再看看 Nix 包管理器的提交记录，它是 NixOS 的底层技术： 上图显示 Nix 项目的活跃度在 2020 年明显上升，Top 6 贡献者中有 5 位都是在 2020 年之后才开始大量贡献代码的。 再看看 Google Trends 中 NixOS 这个关键词的搜索热度： 这个图显示 NixOS 的搜索热度有几个明显的上升时间点： 2021 年 12 年 这大概率是因为在 2021 年 11 月Nix 2.4 发布了，它带来了实验性的 Flakes 特性与新版 CLI，Flakes 使得 NixOS 的可复现能力得到了极大的提升，新 CLI 也更符合用户直觉。 2023 年 6 月 最重要的原因应该是，Youtube 上 Linux 相关的热门频道在这个时间点推出了好几个关于 NixOS 的视频，截至 2024-02-23，Youtube 上播放量最高的三个 NixOS 相关视频都是在 2023-06 ~ 2023-07 这个时间段推出的，它们的播放量之和超过了 130 万。 China 的兴趣指数在近期最高，这可能是因为国内的用户群一直很少，然后我在 6 月份发布了NixOS 与 Flakes - 一份非官方的新手指南， 并且在 科技爱好者周刊 等渠道做了些推广，导致 NixOS 的相对指数出现明显上升。 2024 年 1 月 这个我目前不太确定原因。 再看看 Nix/NixOS 社区从 2022 年启用的年度用户调查。 2022 Nix Survey Results， 根据其中数据计算可得出： 74.5% 的用户是在三年内开始使用 Nix/NixOS 的。 关于如何拓展 Nixpkgs 的调查中，36.7% 的用户使用 Flakes 特性拓展 Nixpkgs，仅次于传统的 overlays. Nix Community Survey 2023 Results， 简单计算可得出， 54.1% 的用户是在三年内开始使用 Nix/NixOS 的。 关于如何拓展 Nixpkgs 的调查中，使用 Flakes 特性的用户占比为 49.2%，超过了传统的 Overlays. 关于实验特性的调查中，使用 Flakes 特性的用户占比已经达到了 59.1%. 2024-04-12 更新：NixCon 2024 也有一个演讲提供了 Nix 社区的各种历史数据：Nix, State of the Union - NixCon 2024 另外 GitHub 的Octoverse 2023 也难得地提了一嘴 Nixpkgs: Developers see benefits to combining packages and containerization. As we noted earlier, 4.3 million repositories used Docker in 2023. \u003e On the other side of the coin, Linux distribution NixOS/nixpkgs has been on the top list of open source projects by contributor for the last two years. 这些数据与我们前面提到的 Nixpkgs 与 Nix 项目的活跃度相符，都显示 Nix/NixOS 社区在 2021 年之后开始迅速增长壮大。 结合上面这些数据看，我对 NixOS 的未来持很乐观的态度。 ","date":"2024-02-21","objectID":"/posts/my-experience-of-nixos/:7:0","series":["NixOS 与 Nix Flakes"],"tags":["NixOS","Nix","Flakes","Linux","DevOps"],"title":"OS as Code - 我的 NixOS 使用体会","uri":"/posts/my-experience-of-nixos/#nixos-future"},{"categories":["tech"],"content":" 总结从决定入坑 NixOS 到现在，短短 10 个月，我在 Linux 上取得的收获远超过去三年。我已经在 PC 上尝试了非常多的新技术新工具，我的 Homelab 内容也丰富了非常多（我目前已经有了十多台 NixOS 主机），我对 Linux 系统结构的了解也越来越深刻。 光是这几点收获，就完全值回票价了，欢迎入坑 NixOS~ ","date":"2024-02-21","objectID":"/posts/my-experience-of-nixos/:8:0","series":["NixOS 与 Nix Flakes"],"tags":["NixOS","Nix","Flakes","Linux","DevOps"],"title":"OS as Code - 我的 NixOS 使用体会","uri":"/posts/my-experience-of-nixos/#conclusion"},{"categories":["tech"],"content":" 零、前言在接触电脑以来很长的一段时间里，我都没怎么在意自己的数据安全。比如说： 长期使用一个没有 passphrase 保护的 SSH 密钥（RSA 2048 位），为了方便我还把它存到了 onedrive 里，而且在各种需要访问 GitHub/Gitee 或 SSH 权限的虚拟机跟 PC 上传来传去。 Homelab 跟桌面 PC 都从来没开过全盘加密。 在 2022 年我的 Homelab 坏掉了两块国产固态硬盘（阿斯加特跟光威弈 Pro 各一根），都是系统一启动就挂，没法手动磁盘格式化，走售后直接被京东换货了。因为我的数据是明文存储的，这很可能导致我的个人数据泄露… 几个密码在各种站点上重复使用，其中重要账号的随机密码还是我在十多年前用 lastpass 生成的，到处用了这么多年，很难说这些密码有没有泄露（lastpass 近几年爆出的泄漏事故就不少…） GitHub, Google, Jetbrains 等账号的 Backup Code 被我明文存储到了百度云盘，中间发现百度云盘安全性太差又转存到了 OneDrive，但一直是明文存储，从来没加过密。 一些银行账号之类的随机密码，因为担心遗忘，长期被我保存在一份印象笔记的笔记里，也是明文存储，仅做了些简单的内容替换，要猜出真正的密码感觉并不是很难。 以前也有过因为对 Git 操作不熟悉或者粗心大意，在公开仓库中提交了一些包含敏感信息的 commit，比如说 SSH 密钥、密码等等，有的甚至很长时间都没发现。 现在在 IT 行业工作了几年，从我当下的经验来看，企业后台的管理员如果真有兴趣，查看用户的数据真的是很简单的一件事，至少国内大部分公司的用户数据，都不会做非常严格的数据加密与权限管控。就算真有加密，那也很少是用户级别的，对运维人员或开发人员而言这些数据仍旧与未加密无异。对系统做比较大的迭代时，把小部分用户数据导入到测试环境进行测试也是挺常见的做法… 总之对我而言，这些安全隐患在过去并不算大问题，毕竟我 GitHub, Google 等账号里也没啥重要数据，银行卡里也没几分钱。 但随着我个人数据的积累与在 GitHub, Google 上的活动越来越多、银行卡里 Money 的增加（狗头），这些数据的价值也越来越大。比如说如果我的 GitHub 私钥泄漏，仓库被篡改甚至删除，以前我 GitHub 上没啥数据也没啥 stars 当然无所谓，但现在我已经无法忍受丢失 GitHub 两千多个 stars 的风险了。 在 2022 年的时候我因为对区块链的兴趣顺便学习了一点应用密码学，了解了一些密码学的基础知识， 然后年底又经历了几次可能的数据泄漏，这使我意识到我的个人数据安全已经是一个不可忽视的问题。因此，为了避免 GitHub 私钥泄漏、区块链钱包助记词泄漏、个人隐私泄漏等可能，我在 2023 年 5 月做了全面强化个人数据安全的决定，并在 0XFFFF 社区发了篇帖子征求意见——学习并强化个人的数据安全性（持续更新）。 现在大半年过去，我已经在个人数据安全上做了许多工作，目前算是达到了一个比较不错的状态。 我的个人数据安全方案，有两个核心的指导思想： 零信任：不信任任何云服务提供商、本地硬盘、网络等的可靠性与安全性，因此任何数据的落盘、网络传输都应该加密，任何数据都应该有多个副本（本地与云端）。 基于这一点，应该尽可能使用经过广泛验证的开源工具，因为开源工具的安全性更容易被验证， 也避免被供应商绑架。 Serverless: 尽可能利用已有的各种云服务或 Git 之类的分布式存储工具来存储数据、管理数据版本。 实际上我个人最近三四年都没维护过任何个人的公网服务器，这个博客以及去年搭建的 NixOS 文档站全都是用的 Vercel 免费静态站点服务，各种数据也全都优先选用 Git 做存储与版本管理。 我 Homelab 算力不错，但每次往其中添加一个服务前，我都会考虑下这是否有必要，是否能使用已有的工具完成这些工作。毕竟跑的服务越多，维护成本越高，安全隐患也越多。 这篇文章记录下我所做的相关调研工作、我在这大半年的实践过程中逐渐摸索出的个人数据安全方案以及未来可能的改进方向。 注意这里介绍的并不是什么能一蹴而就获得超高安全性的傻瓜式方案，它需要你需要你有一定的技术背景跟时间投入，是一个长期的学习、实践与方案迭代的过程。另外如果你错误地使用了本文中介绍的工具或方案，可能反而会降低你的数据安全性，由此产生的任何损失与风险皆由你自己承担。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:1:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#零前言"},{"categories":["tech"],"content":" 一、个人数据安全包含哪些部分？数据安全大概有这些方面： 保障数据不会泄漏——也就是加密 保障数据不会丢失——也就是备份 就我个人而言，我的数据安全主要考虑以下几个部分： SSH 密钥管理 各种网站、APP 的账号密码管理 灾难恢复相关的数据存储与管理 比如说 GitHub, Twitter, Google 等重要账号的二次认证恢复代码、账号数据备份等，日常都不需要用到，但非常重要，建议离线加密存储 需要在多端访问的重要个人数据 比如说个人笔记、图片、视频等数据，这些数据具有私密性，但又需要在多端访问。可借助支持将数据加密存储到云端的工具来实现 个人电脑與 Homelab 的数据安全与灾难恢复 我主要使用 macOS 与 NixOS，因此主要考虑的是这两个系统的数据安全与灾难恢复 下面就分别就这几个部分展开讨论。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:2:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#一个人数据安全包含哪些部分"},{"categories":["tech"],"content":" 二、是否需要使用 YubiKey 等硬件密钥？硬件密钥的好处是可以防止密钥泄漏，但 YubiKey 在国内无官方购买渠道，而且价格不菲，只买一个 YubiKey 的话还存在丢失的风险。 另一方面其实基于现代密码学算法的软件密钥安全性对我而言是足够的，而且软件密钥的使用更加方便。或许在未来，我会考虑使用canokey-core、OpenSK、solokey 等开源方案 DIY 几个硬件密钥，但目前我并不觉得有这必要。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:3:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#二是否需要使用-yubikey-等硬件密钥"},{"categories":["tech"],"content":" 三、SSH 密钥管理","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:4:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#三ssh-密钥管理"},{"categories":["tech"],"content":" 2.1 SSH 密钥的生成我们一般都是直接使用 ssh-keygen 命令生成 SSH 密钥对，OpenSSH 目前主要支持两种密钥算法： RSA: 目前你在网上看到的大部分教程都是使用的 RSA 2048 位密钥，但其破解风险在不断提升，目前仅推荐使用 3072 位及以上的 RSA 密钥。 ED25519: 这是密码学家 Dan Bernstein 设计的一种新的签名算法，其安全性与 RSA 3072 位密钥相当，但其签名速度更快，且密钥更短，因此目前推荐使用 ED25519 密钥。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:4:1","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#21-ssh-密钥的生成"},{"categories":["tech"],"content":" 2.2 SSH 密钥的安全性RSA 跟 ED25519 都是被广泛使用的密码学算法，其安全性都是经过严格验证的，因此我们可以放心使用。但为了在密钥泄漏的情况下，能够尽可能减少损失，强烈建议给个人使用的密钥添加 passphrase 保护。 那这个 passphrase 保护到底有多安全呢？ 有一些密码学知识的人应该知道，passphrase 保护的实现原理通常是：通过 KDF 算法（或者叫慢哈希算法、密码哈希算法）将用户输入的 passphrase 字符串转换成一个二进制对称密钥，然后再用这个密钥加解密具体的数据。 因此，使用 passphrase 加密保护的 SSH Key 的安全性，取决于： passphrase 的复杂度，这对应其长度、字符集、是否包含特殊字符等。这由我们自己控制。 所使用的 KDF 算法的安全性。这由 OpenSSH 的实现决定。 那么，OpenSSH 的 passphrase 是如何实现的？是否足够安全？ 我首先 Google 了下，找到一些相关的文章（注意如下文章内容与其时间点相关，OpenSSH 的新版本会有些变化）： (2018)The default OpenSSH key encryption is worse than plaintext: OpenSSH 默认的 SSH RSA 密钥格式直接使用 MD5 来派生出用于 AES 加密的对称密钥，再用这个密钥加密你的 RSA 私钥，这意味着它的破解速度将会相当的快。 (2021)Password security of encrypted SSH private key: How to read round number or costfactor of bcrypt: 这里有个老哥在回答中简单推算了下，以说明他认为 OpenSSH 默认的 passphrase 加密相当安全。 在 OpenSSH release notes 中搜索 passphrase 跟 kdf 两个关键字，找到些关键信息如下： text OpenSSH 9.4/9.4p1 (2023-08-10) * ssh-keygen(1): increase the default work factor (rounds) for the bcrypt KDF used to derive symmetric encryption keys for passphrase protected key files by 50%. ---------------------------------- OpenSSH 6.5/6.5p1 (2014-01-30) * Add a new private key format that uses a bcrypt KDF to better protect keys at rest. This format is used unconditionally for Ed25519 keys, but may be requested when generating or saving existing keys of other types via the -o ssh-keygen(1) option. We intend to make the new format the default in the near future. Details of the new format are in the PROTOCOL.key file. 时间阶段 (OpenSSH 版本) ssh-keygen 默认密钥类型 ssh-keygen 默认密钥长度 私钥 KDF 算法 (带 passphrase 时) 默认/主要 KEX 算法 默认/主要对称加密算法 OpenSSH 4.x (约2005 - 2008) RSA 2048 位 (RSA, 自 4.0 起) 基于 MD5 (OpenSSL PEM 格式) diffie-hellman-group1-sha1, diffie-hellman-group-exchange-sha1 AES-CBC (更普遍), 3DES-CBC; HMAC-SHA1 OpenSSH 5.x (约2009 - 2013) RSA 2048 位 (RSA) 基于 MD5 (OpenSSL PEM 格式) ecdh-sha2-nistp256 等 ECDH 系列引入 (自 5.7), DH 仍常见 AES-CTR (自 5.2 起优先于 CBC), AES-CBC; HMAC-SHA1 OpenSSH 6.x (约2014 - 2015) RSA (Ed25519 于 6.5 引入) 2048 位 (RSA) bcrypt_pbkdf (新格式, 自 6.5; Ed25519 默认, RSA 需 -o) curve25519-sha256 (自 6.5 起引入并优先), ECDH 系列 chacha20-poly1305@openssh.com (自 6.5), AES-GCM (自 ~6.2); CBC 模式于 6.7 默认禁用 OpenSSH 7.x (约2015 - 2018) RSA 2048 位 (RSA) bcrypt_pbkdf (自 7.8 起所有新密钥默认) curve25519-sha256, ECDH 系列; diffie-hellman-group1-sha1 于 7.0 禁用; rsa-sha2-256/512 签名 (自 7.2) chacha20-poly1305, AES-GCM (AEAD 优先); 3DES-CBC 从客户端默认移除 (7.4) OpenSSH 8.x (约2019 - 2021) RSA (Ed25519 逐渐流行) 3072 位 (RSA, 自 8.0 起) bcrypt_pbkdf curve25519-sha256, ECDH 系列; ssh-rsa (SHA1 签名) 于 8.8 禁用主机认证; Ed25519 签名优先 (自 8.5) chacha20-poly1305, AES-GCM OpenSSH 9.x (约2022 - 至今) Ed25519 (自 9.5 起) 256 位 (Ed25519); 3072 位 (若选 RSA) bcrypt_pbkdf PQC 混合 KEX: sntrup761x25519-sha512@openssh.com (自 9.0 默认); mlkem768x25519-sha256 (自 9.9 默认提供); Terrapin 缓解 (9.6) chacha20-poly1305, AES-GCM OpenSSH 10.0 (预计 2025年4月) Ed25519 256 位 (Ed25519); 3072 位 (若选 RSA) bcrypt_pbkdf mlkem768x25519-sha256 (PQC KEX 默认); 服务器端默认禁用有限域 DH (modp); DSA 完全移除 chacha20-poly1305 (最优先), AES-GCM (优先于 AES-CTR) 所以从 2014 年 1 月发布的 OpenSSH 6.5 开始，才可使用 ed25519 密钥，它的 passphrase 默认使用 bcrypt_pbkdf 生成的。而对于 RSA 类型的密钥，一直到 2018-08-24 发布的 OpenSSH 7.8 才从 MD5 改到 bctypt_pbkdf. 即使 2023-08-10 发布的 9.4 版本增加了默认的 bcrypt KDF rounds 次数，它的安全性仍然很值得怀疑。bcrypt 本身的安全性就越来越差，现代化的加密工具基本都已经升级到了 scrypt 甚至 argon2. 因此要想提升安全性，最好是能更换更现代的 KDF 算法，或者至少增加 bcrypt_pbkdf 的 rounds 数量。 我进一步看了 man ssh-keygen 的文档，没找到任何修改 KDF 算法的参数，不过能通过 -a 参数来修改 KDF 的 rounds 数量，OpeSSh 9.4 的 man 信息中写了默认使用 16 rounds. 我们再了解下 ssh-keygen 默认参数，在 release note 中我进一步找到这个： text OpenSSH 9.5/9.5p1 (2023-10-04) Potentially incompatible changes -------------------------------- * ssh-keygen(1): generate Ed25519 keys by default. Ed25519 public keys are very convenient due to their small size. Ed25519 keys are specified in RFC 8709 and OpenSSH has supported them since version 6.5 (January 2014). 也就是说从 2023-10-04 发布的 9.5 开始，OpenSSH 才默认使用 ED25519。 再看下各主流操作系统与 OpenSSH 的对应关系： OS Distro Version Year 大致的 OpenSSH 版本 Ubuntu (LTS) Ubuntu 18.04 LTS 2018 OpenSSH 7.6p1 Ubuntu 20.04 L","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:4:2","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#22-ssh-密钥的安全性"},{"categories":["tech"],"content":" 2.3 SSH 密钥的分类管理在所有机器上使用同一个 SSH 密钥，这是我过去的做法，但这样做有几个问题： 一旦某台机器的密钥泄漏，那么就需要重新生成并替换所有机器上的密钥，这很麻烦。 密钥需要通过各种方式传输到各个机器上，这也存在泄漏的风险。 因此，我现在的做法是： 对所有桌面电脑跟笔记本，都在其本地生成一个专用的 SSH 密钥配置到 GitHub 跟常用的服务器上。这个 SSH 私钥永远不会离开这台机器。 对于一些相对不重要的 Homelab 服务器，额外生成一个专用的 SSH 密钥，配置到这些服务器上。在一些跳板机跟测试机上会配置这个密钥方便测试与登录到其他机器。 上述所有 SSH 密钥都添加了 passphrase 保护，且使用了 bcrypt 256 rounds 生成加密密钥。 我通过这种方式缩小了风险范围，即使某台机器的密钥泄漏，也只需要重新生成并替换这台机器上的密钥即可。 最后再说明一点：OpenSSH 密钥并不是生成一次然后就可以高枕无忧了，为了确保足够安全性，也必须隔几年更换一次新密钥。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:4:3","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#23-ssh-密钥的分类管理"},{"categories":["tech"],"content":" 2.4 SSH CA - 更安全合理的 SSH 密钥管理方案？搜到些资料： SSH 证书登录教程 TODO 待研究。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:4:4","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#24-ssh-ca---更安全合理的-ssh-密钥管理方案"},{"categories":["tech"],"content":" 四、个人的账号密码管理我曾经大量使用了 Chrome/Firefox 自带的密码存储功能，但用到现在其实也发现了它们的许多弊端。有同事推崇 1Password 的使用体验，它的自动填充跟同站点的多密码管理确实做得非常优秀，但一是要收费，二是它是商业的在线方案，基于零信任原则，我不太想使用这种方案。 作为开源爱好者，我最近找到了一个非常适合我自己的方案：password-store. 这套方案使用 gpg 加密账号密码，每个文件就是一个账号密码，通过文件树来组织与匹配账号密码与 APP/站点的对应关系，并且生态完善，对 firefox/chrome/android/ios 的支持都挺好。 缺点是用 GPG 加密，上手有点难度，但对咱来说完全可以接受。 我在最近使用 pass-import 从 firefox/chrome 中导入了我当前所有的账号密码，并对所有的重要账号密码进行了一次全面的更新，一共改了二三十个账号，全部采用了随机密码。 当前的存储同步与多端使用方式： pass 的加密数据使用 GitHub 私有仓库存储，pass 原生支持基于 Git 的存储方案。 因为数据全都是使用 ECC Curve 25519 GPG 加密的，即使仓库内容泄漏，数据的安全性仍然有保障。 在浏览器与移动端，则分别使用这些客户端来读写 pass 中的密码： Android: https://github.com/android-password-store/Android-Password-Store IOS： https://github.com/mssun/passforios Brosers(Chrome/Firefox): https://github.com/browserpass/browserpass-extension 基於雞蛋不放在同一個籃子裏的原則，otp/mfa 的動態密碼則使用 google authenticator 保存與多端同步，並留有一份離線備份用於災難恢復。登錄 Google 賬號目前需要我 Android 手機或短信驗證，因此安全性符合我的需求。 我的详细 pass 配置见ryan4yin/nix-config/password-store. 其他相关资料： awesome-password-store https://github.com/gopasspw/gopass: reimplement in go, with more features. 遇到过的一些问题与解法： passforios - Merge conflicts ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:5:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#四个人的账号密码管理"},{"categories":["tech"],"content":" 3.1 pass 使用的 GPG 够安全么？GnuPG 是一个很有历史，而且使用广泛的加密工具，但它的安全性如何呢？ 我找到些相关文档： 2021年，用更现代的方法使用PGP（上） Predictable, Passphrase-Derived PGP Keys OpenPGP - The almost perfect key pair 简单总结下，GnuPG 的每个 secret key 都是随机生成的，互相之间没有关联（即不像区块链钱包那样具有确定性）。生成出的 key 被使用 passphrase 加密保存，每次使用时都需要输入 passphrase 解密。 那么还是之前在调研 OpenSSH 时我们提到的问题：它使用的 KDF 算法与参数是否足够安全？ OpenPGP 标准定义了String-to-Key (S2K) 算法用于从 passphrase 生成对称加密密钥，GnuPG 遵循该规范，并且提供了相关的参数配置选项，相关参数的文档OpenPGP protocol specific options 内容如下： text --s2k-cipher-algo name Use name as the cipher algorithm for symmetric encryption with a passphrase if --personal-cipher-preferences and --cipher-algo are not given. The default is AES-128. --s2k-digest-algo name Use name as the digest algorithm used to mangle the passphrases for symmetric encryption. The default is SHA-1. --s2k-mode n Selects how passphrases for symmetric encryption are mangled. If n is 0 a plain passphrase (which is in general not recommended) will be used, a 1 adds a salt (which should not be used) to the passphrase and a 3 (the default) iterates the whole process a number of times (see --s2k-count). --s2k-count n Specify how many times the passphrases mangling for symmetric encryption is repeated. This value may range between 1024 and 65011712 inclusive. The default is inquired from gpg-agent. Note that not all values in the 1024-65011712 range are legal and if an illegal value is selected, GnuPG will round up to the nearest legal value. This option is only meaningful if --s2k-mode is set to the default of 3. 默认仍旧使用 AES-128 做 passphrase 场景下的对称加密，数据签名还是用的 SHA-1，这俩都已经不太安全了，尤其是 SHA-1，已经被证明存在安全问题。因此，使用默认参数生成的 GPG 密钥，其安全性是不够的。 为了获得最佳安全性，我们需要： 使用如下参数生成 GPG 密钥： text gpg --s2k-mode 3 --s2k-count 65011712 --s2k-digest-algo SHA512 --s2k-cipher-algo AES256 ... 加密、签名、认证都使用不同的密钥，每个密钥只用于特定的场景，这样即使某个密钥泄漏，也不会影响其他场景的安全性。 为了在全局使用这些参数，可以将它们添加到你的 ~/.gnupg/gpg.conf 配置文件中。 详见我的 gpg 配置ryan4yin/nix-config/gpg ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:5:1","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#31-pass-使用的-gpg-够安全么"},{"categories":["tech"],"content":" 五、跨平台的加密备份同步工具的选择我日常同时在使用 macOS 与 NixOS，因此不论是需要离线存储的灾难恢复数据，还是需要在多端访问的个人数据，都需要一个跨平台的加密备份与同步工具。 前面提到的 pass 使用 GnuPG 进行文件级别的加密，但在很多场景下这不太适用，而且 GPG 本身也太重了，还一堆历史遗留问题，我不太喜欢。 为了其他数据备份与同步的需要，我需要一个跨平台的加密工具，目前调研到有如下这些： 文件级别的加密 这个有很多现成的现代加密工具，比如 age/sops, 都挺不错，但是针对大量文件的情况下使用比较繁琐。 全盘加密，或者支持通过 FUSE 模拟文件系统 首先 LUKS 就不用考虑了，它基本只在 Linux 上能用。 跨平台且比较活跃的项目中，我找到了 rclone 与 restic 这两个项目，都支持云同步，各有优缺点。 restic 相对 rclone 的优势，主要是天然支持增量 snapshots 的功能，可以保存备份的历史快照，并设置灵活的历史快照保存策略。这对可能有回滚需求的数据而言是很重要的。比如说 PVE 虚拟机快照的备份，有了 restic 我们就不再需要依赖 PVE 自身孱弱的快照保留功能，全交给 restic 实现就行。 多端加密同步 上面提到的 rclone 与 restic 都支持各种云存储，因此都是不错的多端加密同步工具。 最流行的开源数据同步工具貌似是 synthing，但它对加密的支持还不够完善，暂不考虑。 进一步调研后，我选择了 age, rclone 与 restic 作为我的跨平台加密备份与同步工具。这三个工具都比较活跃，stars 很高，使用的也都是比较现代的密码学算法： age: 对于对称加密的场景，使用 ChaCha20-Poly1305 AEAD 加密方案，对称加密密钥使用 scrypt KDF 算法生成。 rclone: 使用基于 XSalsa20-Poly1305 的 AEAD 加密方案，key 通过 scrypt KDF 算法生成，并且默认会加盐。 restic: 使用 AES-256-CTR 加密，使用 Poly1305-AES 认证数据，key 通过 scrypt KDF 算法生成。 对于 Nix 相关的 secrets 配置，我使用了 age 的一个适配库 agenix 完成其自动加解密配置，并将相关的加密数据保存在我的 GitHub 私有仓库中。详见 ryan4yin/nix-config/secrets. 关于这个仓库的详细加解密方法，在后面第八节「桌面电脑的数据安全」中会介绍。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:6:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#五跨平台的加密备份同步工具的选择"},{"categories":["tech"],"content":" 六、灾难恢复相关的数据存储与管理相关数据包括：GitHub, Twitter, Google 等重要账号的二次认证恢复代码、账号数据备份、PGP 主密钥与吊销证书等等。 这些数据日常都不需要用到，但在账号或两步验证设备丢失时就非要使用到其中的数据才能找回账号或吊销某个证书，是非常重要的数据。 我目前的策略是：使用 rclone + 1024bits 随机密码加密存储到两个 U 盘中（双副本），放在不同的地方，并且每隔半年到一年检查一遍数据。 对应的 rclone 解密配置本身也设置了比较强的 passphrase 保护，并通过我的 secrets 私有 Git 仓库多端加密同步。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:7:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#六灾难恢复相关的数据存储与管理"},{"categories":["tech"],"content":" 七、需要在多端访问的重要个人数据相关数据包括：个人笔记、重要的照片、录音、视频、等等。 因为日常就需要在多端访问，因此显然不能离线存储。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:8:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#七需要在多端访问的重要个人数据"},{"categories":["tech"],"content":" 1. 个人笔记不包含个人隐私的笔记，我直接用公开 GitHub 仓库 [ryan4yin/knowledge] (https://github.com/ryan4yin/knowledge/) 存储了，不需要加密。 对于不便公开的个人笔记，有这些考虑： 我的个人笔记目前主要是在移动端编辑，因此支持 Android/iOS 的客户端是必须的。 要能支持 Markdown/Orgmode 等通用的纯文本格式，纯文本格式更容易编写与分析，而通用格式则可以避免被平台绑定。 因为主要是移动端编辑，其实不需要多复杂的功能。 以后可能会希望在桌面端做富文本编辑，但目前还没这种私人笔记的需求。 希望具有类似 Git 的分布式存储与同步、笔记版本管理功能，如果能直接使用 Git 那肯定是最好的。 端到端的加密存储与同步 如果有类似 Git 的 Diff 功能就更好了。 我一开始考虑直接使用基于 Git 仓库的方案，能获得 Git 的所有功能，同时还避免额外自建一个笔记服务。找到个 GitJournal ，数据存在 GitHub 私有仓库用了一个月，功能不太多但够用。但发现它项目不咋活跃，基于 SSH 协议的 Git 同步在大仓库上也有些毛病，而且数据明文存在 Git 仓库里，安全性相对差一些。 另外找到个 git-crypt 能在 Git 上做一层透明加密，但没找到支持它的移动端 APP，而且项目也不咋活跃。 在 https://github.com/topics/note-taking 下看了些流行项目，主要有这些： Joplin 支持 S3/WebDAV 等多种协议同步数据，支持端到端加密 Outline 等 Wiki 系统 它直接就是个 Web 服务，主要面向公开的 Wiki，不适合私人笔记 Logseq/Obsidian 等双链笔记软件（其中 Obsidian 是闭源软件） 都是基于本地文件的笔记系统，也没加密工具，需要借助其他工具实现数据加密与同步 其中 Logseq 是大纲流，一切皆列表。而 Obsidian 是文档流，比较贴近传统的文档编辑体验。 Obsidian 跟 Logseq 的 Sync 功能都是按月收费，相当的贵。社区有通过 Git 同步的方案，但都很 trickk，也不稳定。 AppFlowy/Affine/apitable 等 Notion 替代品 都是富文本编辑，不适合移动端设备 在移动端使用 Synthing 或 Git 等第三方工具同步笔记数据，都很麻烦，而且安全性也不够。因此目前看在移动端也能用得舒服的话，最稳妥的选择是第一类笔记 APP，简单试用后我选择了最流行的 Joplin. ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:8:1","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#1-个人笔记"},{"categories":["tech"],"content":" 2. 照片、视频等其他个人数据 Homelab 中的 Windows-NAS-Server，两个 4TB 的硬盘，通过 SMB 局域网共享，公网所有客户端 （包括移动端）都能通过 tailscale + rclone 流畅访问。 部分重要的数据再通过 rclone 加密备份一份到云端，可选项有： 青云对象存储 与七牛云对象存储 Kodo，它们都有每月 10GB 的免费存储空间，以及 1GB-10GB 的免费外网流量。 阿里云 OSS 也能免费存 5GB 数据以及每月 5GB 的外网流量，可以考虑使用。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:8:2","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#2-照片视频等其他个人数据"},{"categories":["tech"],"content":" 八、桌面电脑與 Homelab 的数据安全我的桌面电脑都是 macOS 与 NixOS，Homlab 虚拟机也已经 all in NixOS，另外我目前没有任何云上服务器。 另外虽然也有两台 Windows 虚拟机，但极少对它们做啥改动，只要做好虚拟机快照的备份就 OK 了。 对于 NixOS 桌面系统与 Homelab 虚拟机，我当前的方案如下： 桌面主机 启用 LUKS2 全盘加密 + Secure Boot，在系统启动阶段需要输入 passphrase 解密 NixOS 系统盘才能正常进入系统。 LUKS2 的 passphrase 为一个比较长的密码学随机字符串。 LUKS2 的所有安全设置全拉到能接受的最高（比较重要的是 --iter-time，计算出 unlock key 的用时，默认 2s，安全起见咱设置成了 5s） text cryptsetup --type luks2 --cipher aes-xts-plain64 --hash sha512 --iter-time 5000 --key-size 256 --pbkdf argon2id --use-urandom --verify-passphrase luksFormat device LUKS2 使用的 argon2id 是比 scrypt 更强的 KDF 算法，其安全性是足够的。 桌面主機使用 tmpfs 作为根目录，所有未明确声明持久化的数据，都会在每次重启后被清空，这强制我去了解自己装的每个软件都存了哪些数据，是否需要持久化，使整个系统更白盒，提升了整个系统的环境可信度。 Homelab Proxmox VE 物理机全部重装为 NixOS，启用 LUKS 全盘加密与 btrfs + zstd 压缩，买几个便宜的 U 盘用于自动解密（注意解密密钥的离线加密备份）。使用 K3s + KubeVirt 管理 QEMU/KVM 虚拟机。 Secrets 說明 重要的通用 secrets，都加密保存在我的 secrets 私有仓库中，在部署我的 nix-config 时使用主机本地的 SSH 系统私钥自动解密。 也就是说要在一台新电脑（不論是桌面主機還是 NixOS 虛擬機）上成功部署我的 nix-config 配置，需要的准备流程： 本地生成一个新的 ssh key，将公钥配置到 GitHub，并 ssh-add 这个新的私钥，使其能够访问到我的私有 secrets 仓库。 将新主机的系统公钥 /etc/ssh/ssh_host_ed25519_key.pub 发送到一台旧的可解密 secrets 仓库数据的主机上。如果该文件不存在则先用 sudo ssh-keygen -A 生成。 在旧主机上，将收到的新主机公钥添加到 secrets 仓库的 secrets.nix 配置文件中，并使用 agenix 命令 rekey 所有 secrets 数据，然后 commit \u0026 push。 现在新主机就能够通过 nixos-rebuild switch 或 darwin-rebuild switch 成功部署我的 nix-config 了，agenix 会自动使用新主机的系统私钥/etc/ssh/ssh_host_ed25519_key 解密 secrets 仓库中的数据并完成部署工作。 这份 secrets 配置在 macOS 跟 NixOS 上通用，也与 CPU 架构无关，agenix 在这两个系统上都能正常工作。 基于安全性考虑，对 secrets 进行分类管理与加密： 桌面电脑能解密所有的 secrets Homelab 中的跳板机只能解密 Homelab 相关的所有 secrets 其他所有的 NixOS 虚拟机只能解密同类别的 secrets，比如一台监控机只能解密监控相关的 secrets. 对于 macOS，它本身的磁盘安全我感觉就已经做得很 OK 了，而且它能改的东西也比较有限。我的安全设置如下： 启用 macOS 的全盘加密功能 常用的 secrets 的部署与使用方式，与前面 NixOS 的描述完全一致 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:9:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#八桌面电脑與-homelab-的数据安全"},{"categories":["tech"],"content":" macOS/NixOS 数据的灾难恢复？在使用 nix-darwin 跟 NixOS 的情况下，整个 macOS/NixOS 的系统环境都是通过我的ryan4yin/nix-config 声明式配置的，因此桌面电脑的灾难恢复根本不是一个问题。 只需要简单的几行命令就能在一个全新的系统上恢复出我的 macOS / NixOS 桌面环境，所有密钥也会由 agenix 自动解密并放置到正确的位置。 要说有恢复难题的，也就是一些个人数据了，这部分已经在前面第七小节介绍过了，用 rclone/restic 就行。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:9:1","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#macosnixos-数据的灾难恢复"},{"categories":["tech"],"content":" 九、总结下我的数据存在了哪些地方 secrets 私有仓库: 它会被我的 nix-config 自动拉取并部署到所有主力电脑上，包含了 homelab ssh key, GPG subkey, 以及其他一些重要的 secrets. 它通过我所有桌面电脑的 /etc/ssh/ssh_host_ed25519_key.pub 公钥加密，在部署时自动使用对应的私钥解密。 此外该仓库还添加了一个灾难恢复用的公钥，确保在我所有桌面电脑都丢失的极端情况下，仍可通过对应的灾难恢复私钥解密此仓库的数据。该私钥在使用 age 加密后（注：未使用 rclone 加密）与我其他的灾难恢复数据保存在一起。 password-store: 我的私人账号密码存储库，通过 pass 命令行工具管理，使用 GPG 加密，GPG 密钥备份被通过 age/agenix 加密保存在上述 secrets 仓库中。 由于 GnuPG 自身导出的密钥备份数据安全性欠佳，因此我使用了 age + passphrase 对其进行了二次对称加密，然后再通过 agenix 加密（第三次加密，使用非对称加密算法）保存在 secrets 仓库中。这保障了即使我的 GPG 密钥在我所有的桌面电脑上都存在，但安全性仍旧很够。 rclone 加密的备份 U 盘（双副本）：离线保存一些重要的数据。其配置文件被加密保存在 secrets 仓库中，其配置文件的解密密码被加密保存在 password-store 仓库中。 这套方案的大部分部署工作都是由我的 Nix 配置自动完成的，整个流程的自动化程度很高，所以这套方案带给我的额外负担并不大。 secrets 这个私有仓库是整个方案的核心，它包含了所有重要数据（password-store/rclone/…）的解密密钥。如果它丢失了，那么所有的数据都无法解密。 但好在 Git 仓库本身是分布式的，我所有的桌面电脑上都有对应的完整备份，我的灾难恢复存储中也会定期备份一份 secrets/password-store 两个仓库的数据过去以避免丢失。 另外需要注意的是，为了避免循环依赖，secrets 与 password-store 这两个仓库的备份不应该使用 rclone 再次加密，而是直接使用 age 对称加密。这样只要我还记得 age 的解密密码、gpg 密钥的 passphrase 等少数几个密码，就能顺着整条链路解密出所有的数据。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:10:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#九总结下我的数据存在了哪些地方"},{"categories":["tech"],"content":" 十、这套方案下需要记忆几个密码？这些密码该如何设计？绝大部分密码都建议设置为包含大小写跟部分特殊字符的密码学随机字符串，通过 pass 加密保存与多端同步与自动填充，不需要额外记忆。考虑到我们基本不会需要手动输入这些密码，因此它们的长度可以设置得比较长，比如 16-24 位（不使用更长密码的原因是，许多站点或 APP 都限制了密码长度，这种长度下使用 passphrase 单词组的安全性相对会差一点，因此也不推荐）。 再通过一些合理的密码复用手段，可以将需要记忆的密码数量降到 3 - 5 个，并且确保日常都会输入，避免遗忘。 不过这里需要注意一点，就是 SSH 密钥、GPG 密钥、系统登录密码这三个密码最好不要设成一样。前面我们已经做了分析，这三个 passphrase 的加密强度区别很大，设成一样的话，使用 bcrypt 的 SSH 密钥将会成为整个方案的短板。 而关于密码内容的设计，这个几核心 passphrase 的长度都是不受限的，有两个思路（注意不要在密码中包含任何个人信息）： 使用由一个个单词组成的较长的 passphrase，比如don't-do-evil_I-promise-this-would-become-not-a-dark-corner 这样的。 使用字母大小写加数字、特殊字符组成的密码学随机字符串，比如 fsD!.*v_F*sdn-zFkJM)nQ 这样的。 第一种方式的优点是，这些单词都是常用单词，记忆起来会比较容易，而且也不容易输错。 第二种方式的优点是，密码学随机字符串可以以更短的长度达到与第一种方式相当的安全性。但它的缺点也比较明显——容易输错，而且记忆起来也不容易。 两种方式是都可以，如果你选择第二种方式，可以专门编些小故事来通过联想记忆它们，hint 中也能加上故事中的一些与密码内容无直接关联的关键字帮助回忆。毕竟人类擅长记忆故事，但不擅长记忆随机字符。举个例子，上面的密码 fsD!.*v_F*sdn-zFkJM)nQ，可以找出这么些联想： fs: 「佛说」这首歌里面的歌词 D!: 头文字D! .*: 地面上的光斑(.)，天上的星光(*) v_: 嘴巴张开（v）睡得很香的样子，口水都流到地上了(_) F*sdn: F*ck 软件定义网络(sdn) zFkJM: 在政府（zf）大门口（k），看(k) 见了 Jack Ma (JM) 在跳脱yi舞… )nQ: 宁静的夏夜，凉风习习，天上一轮弯月，你(n)问(Q)我，当下这一刻是否足够 把上面这些联想串起来，就是一个怀旧、雷人、结尾又有点温馨的无厘头小故事了，肯定能令你自己印象深刻。故事写得够离谱的话，你可能想忘都忘不掉了。 总之就是用这种方式，然后把密码中的每个字符都与故事中的某个关键字联系起来，这样就能很容易地记住这个密码了。如果你对深入学习如何记忆这类复杂的东西感兴趣，可以看看这本我最想要的记忆魔法书. 最后一点，就是定期更新一遍这些密码、SSH 密钥、GPG 密钥。所有数据的加密安全性都是随着时间推移而降低的，曾经安全的密码学算法在未来也可能会变得不再安全（这方面 MD5, SHA-1 都是很好的例子），因此定期更新这些密码跟密钥是很有必要的。 几个核心密码更新起来会简单些，可以考虑每年更新一遍，而密钥可以考虑每两三年更新一遍（时间凭感觉说的哈，没有做论证）。其他密码密钥则可以根据数据的重要性来决定更新频率。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:11:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#十这套方案下需要记忆几个密码这些密码该如何设计"},{"categories":["tech"],"content":" 十一、为了落地这套方案，我做了哪些工作？前面已经基本都提到了，这里再总结下： 重新生成了所有的 SSH Key，增强了 passphrase 强度，bcrypt rounds 增加到 256，通过ssh-add 使用，只需要在系统启动后输入一次密码即可，也不麻烦。 重新生成了所有的 PGP Key，主密钥离线加密存储，本地只保留了加密、签名、认证三个 PGP 子密钥。 重新生成了所有重要账号的密码，全部使用随机密码，一共改了二三十个账号。考虑到旧的 backup code 可能已经泄漏，我也重新生成了所有重要账号的 backup code. 重装 NixOS，使用 LUKS2 做全盘加密，启用 Secure Boot. 同时使用 tmpfs 作为根目录，所有未明确声明持久化的数据，都会在每次重启后被清空。 使用 nix-darwin 与 home-manager 重新声明式地配置了我的两台 MacBook Pro（Intel 跟 Apple Silicon 各一台），与我的 NixOS 共用了许多配置，最大程度上保持了所有桌面电脑的开发环境一致性，也确保了我始终能快速地在一台新电脑上部署我的整个开发环境。 注销印象笔记账号，使用 evernote-backup 跟 evernote2md 两个工具将个人的私密笔记遷移到了 Joplin + OneDrive 上，Homelab 中設了通過 restic 定期自動加密備份 OneDrive 中的 Joplin 數據。 比较有价值的 GitHub 仓库，都设置了禁止 force push 主分支，并且添加了 GitHub action 自动同步到国内 Gitee. All in NixOS，将 Homelab 中的 PVE 全部使用 NixOS + K3s + KubeVirt 替换。从偏黑盒且可复现性差的 Ubuntu、Debian, Proxmox VE, OpenWRT 等 VM 全面替换成更白盒且可复现性强的 NixOS、KubeVirt，提升我对内网环境的掌控度，进而提升内网安全性。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:12:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#十一为了落地这套方案我做了哪些工作"},{"categories":["tech"],"content":" 十二、灾难恢复预案这里考虑我的 GPG 子密钥泄漏了、pass 密码仓库泄漏了等各种情况下的灾难恢复流程。 TODO 后续再慢慢补充。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:13:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#十二灾难恢复预案"},{"categories":["tech"],"content":" 十三、未来可能的改进方向目前我的主要个人数据基本都已经通过上述方案进行了安全管理。但还有这些方面可以进一步改进： 针对 Homelab 的虚拟机快照备份，从我旧的基于 rclone + crontab 的明文备份方案，切换到了基于 restic 的加密备份方案。 手机端的照片视频虽然已经在上面设计好了备份同步方案，但仍未实施。考虑使用 roundsync 加密备份到云端，实现多端访问。 进一步学习下 appamor, bubblewrap 等 Linux 下的安全限制方案，尝试应用在我的 NixOS PC 上。 当前成果nix-config/hardening Git 提交是否可以使用 GnuPG 签名，目前没这么做主要是觉得 PGP 这个东西太重了，目前我也只在 pass 上用了它，而且还在研究用 age 取代它。 尝试通过 hashcat,wifi-cracking 等手段破解自己的重要密码、SSH 密钥、GPG 密钥等数据，评估其安全性。 使用一些流行的渗透测试工具测试我的 Homelab 与内网环境，评估其安全性。 安全总是相对的，而且其中涉及的知识点不少，我 2022 年学了密码学算是为此打下了个不错的基础， 但目前看前头还有挺多知识点在等待着我。我目前仍然打算以比较 casual 的心态去持续推进这件事情，什么时候兴趣来了就推进一点点。 这套方案也可能存在一些问题，欢迎大家审阅指正。 ","date":"2024-01-30","objectID":"/posts/an-incomplete-guide-to-data-security/:14:0","series":["写给开发人员的实用密码学"],"tags":["安全","密码学","Linux","SSH","PGP","密码管理","LUKS","全盘加密","零信任","rclone"],"title":"个人数据安全不完全指南","uri":"/posts/an-incomplete-guide-to-data-security/#十三未来可能的改进方向"},{"categories":["tech"],"content":" 文章是 2023-08-07 写的，后面就完全忘掉这回事了，今天偶然翻到它才想起要整理发布下…所以注意文章中的时间线是 2023 年 8 月。 ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:0:0","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#"},{"categories":["tech"],"content":" 零、前言我从今年 5 月份初收到了内测板的 Lichee Pi 4A，这是当下性能最高的 RISC-V 开发板之一，不过当时没怎么折腾。 6 月初的时候我开始尝试在 Orange Pi 5 上运行 NixOS，在NixOS on ARM 的 Matrix 群组 中得到了俄罗斯老哥 @K900 的帮助，没费多大劲就成功了，一共就折腾了三天。 于是我接着尝试在 Lichee Pi 4A 上运行 NixOS，因为已经拥有了 Orange Pi 5 上的折腾经验，我以为这次会很顺利。但是实际难度远远超出了我的预期，我从 6 月 13 号开始断断续续折腾到 7 月 3 号，接触了大量的新东西，包括 U-Boot、OpenSBI、SPL Flash、RISCV Boot Flows 等等，还参考了 @chainsx 的 Fedora for Lichee Pi 4A 方案，请教了 @NickCao 许多 NixOS 相关的问题，@revy 帮我修了好几个 revyos/thead-kernel 在标准工具链上编译的 bug，期间也请教过 @HougeLangley 他折腾 Lichee Pi 4A 的经验。我在付出了这么多的努力后，才最终成功编译出了 NixOS 的系统镜像（包含 boot 跟 rootfs 两个分区）。 但是！现在要说「但是」了。 镜像是有了，系统却无法启动…找了各种资料也没解决，也没好意思麻烦各位大佬，搞得有点心灰意冷，就先把这部分工作放下了。 接着就隔了一个多月没碰 Lichee Pi 4A，直到 8 月 5 号，外国友人 @JayDeLux 在Mainline Linux for RISC-V TG 群组中询问我 NixOS 移植工作的进展如何（之前有在群里提过我在尝试移植），我才决定再次尝试一下。 在之前工作的基础上一番骚操作后，我在 8 月 6 号晚上终于成功启动了 NixOS，这次意外的顺利，后续也成功通过一份 Nix Flake 配置编译出了可用的 NixOS 镜像。 最终成果：https://github.com/ryan4yin/nixos-licheepi4a 整个折腾过程相当曲折，虽然最终达成了目标，但是期间遭受的折磨也真的不少。总的来说仍然是一次很有趣的经历，既学到了许多新技术知识、认识了些有趣的外国友人（@JayDeLux 甚至还给我打了 $50 美刀表示感谢），也跟 @HougeLangley 、@chainsx 、@Rabenda(revy) 等各位大佬混了个脸熟。 这篇文章就是记录下我在这个折腾过程中学到的所有知识，以飨读者，同时也梳理一下自己的收获。 本文的写作思路是自顶向下的，先从 NixOS 镜像的 boot 分区配置、启动脚本开始分析，过渡到实际的启动日志，再接续分析下后续的启动流程。NixOS 分析完了后，再看看与 RISC-V 相关的硬件固件与 bootloader 部分要如何与 NixOS 协同工作，使得 NixOS 能够在 Lichee Pi 4A 上正常启动。 ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:1:0","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#零前言"},{"categories":["tech"],"content":" 一、基础知识介绍","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:2:0","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#一基础知识介绍"},{"categories":["tech"],"content":" 1. Lichee Pi 4A 介绍LicheePi 4A 是当前市面上性能最高的 RISC-V Linux 开发板之一，它以 TH1520 为主控核心 （4xC910@1.85G， RV64GCV，4TOPS@int8 NPU， 50GFLOP GPU），板载最大 16GB 64bit LPDDR4X，128GB eMMC，支持 HDMI+MIPI 双4K 显示输出，支持 4K 摄像头接入，双千兆网口（其中一个支持POE供电）和 4 个 USB3.0 接口，多种音频输入输出（由专用 C906 核心处理）。 以上来自 Lichee Pi 4A 官方文档Lichee Pi 4A - Sipeed Wiki. 总之它是我手上性能最高的 RISC-V 开发板。 LicheePi 4A 官方主要支持 RevyOS—— 一款针对 T-Head 芯片生态的 Debian 优化定制发行版。根据猴哥（@HougeLangley）文章介绍，它也是目前唯一且确实能够启用 Lichee Pi 4A 板载 GPU 的发行版， ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:2:1","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#1-lichee-pi-4a-介绍"},{"categories":["tech"],"content":" 2. NixOS 介绍这个感觉就不用多说了，我在这几个月已经给 NixOS 写了非常多的文字了，感兴趣请直接移步ryan4yin/nixos-and-flakes-book. 在 4 月份接触了 NixOS 后，我成了 NixOS 铁粉。作为一名铁粉，我当然想把我手上的所有性能好点的板子都装上 NixOS，Lichee Pi 4A 自然也不例外。 我目前主要完成了两块板子的 NixOS 移植工作，一块是 Orange Pi 5，另一块就是 Lichee Pi 4A。 Orange Pi 5 是 ARM64 架构的，刚好也遇到了拥有该板子的 NixOS 用户 @K900，在他的帮助下我很顺利地就完成了移植工作。 而 Lichee Pi 4A 就比较曲折，也比较有话题性。所以才有了这篇文章。 ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:2:2","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#2-nixos-介绍"},{"categories":["tech"],"content":" 二、移植思路一个完整的嵌入式 Linux 系统，通常包含了 U-Boot、kernel、设备树以及根文件系统（rootfs）四个部分。 其中 U-Boot，kernel 跟设备树，都是与硬件相关的，需要针对不同的硬件进行定制。而 rootfs 的大部分内容（比如说 NixOS 系统的 rootfs 本身），都是与硬件无关的，可以通用。 我的移植思路是，从 LicheePi4A 官方使用的 RevyOS 中拿出跟硬件相关的部分（也就是 U-Boot, kernel 跟设备树这三个），再结合上跟硬件无关的 NixOS rootfs，组合成一个完整的、可在 LicheePi4A 上正常启动运行的 NixOS 系统。 RevyOS 针对 LicheePi4A 定制的几个项目源码如下： https://github.com/revyos/thead-kernel.git https://github.com/revyos/thead-u-boot.git https://github.com/revyos/thead-opensbi.git 思路很清晰，但因为 NixOS 本身的特殊性，实际操作起来，现有的 Gentoo, Arch Linux, Fedora 的移植仓库代码全都无法直接使用，需要做的工作还是不少的。 ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:3:0","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#二移植思路"},{"categories":["tech"],"content":" 三、NixOS 启动流程分析要做移植，首先就要了解 NixOS 系统本身的文件树结构以及系统启动流程，搞明白它跟 Arch Linux, Fedora 等其他发行版的区别，这样才好参考其他发行版的移植工作，搞明白该如何入手。 ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:4:0","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#三nixos-启动流程分析"},{"categories":["tech"],"content":" 1. Bootloader 配置与系统文件树分析这里方便起见，我直接使用我自己为 LicheePi4A 构建好的 NixOS 镜像进行分析。首先参照ryan4yin/nixos-licheepi4a 的 README 下载解压镜像，再使用 losetup 跟 mount 直接挂载镜像中的各分区进行初步分析： bash # 解压镜像 › mv nixos-licheepi4a-sd-image-*-riscv64-linux.img.zst nixos-lp4a.img.zst › zstd -d nixos-lp4a.img.zst # 将 img 文件作为虚拟 loop 设备连接到系统中 › sudo losetup --find --partscan nixos-lp4a.img # 查看挂载的 loop 设备 › lsblk | grep loop loop0 7:0 0 1.9G 0 loop ├─loop0p1 259:8 0 200M 0 part └─loop0p2 259:9 0 1.7G 0 part # 分别挂载镜像中的 boot 跟 rootfs 分区 › mkdir boot root › sudo mount /dev/loop0p1 boot › sudo mount /dev/loop0p2 root # 查看 boot 分区内容 › ls boot/ ╭───┬───────────────────────────┬──────┬─────────┬──────────────╮ │ # │ name │ type │ size │ modified │ ├───┼───────────────────────────┼──────┼─────────┼──────────────┤ │ 0 │ boot/extlinux │ dir │ 4.1 KB │ 44 years ago │ │ 1 │ boot/fw_dynamic.bin │ file │ 85.9 KB │ 24 years ago │ │ 2 │ boot/light_aon_fpga.bin │ file │ 50.3 KB │ 24 years ago │ │ 3 │ boot/light_c906_audio.bin │ file │ 16.4 KB │ 24 years ago │ │ 4 │ boot/nixos │ dir │ 4.1 KB │ 44 years ago │ ╰───┴───────────────────────────┴──────┴─────────┴──────────────╯ # 查看 root 分区内容 › ls root/ ╭───┬────────────────────────────┬──────┬──────────┬──────────────╮ │ # │ name │ type │ size │ modified │ ├───┼────────────────────────────┼──────┼──────────┼──────────────┤ │ 0 │ root/boot │ dir │ 4.1 KB │ 54 years ago │ │ 1 │ root/lost+found │ dir │ 16.4 KB │ 54 years ago │ │ 2 │ root/nix │ dir │ 4.1 KB │ 54 years ago │ │ 3 │ root/nix-path-registration │ file │ 242.0 KB │ 54 years ago │ ╰───┴────────────────────────────┴──────┴──────────┴──────────────╯ 可以看到 NixOS 整个根目录（/root）下一共就四个文件夹，其中真正保存有系统数据的文件夹只有/boot 跟 /nix 这两个，这与传统的 Linux 发行版大相径庭。有一点 Linux 使用经验的朋友都应该清楚，传统的 Linux 发行版遵循 UNIX 系统的FHS 标准，根目录下会有很多文件夹，比如/bin、/etc、/home、/lib、/opt、/root、/sbin、/srv、/tmp、/usr、/var 等等。 那 NixOS 它这么玩，真的能正常启动么？这就是我在构建出镜像后却发现无法在 LicheePi 4A 上启动时，最先产生的疑问。在询问 @chainsx 跟 @revy 系统无法启动的解决思路的时候，他们也一脸懵逼，觉得这个文件树有点奇葩，很怀疑是我构建流程有问题导致文件树不完整。 但实际上 NixOS 就是这么玩的，它 rootfs 中所有的数据全都存放在 /nix/store 这个目录下并且被挂载为只读，其他的文件夹以及其中的文件都是在运行时动态创建的。这是它实现声明式系统配置、可回滚更新、可并行安装多个版本的软件包等等特性的基础。 下面继续分析，先仔细看下 /boot 的内容： bash › tree boot boot ├── extlinux │ └── extlinux.conf ├── fw_dynamic.bin ├── light_aon_fpga.bin ├── light_c906_audio.bin └── nixos ├── 2n6fjh4lhzaswbyacaf72zmz6mdsmm8l-initrd-k-riscv64-unknown-linux-gnu-initrd ├── l18cz7jd37n35dwyf8wc8divm46k7sdf-k-riscv64-unknown-linux-gnu-dtbs │ ├── sifive │ │ └── hifive-unleashed-a00.dtb │ └── thead │ ├── fire-emu-crash.dtb │ ├── fire-emu.dtb │ ├── ...... (省略) │ ├── light-fm-emu-audio.dtb │ ├── light-fm-emu-dsi0-hdmi.dtb │ ├── light-fm-emu-dsp.dtb │ ├── light-fm-emu-gpu.dtb │ ├── light-fm-emu-hdmi.dtb │ ├── light-lpi4a-ddr2G.dtb │ └── light_mpw.dtb └── l18cz7jd37n35dwyf8wc8divm46k7sdf-k-riscv64-unknown-linux-gnu-Image 6 directories, 64 files 可以看到： 它使用 /boot/extlinux/extlinux.conf 作为 U-Boot 的启动项配置，据U-Boot 官方的 Distro 文档 所言，这是 U-Boot 的标准配置文件。 另外还有一些 xxx.bin 文件，这些是一些硬件固件，其中的 light_c906_audio.bin 显然是玄铁 906 这个 IP 核的音频固件，其他的后面再研究。 NixOS 的 initrd, dtbs 以及 Image 文件都是在 /boot/nixos 下，这三个文件也都是跟 Linux 的启动相关的，现在不用管它们，下一步会分析。 再看下 /boot/extlinux/extlinux.conf 的内容： bash › cat boot/extlinux/extlinux.conf # Generated file, all changes will be lost on nixos-rebuild! # Change this to e.g. nixos-42 to temporarily boot to an older configuration. DEFAULT nixos-default MENU TITLE ------------------------------------------------------------ TIMEOUT 50 LABEL nixos-default MENU LABEL NixOS - Default LINUX ../nixos/l18cz7jd37n35dwyf8wc8divm46k7sdf-k-riscv64-unknown-linux-gnu-Image INITRD ../nixos/2n6fjh4lhzaswbyacaf72zmz6mdsmm8l-initrd-k-riscv64-unknown-linux-gnu-initrd APPEND init=/nix/store/71wh9lvf94i1jcd6qpqw228fy5s8fv24-nixos-system-lp4a-23.05.20230806.240472b/init console=ttyS0,115200 root=UUID=14e19a7b-0ae0-484d-9d54-43bd6fdc20c7 rootfstype=ext4 rootwait rw earlycon clk_ignore_unused eth=$ethaddr rootrwoptions=rw,noatime rootrwreset=yes loglevel=4 FDT ../nixos/l18cz7jd37n35dwyf8wc8divm46k7sdf-k-riscv64-unknown-linux-gnu-dtb","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:4:1","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#1-bootloader-配置与系统文件树分析"},{"categories":["tech"],"content":" 2. 实际启动日志分析为了方便后续内容的理解，先看下 NixOS 系统在 LicheePi 4A 上的实际启动日志是个很不错的选择。 按我项目中的 README 正常烧录好系统后，使用 USB 转串口工具连接到 LicheePi 4A 的 UART0 串口，然后启动系统，就能看到 NixOS 的启动日志。 接线示例： LicheePi4A UART 调试接线 - 正面 LicheePi4A UART 调试接线 - 反面 接好线后使用 minicom 查看日志： bash › ls /dev/ttyUSB0 ╭───┬──────────────┬─────────────┬──────┬───────────────╮ │ # │ name │ type │ size │ modified │ ├───┼──────────────┼─────────────┼──────┼───────────────┤ │ 0 │ /dev/ttyUSB0 │ char device │ 0 B │ 6 minutes ago │ ╰───┴──────────────┴─────────────┴──────┴───────────────╯ › minicom -d /dev/ttyusb0 -b 115200 一个正常的启动日志示例如下： text Welcome to minicom 2.8 brom_ver 8 [APP][E] protocol_connect failed, exit. OpenSBI v0.9 ____ _____ ____ _____ / __ \\ / ____| _ \\_ _| | | | |_ __ ___ _ __ | (___ | |_) || | | | | | '_ \\ / _ \\ '_ \\ \\___ \\| _ \u003c | | | |__| | |_) | __/ | | |____) | |_) || |_ \\____/| .__/ \\___|_| |_|_____/|____/_____| | | |_| Platform Name : T-HEAD Light Lichee Pi 4A configuration for 8GB DDR board Platform Features : mfdeleg Platform HART Count : 4 Platform IPI Device : clint Platform Timer Device : clint Platform Console Device : uart8250 Platform HSM Device : --- Platform SysReset Device : thead_reset Firmware Base : 0x0 Firmware Size : 132 KB Runtime SBI Version : 0.3 Domain0 Name : root Domain0 Boot HART : 0 Domain0 HARTs : 0*,1*,2*,3* Domain0 Region00 : 0x000000ffdc000000-0x000000ffdc00ffff (I) Domain0 Region01 : 0x0000000000000000-0x000000000003ffff () Domain0 Region02 : 0x0000000000000000-0xffffffffffffffff (R,W,X) Domain0 Next Address : 0x0000000000200000 Domain0 Next Arg1 : 0x0000000001f00000 Domain0 Next Mode : S-mode Domain0 SysReset : yes Boot HART ID : 0 Boot HART Domain : root Boot HART ISA : rv64imafdcvsux Boot HART Features : scounteren,mcounteren,time Boot HART PMP Count : 0 Boot HART PMP Granularity : 0 Boot HART PMP Address Bits: 0 Boot HART MHPM Count : 16 Boot HART MHPM Count : 16 Boot HART MIDELEG : 0x0000000000000222 Boot HART MEDELEG : 0x000000000000b109 [ 0.000000] Linux version 5.10.113 (nixbld@localhost) (riscv64-unknown-linux-gnu-gcc (GCC) 13.1.0, GN0 [ 0.000000] OF: fdt: Ignoring memory range 0x0 - 0x200000 [ 0.000000] earlycon: uart0 at MMIO32 0x000000ffe7014000 (options '115200n8') [ 0.000000] printk: bootconsole [uart0] enabled [ 2.292495] (NULL device *): failed to find vdmabuf_reserved_memory node [ 2.453953] spi-nor spi0.0: unrecognized JEDEC id bytes: ff ff ff ff ff ff [ 2.460971] dw_spi_mmio ffe700c000.spi: cs1 \u003e= max 1 [ 2.466001] spi_master spi0: spi_device register error /soc/spi@ffe700c000/spidev@1 [ 2.497453] sdhci-dwcmshc ffe70a0000.sd: can't request region for resource [mem 0xffef014064-0xffef01] [ 2.509014] misc vhost-vdmabuf: failed to find vdmabuf_reserved_memory node [ 3.386036] debugfs: File 'SDOUT' in directory 'dapm' already present! [ 3.392692] debugfs: File 'Playback' in directory 'dapm' already present! [ 3.399524] debugfs: File 'Capture' in directory 'dapm' already present! [ 3.406262] debugfs: File 'Playback' in directory 'dapm' already present! [ 3.413067] debugfs: File 'Capture' in directory 'dapm' already present! [ 3.425466] aw87519_pa 5-0058: aw87519_parse_dt: no reset gpio provided failed [ 3.432752] aw87519_pa 5-0058: aw87519_i2c_probe: failed to parse device tree node \u003c\u003c\u003c NixOS Stage 1 \u003e\u003e\u003e running udev... Starting systemd-udevd version 253.6 kbd_mode: KDSKBMODE: Inappropriate ioctl for device Gstarting device mapper and LVM... checking /dev/disk/by-label/NIXOS_SD... fsck (busybox 1.36.1) [fsck.ext4 (1) -- /mnt-root/] fsck.ext4 -a /dev/disk/by-label/NIXOS_SD NIXOS_SD: recovering journal NIXOS_SD: clean, 148061/248000 files, 818082/984159 blocks mounting /dev/disk/by-label/NIXOS_SD on /... \u003c\u003c\u003c NixOS Stage 2 \u003e\u003e\u003e running activation script... setting up /etc... ++ /nix/store/2w8nachmhqvbjswrrsdia5cx1afxxx60-util-linux-riscv64-unknown-linux-gnu-2.38.1-bin/bin/findm/ + rootPart=/dev/disk/by-label/NIXOS_SD ++ lsblk -npo PKNAME /dev/disk/by-label/NIXOS_SD + bootDevice=/dev/mmcblk1 ++ lsblk -npo MAJ:MIN /dev/disk/by-label/NIX","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:4:2","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#2-实际启动日志分析"},{"categories":["tech"],"content":" 3. init 程序分析有了上面这些信息，我们就可以比较容易地理解 init 这个程序了，它主要对应前面日志中的 NixOS Stage 2，即在真正挂载根文件系统之后，执行的第一个用户态程序。 在 NixOS 中这个 init 程序实际上是一个 shell 脚本，可以直接通过 cat 或者 vim 来查看它的内容： bash › cat /nix/store/a5gnycsy3cq4ix2k8624649zj8xqzkxc-nixos-system-nixos-23.05.20230624.3ef8b37/init #! /nix/store/91hllz70n1b0qkb0r9iw1bg9xzx66a3b-bash-5.2-p15-riscv64-unknown-linux-gnu/bin/bash systemConfig=/nix/store/71wh9lvf94i1jcd6qpqw228fy5s8fv24-nixos-system-lp4a-23.05.20230806.240472b export HOME=/root PATH=\"/nix/store/fifbf1h3i83jvan2vkk7xm4fraq7drm7-coreutils-riscv64-unknown-linux-gnu-9.1/bin:/nix/store/2w8nachmhqvbjswrrsdia5cx1afxxx60-util-linux-riscv64-unknown-linux-gnu-2.38.1-bin/bin\" if [ \"${IN_NIXOS_SYSTEMD_STAGE1:-}\" != true ]; then # Process the kernel command line. for o in $(\u003c/proc/cmdline); do case $o in boot.debugtrace) # Show each command. set -x ;; esac done # Print a greeting. echo echo -e \"\\e[1;32m\u003c\u003c\u003c NixOS Stage 2 \u003e\u003e\u003e\\e[0m\" echo # Normally, stage 1 mounts the root filesystem read/writable. # However, in some environments, stage 2 is executed directly, and the # root is read-only. So make it writable here. if [ -z \"$container\" ]; then mount -n -o remount,rw none / fi fi # Likewise, stage 1 mounts /proc, /dev and /sys, so if we don't have a # stage 1, we need to do that here. if [ ! -e /proc/1 ]; then specialMount() { local device=\"$1\" local mountPoint=\"$2\" local options=\"$3\" local fsType=\"$4\" # We must not overwrite this mount because it's bind-mounted # from stage 1's /run if [ \"${IN_NIXOS_SYSTEMD_STAGE1:-}\" = true ] \u0026\u0026 [ \"${mountPoint}\" = /run ]; then return fi install -m 0755 -d \"$mountPoint\" mount -n -t \"$fsType\" -o \"$options\" \"$device\" \"$mountPoint\" } source /nix/store/vn0sga6rn69vkdbs0d2njh0aig7zmzi6-mounts.sh fi if [ \"${IN_NIXOS_SYSTEMD_STAGE1:-}\" = true ]; then echo \"booting system configuration ${systemConfig}\" else echo \"booting system configuration $systemConfig\" \u003e /dev/kmsg fi # Make /nix/store a read-only bind mount to enforce immutability of # the Nix store. Note that we can't use \"chown root:nixbld\" here # because users/groups might not exist yet. # Silence chown/chmod to fail gracefully on a readonly filesystem # like squashfs. chown -f 0:30000 /nix/store chmod -f 1775 /nix/store if [ -n \"1\" ]; then if ! [[ \"$(findmnt --noheadings --output OPTIONS /nix/store)\" =~ ro(,|$) ]]; then if [ -z \"$container\" ]; then mount --bind /nix/store /nix/store else mount --rbind /nix/store /nix/store fi mount -o remount,ro,bind /nix/store fi fi if [ \"${IN_NIXOS_SYSTEMD_STAGE1:-}\" != true ]; then # Use /etc/resolv.conf supplied by systemd-nspawn, if applicable. if [ -n \"\" ] \u0026\u0026 [ -e /etc/resolv.conf ]; then resolvconf -m 1000 -a host \u003c/etc/resolv.conf fi # Log the script output to /dev/kmsg or /run/log/stage-2-init.log. # Only at this point are all the necessary prerequisites ready for these commands. exec {logOutFd}\u003e\u00261 {logErrFd}\u003e\u00262 if test -w /dev/kmsg; then exec \u003e \u003e(tee -i /proc/self/fd/\"$logOutFd\" | while read -r line; do if test -n \"$line\"; then echo \"\u003c7\u003estage-2-init: $line\" \u003e /dev/kmsg fi done) 2\u003e\u00261 else mkdir -p /run/log exec \u003e \u003e(tee -i /run/log/stage-2-init.log) 2\u003e\u00261 fi fi # Required by the activation script install -m 0755 -d /etc /etc/nixos install -m 01777 -d /tmp # Run the script that performs all configuration activation that does # not have to be done at boot time. echo \"running activation script...\" $systemConfig/activate # Record the boot configuration. ln -sfn \"$systemConfig\" /run/booted-system # Run any user-specified commands. /nix/store/91hllz70n1b0qkb0r9iw1bg9xzx66a3b-bash-5.2-p15-riscv64-unknown-linux-gnu/bin/bash /nix/store/cmvnjz39iq4bx4cq3lvri2jj0sjq5h24-local-cmds # Ensure systemd doesn't try to populate /etc, by forcing its first-boot # heuristic off. It doesn't matter what's in /etc/machine-id for this purpose, # and systemd will immediately fill in the file when it starts, so just # creating it is enough. This `: \u003e\u003e` pattern avoids forking and avoids changing # the mtime if the file already exists. : \u003e\u003e /etc/machine-id","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:4:3","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#3-init-程序分析"},{"categories":["tech"],"content":" 4. activate 程序分析前面的 init 程序其实没干啥，根据我们看过的启动日志，大部分的功能应该都是在$systemConfig/activate 这个程序中完成的。 再看看其中的 $systemConfig/activate 的内容，它同样是一个 shell 脚本，直接 cat/vim 查看下： bash › cat root/nix/store/71wh9lvf94i1jcd6qpqw228fy5s8fv24-nixos-system-lp4a-23.05.20230806.240472b/activate #!/nix/store/91hllz70n1b0qkb0r9iw1bg9xzx66a3b-bash-5.2-p15-riscv64-unknown-linux-gnu/bin/bash systemConfig='/nix/store/71wh9lvf94i1jcd6qpqw228fy5s8fv24-nixos-system-lp4a-23.05.20230806.240472b' export PATH=/empty for i in /nix/store/fifbf1h3i83jvan2vkk7xm4fraq7drm7-coreutils-riscv64-unknown-linux-gnu-9.1 /nix/store/x3hfwbwcqgl9zpqrk8kvm3p2kjns9asm-gnugrep-riscv64-unknown-linux-gnu-3.7 /nix/store/qn0yhj5d7r432rdh1885cn40gz184ww9-findutils-riscv64-unknown-linux-gnu-4.9.0 /nix/store/slwk77dzar2l1c4h9fikdw93ig4wdfy1-getent-glibc-riscv64-unknown-linux-gnu-2.37-8 /nix/store/yrf57f5h1rwmf3q70msx35a2p9f0rsjr-glibc-riscv64-unknown-linux-gnu-2.37-8-bin /nix/store/9al8xczxbm72i5q63n91fli5rynrfprl-shadow-riscv64-unknown-linux-gnu-4.13 /nix/store/2imxx6v9xhy8mbbx9q1r2d991m81inar-net-tools-riscv64-unknown-linux-gnu-2.10 /nix/store/2w8nachmhqvbjswrrsdia5cx1afxxx60-util-linux-riscv64-unknown-linux-gnu-2.38.1-bin; do PATH=$PATH:$i/bin:$i/sbin done _status=0 trap \"_status=1 _localstatus=\\$?\" ERR # Ensure a consistent umask. umask 0022 #### Activation script snippet specialfs: _localstatus=0 specialMount() { local device=\"$1\" local mountPoint=\"$2\" local options=\"$3\" local fsType=\"$4\" if mountpoint -q \"$mountPoint\"; then local options=\"remount,$options\" else mkdir -m 0755 -p \"$mountPoint\" fi mount -t \"$fsType\" -o \"$options\" \"$device\" \"$mountPoint\" } source /nix/store/vn0sga6rn69vkdbs0d2njh0aig7zmzi6-mounts.sh if (( _localstatus \u003e 0 )); then printf \"Activation script snippet '%s' failed (%s)\\n\" \"specialfs\" \"$_localstatus\" fi #### Activation script snippet binfmt: _localstatus=0 mkdir -p -m 0755 /run/binfmt if (( _localstatus \u003e 0 )); then printf \"Activation script snippet '%s' failed (%s)\\n\" \"binfmt\" \"$_localstatus\" fi #### Activation script snippet stdio: _localstatus=0 if (( _localstatus \u003e 0 )); then printf \"Activation script snippet '%s' failed (%s)\\n\" \"stdio\" \"$_localstatus\" fi #### Activation script snippet binsh: _localstatus=0 # Create the required /bin/sh symlink; otherwise lots of things # (notably the system() function) won't work. mkdir -m 0755 -p /bin ln -sfn \"/nix/store/4y83vxk3mfk216d1jjfjgckkxwrbassi-bash-interactive-5.2-p15-riscv64-unknown-linux-gnu/bin/sh\" /bin/.sh.tmp mv /bin/.sh.tmp /bin/sh # atomically replace /bin/sh if (( _localstatus \u003e 0 )); then printf \"Activation script snippet '%s' failed (%s)\\n\" \"binsh\" \"$_localstatus\" fi #### Activation script snippet check-manual-docbook: _localstatus=0 if [[ $(cat /nix/store/xzgmgymf510dicgppghq27lrh9fjpxfi-options-used-docbook) = 1 ]]; then echo -e \"\\e[31;1mwarning\\e[0m: This configuration contains option documentation in docbook.\" \\ \"Support for docbook is deprecated and will be removed after NixOS 23.05.\" \\ \"See nix-store --read-log /nix/store/n232fhpqqqnlfjl0rj59xxms419glja2-options.json.drv\" fi if (( _localstatus \u003e 0 )); then printf \"Activation script snippet '%s' failed (%s)\\n\" \"check-manual-docbook\" \"$_localstatus\" fi #### Activation script snippet domain: _localstatus=0 if (( _localstatus \u003e 0 )); then printf \"Activation script snippet '%s' failed (%s)\\n\" \"domain\" \"$_localstatus\" fi #### Activation script snippet users: _localstatus=0 install -m 0700 -d /root install -m 0755 -d /home /nix/store/6fap9xv6snx5fr2m7m804v4gc23pb1jh-perl-riscv64-unknown-linux-gnu-5.36.0-env/bin/perl \\ -w /nix/store/gx91fdp4a099jpfwdkbdw2imvl3lalsk-update-users-groups.pl /nix/store/1zj6fk93qkqd3z8n34s4r40xnby2ci21-users-groups.json if (( _localstatus \u003e 0 )); then printf \"Activation script snippet '%s' failed (%s)\\n\" \"users\" \"$_localstatus\" fi #### Activation script snippet groups: _localstatus=0 if (( _localstatus \u003e 0 )); then printf \"Activation script snippet '%s' failed (%s)\\n\" \"groups\" \"$_localstatus\" fi #### Activation script snippet etc:","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:4:4","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#4-activate-程序分析"},{"categories":["tech"],"content":" 四、硬件驱动部分NixOS 要能在 LicheePi 4A 上正常启动，还需要有硬件固件的支持，因此光了解 NixOS 的启动流程还不够，还需要了解硬件固件的启动流程。这里简要介绍下 Linux 在 RISC-V 上的启动流程。 ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:5:0","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#四硬件驱动部分"},{"categories":["tech"],"content":" 1. u-boot，u-boot-spl，u-boot-tpl 的关系U-Boot 是嵌入式领域最常用的 bootloader， 对于一般嵌入式系统而言只需要一个 u-boot 作为 bootloader 即可，但入今的嵌入式 IC 已经转向 SOC 片上系统，其内部不仅仅是一颗 CPU 核，还可能包含各种各样的其他 IP，因而相关的上层软件也需要针对性的划分不同的功能域，操作域，安全域等上层应用。为了支持这些复杂而碎片化的应用需求，又或者因为 SRAM 太小以致无法放下整个 bootloader，SOC 的 Boot 阶段衍生出了多级 BootLoader，u-boot 为此定义了二三级加载器: spl：Secondary Program Loader，二级加载器 tpl：Tertiary Program Loader，三级加载器 spl 和 tpl 走 u-boot 完全相同的 boot 流程，不过在 spl 和 tpl 中大多数驱动和功能被去除了， 根据需要只保留一部分 spl 和 tpl 需要的功能，通过 CONFIG_SPL_BUILD 和 CONFIG_TPL_BUILD 控制；一般只用 spl 就足够了，spl 完成 ddr 初始化，并完成一些外设驱动初始化，比如 usb，emmc， 以此从其他外围设备加载 u-boot，但是如果对于小系统 spl 还是太大了，则可以继续加入 tpl，tpl 只做 ddr 等的特定初始化保证代码体积极小，以此再次从指定位置加载 spl，spl 再去加载 u-boot。 LicheePi4A 就使用了二级加载器，它甚至写死了 eMMC 的分区表，要求我们使用 fastboot 往对应的分区写入 u-boot-spl.bin，官方给出的命令如下： bash # flash u-boot into spl partition sudo fastboot flash ram u-boot-with-spl.bin sudo fastboot reboot # flash uboot partition sudo fastboot flash uboot u-boot-with-spl.bin ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:5:1","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#1-u-bootu-boot-splu-boot-tpl-的关系"},{"categories":["tech"],"content":" 2. RISC-V 的启动流程网上找到的一个图，涉及到一些 RISC-V 指令集相关的知识点： RISCV 开发版当前的引导流程 根据我们前面的 NixOS 启动日志，跟这个图还是比较匹配的，但我们没观察到任何 U-Boot 日志，有可能是因为 U-Boot 没开日志，暂时不打算细究。 ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:5:2","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#2-risc-v-的启动流程"},{"categories":["tech"],"content":" 3. OpenSBI前面的 NixOS 启动日志跟启动流程图中都出现了 OpenSBI，那么 OpenSBI 是什么呢？为什么 ARM 开发版的启动流程中没有这么个玩意儿？ 查了下资料，大概是说因为 RISC-V 是一个开放指令集，任何人都可以基于 RISC-V 开发自己的定制指令集，或者定制 IC 布局。这显然存在很明显的碎片化问题。OpenSBI 就是为了避免此问题而设计的， 它提供了一个标准的接口，即 Supervisor Binary Interface, SBI. 上层系统只需要适配 SBI 就可以了，不需要关心底层硬件的细节。IC 开发商也只需要实现 SBI 的接口，就可以让任何适配了 SBI 的上层系统能在其硬件平台上正常运行。 而 OpenSBI 则是 SBI 标准的一个开源实现，IC 开发商只需要将 OpenSBI 移植到自己的硬件平台上即可支持 SBI 标准。 而 ARM 跟 X86 等指令集则是封闭的，不允许其他公司修改与拓展其指令集，因此不存在碎片化的问题，也就不需要 OpenSBI 这样的东西。 ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:5:3","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#3-opensbi"},{"categories":["tech"],"content":" 4. fw_dynamic.bin 跟 u-boot-spl.bin 两个文件 fw_dynamic.bin: 我们 NixOS 镜像的 /boot 中就有这个固件，它是 OpenSBI 的编译产物。 RevyOS 的定制 OpenSBI 构建方法：https://github.com/revyos/thead-opensbi/blob/lpi4a/.github/workflows/build.yml u-boot-spl.bin: 这个文件是 u-boot 的编译产物，它是二级加载器。 RevyOS 的定制 u-boot 构建方法：https://github.com/revyos/thead-u-boot/blob/lpi4a/.github/workflows/build.yml ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:5:4","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#4-fw_dynamicbin-跟-u-boot-splbin-两个文件"},{"categories":["tech"],"content":" 5. T-Head 官方的编译工具链因为历史原因，TH1520 设计时貌似 RVV 还没出正式的规范，因此它使用了一些非标准的指令集，GCC 官方貌似宣称了永远不会支持这些指令集…（个人理解，可能有误哈） 因此为了获得最佳性能，LicheePi4A 官方文档建议使用 T-Head 提供的工具链编译整个系统。 但我在研究了 NixOS 的工具链实现，以及咨询了 @NickCao 后，确认了在 NixOS 上这几乎是不可行的。NixOS 因为不遵循 FHS 标准，它对 GCC 等工具链做了非常多的魔改，要在 NixOS 上使用 T-Head 的工具链，就要使这一堆魔改的东西在 T-Head 的工具链上也能 Work，这个工作量很大，也很有技术难度。 所以最终选择了用 NixOS 的标准工具链编译系统，@revy 老师也为此帮我做了些适配工作，解决了一些标准工具链上的编译问题。 Issue 区也有人提到了这个问题，Revy 老师也帮助补充了些相关信息：https://github.com/ryan4yin/nixos-licheepi4a/issues/14 ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:5:5","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#5-t-head-官方的编译工具链"},{"categories":["tech"],"content":" 五、我是如何构建出一个可以在 LicheePi 4A 上运行的 NixOS 镜像的到这里，NixOS 在 LicheePI4A 上启动的整个流程就基本讲清楚了， NixOS 跟其他传统发行版在启动流程中最大的区别是它自定义了一个 init 脚本，在启动 systemd 之前，它会先执行这个脚本进行文件系统的初始化操作，准备好最基础的 FHS 目录结构，使得后续的 systemd 以及其他服务能正常启动。正是因为这个 init 脚本，NixOS 才能在仅有 /boot 与 /nix 这两个目录的情况下正常启动整个系统。 NixOS 数据的集中化只读存储使更多的骚操作成为可能，比如直接使用 tmpfs 作为根文件系统，将需要持久化的目录挂载到外部存储设备上，这样每次重启系统时，所有预期之外的临时数据都会被清空，进一步保证了系统的可复现性与安全性。如果你有系统洁癖，而且有兴趣折腾，那就快来看看 @LanTian 写的NixOS 系列（四）：「无状态」操作系统 吧~ 最终在 LicheePi4A 成功启动后的登录的截图： NixOS 成功启动 那么基于我们到目前为止学到的知识，要如何构建出一个可以在 LicheePi 4A 上运行的 NixOS 镜像呢？ 这个讲起来就很费时间了，涉及到了 NixOS 的交叉编译系统，内核 override,flakes,镜像构建等等，要展开讲的话也是下一篇文章了，有兴趣的可以直接看我的 NixOS on LicheePi4A 仓库：https://github.com/ryan4yin/nixos-licheepi4a. 简单的说，NixOS 跟传统 Linux 发行版的系统镜像构建思路是一致的，但因为其声明式与可复现性的特点，实际实现时出现了非常大的区别。以我的项目仓库为例，整个项目完全使用 Nix 语言声明式编写（内嵌了部分 Shell 脚本…），而且这份配置也可用于系统后续的持续声明式更新部署（我还给出了一个 demo）。 最后，再推荐一波我的 NixOS 入门指南：ryan4yin/nixos-and-flakes-book， 对 NixOS 感兴趣的读者们，快进我碗里来（ ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:6:0","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#五我是如何构建出一个可以在-licheepi-4a-上运行的-nixos-镜像的"},{"categories":["tech"],"content":" 参考 Systemd Stage 1 - NixCon NA 2024 - California LicheePi 4A —— 这个小板有点意思（第一部分） - HougeLangley Analyzing the Linux boot process - By Alison Chaiken OpenSBI Platform Firmwares An Introduction to RISC-V Boot flow: Overview, Blob vs Blobfree standards 基于 qemu-riscv 从 0 开始构建嵌入式 linux 系统 ch5-1. 什么是多级 BootLoader 与 opensbi(上)¶ Using the initial RAM disk (initrd) - kernel.org Differences Between vmlinux, vmlinuz, vmlinux.bin, zimage, and bzimage U-Boot 官方的 Distro 文档 ","date":"2024-01-29","objectID":"/posts/how-nixos-start-on-licheepi4a/:7:0","series":["NixOS 与 Nix Flakes"],"tags":["Linux","NixOS","LicheePi4A","Embedded","U-Boot","RISC-V"],"title":"NixOS 在 Lichee Pi 4A 上是如何启动的","uri":"/posts/how-nixos-start-on-licheepi4a/#参考"},{"categories":["life","tech"],"content":" 闲言碎语啊呀，又到了一年一度的传统节目——年终总结时间。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:1:0","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#闲言碎语"},{"categories":["life","tech"],"content":" 2023 年流水账还是跟去年一样，先简单过一下我 2023 年的流水账（基本都摘抄自我的 /history，类似日记？）： 1 月 再一次完成了公司 K8s 集群一年一度的升级，虽然仍然有比较大的压力，但这次的过程相当顺利。 然后就是朋友约饭，玩耍，回家过春节。 2 月 延续去年底开始对嵌入式硬件的兴趣，继续折腾 stm32 / orange pi 5 / esp32 等嵌入式硬件。 用 STM32 点亮了 TFT 液晶屏，以及搞定了使用 printf 打印日志到串口 -ryan4yin/learn-stm32f103c8t6 研究在 orangepi5(rk3558s) 上用 npu 跑 AI 任务，写了点笔记demos_rk3588 折腾 Proxmox VE 集群 主力节点 UM560 固态翻车了，是才用了三个月的 Asgard 512G SSD，颗粒是长江存储的。走京东售后了。（上次翻车是 2022-11-02 炸了根光威弈 Pro 1T，这也没隔多久啊…） 2022-11-02 翻车记录 - 系统无法启动 2023-02-03 翻车记录 - 系统能启动但是文件损坏 研究 Homelab 备份与数据同步方案，写了点笔记数据备份与同步策略 发布文章EE 入门（二） - 使用 ESP32 + SPI 显示屏绘图、显示图片、跑贪吃蛇 简单玩了玩 Stable-Diffusion-WebUI 跟 sd-webui-controlnet，抄了些网上的提示词，效果确实很惊艳。不过玩了一波就没啥兴趣了，不太想花很多精力去折腾它。 3 月 生活上 读完了「The Moon and Sixpence」 跟朋友到游泳馆游泳，算是开年以来最剧烈的一次运动… 跟同事们约着一起穿越了东西冲海岸线，这是我第三次走这条线，风景仍旧很美，当然也走得挺累… 买了块冲浪训练平衡板，练习了一段时间，挺有意思。 业余技术上 仍旧是折腾各种嵌入式设备，新入手了 Sipeed MAIX-III AXera-Pi AX620A（爱芯派）+ Maix M0s MCU, 野火鲁班猫 0，荔枝糖 Nano 9K、M1s Dock、Longan Nano 等一堆大小开发板，折腾各种 Linux 编译、嵌入式开发的内容。 被 Copilot X 小小震撼了下，花了 100 美刀买了个 1 年的订阅，价格不贵，希望能切实帮我提升写代码的效率。 发了篇新博客：Linux 上的 WireGuard 网络分析（一） 读了一点Linux Device Drivers Development 2nd Edition 4 月 在业余爱好上投入的精力越来越多，工作上相对的越来越懈怠，感觉碰到了瓶颈，但搞不明白该怎么解决。 听了 wd 佬的建议整了个达尔优 A87Pro 天空轴 v3，一番体验这个天空轴 v3 手感确实贼爽、声音也小，感觉可能有点类似静电容了（虽然我没用过静电容 emmm）。 我毕业以来就 19 年跟 20 年分别买过两把 IKBC 的茶轴跟红轴，茶轴放家里了，红轴一直放在公司用。当时国产轴感觉还不太出名，但是现在我聊键盘的朋友都看不上 cherry 轴了，网上搜了下 cheery 轴也有各种品控下降、轴心不稳、杂音大的诟病。 结合朋友推荐，另外看到 v2ex 上聊键盘的朋友也有说天空轴 v3 好用的，还在知乎上也看到有人说这个轴不错，于是就按捺不住心思下单了。到手确实很惊艳，甚至让我再一次喜欢上了打字的感觉！打了几篇小鹤练习群的赛文享受这种飘逸的 feel~ 搞了个 chatglm-6b int4 量化版，本地用我的拯救者笔记本（16G RAM + RTX3070 8G）玩了下， 响应速度感觉可以，确实有一定的上下文联想能力，简单的问题也能解答，但是有点不聪明的样子，内容投毒比较严重。 玩 AI 联想到淘垃圾显卡，看嗨了就直接整了套新主机新显示器（我的第一台 PC 主机，以前只买过笔记本电脑），玻璃侧透机箱，RTX 4090，双水冷，27 寸 4K 显示器。组装了大半夜，后面又折腾了好几天，机箱直接当手办展示柜了，效果相当惊艳！缺点一是套下来貌似花了两万多， 罪魁祸首是 RTX4090… 主机配置 机箱展示 机箱展示 去听了个 Live House，乐队叫迎春归，青岛的乐队，不过前面许多歌我都觉得一般般，主唱唱功也差了点，全靠架子鼓贝斯烘托。不过末尾几首歌还挺好听的。 天依手办到货，很飒～ 洛天依 秘境花庭 常服手办 新主机装了个 Endeavour OS 遇到些奇怪的问题，一怒之下决定换 OS，刚好朋友提到了 NixOS， 听说过这玩意儿能做到「可复现」，直接就在 Homelab 里开了个 NixOS 虚拟机开始折腾，由此开始了我的 NixOS 之旅。 用新主机试玩了米忽悠的新游戏「星穹铁道」，还是熟悉的画风跟 UI，制作质量也很高，回合式对战的玩法我本以为会枯燥，不过也还 OK。最重要是 4090 画质够高，很多可爱的角色，游戏的动画跟剧情也都很在线，总体很 Nice! 用新主机连 Quest 2 打 VR 游戏，发现做过参数优化后，RTX4090 跑 beta saber，Quest 2 的画质参数全调到最高， 5K 120 帧无压力，相当流畅。 用 RTX4090 玩 Cyperpunk 2077，顶配光追画质（叫啥 onedrive）贼棒，真的非常还原真实环境，在 GeForce Experience 上调了个啥优化参数后，4K 差不多能稳定在 100 帧，看半天风景。 5 月 月初，在虚拟机里折腾了大半个月 NixOS 后，成功地用几条简单的命令，在我的新主机上复现了整个 NixOS 环境，那一刻真的超级开心，半个月的努力终于得到了回报！ 在新主机上成功复现出我的 NixOS 环境后，紧接着发布了我的系统配置ryan4yin/nix-config/v0.0.2) 以及这大半个月的学习笔记NixOS 与 Nix Flakes 新手入门， 然后事情就变得越来越有趣起来了！随着读者的反馈以及我对它的不断迭代，这份学习笔记逐渐膨胀成一篇一万多字的博文，并且有了中英双语，然后又转变成一本开源书藉nixos-and-flakes-book，在 NixOS 国际社区获得了大量好评！它给我带来了巨大的成就感以及社区参与感。 NixOS \u0026 Flakes Book 的部分评论 - Reddit NixOS \u0026 Flakes Book 的部分评论 - Discourse 与 GitHub NixOS \u0026 Flakes Book 的部分评论 - Discord 在 NixOS 上尝试了 i3 与 Hyprland 两个窗口管理器，并且使用 agenix 管理了系统中的敏感信息，比如密码、私钥、wireguard 配置等。 agenix 确实 OK，但它纯 bash 脚本实现的核心功能，体验太差了，错误信息一团糟，解决错误全靠自己摸索。 6 月 立了个 flag - 把 NixOS 移稙到我手上的两块开发版上跑起来，一块 ARM64 架构的 Orange Pi 5，以及另一块 RISC-V 架构的 LicheePi 4A. 花了好几天时间研究，在俄罗斯网友的耐心帮助下，终于在 6/4 晚上在 Orange Pi 5 上把 NixOS 跑起来了，还挺有成就感的（虽然现在也不知道拿这板子用来干啥…） 之后断断续续折腾了一个月的 NixOS on LicheePi 4A，试了很多方案，还请教了HougeLangley、@nickcao、@chainsx 等大佬，学会了很多 Linux 相关的东西，费尽千辛万苦终于成功把 rootfs 编译出来了，但死活引导不成功。感觉是 uboot-spl 跟 boot 分区这两个地方的内容有问题，但不知道怎么解决，累觉不爱。 收到一封来自 2018 年的我在 futureme.org 发送的邮件，回想起来，当时我是真迷茫哪。 2018 年写给 5 年后的我的邮件 受读者评论启发，将之前的 NixOS 笔记做成了一个单独的文档站点 + GitHub 仓库，nixos-and-flakes-book，也对其内容做了大量更新，用 ChatGPT 3.5 全面优化了英文内容，阅读体验大大提升（英文苦手默默路过…） NixOS \u0026 Flakes Book 7 月 NixOS 系统配置 ryan4yin/nix-config 迭代： 把办公电脑 Macbook Pro 2020 重裝了一遍系統，新系统環境完全通過 nix-darwin 安裝管理， 就連大部分的 macOS 系統配置也完全声明式管理了。至此，我的常用电脑环境（NixOS+macOS） 全部都使用同一份 nix 配置管理起来了，感覺非常香！ Linux 与 macOS 都使用了同一份小鹤音形的 rime 配置，现在输入法的跨平台体验也完全一致了，非常棒！ nixpkgs 对 macOS 的支持有限，因此常用的 GUI 程序都通过 nix-darwin 调用 homebrew 进行安装管理。 所有命令行工具的主题，全部统一为了catppuccin-mocha. 壁纸文件太大了，将它们拆分到单独的仓库中，方便管理。同时还添加了随机切换壁纸的功能。 添加了三台在 Proxmox VE 上运行的 NixOS 虚拟机，并且尝试用它们组建 NixOS 的分布式构建集群，挺有意思。 发现之前用的 alacritty 功能有限，于是将主力终端换成了 kitty，wezterm 作为备用选择， 而 alacritty 就基本不使用了。 主力编","date":"2023-12-31","objectID":"/posts/2023-summary/:2:0","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#2023-年流水账"},{"categories":["life","tech"],"content":" 2023 年 Highlights","date":"2023-12-31","objectID":"/posts/2023-summary/:3:0","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#2023-年-highlights"},{"categories":["life","tech"],"content":" 1. 业余技术技术方面我今年的进展还是挺大的，可跟去年写的展望几乎没啥关联，人生总是充满了意外哈哈… 今年的主要技术成就基本完全集中在 NixOS 这一块，新建的几个项目都收到了挺多 stars 跟好评。 截止 2023/12/31，我 stars 比较高的几个项目如下： ryan4yin/nixos-and-flakes-book: 这本开源小书的仓库于 2023/6/23 创建，目前获得了 15 个 issues，24 位贡献者，43 个 forks，923 个 stars，以及 4 位国外读者的共计 $80 零花钱赞助。 是我目前 stars 数最高的项目 它的文档站目前稳定在每天 150 UV ryan4yin/knowledge: 这份个人笔记我从 2019 年工作开始写，目前有了 38 个 forks，363 个 stars. ryan4yin/nix-config: 这份 NixOS 系统配置仓库于 2023/4/23 创建，目前获得了 6 位贡献者，23 个 forks，以及 297 个 stars. ryan4yin/nix-darwin-kickstarter: 我于 2023/7/19 创建的一个 Nix-Darwin 模板仓库，目前 133 stars. ryan4yin/nixos-rk3588: 这是我在 2023/6/4 创建的一个 NixOS 移植项目，目前支持了三块 RK3588 开发板，获得了 2 位贡献者，9 个 forks，11 个 issues，以及 49 stars. ryan4yin/nixos-licheepi4a: 同样是一个 NixOS 移植项目，但目标是基于 RISC-V 指令集的 LicheePi 4A 开发板。目前获得了 3 位贡献者与 23 stars，其中一位贡献者还赞助了 $50 给我。 这个项目断断续续花了两个月才搞定，用时远超预料…不过成功后获得的成就感也是巨大的！ 对比下从 2023 年 1 月 1 日到现在，我的 GitHub Metrics 统计数据： 2023/01/01 GitHub 统计数据 2023/12/31 GitHub 统计数据 几个关键指标的变化： Stars: 312 =\u003e 2072, 涨幅 564%. Followers: 152 =\u003e 468, 涨幅 208%. Forkers: 97 =\u003e 203, 涨幅 109%. Watchers: 39 =\u003e 102, 涨幅 161%. 在折腾 NixOS 的过程中我写的入门指南 （nixos-and-flakes-book）获得了国内外社区的大量好评，其他项目也各有斩获；另外认识了好几位国内外的 NixOS 资深用户、开源项目作者以及嵌入式开发者，还收到了一些外国读者的打赏。 这些成绩给我带来了巨大的成就感以及社区参与感，也完全契合了我年初给自己的期许——「认识更多有趣的人，见识下更宽广的世界」。 今年不仅给 AstroNvim, ESP-IDF 等知名项目贡献了少许代码，甚至还创造了这么多个受欢迎的新项目、收到了几十个 PR。之前定的给一些开源项目贡献代码的目标，完全是超额完成了。 在折腾上 NixOS 后，年中的时候又顺带将 VSCode 彻底换成了 Zellij + Neovim，甚至年底又开始折腾 Guix 系统、Emacs 编辑器，以及 nushell 等新鲜玩意儿。 总的来说，业余技术今年搞到这个程度，相比去年，能称得上是「优秀」、「超出预期」。 要说我在搞技术这方面有啥诀窍可分享的话，那应该就是 Learning by Teaching，我 GitHub 上目前 stars 最高的两个项目（NixOS \u0026 Flakes Book 以及 knowledge）以及这个博客站点就是我在践行它最好的证明，而它们获得的评论、 stars、以及零花钱赞助，则证明了它的价值。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:3:1","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#1-业余技术"},{"categories":["life","tech"],"content":" 2. 工作工作上只能说马马虎虎，上半年业余在 NixOS 上做的成果得到了非常多的认可，相当有成就感，花了大量的精力在 NixOS 上，也创建了许多相关项目。但另一方面，精力就这么多，我也一直做不到平衡好工作与生活/业余爱好，其结果就是那段时间没啥心思在工作上，把工作搞得一团糟。当时觉得自己进入了一个瓶颈期，在工作上找不到什么成就感，业余爱好虽然做出了不错的成绩，但又不能靠这个吃饭。 在折腾业余爱好期间，一种找不到方向的焦虑感一直萦绕着我，有跟一些朋友、同事沟通过这个问题， 但大道理谁都懂，真要做起来又是另一回事了。 因为业余搞了些嵌入式硬件感觉有意思，也有隐约考虑过转行搞硬件，但只是些粗浅的想法。到 8 月份的时候，做的几个 NixOS 项目收到些赞助，让我可能有点异想天开？了解了些「如何通过开源项目养活自己」类似的信息，8 月中下旬的时候在苏洋的折腾群里提到这个想法，被洋哥泼了冷水 emmm 冷静下来后回想，洋哥说的挺在理的，靠开源用爱发电真能养活自己的凤毛麟角，如果专门往商业项目的方向做，又没了那份折腾的快乐了。 8 月底的时候，苏洋的折腾群里发起一场自我介绍活动，读了许多群友的自我介绍，很有感触，于是基于我自己在群里发的自我介绍调整扩写，成果就是这篇人生已过四分之一。 当时文章完成后发出来，真觉得自己想明白不少，也跟领导同事做了些沟通，工作上状态有所好转。但还是很难集中注意力，分心的情况仍然相当严重。也尝试了通过番茄钟之类的方式来提高工作效率，但效果不佳。当时有点认命了，想着人生可能就是这样永远在这样未知的道路上的挣扎着前进，也有痛苦，也有快乐。 转折点是国庆前跟朋友提了嘴感觉自己有 ADHD，国庆后就被@咩咩催促去看心理医生，之后就确诊了 ADHD 并开始服药。确诊让我的心态出现巨大的改变，业余爱好因此放下了一个多月。而服药则帮助我扭转了工作状态，我的专注能力有了质的提升，这也是我今年最大的收获之一。 总体上，我今年的工作做得比去年差，尤其是上半年，总体上只能算「及格」吧。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:3:2","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#2-工作"},{"categories":["life","tech"],"content":" 3. 生活 3.1. 确诊 ADHD 以及治疗前文提了，我今年确诊了 ADHD，这是我今年最大的收获之一。它让我搞明白了，原来我一直存在的注意力问题，并不是什么人格缺陷或者意志力不够，而是一种有挺多人都存在的功能失调（disorder）， 可以通过药物等方式治疗缓解。 考虑到 ADHD 的遗传特性，跟妹妹、父母一番沟通后，带我妹来深圳看医生，确诊了 ADHD 以及抑郁症。说真的，我一直知道我妹妹情绪比较敏感，但从没想过是因为抑郁症。 这之后，除了工作外，我大部分的精力都花在了关心家人、运动、学习心理学等事情上。这是一个非常明显的转变，我跟我妹的关系更好了，另外持续好几个月的治疗跟每个月带她在深圳散散心，她的情绪也有了很大的改善。 3.2. 参与公益活动因缘际会跟 @Manjusaka_Lee 以及另一位朋友聊到了公益， 当时在工作上缺乏动力，想在其他的事情上找找感觉，公益本身也是一件很有意义的事情，那段时间学习了解了许多公益资料，参加了一些公益会展与捐款活动，还一度考虑做志愿者。 今年算是迈出了参与公益活动的第一步，这拓展了我的视野，让我更加了解了社会。在学习了解公益期间也结识了一些志同道合的朋友，这也挺符合我去年给自己的期许——「认识更多有趣的人，见识下更宽广的世界」。 做事情，最难的就是从 0 到 1，因此今年的这一进展就显得尤为可贵，称得上是「优秀」。 3.3. 阅读与写作首先是我 2023 年的读书记录： 2023-03-09 - The Moon and Sixpence 你是要地上的六便士，还是要天上的月亮？ 2023-08-31 -How to Do Great Work - Paul Graham 黑客与画家一篇两万多字的长文，也算是一本小书了吧，干货满满。 2023-09-29 - 《被讨厌的勇气》 一本通过对话的形式讲述阿勒德心理学的书，这门心理学与现代科学心理学不同，它更偏向哲学。 2023-10-08 - 《叫魂：1768 年中国妖术大恐慌》 从 1768 年的叫魂案出发，分析了乾隆时期的中国社会的许多方面。比如因各种原因导致人口过度增长、人均资源减少、社会矛盾激化导致的普遍恐慌，以及官僚君主制的运作机制、存在的问题。此书以史为鉴，能帮助我们理解现代中国政治与中国社会的一些基本问题。 2023-10-17 - 《分心不是我的错》 介绍 ADHD 的一本书，列举了很多 ADHD 案例，也给出了诊疗建议，对我相当有用！ 以及一些未读完的书： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig The Great Gatsby - 10/41 《复杂 - 梅拉尼 米歇尔》 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 2023 年我读的书籍数量不多，没达成每月读一本书的目标。而写作方面，算上这篇总结，今年我一共写了 9 篇博文，也没达成每月写一篇的目标。 但鉴于我今年写了一本挺受欢迎的小书NixOS \u0026 Flakes Book，它得到了 NixOS 社区诸多好评，还在 10 月份上了 Hacker News 热榜，甚至被官方文档nix.dev 列入推荐阅读，我姑且将今年「阅读与写作」这一项的评分定为「超出期待」！ 3.4. 其他 运动方面乏善可陈，穿越了一次东西冲海岸线，游了几次泳，12 月初晨跑了一周但因为是空腹跑， 胃炎给跑犯了，就没再跑了。体重全年都在 60kg 波动，没啥变化。 给我老爸全款买了我们全家的第一辆小轿车（自己没买，一是天天坐地铁用不到，二是我对车也缺乏兴趣）。 计划开始给父母约年度体检，待实施。 3.5. 总结生活上今年达成了这么多有意义的成就（确诊 ADHD、将更多精力花在关心家人上、参与公益活动、给家里买车等等），我给自己的评价当然是「优秀」。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:3:3","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#3-生活"},{"categories":["life","tech"],"content":" 3. 生活 3.1. 确诊 ADHD 以及治疗前文提了，我今年确诊了 ADHD，这是我今年最大的收获之一。它让我搞明白了，原来我一直存在的注意力问题，并不是什么人格缺陷或者意志力不够，而是一种有挺多人都存在的功能失调（disorder）， 可以通过药物等方式治疗缓解。 考虑到 ADHD 的遗传特性，跟妹妹、父母一番沟通后，带我妹来深圳看医生，确诊了 ADHD 以及抑郁症。说真的，我一直知道我妹妹情绪比较敏感，但从没想过是因为抑郁症。 这之后，除了工作外，我大部分的精力都花在了关心家人、运动、学习心理学等事情上。这是一个非常明显的转变，我跟我妹的关系更好了，另外持续好几个月的治疗跟每个月带她在深圳散散心，她的情绪也有了很大的改善。 3.2. 参与公益活动因缘际会跟 @Manjusaka_Lee 以及另一位朋友聊到了公益， 当时在工作上缺乏动力，想在其他的事情上找找感觉，公益本身也是一件很有意义的事情，那段时间学习了解了许多公益资料，参加了一些公益会展与捐款活动，还一度考虑做志愿者。 今年算是迈出了参与公益活动的第一步，这拓展了我的视野，让我更加了解了社会。在学习了解公益期间也结识了一些志同道合的朋友，这也挺符合我去年给自己的期许——「认识更多有趣的人，见识下更宽广的世界」。 做事情，最难的就是从 0 到 1，因此今年的这一进展就显得尤为可贵，称得上是「优秀」。 3.3. 阅读与写作首先是我 2023 年的读书记录： 2023-03-09 - The Moon and Sixpence 你是要地上的六便士，还是要天上的月亮？ 2023-08-31 -How to Do Great Work - Paul Graham 黑客与画家一篇两万多字的长文，也算是一本小书了吧，干货满满。 2023-09-29 - 《被讨厌的勇气》 一本通过对话的形式讲述阿勒德心理学的书，这门心理学与现代科学心理学不同，它更偏向哲学。 2023-10-08 - 《叫魂：1768 年中国妖术大恐慌》 从 1768 年的叫魂案出发，分析了乾隆时期的中国社会的许多方面。比如因各种原因导致人口过度增长、人均资源减少、社会矛盾激化导致的普遍恐慌，以及官僚君主制的运作机制、存在的问题。此书以史为鉴，能帮助我们理解现代中国政治与中国社会的一些基本问题。 2023-10-17 - 《分心不是我的错》 介绍 ADHD 的一本书，列举了很多 ADHD 案例，也给出了诊疗建议，对我相当有用！ 以及一些未读完的书： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig The Great Gatsby - 10/41 《复杂 - 梅拉尼 米歇尔》 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 2023 年我读的书籍数量不多，没达成每月读一本书的目标。而写作方面，算上这篇总结，今年我一共写了 9 篇博文，也没达成每月写一篇的目标。 但鉴于我今年写了一本挺受欢迎的小书NixOS \u0026 Flakes Book，它得到了 NixOS 社区诸多好评，还在 10 月份上了 Hacker News 热榜，甚至被官方文档nix.dev 列入推荐阅读，我姑且将今年「阅读与写作」这一项的评分定为「超出期待」！ 3.4. 其他 运动方面乏善可陈，穿越了一次东西冲海岸线，游了几次泳，12 月初晨跑了一周但因为是空腹跑， 胃炎给跑犯了，就没再跑了。体重全年都在 60kg 波动，没啥变化。 给我老爸全款买了我们全家的第一辆小轿车（自己没买，一是天天坐地铁用不到，二是我对车也缺乏兴趣）。 计划开始给父母约年度体检，待实施。 3.5. 总结生活上今年达成了这么多有意义的成就（确诊 ADHD、将更多精力花在关心家人上、参与公益活动、给家里买车等等），我给自己的评价当然是「优秀」。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:3:3","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#31-确诊-adhd-以及治疗"},{"categories":["life","tech"],"content":" 3. 生活 3.1. 确诊 ADHD 以及治疗前文提了，我今年确诊了 ADHD，这是我今年最大的收获之一。它让我搞明白了，原来我一直存在的注意力问题，并不是什么人格缺陷或者意志力不够，而是一种有挺多人都存在的功能失调（disorder）， 可以通过药物等方式治疗缓解。 考虑到 ADHD 的遗传特性，跟妹妹、父母一番沟通后，带我妹来深圳看医生，确诊了 ADHD 以及抑郁症。说真的，我一直知道我妹妹情绪比较敏感，但从没想过是因为抑郁症。 这之后，除了工作外，我大部分的精力都花在了关心家人、运动、学习心理学等事情上。这是一个非常明显的转变，我跟我妹的关系更好了，另外持续好几个月的治疗跟每个月带她在深圳散散心，她的情绪也有了很大的改善。 3.2. 参与公益活动因缘际会跟 @Manjusaka_Lee 以及另一位朋友聊到了公益， 当时在工作上缺乏动力，想在其他的事情上找找感觉，公益本身也是一件很有意义的事情，那段时间学习了解了许多公益资料，参加了一些公益会展与捐款活动，还一度考虑做志愿者。 今年算是迈出了参与公益活动的第一步，这拓展了我的视野，让我更加了解了社会。在学习了解公益期间也结识了一些志同道合的朋友，这也挺符合我去年给自己的期许——「认识更多有趣的人，见识下更宽广的世界」。 做事情，最难的就是从 0 到 1，因此今年的这一进展就显得尤为可贵，称得上是「优秀」。 3.3. 阅读与写作首先是我 2023 年的读书记录： 2023-03-09 - The Moon and Sixpence 你是要地上的六便士，还是要天上的月亮？ 2023-08-31 -How to Do Great Work - Paul Graham 黑客与画家一篇两万多字的长文，也算是一本小书了吧，干货满满。 2023-09-29 - 《被讨厌的勇气》 一本通过对话的形式讲述阿勒德心理学的书，这门心理学与现代科学心理学不同，它更偏向哲学。 2023-10-08 - 《叫魂：1768 年中国妖术大恐慌》 从 1768 年的叫魂案出发，分析了乾隆时期的中国社会的许多方面。比如因各种原因导致人口过度增长、人均资源减少、社会矛盾激化导致的普遍恐慌，以及官僚君主制的运作机制、存在的问题。此书以史为鉴，能帮助我们理解现代中国政治与中国社会的一些基本问题。 2023-10-17 - 《分心不是我的错》 介绍 ADHD 的一本书，列举了很多 ADHD 案例，也给出了诊疗建议，对我相当有用！ 以及一些未读完的书： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig The Great Gatsby - 10/41 《复杂 - 梅拉尼 米歇尔》 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 2023 年我读的书籍数量不多，没达成每月读一本书的目标。而写作方面，算上这篇总结，今年我一共写了 9 篇博文，也没达成每月写一篇的目标。 但鉴于我今年写了一本挺受欢迎的小书NixOS \u0026 Flakes Book，它得到了 NixOS 社区诸多好评，还在 10 月份上了 Hacker News 热榜，甚至被官方文档nix.dev 列入推荐阅读，我姑且将今年「阅读与写作」这一项的评分定为「超出期待」！ 3.4. 其他 运动方面乏善可陈，穿越了一次东西冲海岸线，游了几次泳，12 月初晨跑了一周但因为是空腹跑， 胃炎给跑犯了，就没再跑了。体重全年都在 60kg 波动，没啥变化。 给我老爸全款买了我们全家的第一辆小轿车（自己没买，一是天天坐地铁用不到，二是我对车也缺乏兴趣）。 计划开始给父母约年度体检，待实施。 3.5. 总结生活上今年达成了这么多有意义的成就（确诊 ADHD、将更多精力花在关心家人上、参与公益活动、给家里买车等等），我给自己的评价当然是「优秀」。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:3:3","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#32-参与公益活动"},{"categories":["life","tech"],"content":" 3. 生活 3.1. 确诊 ADHD 以及治疗前文提了，我今年确诊了 ADHD，这是我今年最大的收获之一。它让我搞明白了，原来我一直存在的注意力问题，并不是什么人格缺陷或者意志力不够，而是一种有挺多人都存在的功能失调（disorder）， 可以通过药物等方式治疗缓解。 考虑到 ADHD 的遗传特性，跟妹妹、父母一番沟通后，带我妹来深圳看医生，确诊了 ADHD 以及抑郁症。说真的，我一直知道我妹妹情绪比较敏感，但从没想过是因为抑郁症。 这之后，除了工作外，我大部分的精力都花在了关心家人、运动、学习心理学等事情上。这是一个非常明显的转变，我跟我妹的关系更好了，另外持续好几个月的治疗跟每个月带她在深圳散散心，她的情绪也有了很大的改善。 3.2. 参与公益活动因缘际会跟 @Manjusaka_Lee 以及另一位朋友聊到了公益， 当时在工作上缺乏动力，想在其他的事情上找找感觉，公益本身也是一件很有意义的事情，那段时间学习了解了许多公益资料，参加了一些公益会展与捐款活动，还一度考虑做志愿者。 今年算是迈出了参与公益活动的第一步，这拓展了我的视野，让我更加了解了社会。在学习了解公益期间也结识了一些志同道合的朋友，这也挺符合我去年给自己的期许——「认识更多有趣的人，见识下更宽广的世界」。 做事情，最难的就是从 0 到 1，因此今年的这一进展就显得尤为可贵，称得上是「优秀」。 3.3. 阅读与写作首先是我 2023 年的读书记录： 2023-03-09 - The Moon and Sixpence 你是要地上的六便士，还是要天上的月亮？ 2023-08-31 -How to Do Great Work - Paul Graham 黑客与画家一篇两万多字的长文，也算是一本小书了吧，干货满满。 2023-09-29 - 《被讨厌的勇气》 一本通过对话的形式讲述阿勒德心理学的书，这门心理学与现代科学心理学不同，它更偏向哲学。 2023-10-08 - 《叫魂：1768 年中国妖术大恐慌》 从 1768 年的叫魂案出发，分析了乾隆时期的中国社会的许多方面。比如因各种原因导致人口过度增长、人均资源减少、社会矛盾激化导致的普遍恐慌，以及官僚君主制的运作机制、存在的问题。此书以史为鉴，能帮助我们理解现代中国政治与中国社会的一些基本问题。 2023-10-17 - 《分心不是我的错》 介绍 ADHD 的一本书，列举了很多 ADHD 案例，也给出了诊疗建议，对我相当有用！ 以及一些未读完的书： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig The Great Gatsby - 10/41 《复杂 - 梅拉尼 米歇尔》 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 2023 年我读的书籍数量不多，没达成每月读一本书的目标。而写作方面，算上这篇总结，今年我一共写了 9 篇博文，也没达成每月写一篇的目标。 但鉴于我今年写了一本挺受欢迎的小书NixOS \u0026 Flakes Book，它得到了 NixOS 社区诸多好评，还在 10 月份上了 Hacker News 热榜，甚至被官方文档nix.dev 列入推荐阅读，我姑且将今年「阅读与写作」这一项的评分定为「超出期待」！ 3.4. 其他 运动方面乏善可陈，穿越了一次东西冲海岸线，游了几次泳，12 月初晨跑了一周但因为是空腹跑， 胃炎给跑犯了，就没再跑了。体重全年都在 60kg 波动，没啥变化。 给我老爸全款买了我们全家的第一辆小轿车（自己没买，一是天天坐地铁用不到，二是我对车也缺乏兴趣）。 计划开始给父母约年度体检，待实施。 3.5. 总结生活上今年达成了这么多有意义的成就（确诊 ADHD、将更多精力花在关心家人上、参与公益活动、给家里买车等等），我给自己的评价当然是「优秀」。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:3:3","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#33-阅读与写作"},{"categories":["life","tech"],"content":" 3. 生活 3.1. 确诊 ADHD 以及治疗前文提了，我今年确诊了 ADHD，这是我今年最大的收获之一。它让我搞明白了，原来我一直存在的注意力问题，并不是什么人格缺陷或者意志力不够，而是一种有挺多人都存在的功能失调（disorder）， 可以通过药物等方式治疗缓解。 考虑到 ADHD 的遗传特性，跟妹妹、父母一番沟通后，带我妹来深圳看医生，确诊了 ADHD 以及抑郁症。说真的，我一直知道我妹妹情绪比较敏感，但从没想过是因为抑郁症。 这之后，除了工作外，我大部分的精力都花在了关心家人、运动、学习心理学等事情上。这是一个非常明显的转变，我跟我妹的关系更好了，另外持续好几个月的治疗跟每个月带她在深圳散散心，她的情绪也有了很大的改善。 3.2. 参与公益活动因缘际会跟 @Manjusaka_Lee 以及另一位朋友聊到了公益， 当时在工作上缺乏动力，想在其他的事情上找找感觉，公益本身也是一件很有意义的事情，那段时间学习了解了许多公益资料，参加了一些公益会展与捐款活动，还一度考虑做志愿者。 今年算是迈出了参与公益活动的第一步，这拓展了我的视野，让我更加了解了社会。在学习了解公益期间也结识了一些志同道合的朋友，这也挺符合我去年给自己的期许——「认识更多有趣的人，见识下更宽广的世界」。 做事情，最难的就是从 0 到 1，因此今年的这一进展就显得尤为可贵，称得上是「优秀」。 3.3. 阅读与写作首先是我 2023 年的读书记录： 2023-03-09 - The Moon and Sixpence 你是要地上的六便士，还是要天上的月亮？ 2023-08-31 -How to Do Great Work - Paul Graham 黑客与画家一篇两万多字的长文，也算是一本小书了吧，干货满满。 2023-09-29 - 《被讨厌的勇气》 一本通过对话的形式讲述阿勒德心理学的书，这门心理学与现代科学心理学不同，它更偏向哲学。 2023-10-08 - 《叫魂：1768 年中国妖术大恐慌》 从 1768 年的叫魂案出发，分析了乾隆时期的中国社会的许多方面。比如因各种原因导致人口过度增长、人均资源减少、社会矛盾激化导致的普遍恐慌，以及官僚君主制的运作机制、存在的问题。此书以史为鉴，能帮助我们理解现代中国政治与中国社会的一些基本问题。 2023-10-17 - 《分心不是我的错》 介绍 ADHD 的一本书，列举了很多 ADHD 案例，也给出了诊疗建议，对我相当有用！ 以及一些未读完的书： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig The Great Gatsby - 10/41 《复杂 - 梅拉尼 米歇尔》 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 2023 年我读的书籍数量不多，没达成每月读一本书的目标。而写作方面，算上这篇总结，今年我一共写了 9 篇博文，也没达成每月写一篇的目标。 但鉴于我今年写了一本挺受欢迎的小书NixOS \u0026 Flakes Book，它得到了 NixOS 社区诸多好评，还在 10 月份上了 Hacker News 热榜，甚至被官方文档nix.dev 列入推荐阅读，我姑且将今年「阅读与写作」这一项的评分定为「超出期待」！ 3.4. 其他 运动方面乏善可陈，穿越了一次东西冲海岸线，游了几次泳，12 月初晨跑了一周但因为是空腹跑， 胃炎给跑犯了，就没再跑了。体重全年都在 60kg 波动，没啥变化。 给我老爸全款买了我们全家的第一辆小轿车（自己没买，一是天天坐地铁用不到，二是我对车也缺乏兴趣）。 计划开始给父母约年度体检，待实施。 3.5. 总结生活上今年达成了这么多有意义的成就（确诊 ADHD、将更多精力花在关心家人上、参与公益活动、给家里买车等等），我给自己的评价当然是「优秀」。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:3:3","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#34-其他"},{"categories":["life","tech"],"content":" 3. 生活 3.1. 确诊 ADHD 以及治疗前文提了，我今年确诊了 ADHD，这是我今年最大的收获之一。它让我搞明白了，原来我一直存在的注意力问题，并不是什么人格缺陷或者意志力不够，而是一种有挺多人都存在的功能失调（disorder）， 可以通过药物等方式治疗缓解。 考虑到 ADHD 的遗传特性，跟妹妹、父母一番沟通后，带我妹来深圳看医生，确诊了 ADHD 以及抑郁症。说真的，我一直知道我妹妹情绪比较敏感，但从没想过是因为抑郁症。 这之后，除了工作外，我大部分的精力都花在了关心家人、运动、学习心理学等事情上。这是一个非常明显的转变，我跟我妹的关系更好了，另外持续好几个月的治疗跟每个月带她在深圳散散心，她的情绪也有了很大的改善。 3.2. 参与公益活动因缘际会跟 @Manjusaka_Lee 以及另一位朋友聊到了公益， 当时在工作上缺乏动力，想在其他的事情上找找感觉，公益本身也是一件很有意义的事情，那段时间学习了解了许多公益资料，参加了一些公益会展与捐款活动，还一度考虑做志愿者。 今年算是迈出了参与公益活动的第一步，这拓展了我的视野，让我更加了解了社会。在学习了解公益期间也结识了一些志同道合的朋友，这也挺符合我去年给自己的期许——「认识更多有趣的人，见识下更宽广的世界」。 做事情，最难的就是从 0 到 1，因此今年的这一进展就显得尤为可贵，称得上是「优秀」。 3.3. 阅读与写作首先是我 2023 年的读书记录： 2023-03-09 - The Moon and Sixpence 你是要地上的六便士，还是要天上的月亮？ 2023-08-31 -How to Do Great Work - Paul Graham 黑客与画家一篇两万多字的长文，也算是一本小书了吧，干货满满。 2023-09-29 - 《被讨厌的勇气》 一本通过对话的形式讲述阿勒德心理学的书，这门心理学与现代科学心理学不同，它更偏向哲学。 2023-10-08 - 《叫魂：1768 年中国妖术大恐慌》 从 1768 年的叫魂案出发，分析了乾隆时期的中国社会的许多方面。比如因各种原因导致人口过度增长、人均资源减少、社会矛盾激化导致的普遍恐慌，以及官僚君主制的运作机制、存在的问题。此书以史为鉴，能帮助我们理解现代中国政治与中国社会的一些基本问题。 2023-10-17 - 《分心不是我的错》 介绍 ADHD 的一本书，列举了很多 ADHD 案例，也给出了诊疗建议，对我相当有用！ 以及一些未读完的书： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig The Great Gatsby - 10/41 《复杂 - 梅拉尼 米歇尔》 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 2023 年我读的书籍数量不多，没达成每月读一本书的目标。而写作方面，算上这篇总结，今年我一共写了 9 篇博文，也没达成每月写一篇的目标。 但鉴于我今年写了一本挺受欢迎的小书NixOS \u0026 Flakes Book，它得到了 NixOS 社区诸多好评，还在 10 月份上了 Hacker News 热榜，甚至被官方文档nix.dev 列入推荐阅读，我姑且将今年「阅读与写作」这一项的评分定为「超出期待」！ 3.4. 其他 运动方面乏善可陈，穿越了一次东西冲海岸线，游了几次泳，12 月初晨跑了一周但因为是空腹跑， 胃炎给跑犯了，就没再跑了。体重全年都在 60kg 波动，没啥变化。 给我老爸全款买了我们全家的第一辆小轿车（自己没买，一是天天坐地铁用不到，二是我对车也缺乏兴趣）。 计划开始给父母约年度体检，待实施。 3.5. 总结生活上今年达成了这么多有意义的成就（确诊 ADHD、将更多精力花在关心家人上、参与公益活动、给家里买车等等），我给自己的评价当然是「优秀」。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:3:3","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#35-总结"},{"categories":["life","tech"],"content":" 4. 各种站点的统计数据首先是我的博客站点 https://thiscute.world/ 的统计数据： thiscute.world - 2023 年 Google Analytics 访问统计 thiscute.world - 2023 年 Google Analytics 访问统计 - 按国家分类 thiscute.world - 2023 年 Google Search 统计数据 另外是我今年 6 月份新建的 NixOS 笔记站点 https://nixos-and-flakes.thiscute.world 的统计数据（国外读者相当多，这跟 stars 以及收到的赞助情况也比较匹配）： NixOS \u0026 Flakes Book - 2023 年 Google Analytics 访问统计 NixOS \u0026 Flakes Book - 2023 年 Google Analytics 访问统计 - 按国家分类 NixOS \u0026 Flakes Book - 2023 年 Google Search 访问统计 以及两个站点全年在 Vercel 上的流量统计（感谢 Vercel 每个月的 100G 免费流量，目前看白嫖阶段流量还有挺大增长空间哈哈）： Vercel - 2023 年流量统计 还有文章阅读排行统计： 2023 年文章阅读排行 此外，我今年在 Twitter(X) 上比较活跃，也新增了不少粉丝： 2023 年 Twitter 统计数据 最后，是一些赞助平台上收到的零花钱统计： 2023 年 buymeacoffee 收入统计 2023 年 patreon 收入统计 2023 年爱发电收入统计 另有加密货币 $50 没有好的统计页面，就不放截图了。以及部分国外读者希望使用我未使用的支付方式赞助，我比较懒没折腾了… ","date":"2023-12-31","objectID":"/posts/2023-summary/:3:4","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#4-各种站点的统计数据"},{"categories":["life","tech"],"content":" 2024 年展望我当下工作维持着不错的状态，业余兴趣不减，生活上也挺满意。所以其实对明年反而没啥特别的期望，保持现在的状态就挺好的（这大概就是「现充」吧 emmm） 但总不能因为这个就摆烂，还是要给自己定一些目标，能否达成就看缘分了。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:4:0","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#2024-年展望"},{"categories":["life","tech"],"content":" 1. 业余技术遵循兴趣优先的原则，简单列一下： 2023 年 AIGC 飞速发展，预期 2024 年很多新兴的 AIGC 技术将会落地到各种产品中，我也希望能玩一玩。不过貌似工作上就能玩得上，所以就不列在这里了 emmmm. 2023 年既然意外地点亮了 NixOS 跟 Neovim 这两项技能点，希望 2024 年能接着深入学习使用 NixOS 以及参与社区贡献。 操作系统（ARM64 + RISCV64） 2022 年看了一半《Linux/Unix 系统编程手册 - 上册》，2023 全年没动这本书，2024 年总该看完了吧… MIT6.S081 Operating System Engineering (Fall 2020) 之前定了个目标学一遍但完全没开始，2024 年继续… （低优先级）2024 年有机会的话用 Rust 语言整点活 （低优先级）2024 年在 EE 与嵌入式方面，也希望能更深入些，设计一点自己的板子玩玩，做点有意思的东西，比如无人机啊、智能机械臂啊啥的。 总结下主要就是三个学习目标：：搞搞 AIGC、学学操作系统、尝试更深入地使用 NixOS 以及参与社区贡献. ","date":"2023-12-31","objectID":"/posts/2023-summary/:4:1","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#1-业余技术-1"},{"categories":["life","tech"],"content":" 2. 业余生活生活上，2024 年希望能： 首先仍然是每年固定的目标：每月读一本书、写一篇博客。 新增的旅游目标：带家人至少出省旅游三次。 运动：2023 年意识到了身体健康的重要性，在 2024 年想多多运动，晨跑就是个不错的选择（但可别空腹晨跑，胃炎犯了很难受）。 学学心理学：要往「久病成医」的方向发展了 emmmm. 2024 年想入门个心理学，帮我更好地照顾自己、关心家人、处理人际关系。 音乐：今年买了个 Quest 3 VR 头显，它有个 AR 弹琴的游戏挺不错，希望 2023 年能借此学会些简单的钢琴曲。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:4:2","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#2-业余生活"},{"categories":["life","tech"],"content":" 3. 工作 这部分于 2024-01-13 新增 其实全文写下来，基本完全没提到我 2023 年的主要工作。主要还是因为前面提到的一些心态问题，以及工作部分相对而言没那么亮眼，所以没提。 但最近年终嘛，回顾了下我 2023 年下半年的工作，主要是在 infra 层面支撑 AIGC 团队，期间开开心心地折腾了挺多 Nvidia、PyTorch 之类的新鲜东西，也拿到了不错的成果。另一方面又了解到工作上 2024 年我会与 AIGC 团队更深入地合作，更深入地折腾 AIGC、Nvidia 生态、Pytorch 相关的东西。目前感觉还是挺有意思的，所以我工作上姑且也给自己定个目标吧：既学学新鲜热门的 AIGC 涨涨见识，又借此更好地支撑 AIGC 团队，Win Win! ","date":"2023-12-31","objectID":"/posts/2023-summary/:4:3","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#3-工作"},{"categories":["life","tech"],"content":" 结语2022 年的年终总结文末，我给自己 2023 年的期许是「认识更多有趣的人，见识下更宽广的世界」，感觉确实应验了。由衷感谢在 2023 年给我帮助与支持的亲人、朋友、同事，以及努力探索未知的我自己！ 那么在 2024 年，我希望自己能够「工作与业余爱好上稳中求进，生活上锻炼好身体、多关心家人」~ Carpe Diem. Seize The Day, Boys. Make Your Lives Extraordinary. – 《死亡诗社》 文末推荐一个年终回顾与展望的帮助手册，感觉设计得很好：https://yearcompass.com/cn/ ","date":"2023-12-31","objectID":"/posts/2023-summary/:5:0","series":["年终总结"],"tags":["总结"],"title":"我的 2023 - 认识更多有趣的人，见识更宽广的世界","uri":"/posts/2023-summary/#结语"},{"categories":["life"],"content":" 本文稍有点长，推荐配合歌曲《夜空中最亮的星——逃跑计划》食用。我曾在无数个白天夜晚，听着这首歌，想着自己的人生，书写本文时也不例外。 2023 年，按我能长命百岁来计算，我已经走过了四分之一的人生路。 如果要我用一句话总结我过去这四分之一的人生，我想用这句诗再合适不过了： 两岸猿声啼不住，轻舟已过万重山。 我想大部分人前四分之一的人生，主旋律都是求学，我也不例外。 我的求学之路并不顺利，小学初中时我不知道自己想要什么，高中时压力太大几乎退学，大学时我又因为自己的问题无法毕业。但是在工作后我反而逐渐建立起了自信心，就像是突然进入了康庄大道。 最近我又经历了许多，受到了一些启发，觉得到了一个合适的时机，因此写下这篇文章，既是记录我的过去，也同时思考下未来的路该怎么走。 这篇文章主要是写给我自己看的，但如果也能带给你一些启发，那就再好不过了。 ","date":"2023-08-19","objectID":"/posts/a-quarter-of-the-way-through-life/:0:0","series":["年终总结"],"tags":["总结"],"title":"两岸猿声啼不住，轻舟已过万重山——我的四分之一人生","uri":"/posts/a-quarter-of-the-way-through-life/#"},{"categories":["life"],"content":" 我的高中2015 年，我高三，那时候我是一个科幻迷，并且刚刚经历过一次退学风波。为什么会闹出这样的事情？简单的说就是因为我抗压能力差，高三巨大的压力压得我喘不过气来，我想要逃避。 当时班主任找我谈话，怎么劝都劝不动我，劝不动他就开始骂，把我骂个狗血淋头，这一骂把我给骂醒了，我因此得以坚持到高考结束。至今仍然相当感谢我的班主任王老师，他骂醒了我，让我不至于走上另一条更艰难的路。 但是即使如此，我还是没有任何动力去坚持备考。当时整个班级的学习强度都越来越高，只有我仿佛活在另一个世界。整个高三下学期，只有各种考试跟测验我会认真完成，其他时间我都在看各种与学习无关的书。 我读了很多的书。 我读刘慈欣的《三体》、山田正纪的《艾达》、保罗·巴奇加卢皮的《发条女孩》跟《拆船工》、以及《科幻世界》，我读川端康成的《雪国》跟《千只鹤》、村上春树的《当我谈跑步时我谈些什么》，读卡勒德·胡赛尼的《追风筝的人》、马尔克斯的《百年孤独》，读江南的《龙族》、余秋雨的《中国戏剧史》。因为压力太大，我甚至开始对哲学、禅宗感兴趣，我读了妙莉叶·芭贝里的《刺猬的优雅》、乔斯坦·贾德的《苏菲的世界》，我甚至开始读《心之道》、《学禅方便谭》、《新世界 : 灵性的觉醒》。 读了村上春树的《当我谈跑步时我谈些什么》后，我开始把跑步当成一种宣泄压力的方式，我经常晚自习时偷偷溜去操场跑步。跑了五圈、十圈、二十圈，跑到感觉脚下力重千钧，呼吸火辣辣地痛，这些痛苦与不适，让我忘记了高考的压力。 我还喜欢上了看星星，经常在晚上熄灯后，偷偷爬上宿舍天台，用手机的《星图》APP，寻找各种星宿、寻找牛郎织女、北斗七星、大角星、看可见卫星过境。 我考的是理科，高考结束后，我拿网上的答案随便估了个分，尽量往低了估，算出二本都不够的。但这也没啥，我自认只能做到这个程度，当时的想法就是尽人事，听天命。 然后是浑浑噩噩地等待高考成绩，中间也胡思乱想过一些要不要考虑复读的问题，但最终还是决定不复读了，因为我觉得我不可能再坚持一年了。 或许是因为我心态很平稳（心如死灰），考试时发挥得很好，成绩出来居然超过了一本线十多分。 之后就是填报志愿了，我不知道自己想学什么，可能是电子信息？毕竟我小时候挺喜欢捣鼓各种电子设备。 偶然想起在学校阅览室读杂志时，曾被科幻世界 2013 年 12 期里沖氏武彦的《在回声中重历》给打动——用耳朵“看见”世界实在是太奇妙了，我当时痴痴地幻想了好几天。 这样我开始考虑选择声学。 我从高中同桌推荐的《刀剑神域》开始接触日本的 ACG 文化，后来接触到初音未来和洛天依，就对歌声合成(singing synthesis)产生了很大的兴趣，仔细一想发现这也应该是声学的范畴，这使我坚定了我选择声学的想法。 家里父母都没学问，他们的意见就是听我的，于是我的第一志愿就填了安徽建筑大学的声学专业。 声学是一个非常冷门的专业，我很顺利地被录取了。 就这样，我开始了我错位的大学生活，并为我自己草率的选择付出了巨大的代价。 ","date":"2023-08-19","objectID":"/posts/a-quarter-of-the-way-through-life/:1:0","series":["年终总结"],"tags":["总结"],"title":"两岸猿声啼不住，轻舟已过万重山——我的四分之一人生","uri":"/posts/a-quarter-of-the-way-through-life/#我的高中"},{"categories":["life"],"content":" 我的大学我凭着自己粗浅的想象与一时热血，填报了声学专业，但现实不同于想象，我发现我并不喜欢声学专业。 一开始，到新的学校，我接触到的一切都让我觉得自己就像是刘姥姥进大观园，处处都是新鲜事物——山里的孩子出来，第一次发现大家都只讲普通话，第一次见识到平原的广阔，第一次见识到城市的繁华， 第一次见识到大学的自由。 但渐渐的问题就变得明显了，我发现学校教授的声学课程与我想象的完全不同，教的东西跟声学成像、声音合成几乎毫无关联，而且大学也并没有我想象的那般神圣无垢，它照样存在着各种各样的问题（比如一些形式主义、一些水课）。另一方面我刚进大学时，为了弥补自己高三的遗憾，如同苦行僧一般的学习，而这种状态完全无法持续。 我很快就出了问题，这从我以前写过的文章中可见一斑： 2017-03-07 - 我患上了阅读焦虑症 2017-02-06 - 忽而假末 另外大学期间我迷上了编程，开始自学各种编程技术，最开始是 C，后来有一次学校校友、在美国常春滕读天体物理学博士的季索清学长（现在已经是 Professor of Astrophysics 了）回校演讲，谈到了他们实验室现在都用 Python 了，Matlab 都没人用了，于是我开始学习 Python. 后来又跟着知乎上的推荐学习路线学习过 SICP、Linux C 编程等各种内容。 大学四年，学校对我帮助最大的，一是让我接触到了更大的世界，二是图书馆藏书丰富，尤其是计算机类书籍。四年时间我读了很多很多的书，学校的图书馆成了我最喜欢的地方。 但我始终无法平衡学业与编程，我开始彻底忽略学业。 问题愈演愈烈，大三时我尝试过申请休学，但是又被劝住了。到大四临近毕业时，我心理问题已经相当严重。尤其因为我性格还比较轴，觉得这个世界不应该是这样的，不想学的课程我一个字都不想看，相关的考试我要么缺考要么交白卷。 结果就是我完全无法毕业，觉得前途一片灰暗，见不到任何光亮，甚至感觉自己要得抑郁症，那是我这四分之一人生中最黑暗的时期。 某天跟在深圳工作的初中同学聊天，他说到深圳这方面的公司挺多的，建议我考虑下。 于是我买了张火车票直接就奔去了深圳，还写下了逃离我的大学，现在回想起来，当时的想法大概是，「做什么都好，总之再也不想继续待在这个对我而言暗无天日的地方了。」 在学校的经历让我对学校产生了极大的反感，我拉黑了学校的所有联系方式。这完全是我自己的问题， 我的导员对我很好，我在学校也有过许多美好的回忆，但我就是心理上无法接受再跟这个地方有任何联系。 工作后也有过很多朋友觉得我应该回去把学位拿了，每次我都会回答，如果能做到我也想，但我真的做不到，再在学校呆下去我真的要疯掉了。至今我仍然觉得这是我当初做的最正确的决定。 导员联系不上我，就让同学给我传话，又为我着想给我办理了延期，但后面的一年我也一门网课都没听过，又一年后，我的学历状态就变成了结业。 ","date":"2023-08-19","objectID":"/posts/a-quarter-of-the-way-through-life/:2:0","series":["年终总结"],"tags":["总结"],"title":"两岸猿声啼不住，轻舟已过万重山——我的四分之一人生","uri":"/posts/a-quarter-of-the-way-through-life/#我的大学"},{"categories":["life"],"content":" 我的打工人生活我初中同学是读书不好的那种，高中就直接读了武术学校，在深圳教跆拳道。我跟他一张床挤着睡了三天（很感谢他愿意为我做到这个程度），期间投了几十份简历。 我大学时自学涉猎过 Python Java Linux，因此后端、运维都有投，但只收到一家 Python 自动化运维的面试邀请，面试对我而言难度不高，很轻易就通过了，这家小创业公司也不看学历。不知该说是运气好还是不好，我之后再也没收到过任何面试邀请。 就这样，我开始了我的打工人生活。 我在上大学时虽然自学了许多计算机与编程知识，但是那个时候全是我自己单干，现实中几乎没接触过其他做 IT 的人。另一方面大学读得一团糟，因此刚工作时我相当不自信甚至可以说是自卑。在工作后，我发现周围都是同样做技术的人，这种一起解决技术问题、完成技术目标的感觉是我从未体验过的，另一方面我的工作成果也获得了大家的认可，这让我在工作期间一直非常开心（虽然工资真的相当低，而且加班很严重。毕竟我没学历，当时能找到个工作都谢天谢地了）。 在这第一份正式工作期间，我学会了 Kubernetes、Istio、Docker、阿里云、Terraform/Pulumi、Argo Workflows 等云原生技术，Jenkins 等自动化运维技术，写了很多 Python 代码锻炼了自己的代码能力，还折腾了洋垃圾服务器、组装了公司办公电脑，工作能力也得到了领导同事的认可。 我很感谢这家公司，它是我 IT 生涯的起点，在这里我学到了令我足以在 IT 行业立足的技术，也建立起了技术自信。但它给的工资实在太低了，还加班严重、画饼充饥，很多东西都不规范，处处透露着小作坊的气息，因此在工作了一年零八个月后，我决定离职。 离职后我休息了一个多月，给自己放了一个长假，期间也写了我在创业公司做技术一年多的一点体会 跟 脚踏实地，仰望星空。现在看来，当时这些文章也是写得纯真又幼稚。或许再过几年回头看，我会觉得现在这篇文章也纯真又幼稚？那就再好不过了——我对这个世界的认知又更正确了一点，我得以再次优化指导我行动的「人生模型」，避免在未来因此造成更大的问题（笑 之后我开始找工作。我当时很自信地（其实也有点忐忑）在简历上写下了「本人因学分不足，未能取得学位」，实话说，这句话帮我省去了不少无意义的沟通，大概 50% 的公司在确认了情况后会直接忽略我。即使如此，我仍然拿到好几份 offer，也得以进入现在这家公司，职位是 SRE。 在新公司这几年的经历相当丰富，我简单总结如下： 2021 年： 年初的期许：拆破玉笼飞彩凤，顿开金锁走蛟龙。 工作上我的工资相比之前翻了数倍，工作环境也好了太多。同事中不乏 985 211、海归甚至清北的大佬（同事的 title 可能代表不了什么，但这确实让我很有成就感），跟他们学到了许多东西， 熟悉了全新的工作文化（OKR 等），接触到了拥有数千万日活、十多万 QPS 的云上系统架构，并且完成了其中核心组件 K8s 集群的运维升级工作，获得了许多牛逼同事与领导对我专业能力的认可。 业余生活上被同事带着第一次海边冲浪、烧烤，又开始玩轮滑，还学上了泡茶。 2021 年年终总结 2022 年： 年初的期许：更上一层楼 工作上一是通过了职级晋升，不再是 SRE 萌新了。二是在流量链路上做了很多工作，帮公司省了很多钱，还因此拿了一个 S（公司最高绩效），年终奖也很丰厚。 业余生活上做出了更多的探索，学了很多新技术，认识了很多有趣的人（0xffff 社区），还坚持学了好几个月的英语。 2022 年年终总结 2023 年（虽然还没过完）： 年初的期许：认识更多有趣的人，见识下更宽广的世界 今年在工作上没有做出很好的成绩，马马虎虎。我更多的把时间投入到了业余爱好上。 业余生活上，我又折腾了许多新技术（MCU 开发、各种 ARM/RISCV 开发板、Homelab、NixOS）， 并且因此认识了许多嵌入式领域的大佬。在折腾 NixOS 的过程中我做的开源项目、入门指南更是获得了国内外社区的大量好评，认识了好几位外国朋友，还收到了一些外国读者的打赏。这完全契合了我年初给自己的期许。 我今年写 NixOS 入门指南的经历 两份工作，四年多的时间，我的经历很难通过上面这只言片语就完全概括，中间当然也有过许多挣扎、迷茫、许多心酸苦辣。但就得到的结果来说，在第一家公司我学到了很多很多，接着从进入新公司开始到现在，我每一年年初给自己的期许，也都能如约兑现。 现在，我相信在深圳这四年只是我上升期的第一步，这一步我完成了自信心的构建、眼界的开拓、基础技术能力的积累，也攒下了能让我衣食无忧好几年的少许财富（就在今天，还给我爸全款买了家里的第一辆小轿车，一家子都很开心）。下一个四年或者五年，我一定能收获更多，就像几年前我第一份工作刚结束，好朋友 @Likenttt 送我的诗一样： 拆破玉笼飞彩凤，顿开金锁走蛟龙。 人生还很长，我想一个阶段的失败，可能只是在提前优化我对世界的认知，帮我提前发现并解决我「人生模型」中隐藏的问题，为下一个阶段的成功做铺垫。 ","date":"2023-08-19","objectID":"/posts/a-quarter-of-the-way-through-life/:3:0","series":["年终总结"],"tags":["总结"],"title":"两岸猿声啼不住，轻舟已过万重山——我的四分之一人生","uri":"/posts/a-quarter-of-the-way-through-life/#我的打工人生活"},{"categories":["life"],"content":" 我的未来我过去的这四分之一人生，很难复刻，其中有太多的莽撞、理想主义让我饱尝苦果，其中的转折点也有许多运气与机遇的成分。但我的未来正是构建在此之上。 有的人喜欢稳定，但当今大世，AI 飞速发展、中美摩擦不断、欧洲也各种难民、党争问题，全世界都在变化，真的有什么绝对稳定的东西可以依靠么？世事无常，纷繁复杂，我能做的，是在接受这份无常的同时，仍然能维持自驱力，在这个世界中探索出自己的一片天地。 最近在苏洋的折腾群，从大家的自我介绍里看到了形形色色的人生，年龄从 20+ 到 50+，学历从高中专科到博士，职业从软硬件到电工、灯光师、全职公益人、见证纸媒体消亡的电脑报前编辑，生活地点也遍布全球。我甚至还发现最近跟我深入交流过 NixOS 相关技术问题的群友，在十多年前就做过我当时使用或者接触过的产品，而那个时候我还在上初中甚至小学，这让我感到十分震撼。其中大部群友的年龄都比我大许多，他们的经历给了我很大的启发，让我意识到我往后 3/4 的人生还有很多的可能。 另外随着我近两年逐渐在自己的业余爱好上有所建树，我也越来越觉得工作作为养家糊口的手段，确实很重要，但它只是生活的一部分，在工作之外我还有许多可以做的事。 我一直在践行「兴趣是最好的老师」，虽然因为太过兴趣驱动以及一些其他原因导致我大学读得比较糟糕，但是能让我达成现在的成就的同样是兴趣，让我最近几个月接触 NixOS 并且获得了众多好评与感谢的同样是兴趣。最近有推友分享了一篇很实用的长文How to Do Great Work - Paul Graham（中译【实用指南】Paul Graham 两万字新文：如何取得杰出成就 ）， 我读了个开头，还没看完，但是发觉它很契合我，它与我的经历能相互印证，也对我未来的行动很有指导意义。其中我目前读到印象最深刻的一句话就是： The three most powerful motives are curiosity, delight, and the desire to do something impressive. Sometimes they converge, and that combination is the most powerful of all. 三个最强大的内在动机是好奇心、快乐和做出令人印象深刻的事情的欲望，当它们汇聚在一起时，会成为最强大的组合。 写这篇文章花了我一整天时间，第二天我又做了不少修修补补的工作。写作时我回想了很多的东西，也翻阅了我自己过往的各种日记、随笔，往事历历在目。 我甚至有一点使命感，能感觉到这是一件相当有意义的事情。 文章写完后，我又反复读了好多遍，越读我越喜欢它，觉得它会成为我的一个人生里程碑。这个里程碑不只有纪念意义，它更是对我未来方向的指引。迈过这个里程碑，我对仍旧未知的未来，有了更多的期待。 ","date":"2023-08-19","objectID":"/posts/a-quarter-of-the-way-through-life/:4:0","series":["年终总结"],"tags":["总结"],"title":"两岸猿声啼不住，轻舟已过万重山——我的四分之一人生","uri":"/posts/a-quarter-of-the-way-through-life/#我的未来"},{"categories":["life"],"content":" 后记因着今天发现我认识的网友中就有人在深圳做了多年全职公益人，我想起了去年 8 月份看过的《在峡江的转弯处——陈行甲人生笔记》，作者现在也在深圳做公益。我又把书翻出来略读了一遍，很有些感触。 偶尔回忆起自己当初的自卑、迷茫、挣扎，我会意识到现在的我虽然不再自卑，但仍然会迷茫、挣扎， 怀疑自己的想法是否正确。但我不觉得这是坏事，这正说明我走在了正确的路上。经常会有人说要「走出舒适区」，有这种迷茫、挣扎的感觉，说明我正在这么做。 正因为曾经经历过人生的灰暗时刻，所以我更希望自己能记住，这是一个可爱的世界，这正是我博客域名 thiscute.world 的由来，今后我也会牢记这一点。 其实这段人生是最美好的，以后可能没有这么好的日子了。 —— v2ex 网友的评论，留做警示。最近几年过得一帆风顺，我确实是有点飘了，应该「居安思危」。 文章的最后，我想我应该再次感谢，感谢这一路走来，帮助过我的老师、同学、朋友，认可我工作的同事跟领导，鼓励过我的家人、朋友、网友，感谢你们！没有你们，我可能早就迷失了方向，更不会有现在的成绩了。 四年多前，我从学校不辞而别，我欠我的导员圆圆姐一个道歉，一份感谢，一个交代。这次，我也会一并补上已经补上了： 人是社会性动物，我们互相成就。我今后也会争取交到更多有趣的朋友，认识更多有趣的人，见识这个宽广、可爱的世界。 ","date":"2023-08-19","objectID":"/posts/a-quarter-of-the-way-through-life/:5:0","series":["年终总结"],"tags":["总结"],"title":"两岸猿声啼不住，轻舟已过万重山——我的四分之一人生","uri":"/posts/a-quarter-of-the-way-through-life/#后记"},{"categories":["life"],"content":" 评论区我也在其他平台分享了这篇文章，其中评论有些不礼貌的 judge（直接无视掉就好，有的人这辈子也就剩这点东西了），但也不乏好的内容。其中许多的留言相当治愈，让人心里暖暖的，有些留言让我会心一笑，这些内容都让我觉得，能够把文章分享到这些平台，让大家看到，真的是太好了！在跟评论区一些朋友交流时，也碰撞出了许多思想的火花，这也让我相当开心。 为着让大家都能看到其中好的内容，我把它们都列在这里。 0xffff 社区: 0xffff 评论区真的有很多真知灼见，强烈推荐一读！ v2ex: 有些很治愈的评论，也有个别不好的，我觉得这些评论都挺有意思。 博客园 关于这些不好的评论（譬如 v2ex 上有人评论我是在「无病呻吟」，还挺多人点赞。另外 Twitter 上有人发推喷我「谁 TM 在意你的人生怎么样」，我觉得都挺好笑的），我想在这里为一些因此不快意的读者解释下。我其实感觉到，在我自己的体系能够自洽后，看待这类评论时我更像是一个旁观者。我甚至完全不觉得这些评论冒犯了我（笑）。这样的深入剖析自我的文章，肯定会刺痛到一些被生活磨去了棱角，迷失了自己的人。这种人别说跟我共情了，他们甚至下意识就要攻击我、反驳我。 以前看过朋友推荐的一本小说，里面有一句话我印象很深刻：「正如纯氧对生物有害，毫无保留的真相，只会把人的精神击溃。一比五（四）的氧与氮，才是可供呼吸的空气。同样，呼吸着以戏言稀释的少量真实，人才能维持健全的心。」 对这种被世界的真相击溃的人，我没啥好说的。以铜为镜，可以正衣冠；以人为镜，可以明得失。这些评论在警示我，不要成为这样的人， 看清世界的真相后，仍要热爱生活。 如果说体系自洽有点不好懂，那我可以用个简单的类比来说明这一点：面对这种品头论足，我觉得我简直是在对牛弹琴。牛的哞哞叫会让我感到不开心么？我有必要跟牛解释我弹的曲子么？它听不懂关我何事？ ","date":"2023-08-19","objectID":"/posts/a-quarter-of-the-way-through-life/:6:0","series":["年终总结"],"tags":["总结"],"title":"两岸猿声啼不住，轻舟已过万重山——我的四分之一人生","uri":"/posts/a-quarter-of-the-way-through-life/#评论区"},{"categories":["tech","life"],"content":"我折腾过许多的小众技术，而今年新折腾的主要有 NixOS、窗口管理器 i3 / hyprland、以及 Neovim，其中 NixOS 我甚至折腾到了一个新境界——出了一本帮助新手入门的中英双语开源书籍nixos-and-flakes-book，还搞了好几个 NixOS 相关的开源项目（比如nix-darwin-kickstarter 跟ryan4yin/nix-config），都收到了许多好评。 结合我自己折腾这些小众技术的经历，以及我经常被问到的问题（为什么你选择用NixOS / Neovim /小鹤音形中文输入法？它有什么好处？它真的能提升效率吗？等等），我想在这里简单谈谈我对它们的看法。 ","date":"2023-08-01","objectID":"/posts/why-i-choose-niche-products/:0:0","series":[],"tags":["Vim","Neovim","VSCode","Editor","IDE","Linux","中文输入法"],"title":"为什么我折腾这些小众技术？","uri":"/posts/why-i-choose-niche-products/#"},{"categories":["tech","life"],"content":" 什么是小众技术？小众，是相对于大众而言的。小众技术，指在该领域中用户占比较相对较小的技术。 基于这样的定义，我可以列举出我接触过的不同领域的一些小众技术： 领域 小众技术 大众技术 编辑器 Neovim、Emacs VSCode、PyCharm、IDEA 中文输入方案 双拼、小鹤音形、五笔、二笔、郑码、灵形速影 智能拼音 Linux 操作系统 NixOS、Gentoo、Arch Linux Ubuntu、Fedora 窗口管理器 i3、hyprland KDE、GNOME 大多数人在使用这些领域的技术时，都会选择大众技术，因为它们的入门门槛低，使用起来也比较方便。我曾经也是这大多数人之一，但是我渐渐发现，这些小众技术也有它们的优势，所以我开始尝试使用它们，并逐渐过渡到了它们。 ","date":"2023-08-01","objectID":"/posts/why-i-choose-niche-products/:1:0","series":[],"tags":["Vim","Neovim","VSCode","Editor","IDE","Linux","中文输入法"],"title":"为什么我折腾这些小众技术？","uri":"/posts/why-i-choose-niche-products/#什么是小众技术"},{"categories":["tech","life"],"content":" 这些小众技术有什么特点？小众技术显然得拥有一些优势，才能吸引到一部分用户，让这些用户选择它们而不是大众技术。 从我个人的使用经验来看，我用过的这些小众技术，具有一些比较明显的共同特征。 首先是它们共同的劣势：入门门槛更高，入门阶段需要花费更多的时间去学习、熟悉。 这就过滤掉了大部分用户，只有那些喜欢折腾、喜欢挑战的人才会去尝试这些小众技术。 比如说五笔输入法，它们的入门门槛很高，需要花费大量的时间去记忆它的键位编排、去练习，前期的输入体验会跌到谷底。要想达到你曾经智能拼音的输入速度，感觉至少得每天练习 1 个小时，持续一个月（这很可能还不够）。 其他形码输入法也是一样，我用的小鹤音形算是一个折衷的选择，它的入门门槛比五笔低一些，学会后也能获得类似五笔的输入体验。 再说说它们共同的优势： 定制程度高：用户可以根据自己的需求，自由地定制各种功能。 强烈的掌控感、绝佳的使用体验：高度的自定义，让用户感觉到自己在使用这些技术的过程中，能够完全掌控一切，从而带来绝佳的使用体验。 用户黏性高、社区活跃：用户在使用这些技术的过程中，会不断地去探索、去学习、去定制， 这会让用户对它们产生强烈的归属感。 也因为上面这些原因，用户一旦成功入门某项小众技术（比如说形码输入法、Neovim/Emacs 编辑器），就很难再退回到曾经的大众方案——他们会发现曾经的大众方案用起来，各种不顺手、不爽快。 ","date":"2023-08-01","objectID":"/posts/why-i-choose-niche-products/:2:0","series":[],"tags":["Vim","Neovim","VSCode","Editor","IDE","Linux","中文输入法"],"title":"为什么我折腾这些小众技术？","uri":"/posts/why-i-choose-niche-products/#这些小众技术有什么特点"},{"categories":["tech","life"],"content":" 我为什么折腾这些小众技术？我折腾过许多小众技术，而原因中最大的一部分，应该是好奇心。但好奇心只能让我去尝试，让我留下来的，是它们优秀的使用体验。 比如说最近折腾的 Neovim 编辑器、Hyprland 窗口管理器，让我留下来继续使用它们的原因，一是 Neovim 跟 Hyprland 配置好了之后，真的很漂亮！而且 Neovim 速度真的超快、太快了！一些从没深度体验过 Neovim 的 VSCode / IDEA 用户可能会觉得这种快不过如此，但是一旦你真的体验过，就会发现这种快真的很爽，就像流浪地球 2 中图恒宇的感叹一样（550W 太快了！这速度太快了！） 二是实际入门后，发现它们用起来很爽快，基于键盘的交互，能带给我形码输入法的那种掌控感、流畅感（优雅，太优雅了 hhh）。 我的 NixOS + Hyprland 桌面 我的 Neovim 编辑器 而我折腾并且爱上 NixOS，也是基于类似的原因。拥有声明式、可复现（一致的运行环境）、OS as Code 等这些特点的 NixOS，对于本运维狗而言，真就是理想中的样子，这让我迫不及待地想要使用它，即使发现了问题也希望能尽快完善它，使它能够适用于更多的场景。 前两天在 4chan 上看到某外国网友的这么一段评论（虽然言词有点偏激，但我还真有点认同…）： Completely and utterly unacceptable. Imagine having a tool that can’t even properly undo an operation and then relying on it to manage an operating system.apt, pip, pm, rpm, pacman, whatever are all a mad fucking joke. ","date":"2023-08-01","objectID":"/posts/why-i-choose-niche-products/:3:0","series":[],"tags":["Vim","Neovim","VSCode","Editor","IDE","Linux","中文输入法"],"title":"为什么我折腾这些小众技术？","uri":"/posts/why-i-choose-niche-products/#我为什么折腾这些小众技术"},{"categories":["tech","life"],"content":" 小众工具或技术能提升效率吗？有许多人说，Neovim 编辑器、i3 窗口管理器、形码输入法等这些小众工具或技术，能提升效率，我觉得这是一个误区。相反，其中许多工具或技术，实际上是一个时间销金窟，你会被自己的兴趣驱使着去不断探索它们的边界、调整它的配置使其更契合自己的需求。这导致至少前面较长一段上升期，这些投入的时间会比你效率提升所省下的时间多得多。 所以说到底，想用这些技术来提升效率啥的还是不用想了。它能提升你的效率，但是比较有限，除非你写代码/文档的效率是受限于你的手速 emmm 当然也有些特殊场景，比如说有的人需要经常输入些生僻字，这时候智能拼音就比较鸡肋了，五笔等形码输入法就确实能大大提升输入效率。 或者有人会说，完全熟悉后，vim/emacs 能使你更容易进入心流状态？这个也很难说吧。 ","date":"2023-08-01","objectID":"/posts/why-i-choose-niche-products/:4:0","series":[],"tags":["Vim","Neovim","VSCode","Editor","IDE","Linux","中文输入法"],"title":"为什么我折腾这些小众技术？","uri":"/posts/why-i-choose-niche-products/#小众工具或技术能提升效率吗"},{"categories":["tech","life"],"content":" 那折腾这些东西，到底有什么好处？如果从很功利的角度看的话，确实就没啥好处，就跟打游戏一样，单纯在消遣时光而已。 要说跟做些无聊的事消遣时光有啥区别的话，大概就是还确实能获得点有用的东西。比如我，遇到 AstroNvim 的 bug ，会提 PR 给上游仓库。发现 NixOS 的文档很糟糕，我直接自己写文档并分享出来。发现 NixOS 缺少对我手头某块开发板的支持，我会自己尝试移植。啥时候发现某工具缺少自己想要的功能，我也可能直接自己写一个。 这些折腾过程中获得的经验、创建的开源项目、在上游仓库中留下的 PR 、在社区中收获的感谢，感觉都是有价值的。它不一定有啥业务价值，但是它好玩啊，还能交到朋友，帮到别人，在开源社区留下自己的痕迹，这不是很有意思么？ Linus 最开始写 Linux， 也只是为了好玩（Just For Fun）. ","date":"2023-08-01","objectID":"/posts/why-i-choose-niche-products/:5:0","series":[],"tags":["Vim","Neovim","VSCode","Editor","IDE","Linux","中文输入法"],"title":"为什么我折腾这些小众技术？","uri":"/posts/why-i-choose-niche-products/#那折腾这些东西到底有什么好处"},{"categories":["tech","life"],"content":" 结语你展望人生的时候，不可能把这些点连起来；只有当你回顾人生的时候，才能发现它们之间的联系。所以你必须有信心，相信这些点总会以某种方式，对你的未来产生影响。你必须相信一些事情——你的勇气、命运、人生、缘分等等。这样做从未令我失望，反而决定了我人生中所有与众不同之处。 Stay Hungry. Stay Foolish. ——You’ve got to find what you love, by Steve Jobs, CEO of Apple Computer ","date":"2023-08-01","objectID":"/posts/why-i-choose-niche-products/:6:0","series":[],"tags":["Vim","Neovim","VSCode","Editor","IDE","Linux","中文输入法"],"title":"为什么我折腾这些小众技术？","uri":"/posts/why-i-choose-niche-products/#结语"},{"categories":["tech","life"],"content":" 评论区文末附上来自其他论坛的评论，其中不乏一些有趣的观点： 0xffff.one: https://0xffff.one/d/1595-wei-shen-me-wo-zhe-teng-zhei-xie v2ex: https://www.v2ex.com/t/961562 ","date":"2023-08-01","objectID":"/posts/why-i-choose-niche-products/:7:0","series":[],"tags":["Vim","Neovim","VSCode","Editor","IDE","Linux","中文输入法"],"title":"为什么我折腾这些小众技术？","uri":"/posts/why-i-choose-niche-products/#评论区"},{"categories":["tech"],"content":" 2024-08-30 更新：我已经用 aerospace 替代了 yabai + shkd，体验好了太多，而且也不用关掉 SIP 啥的，强烈推荐一用。 2024-01-28 更新：换了新 Macbook Pro 之后，我又重新把 yabai 装上了，目前体验还不错，比 23 年好了不少。另外我新的配置完全基于 nix-darwin 部署，内容也有些改动，有兴趣的可以看看：ryan4yin/nix-config/darwin/wm 附上 nix-darwin 新手起步模板ryan4yin/nix-darwin-kickstarter 在 Linux 上用了一段时间 i3wm 后，我就有点忍受不了工作电脑的桌面环境了，公司给配的是 Macbook Pro 2020，一番查找发现 yabai 比较符合我的需求，于是开始了折腾之旅。 ","date":"2023-05-22","objectID":"/posts/macos-window-manager-yabai-usage/:0:0","series":[],"tags":["MacOS","窗口管理器","Window Manager","yabai"],"title":"MacOS 窗口管理器 yabai 玩耍笔记","uri":"/posts/macos-window-manager-yabai-usage/#"},{"categories":["tech"],"content":" 使用体验总结我的电脑配置为 Macbook Pro 2020，i5 + 16G RAM + 512G Disk，性能尚可。 一句话总结：体验还不错，但是还不太成熟，Bug 比较多，而且有点吃性能，安装 yabai 后偶尔就会卡顿一下。 自动分屏 + 快捷键自动调整窗口的体验还是很舒服的，劝退我的主要是如下这些问题： 对有些软件，比如企业微信、微信、QQ，自动分屏功能不太行，会出现窗口错位。 如下两个问题逼着我一会儿进入全屏模式，一会儿又要退出全屏，简直离谱。 全屏下 Chrome 搜索框下方的提示栏被会 Chrome 本身遮挡，必须退出全屏功能才能看到。 非全屏下，Chrome 页面中的输入框「自动填充」功能会被 Chrome 遮挡，必须进入全屏模式才能看到… 在右键修改 Firefox Bookmark 中标签时，弹出的修改菜单会被 Bookmark 收藏夹本身的弹窗遮挡，导致有些选项无法点击到。 开始使用 yabai 后，系统经常性地卡顿，或者风扇狂转，说明这玩意儿有点吃性能。 ","date":"2023-05-22","objectID":"/posts/macos-window-manager-yabai-usage/:1:0","series":[],"tags":["MacOS","窗口管理器","Window Manager","yabai"],"title":"MacOS 窗口管理器 yabai 玩耍笔记","uri":"/posts/macos-window-manager-yabai-usage/#使用体验总结"},{"categories":["tech"],"content":" 安装流程首先参考这篇官方 WikiDisabling System Integrity Protection 关闭 SIP，然后参照如下流程安装 yabai 与 skhd。 shell # 安装yabai brew install koekeishiya/formulae/yabai sudo yabai --install-sa # 启动yabai 这时候需要授权辅助功能 brew services start yabai # 安装skhd brew install koekeishiya/formulae/skhd # 启动skhd 这时候需要授权辅助功能 brew services start skhd ########### 为 yabai 添加 sudo 权限 ########### sudo yabai --load-sa sudo visudo -f /private/etc/sudoers.d/yabai # 然后输入以下内容 其中 \u003cuser\u003e 修改为当前 mac 的用户名 # input the line below into the file you are editing. # replace \u003cyabai\u003e with the path to the yabai binary (output of: which yabai). # replace \u003cuser\u003e with your username (output of: whoami). # replace \u003chash\u003e with the sha256 hash of the yabai binary (output of: shasum -a 256 $(which yabai)). # this hash must be updated manually after running brew upgrade. \u003cuser\u003e ALL=(root) NOPASSWD: sha256:\u003chash\u003e \u003cyabai\u003e --load-sa 上面就完成了安装流程，但是到这里还不能使用，还需要为 skhd 与 yabai 添加配置文件，并添加自定义配置。 shell # 创建yabai配置文件 touch ~/.yabairc chmod +x ~/.yabairc # 创建skhd配置文件 touch ~/.skhdrc chmod +x ~/.skhdrc # 之后在 ~/.yabairc 中添加以下命令 cat \u003c\u003cEOF \u003e ~/.yabairc #!/usr/bin/env sh # wiki 要求在配置最前面加这个，看起来是跟 sudo 权限相关的东西 sudo yabai --load-sa yabai -m signal --add event=dock_did_restart action=\"sudo yabai --load-sa\" EOF ","date":"2023-05-22","objectID":"/posts/macos-window-manager-yabai-usage/:2:0","series":[],"tags":["MacOS","窗口管理器","Window Manager","yabai"],"title":"MacOS 窗口管理器 yabai 玩耍笔记","uri":"/posts/macos-window-manager-yabai-usage/#安装流程"},{"categories":["tech"],"content":" 自定义 skhd 与 yabai 配置这里配置的目标是，尽量与 i3wm 的默认快捷键保持一致，因为我在家用的是 Linux，只有办公电脑是 Mac. 我目前的 ~/.yabairc，它用于配置 yabai 的各种行为： shell #!/usr/bin/env sh # wiki 要求在配置最前面加这个，看起来是跟 sudo 权限相关的东西 sudo yabai --load-sa yabai -m signal --add event=dock_did_restart action=\"sudo yabai --load-sa\" ## 输出 debug 日志，出问题时方便排查 yabai -m config debug_output on # 窗口平铺 yabai -m space --layout bsp # 默认拆分规则 first_child second_child yabai -m config window_placement second_child # 窗口间距设置 yabai -m config top_padding 10 yabai -m config bottom_padding 10 yabai -m config left_padding 10 yabai -m config right_padding 10 yabai -m config window_gap 10 # 自动平衡所有窗口始终占据相同的空间 yabai -m config auto_balance off # 如果禁用自动平衡，此项属性定义的是新窗口占用的空间量。0.5意为旧窗口占用50% yabai -m config split_ratio 0.50 # 鼠标修饰键 意思就是按着这个键就可以使用鼠标单独修改窗口大小了 yabai -m config mouse_modifier ctrl # ctrl + 鼠标左键 移动窗口 yabai -m config mouse_action1 move # ctrl + 鼠标右键 调整窗口大小 yabai -m config mouse_action2 resize # 焦点跟随鼠标 默认off: 关闭 autoraise:自动提升 autofocus: 自动对焦 yabai -m config focus_follows_mouse autofocus # 设置鼠标是否跟随当前活动窗口 默认 off: 关闭 on: 开启 yabai -m config mouse_follows_focus on # 浮动窗口问题在顶部 yabai -m config window_topmost on # 修改窗口阴影 on: 打开 off: 关闭 float: 只显示浮动窗口的阴影 yabai -m config window_shadow float # 窗口透明度设置 yabai -m config window_opacity on # 配置活动窗口不透明度 yabai -m config active_window_opacity 0.98 yabai -m config normal_window_opacity 0.9 yabai -m config window_opacity_duration 0.0 # 在所有显示器上的每个空间顶部添加 0 填充 底部添加 0 填充 yabai -m config external_bar all:0:0 # ================================ 规则 ================================ # 打开系统偏好设置，不使用平铺模式 yabai -m rule --add app=\"^系统偏好设置$\" manage=off yabai -m rule --add app=\"^提醒事项$\" manage=off yabai -m rule --add app=\"^关于本机$\" manage=off echo \"yabai configuration loaded..\" 再就是 ~/.skhdrc，它负责配置各种快捷键，如下是我的配置: shell # 配置语法 : \u003cmodifier\u003e - \u003ckey\u003e : \u003ccommand\u003e # modifier 可以是单个键比如 cmd, alt, ctrl, 也可以是组合键比如 ctrl + shift, ctrl + alt # ================================ 打开终端 ================================ # 启动终端 cmd - return : open -a iTerm # 关闭当前窗口，这个不需要加，macOS 默认是 cmd + q，我 Linux 也这么设置的 # ================================ 窗口设置 ================================ # =============== 为了避免快捷键冲突改用了 ctrl 作为 modifier ================= # ctrl + e 切换为平铺模式 ctrl - e : yabai -m space --layout bsp # ctrl + s 切换为堆叠模式 ctrl - s : yabai -m space --layout stack # 浮动/不浮动窗口 float ctrl - f : yabai -m window --toggle float # ================================ 多桌面配置 ================================ # 创建一个新桌面，并把当前活动的窗口发送到新桌面，并且自动跳转到新桌面. 需要 jq 支持 brew install jq shift + cmd - n : yabai -m space --create \u0026\u0026 index=\"$(yabai -m query --spaces --display | jq '.| length')\" \u0026\u0026 yabai -m window --space \"${index}\" \u0026\u0026 yabai -m space --focus \"${index}\" \u0026\u0026 yabai -m space --layout bsp # 在 stack 模式下通过方向键切换窗口 ctrl - down : yabai -m window --focus stack.next || yabai -m window --focus south ctrl - up : yabai -m window --focus stack.prev || yabai -m window --focus north # 在 bsp 模式下通过方向键切换窗口 cmd - left : yabai -m window --focus west cmd - right : yabai -m window --focus east # 在 9 个桌面之间切换 ctrl - 1 : yabai -m space --focus 1 ctrl - 2 : yabai -m space --focus 2 ctrl - 3 : yabai -m space --focus 3 ctrl - 4 : yabai -m space --focus 4 ctrl - 5 : yabai -m space --focus 5 ctrl - 6 : yabai -m space --focus 6 ctrl - 7 : yabai -m space --focus 7 ctrl - 8 : yabai -m space --focus 8 ctrl - 9 : yabai -m space --focus 9 # 将窗口发送到某个其他桌面 ctrl + shift - 1 : yabai -m window --space 1 ctrl + shift - 2 : yabai -m window --space 2 ctrl + shift - 3 : yabai -m window --space 3 ctrl + shift - 4 : yabai -m window --space 4 ctrl + shift - 5 : yabai -m window --space 5 ctrl + shift - 6 : yabai -m window --space 6 ctrl + shift - 7 : yabai -m window --space 7 ctrl + shift - 8 : yabai -m window --space 8 ctrl + shift - 9 : yabai -m window --space 9 配置加好后重启 yabai 与 skhd: shell brew services restart yabai brew services restart skhd 现在就可以随便打开几个程序试试，正常情况下 yabai 会自动帮你分屏。再尝试下添加好的这些快捷键，看看是否生效。 ","date":"2023-05-22","objectID":"/posts/macos-window-manager-yabai-usage/:3:0","series":[],"tags":["MacOS","窗口管理器","Window Manager","yabai"],"title":"MacOS 窗口管理器 yabai 玩耍笔记","uri":"/posts/macos-window-manager-yabai-usage/#自定义-skhd-与-yabai-配置"},{"categories":["tech"],"content":" 问题排查","date":"2023-05-22","objectID":"/posts/macos-window-manager-yabai-usage/:4:0","series":[],"tags":["MacOS","窗口管理器","Window Manager","yabai"],"title":"MacOS 窗口管理器 yabai 玩耍笔记","uri":"/posts/macos-window-manager-yabai-usage/#问题排查"},{"categories":["tech"],"content":" 1. yabai如果 yabai 配置没有生效，有可能是权限问题，可以试下这个命令重启 yabai: shell sudo yabai --uninstall-sa; sudo yabai --load-sa; brew services restart yabai 其他问题可查看 yabai 的日志解决： 错误日志路径: /usr/local/var/log/yabai/yabai.err.log 普通日志路径: /usr/local/var/log/yabai/yabai.out.log ","date":"2023-05-22","objectID":"/posts/macos-window-manager-yabai-usage/:4:1","series":[],"tags":["MacOS","窗口管理器","Window Manager","yabai"],"title":"MacOS 窗口管理器 yabai 玩耍笔记","uri":"/posts/macos-window-manager-yabai-usage/#1-yabai"},{"categories":["tech"],"content":" 2. skhd如果 skhd 配置没有生效，首先可以查看 skhd 的日志: shell cat /usr/local/var/log/skhd/*.log 如果日志文件不存在，可以停止 skhd 服务并手动启动它，看看是否有输出报错： shell brew services stop skhd skhd -c ~/.skhdrc 比如我之前改错了配置，执行上述命令就会报错： text #27:7 expected modifier 提示我配置的第 27 行配置有问题，我就去看了下，发现是我把 cmd - return 写成了cmd + return，改正后再 brew services start skhd 重启 skhd 就好了。 ","date":"2023-05-22","objectID":"/posts/macos-window-manager-yabai-usage/:4:2","series":[],"tags":["MacOS","窗口管理器","Window Manager","yabai"],"title":"MacOS 窗口管理器 yabai 玩耍笔记","uri":"/posts/macos-window-manager-yabai-usage/#2-skhd"},{"categories":["tech"],"content":" 堆叠模式下的可视化yabai 在堆叠模式下的可视化效果不是很好，可以使用stackline 来改善一下。 shell # stackline 依赖 hammerspoon，这是一个 macOS 桌面自动化工具 brew install hammerspoon --cask # 现在将 stackline 安装到 hammerspoon 的配置目录中 git clone https://github.com/AdamWagner/stackline.git ~/.hammerspoon/stackline # Make stackline run when hammerspoon launches cd ~/.hammerspoon echo 'stackline = require \"stackline\"' \u003e\u003e init.lua echo 'stackline:init()' \u003e\u003e init.lua 现在还需要安装下 hammerspoon 的命令行工具 hs，它用于在脚本中执行 stackline 操作，安装方法如下： 首先搜索打开 Hamerspoon 程序，或者使用命令 open -a \"Hammerspoon\" 这里启动时会申请权限，需要手动打开下 同时注意勾选登录时自动启动 在下方的命令输出栏中键入 hs.ipc.cliInstall() 再回车，即可完成安装 现在确认下 hs 命令已经可用： shell which hs ","date":"2023-05-22","objectID":"/posts/macos-window-manager-yabai-usage/:5:0","series":[],"tags":["MacOS","窗口管理器","Window Manager","yabai"],"title":"MacOS 窗口管理器 yabai 玩耍笔记","uri":"/posts/macos-window-manager-yabai-usage/#堆叠模式下的可视化"},{"categories":["tech"],"content":" 使用时的常见问题与解决方法 Chrome/WeChat 等程序的弹窗无法显示: 尝试下进入全屏或者退出全屏，总有一种场景下可以显示弹窗… … ","date":"2023-05-22","objectID":"/posts/macos-window-manager-yabai-usage/:6:0","series":[],"tags":["MacOS","窗口管理器","Window Manager","yabai"],"title":"MacOS 窗口管理器 yabai 玩耍笔记","uri":"/posts/macos-window-manager-yabai-usage/#使用时的常见问题与解决方法"},{"categories":["tech"],"content":" 参考 mac 下的平铺桌面 yabai 使用 - 月青悠 Yabai setup for i3wm users - Krever ","date":"2023-05-22","objectID":"/posts/macos-window-manager-yabai-usage/:7:0","series":[],"tags":["MacOS","窗口管理器","Window Manager","yabai"],"title":"MacOS 窗口管理器 yabai 玩耍笔记","uri":"/posts/macos-window-manager-yabai-usage/#参考"},{"categories":["tech"],"content":"随着文章的更新，文章内容逐渐增多，为了方便阅读，文章内容已经迁移到单独的站点: 文档站: https://nixos-and-flakes.thiscute.world/zh/ GitHub: https://github.com/ryan4yin/nixos-and-flakes-book 非常感谢Reddit、文章评论区、V2EX 以及0xffff.one 等平台上各位朋友的反馈、批评与建议 ❤️ ","date":"2023-05-04","objectID":"/posts/nixos-and-flake-basics/:0:0","series":["NixOS 与 Nix Flakes"],"tags":["NixOS","Nix","Flakes","Linux","DevOps"],"title":"NixOS 与 Nix Flakes 新手入门","uri":"/posts/nixos-and-flake-basics/#"},{"categories":["tech"],"content":" 阅读此文章需要前置知识：Linux 网络基础知识、iptables、conntrack 本文内容部分采用了 Copilot 提示内容，也有部分内容用了 ChatGPT 免费版进行分析，确实都比较有帮助。 最近因为工作需要研究了一波 WireGuard 协议，在这篇文章中简单记录下心得。 ","date":"2023-03-28","objectID":"/posts/wireguard-on-linux/:0:0","series":["计算机网络相关"],"tags":["WireGuard","VPN","Linux","网络","iptables","conntrack"],"title":"Linux 上的 WireGuard 网络分析（一）","uri":"/posts/wireguard-on-linux/#"},{"categories":["tech"],"content":" WireGuard 是什么WireGuard 是极简主义思想下的 VPN 实现，解决了很多现存 VPN 协议存在的问题。它于 2015 年由 Jason A. Donenfeld 设计实现，因其代码实现简洁易懂、配置简单、性能高、安全强度高而受到广泛关注。 WireGuard 在 2020 年初进入 Linux 主线分支，随后成为 Linux 5.6 的一个内核模块，这之后很快就涌现出许多基于 WireGuard 的开源项目与相关企业，各大老牌 VPN 服务商也逐渐开始支持 WireGuard 协议，很多企业也使用它来组建企业 VPN 网络。 基于 WireGuard 的明星开源项目举例： tailscale: 一套简单易用的 WireGuard VPN 私有网络解决方案，强烈推荐！ headscale: tailscale 控制服务器的开源实现，使你可以自建 tailscale 服务。 kilo: 基于 WireGuard 的 Kubernetes 多云网络解决方案。 … 除了上面这些，还有很多其他 WireGuard 项目，有兴趣可以去awesome-wireguard 仓库看看。 WireGuard 本身只是一个点对点隧道协议，只提供点对点通信的能力（这也是其极简主义思想的体现）。而其他网络路由、NAT 穿越、DNS 解析、防火墙策略等功能都是基于 Linux 系统的现有工具来实现的。 在这篇文章里，我将搭建一个简单的单服务器 + 单客户端 WireGuard 网络，然后分析它如何使用 Linux 系统现有的工具，在 WireGuard 隧道上搭建出一个安全可靠的虚拟网络。 文章测试用到的服务器与客户端均为虚拟机，使用 Ubuntu 20.04 系统，内核版本为 5.15，也就是说都包含了 wireguard 内核模块。 ","date":"2023-03-28","objectID":"/posts/wireguard-on-linux/:1:0","series":["计算机网络相关"],"tags":["WireGuard","VPN","Linux","网络","iptables","conntrack"],"title":"Linux 上的 WireGuard 网络分析（一）","uri":"/posts/wireguard-on-linux/#wireguard-是什么"},{"categories":["tech"],"content":" WireGuard 服务端网络分析简单起见，这里使用 docker-compose 启动一个 WireGuard 服务端，使用的镜像是linuxserver/docker-wireguard。 配置文件如下，内容完全参考自此镜像的官方 README： yaml --- version: \"2.1\" services: wireguard: image: lscr.io/linuxserver/wireguard:latest container_name: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - PUID=1000 - PGID=1000 - TZ=Etc/UTC - SERVERURL=auto # 自动确定服务器的外部 IP 地址，在生成客户端配置时会用到 - SERVERPORT=51820 # 服务端监听的端口号 - PEERS=1 # 自动生成 1 个客户端配置 - PEERDNS=auto # 自动确定客户端的 DNS 服务器地址，同样是在生成客户端配置时会用到 - INTERNAL_SUBNET=10.13.13.0 # WireGuard 虚拟网络的网段 - ALLOWEDIPS=0.0.0.0/0 # 这条规则表示允许虚拟网络内的所有客户端将流量发送到此节点 # 众所周知，NAT 网络需要定期发送心跳包来保持 NAT 表内容不过期，俗称连接保活。 # 这里设置为 all 表示所有客户端都开启连接保活。 - PERSISTENTKEEPALIVE_PEERS=all - LOG_CONFS=true # 开启日志 volumes: - ./config:/config - /lib/modules:/lib/modules # 将宿主机的内核模块挂载到容器内，用于加载 WireGuard 内核模块 ports: - 51820:51820/udp sysctls: - net.ipv4.conf.all.src_valid_mark=1 restart: unless-stopped 将上面的配置文件保存为 docker-compose.yml，然后通过如下命令后台启动 WireGuard 服务端： shell docker-compose up -d WireGuard 服务端启动好了，现在查看下服务端容器的日志（我加了详细注释说明）： shell $ docker logs wireguard # ...省略若干内容 .:53 # 这几行日志是启动 CoreDNS，为虚拟网络提供默认的 DNS 服务 CoreDNS-1.10.1 # 实际上 CoreDNS 不是必须的，客户端可以改用其他 DNS 服务器 linux/amd64, go1.20, 055b2c3 [#] ip link add wg0 type wireguard # 创建一个 wireguard 设备 [#] wg setconf wg0 /dev/fd/63 # 设置 wireguard 设备的配置 [#] ip -4 address add 10.13.13.1 dev wg0 # 为 wireguard 设备添加一个 ip 地址 [#] ip link set mtu 1420 up dev wg0 # 设置 wireguard 设备的 mtu [#] ip -4 route add 10.13.13.2/32 dev wg0 # 为 wireguard peer1 添加路由，其地址来自 wireguard 配置的 `allowedIPs` 参数 # 下面这几条 iptables 命令为 wireguard 设备添加 NAT 规则，使其成为 WireGuard 虚拟网络的默认网关 # 并使虚拟网络内的其他 peers 能通过此默认网关访问外部网络。 [#] iptables -A FORWARD -i wg0 -j ACCEPT; iptables -A FORWARD -o wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth+ -j MASQUERADE [ls.io-init] done. 通过日志能看到，程序首先创建了 WireGuard 设备 wg0 并绑定了地址 10.13.13.1。作为 WireGuard 网络中的服务端，它所创建的这个 wg0 的任务是成为整个 WireGuard 虚拟网络的默认网关，处理来自虚拟网络内的其他 peers 的流量，构成一个星型网络。 然后服务端为它所生成的 peer1 添加了一个路由，使得 peer1 的流量能够被正确路由到 wg0 设备上。 最后为了让 WireGuard 虚拟网络内的其他 peers 的流量能够通过 wg0 设备访问外部网络或者互相访问，服务端为 wg0 设备添加了如下的 iptables 规则： iptables -A FORWARD -i wg0 -j ACCEPT; iptables -A FORWARD -o wg0 -j ACCEPT;：允许进出 wg0 设备的数据包通过 netfilter 的 FORWARD 链（默认规则是 DROP，即默认是不允许通过的） iptables -t nat -A POSTROUTING -o eth+ -j MASQUERADE：在 eth+ 网卡上添加 MASQUERADE 规则，即将数据包的源地址伪装成 eth+ 网卡的地址，目的是为了允许 wireguard 的数据包通过 NAT 访问外部网络。 而回来的流量会被 NAT 的 conntrack 链接追踪规则自动允许通过，不过 conntrack 表有自动清理机制，长时间没流量的话会被从 conntrack 表中移除。这就是前面 docker-compose.yml 中的 PERSISTENTKEEPALIVE_PEERS=all 参数解决的问题通过定期发送心跳包来保持 conntrack 表中的连接信息。 这里还涉及到了 NAT 穿越相关内容，就不多展开了，感兴趣的可以自行了解。 WireGuard 的实现中还有一个比较重要的概念叫做 AllowedIPs，它是一个 IP 地址列表，表示允许哪些 IP 地址的流量通过 WireGuard 虚拟网络。为了详细说明这一点，我们先看下服务端配置文件夹中 wg0 的配置： shell $ cat wg0.conf [Interface] Address = 10.13.13.1 ListenPort = 51820 PrivateKey = kGZzt/CU2MVgq19ffXB2YMDSr6WIhlkdlL1MOeGH700= # wg0 隧道启动后添加 iptables 规则 PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth+ -j MASQUERADE # wg0 隧道停止后删除前面添加的 iptables 规则 PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth+ -j MASQUERADE [Peer] # peer1 PublicKey = HR8Kp3xWIt2rNdS3aaCk+Ss7yQqC9cn6h3WS6UK3WE0= PresharedKey = 7mCNCZdMKeRz1Zrpl9bFS08jJAdv6/USazRVq7tjznY= # AllowedIPs 设置为 peer1 的虚拟 IP 地址，表示允许 peer1 的流量通过 WireGuard 虚拟网络 AllowedIPs = 10.13.13.2/32 AllowedIPs 实际就是每个 peer 在服务端路由表中的 ip 地址，它既可以是 ip 也可以是网段，而且能设置多个，这使所有 peer 都可以负责一个甚至多个 ip 段的转发，也就是充当局域网的路由器——VPN 子路由。 WireGuard 本身只是一个点对点隧道协议，它非常通用。通过 AllowedIPs 参数，我们就能在每个 peer 上添加各 peers 的配置与不同的路由规则，构建出各种复杂的网络拓扑，比如星型、环型、树型等等。 ","date":"2023-03-28","objectID":"/posts/wireguard-on-linux/:2:0","series":["计算机网络相关"],"tags":["WireGuard","VPN","Linux","网络","iptables","conntrack"],"title":"Linux 上的 WireGuard 网络分析（一）","uri":"/posts/wireguard-on-linux/#wireguard-服务端网络分析"},{"categories":["tech"],"content":" WireGuard 客户端网络分析现在换台虚拟机跑 WireGuard 客户端，首先需要安装 wireguard 命令行工具： shell sudo apt install wireguard resolvconf 第二步是从服务端的配置文件夹中找到 peer1/peer1.conf，它是服务端容器根据参数 PEERS=1 自动生成的客户端配置文件，先确认下它的内容： shell $ cd ./config/peer1 $ cat peer1.conf [Interface] Address = 10.13.13.2 PrivateKey = +GLDb5QQOHQ2QKWvuFS/4FiWpnivaxzwlm0QmFJIHV8= ListenPort = 51820 DNS = 10.13.13.1 [Peer] PublicKey = t95vF4b11RLCId3ArVVIJoC5Ih9CNbI0VTNuDuEzZyw= PresharedKey = 7mCNCZdMKeRz1Zrpl9bFS08jJAdv6/USazRVq7tjznY= # 需要注意的是这个 Peer Endpoint 的 IP 是否正确 Endpoint = 192.168.5.198:51820 AllowedIPs = 0.0.0.0/0 插入下，这个 Endpoint 的地址也很值得一说，能看到服务端 wg0.conf 的配置中，peer1 并未被设置任何 Endpoint，这实质是表示这个 peer1 的 Endpoint 是动态的，也就是说每次 peer1 发送数据到服务端 wg0 时，服务端通过认证加密技术认证了数据后，就会以数据包的来源 IP 地址作为 peer1 的 Endpoint，这样 peer1 就可以随意更换自己的 IP 地址（Roaming），而 WireGuard 隧道仍然能正常工作（IP 频繁更换的一个典型场景就是手机的网络漫游与 WiFi 切换）。这使 WireGuard 具备了比较明显的无连接特性，也就是说 WireGuard 隧道不需要保持一个什么连接，切换网络也不需要重连，只要数据包能够到达服务端，就能够正常工作。 因为我这里是内网环境测试，配置文件中的 Peer - Endpoint 的 IP 地址直接用服务端的内网 IP 地址就行，也就是 192.168.5.198。 如果你的服务端有公网 IP 地址（比如是云服务器，或者通过端口映射用家庭宽带的动态公网 IP），这个 Endpoint 地址也可以使用该公网 IP 地址，效果是一样的。 配置文件确认无误后，将该配置文件保存到客户端的 /etc/wireguard/peer1.conf 这个路径下，然后使用如下命令启动 WireGuard 客户端： shell sudo wg-quick up peer1 上述命令会自动在 /etc/wireguard/ 目录下找到名为 peer1.conf 的配置文件，然后根据其内容启动一个名为 peer1 的 WireGuard 设备并完成对应配置。 我启动时的日志如下，wg-quick 打印出了它执行的所有网络相关指令（我添加了详细的注释）： shell $ sudo wg-quick up peer1 [#] ip link add peer1 type wireguard # 创建一个名为 peer1 的 WireGuard 设备 [#] wg setconf peer1 /dev/fd/63 # 设置 peer1 设备的配置 [#] ip -4 address add 10.13.13.2 dev peer1 # 设置 peer1 设备的 IP 地址 [#] ip link set mtu 1420 up dev peer1 # 设置 peer1 设备的 MTU [#] resolvconf -a tun.peer1 -m 0 -x # 设置 peer1 设备的 DNS，确保 DNS 能够正常工作 [#] wg set peer1 fwmark 51820 # 将 peer1 设备的防火墙标记设为 51820，用于标记 WireGuard 出站流量 # 在后面的路由策略中会使用该标记使 WireGuard 出站流量走默认路由表 [#] ip -4 route add 0.0.0.0/0 dev peer1 table 51820 # 创建单独的路由表 51820，默认将所有流量转发到 peer1 接口 [#] ip -4 rule add not fwmark 51820 table 51820 # 所有不带 51820 标记的流量（普通流量），都转发到前面新建的路由表 51820 # 也就是所有普通流量都转发到 peer1 接口 [#] ip -4 rule add table main suppress_prefixlength 0 # 流量全都走 main 路由表（即默认路由表），但是排除掉前缀长度（掩码） \u003c= 0 的流量 # 掩码 \u003c= 0 的只有 0.0.0.0/0，即默认路由。所以意思是所有非默认路由策略的流量都走 main 路由表 [#] sysctl -q net.ipv4.conf.all.src_valid_mark=1 # 启用源地址有效性检查，用于防止伪造源地址 [#] nft -f /dev/fd/63 # 配置 nftables 规则，用于确保 WireGuard 流量能正确路由，并防止恶意数据包进入网络 跑完后我们现在确认下状态，应该是能正常走 WireGuard 访问相关网络了，可以 WireShark 抓个包确认下。 如果网络不通，那肯定是中间哪一步配置有问题，可以根据上面的日志一步步排查网络接口、路由表、路由策略、iptables/nftables 的配置，必要时可以通过 WireShark 抓包定位。 现在再检查下系统的网络状态，首先检查下路由表，会发现路由表没任何变化： shell $ ip route ls default via 192.168.5.201 dev eth0 proto static 192.168.5.0/24 dev eth0 proto kernel scope link src 192.168.5.197 但是我们的 WireGuard 隧道已经生效了，这就说明现在我们的流量已经不是直接走上面这个默认路由表了，还有其他配置在起作用。往回看看前面的客户端启动日志，其中显示 wg-quick 创建了一个名为 51820 的路由表，我们来检查下这个表： shell ryan@ubuntu-2004-builder:~$ ip route ls table 51820 default dev peer1 scope link 能看到这个表确实是将所有流量都转发到了 WireGuard 的 peer1 接口，基本能确认现在流量都走了这个路由表。那么问题来了，系统的流量是如何被转发到这个路由表的呢？为什么默认的路由表现在不生效了？ 要理清这个问题，需要补充点知识——Linux 从 2.2 开始支持了多路由表，并通过路由策略数据库来为每个数据包选择正确的路由表，这个路由策略数据库可以通过 ip rule 命令来查看、修改。 前置知识补充完毕，现在来看下系统当前的路由策略，同样我已经补充好了注释： shell $ ip rule show 0: from all lookup local # 0 是最高优先级，`all` 表示所有流量，`lookup local` 表示查找 local 路由表。 # local 是一个特殊路由表，包含对本地和广播地址的优先级控制路由。 32764: from all lookup main suppress_prefixlength 0 # 32764 目前是第二优先级，将所有流量路由到 main 路由表，但是排除掉默认路由（前缀/掩码 \u003c= 0） # 功能是让所有非默认路由的流量都走 main 路由表 # 这条规则前面实际解释过了，它是 wg-quick 在启动隧道时添加的规则。 32765: not from all fwmark 0xca6c lookup 51820 # 所有不带 0xca6c 标记（51820 的 16 进制格式）的流量（普通流量），都走 51820 路由表 # 也就是都转发到 WireGuard peer1 接口。 # 这条规则是前面的 `ip -4 rule add not fwmark 51820 table 51820` 命令添加的。 # 而它所匹配的防火墙标记则是由前面的 `wg set peer1 fwmark 51820` 命令设置的。 32766: from all lookup main # 所有流量都走 main 路由表，当前是不生效状态，因为前面的规则优先级更高。 # main 是系统的默认路由表，通常我们使用 ip route 命令都是在这个表上操作。 32767: from all lookup default # 所有流量都走 default 路由表，当前同样是不生效状态。 # default 是一个系统生成的兜底路由表，默认不包含任何路由规则，可用于自定义路由策略，也可删除。 结合注释看完上面的路由策略，现在你应该理清楚 WireGuard 的路由规则了，它加了条比默认路由策略 32766 优先级更高的路由策略 32765，将所有普通流量都通过它的自定义路由表路由到 peer1 接口。另一方面 ","date":"2023-03-28","objectID":"/posts/wireguard-on-linux/:3:0","series":["计算机网络相关"],"tags":["WireGuard","VPN","Linux","网络","iptables","conntrack"],"title":"Linux 上的 WireGuard 网络分析（一）","uri":"/posts/wireguard-on-linux/#wireguard-客户端网络分析"},{"categories":["tech"],"content":" 结语一通分析，你是否感觉到了 wg-quick 的实现十分巧妙，通过简单几行 iptables/nftables 与 iproute2 命令就在 WireGuard 隧道上实现了一个 VPN 网络，更妙的是只要把新增的这些 iptables/nftables 与 iproute2 规则删除，就能恢复到 WireGuard 未启动的状态，相当于整个工作是完全可逆的（显然前面的 sudo wg-quick down peer1 就是这么干的）。 总之这篇文章简单分析了 wireguard 虚拟网络在 Linux 上的实现，希望对你有所帮助。 下一篇文章（如果有的话…），我会带来更多的 WireGuard 实现细节，敬请期待。 ","date":"2023-03-28","objectID":"/posts/wireguard-on-linux/:4:0","series":["计算机网络相关"],"tags":["WireGuard","VPN","Linux","网络","iptables","conntrack"],"title":"Linux 上的 WireGuard 网络分析（一）","uri":"/posts/wireguard-on-linux/#结语"},{"categories":["tech"],"content":" 参考 wireguard protocol： 官方文档还有官方的白皮书，都写得很清晰易懂。 WireGuard到底好在哪？: 比较深入浅出的随想，值得一读。 Understanding modern Linux routing (and wg-quick): 对 WireGuard 客户端用到的多路由表与路由策略技术做了详细的介绍。 它的中文翻译：WireGuard 基础教程：wg-quick 路由策略解读 - 米开朗基扬 ","date":"2023-03-28","objectID":"/posts/wireguard-on-linux/:5:0","series":["计算机网络相关"],"tags":["WireGuard","VPN","Linux","网络","iptables","conntrack"],"title":"Linux 上的 WireGuard 网络分析（一）","uri":"/posts/wireguard-on-linux/#参考"},{"categories":["tech"],"content":" 零、硬件准备与依赖库调研之前淘货买了挺多显示屏的，本文使用的是这一块： 3.5 寸电阻触摸屏，480 * 320，SPI 协议，显示屏驱动 IC 为 ILI9488 开发板是 ESP-WROOM-32 模组开发板。其他需要的东西：杜邦线、面包板、四个 10 K$\\Omega$ 电阻、四个按键。 至于需要的依赖库，我找到如下几个 stars 数较高的支持 ILI9488 + ESP32 的显示屏驱动库： Bodmer/TFT_eSPI: 一个基于 Arudino 框架的 tft 显示屏驱动，支持 STM32/ESP32 等多种芯片。 lv_port_esp32: lvgl 官方提供的 esp32 port，但是几百年不更新了，目前仅支持到 esp-idf v4，试用了一波被坑了，不建议使用。 esp-idf/peripherals/lcd: ESP 官方的 lcd 示例，不过仅支持部分常见显示屏驱动，比如我这里用的 ili9488 官方就没有。 总之强烈推荐 TFT_eSPI 这个库，很好用，而且驱动支持很齐全。 ","date":"2023-03-05","objectID":"/posts/ee-basics-2-esp32-display/:1:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU","ESP32","SPI","GPIO","贪吃蛇","显示屏"],"title":"EE 入门（二） - 使用 ESP32 与 SPI 显示屏绘图、显示图片、跑贪吃蛇","uri":"/posts/ee-basics-2-esp32-display/#零硬件准备与依赖库调研"},{"categories":["tech"],"content":" 一、开发环境搭建、电路搭建与测试","date":"2023-03-05","objectID":"/posts/ee-basics-2-esp32-display/:2:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU","ESP32","SPI","GPIO","贪吃蛇","显示屏"],"title":"EE 入门（二） - 使用 ESP32 与 SPI 显示屏绘图、显示图片、跑贪吃蛇","uri":"/posts/ee-basics-2-esp32-display/#一开发环境搭建电路搭建与测试"},{"categories":["tech"],"content":" 1. 创建项目并配置好环境ESP32 开发有好几种方式： vscode 的 esp-idf 插件 + 官方的 esp-idf 工具 vscode 的 platformio 插件 + arudino 框架 Bodmer/TFT_eSPI 这个依赖库两种方式都支持，不过看了下官方文档，仓库作者表示 ESP-IDF 的支持是其他人提供的，他不保证能用，所以稳妥起见我选择了 PlatformIO + Arduino 框架作为开发环境。 首先当然是创建一个空项目，点击 VSCode 侧栏的 PlatformIO 图标，再点击列表中的PlatformIO Core CLI 选项进入 shell 执行如下命令： shell pio project init --ide=vscode -d tft_esp32_arduino 这条命令会创建一个空项目，并配置好 vscode 插件相关配置，这样就算完成了一个空的项目框架。 ","date":"2023-03-05","objectID":"/posts/ee-basics-2-esp32-display/:2:1","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU","ESP32","SPI","GPIO","贪吃蛇","显示屏"],"title":"EE 入门（二） - 使用 ESP32 与 SPI 显示屏绘图、显示图片、跑贪吃蛇","uri":"/posts/ee-basics-2-esp32-display/#1-创建项目并配置好环境"},{"categories":["tech"],"content":" 1. 显示屏接线与项目参数配置网上简单搜了下 ESP32 pinout，找到这张图，引脚定义与我的 ESP32 开发板完全一致，用做接线参考： 可以看到这块 ESP32 开发板有两个 SPI 端口：HSPI 跟 VSPI，这里我们使用 HSPI，那么 MOSI/MISO/SCK 三个引脚的接线必须与上图的定义完全一致。而其他引脚随便找个普通 GPIO 口接上就行。 此外背光灯的线我试了下接 GPIO 口不好使，建议直接接在 3V3 引脚上（缺点就是没法通过程序关闭背光，问题不大）。 我的接线如下： 使用 wokwi.com 制作的示意图 接线实操 线接好后需要更新下 PlatformIO 项目根目录 platformio.ini 的配置，使其显示屏引脚相关的参数与我们的接线完全对应起来，这样才能正常驱动这个显示屏。 这里我以驱动库官方提供的模板Bodmer/TFT_eSPI/docs/PlatformIO 为基础，更新了其构建参数对应的引脚，加了点注释，得到的内容如下（如果你的接线与我一致，直接抄就行）： ini [env:esp32dev] platform = espressif32 board = esp32dev framework = arduino lib_deps = bodmer/TFT_eSPI@^2.5.0 Bodmer/TFT_eWidget@^0.0.5 monitor_speed = 115200 build_flags = -Os -DCORE_DEBUG_LEVEL=ARDUHAL_LOG_LEVEL_DEBUG -DUSER_SETUP_LOADED=1 ; Define the TFT driver, pins etc here: ; 显示屏驱动要对得上 -DILI9488_DRIVER=1 # 宽度与高度 -DTFT_WIDTH=480 -DTFT_HEIGHT=320 # SPI 引脚的接线方式， -DTFT_MISO=12 -DTFT_MOSI=13 # SCLK 在显示屏上对应的引脚可能叫 SCK，是同一个东西 -DTFT_SCLK=14 -DTFT_CS=15 # DC 在显示屏上对应的引脚可能叫 RS 或者 DC/RS，是同一个东西 -DTFT_DC=4 -DTFT_RST=2 # 背光暂时直接接在 3V3 上 ; -DTFT_BL=27 # 触摸，暂时不用 ;-DTOUCH_CS=22 -DLOAD_GLCD=1 # 其他配置，保持默认即可 -DLOAD_FONT2=1 -DLOAD_FONT4=1 -DLOAD_FONT6=1 -DLOAD_FONT7=1 -DLOAD_FONT8=1 -DLOAD_GFXFF=1 -DSMOOTH_FONT=1 -DSPI_FREQUENCY=27000000 修好后保存修改，platformio 将会自动检测到配置文件变更，并根据配置文件下载 Arduino/ESP32 工具链，更新构建配置、拉取依赖库（建议开个全局代理，不然下载会贼慢）。 ","date":"2023-03-05","objectID":"/posts/ee-basics-2-esp32-display/:2:2","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU","ESP32","SPI","GPIO","贪吃蛇","显示屏"],"title":"EE 入门（二） - 使用 ESP32 与 SPI 显示屏绘图、显示图片、跑贪吃蛇","uri":"/posts/ee-basics-2-esp32-display/#1-显示屏接线与项目参数配置"},{"categories":["tech"],"content":" 3. 测试验证现在找几个 demo 跑跑看，新建文件 src/main.ino，从如下文件夹中随便找个 demo copy 进去然后编译上传，看看效果： Bodmer/TFT_eSPI - examples/480x320 可以直接从 libdeps 中 copy examples 代码过来测试：cp .pio/libdeps/esp32dev/TFT_eSPI/examples/480\\ x\\ 320/TFT_Meters/TFT_Meters.ino src/main.ino 我跑出来的效果： ","date":"2023-03-05","objectID":"/posts/ee-basics-2-esp32-display/:2:3","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU","ESP32","SPI","GPIO","贪吃蛇","显示屏"],"title":"EE 入门（二） - 使用 ESP32 与 SPI 显示屏绘图、显示图片、跑贪吃蛇","uri":"/posts/ee-basics-2-esp32-display/#3-测试验证"},{"categories":["tech"],"content":" 二、显示图片、文字这需要首先将图片/文字转换成 bitmap 格式的 C 代码，可使用在线工具javl/image2cpp 进行转换，简单演示下： 注意高度与宽度调整为与屏幕大小一致，设置放缩模式，然后色彩改为 RGB565，最后上传图片、生成代码。 将生成好的代码贴到 src/test_img.h 中： c // We need this header file to use FLASH as storage with PROGMEM directive: // Icon width and height const uint16_t imgWidth = 480; const uint16_t imgHeight = 320; // 'evt_source', 480x320px const uint16_t epd_bitmap_evt_source [] PROGMEM = { // 这里省略掉图片内容...... } 然后写个主程序 src/main.ino 显示图像： c #include \u003cTFT_eSPI.h\u003e // Hardware-specific library TFT_eSPI tft = TFT_eSPI(); // Invoke custom library // Include the header files that contain the icons #include \"test_img.h\" void setup() { Serial.begin(115200); tft.begin(); tft.setRotation(1); // landscape tft.fillScreen(TFT_BLACK); // Swap the colour byte order when rendering tft.setSwapBytes(true); // 显示图片 tft.pushImage(0, 0, imgWidth, imgHeight, epd_bitmap_evt_source); delay(2000); } void loop() {} 编译上传，效果如下： ","date":"2023-03-05","objectID":"/posts/ee-basics-2-esp32-display/:3:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU","ESP32","SPI","GPIO","贪吃蛇","显示屏"],"title":"EE 入门（二） - 使用 ESP32 与 SPI 显示屏绘图、显示图片、跑贪吃蛇","uri":"/posts/ee-basics-2-esp32-display/#二显示图片文字"},{"categories":["tech"],"content":" 三、写个极简贪吃蛇游戏N 年前我写的第一篇博客文章，是用 C 语言写一个贪吃蛇，这里把它移植过来玩玩看~ 我的旧文章地址为：贪吃蛇—C—基于easyx图形库(下):从画图程序到贪吃蛇【自带穿墙术】 ， 里面详细介绍了程序的思路。 那么现在开始代码移植，TFT 屏幕前面已经接好了不需要动，要改的只有软件部分，还有就是添加上下左右四个按键的电路。 首先清空 src 文件夹，新建文件 src/main.ino，内容如下，其中主要逻辑均移植自我前面贴的文章： c #include \u003cmath.h\u003e #include \u003cstdio.h\u003e #include \u003cTFT_eSPI.h\u003e // Hardware-specific library #define WIDTH 480 #define HEIGHT 320 // 四个方向键对应的 GPIO 引脚 #define BUTTON_UP_PIN 5 #define BUTTON_LEFT_PIN 18 #define BUTTON_DOWN_PIN 19 #define BUTTON_RIGHT_PIN 21 TFT_eSPI tft = TFT_eSPI(); // Invoke custom library typedef struct Position // 坐标结构 { int x; int y; } Pos; Pos SNAKE[3000] = {0}; Pos DIRECTION; Pos EGG; long SNAKE_LEN; void setup() { Serial.begin(115200); tft.begin(); tft.setRotation(1); // landscape tft.fillScreen(TFT_BLACK); // Swap the colour byte order when rendering tft.setSwapBytes(true); // initialize the pushbutton pin as an input: the default state is LOW pinMode(BUTTON_UP_PIN, INPUT); pinMode(BUTTON_LEFT_PIN, INPUT); pinMode(BUTTON_DOWN_PIN, INPUT); pinMode(BUTTON_RIGHT_PIN, INPUT); init_game(); } void loop() { command(); // 获取按键消息 move(); // 修改头节点坐标-蛇的移动 eat_egg(); draw(); // 作图 eat_self(); delay(100); } void init_game() { // 初始化小蛇 SNAKE_LEN = 1; SNAKE[0].x = random(50, WIDTH - 50); // 头节点位置随机化 SNAKE[0].y = random(50, HEIGHT - 50); DIRECTION.x = pow(-1, random()); // 初始化方向向量 DIRECTION.y = 0; creat_egg(); Serial.println(\"GAM STARTED, Having Fun~\"); } void creat_egg() { while (true) { int ok = 0; EGG.x = random(50, WIDTH - 50); // 头节点位置随机化 EGG.y = random(50, HEIGHT - 50); for (int i = 0; i \u003c SNAKE_LEN; i++) { if (SNAKE[i].x == 0 \u0026\u0026 SNAKE[i].y == 0) continue; if (fabs(SNAKE[i].x - EGG.x) \u003c= 10 \u0026\u0026 fabs(SNAKE[i].y - EGG.y) \u003c= 10) ok = -1; break; } if (ok == 0) return; } } void command() // 获取按键命令命令 { if (digitalRead(BUTTON_LEFT_PIN) == HIGH) { if (DIRECTION.x != 1 || DIRECTION.y != 0) { // 如果不是反方向，按键才有效 Serial.println(\"Turn Left!\"); DIRECTION.x = -1; DIRECTION.y = 0; } } else if (digitalRead(BUTTON_RIGHT_PIN) == HIGH) { if (DIRECTION.x != -1 || DIRECTION.y != 0) { Serial.println(\"Turn Right!\"); DIRECTION.x = 1; DIRECTION.y = 0; } } else if (digitalRead(BUTTON_UP_PIN) == HIGH) { if (DIRECTION.x != 0 || DIRECTION.y != 1) { // 注意 Y 轴，向上是负轴，因为屏幕左上角是原点 (0,0) Serial.println(\"Turn Up!\"); DIRECTION.x = 0; DIRECTION.y = -1; } } else if (digitalRead(BUTTON_DOWN_PIN) == HIGH) { if (DIRECTION.x != 0 || DIRECTION.y != -1) { Serial.println(\"Turn Down!\"); DIRECTION.x = 0; DIRECTION.y = 1; } } } void move() // 修改各节点坐标以达到移动的目的 { // 覆盖尾部走过的痕迹 tft.drawRect(SNAKE[SNAKE_LEN - 1].x - 5, SNAKE[SNAKE_LEN - 1].y - 5, 10, 10, TFT_BLACK); for (int i = SNAKE_LEN - 1; i \u003e 0; i--) { SNAKE[i].x = SNAKE[i - 1].x; SNAKE[i].y = SNAKE[i - 1].y; } SNAKE[0].x += DIRECTION.x * 10; // 每次移动10pix SNAKE[0].y += DIRECTION.y * 10; if (SNAKE[0].x \u003e= WIDTH) // 如果越界，从另一边出来 SNAKE[0].x = 0; else if (SNAKE[0].x \u003c= 0) SNAKE[0].x = WIDTH; else if (SNAKE[0].y \u003e= HEIGHT) SNAKE[0].y = 0; else if (SNAKE[0].y \u003c= 0) SNAKE[0].y = HEIGHT; } void eat_egg() { if (fabs(SNAKE[0].x - EGG.x) \u003c= 5 \u0026\u0026 fabs(SNAKE[0].y - EGG.y) \u003c= 5) { // shade old egg tft.drawCircle(EGG.x, EGG.y, 5, TFT_BLACK); creat_egg(); // add snake node SNAKE_LEN += 1; for (int i = SNAKE_LEN - 1; i \u003e 0; i--) { SNAKE[i].x = SNAKE[i - 1].x; SNAKE[i].y = SNAKE[i - 1].y; } SNAKE[0].x += DIRECTION.x * 10; // 每次移动10pix SNAKE[0].y += DIRECTION.y * 10; } } void draw() // 画出蛇和食物 { for (int i = 0; i \u003c SNAKE_LEN; i++) { tft.drawRect(SNAKE[i].x - 5, SNAKE[i].y - 5, 10, 10, TFT_BLUE); } tft.drawCircle(EGG.x, EGG.y, 5, TFT_RED); } void eat_self() { if (SNAKE_LEN == 1) return; for (int i = 1; i \u003c SNAKE_LEN; i++) if (fabs(SNAKE[i].x - SNAKE[0].x) \u003c= 5 \u0026\u0026 fabs(SNAKE[i].y - SNAKE[0].y) \u003c= 5) { delay(1000); tft.setTextColor(TFT_RED, TFT_BLACK); tft.drawString(\"GAME OVER!\", 200, 150, 4); delay(3000); setup(); break; } } 代码就这么点，没几行，接下来我们来接一下按键电路，这部分是参考了 arduino 的官方文档How to Wire and Program a Button 接线方式如下，主要原理就是通过 GND 接线，使四个方向键对应的 GPIO 口默认值为低电平。当按键按下时，GPIO 口会被拉升成高电平，从而使程序识别到该按键被按下。 接线示意图如下（简单起见，省略了前面的显示屏接线部分）： 使用","date":"2023-03-05","objectID":"/posts/ee-basics-2-esp32-display/:4:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU","ESP32","SPI","GPIO","贪吃蛇","显示屏"],"title":"EE 入门（二） - 使用 ESP32 与 SPI 显示屏绘图、显示图片、跑贪吃蛇","uri":"/posts/ee-basics-2-esp32-display/#三写个极简贪吃蛇游戏"},{"categories":["tech"],"content":" 前言我是从去年 12 月开始玩电子的，起因是 11 月搞了个 Homelab，然后就一路玩到 ESPHome，买了一堆传感器。玩了一阵后 ESPHome 这种小白式玩法就满足不了我了，于是开始学习电路知识，用树莓派跟其他单片机开始折腾遥控小车、简易机械臂、跑马灯等等，可以说是玩得很尽兴。 今年我也打算继续玩一玩这一块，尤其想玩一玩用 ESP32/STM32 自制无人机，如果能搞搞无人机编队飞行就更好了~ 言归正传，这篇文章是我入门电子电路的第一篇笔记，涵盖最基础的电路理论与一些焊接知识，末尾还包含了后续的学习规划。 笔记内容参考了许多网上的资料，主要有如下几个： 纵横向导 - 电路入门 采用类比的方法来讲解电路基础，很适合业余玩家零基础快速入门。 Electrical Engineering Essentials - sparkfun 同样是零基础入门，尤其是还介绍了电烙铁等玩电路板的实用知识。 The Amazing World of Electronics - Only the Cool Stuff :-) 这个是一篇篇零散的文章，每篇文章一个知识点，但是讲得比较深入透彻。 我看完上面的文章后，随着玩得越来越深入，又陆续了解了这些内容： 电路基础 什么是面包板、面包线、杜邦线 如何使用万用表测电压、电流、电阻、电容，判断二极管、三极管引脚。 （N 年前学这玩意儿时用的是最简单易懂的物理指针表，但是实际显然是电子的用着更方便） 回忆下用法：首先调到合适档位，然后测电流要串联到电路中、测电压要与被测器件并联、测电阻直接接在被测器件两端即可。 对于手动量程万用表（如 DT-9205A），它的显示单位为量程名字末尾非数字部分。 比如电阻 200 量程的显示单位就为 Ω；2K/20K/200K 这三个量程的显示单位都为 KΩ；2M/20M/200M 的显示单位都为 MΩ 对于电流/电压/电容也是一样。 什么元件需要防静电，以及有线防静电手环/台垫 如何读色环电阻的阻值？（不会读不如直接万用表走起…） 如何选购、使用电烙铁/吸锡器等焊接工具 … 单片机（MCU）与单板计算机（SBC） 什么是开发版 什么是 TTL 串口、串口驱动、波特率 什么是 SPI/UART/I$^{2}$C 数据传输协议 什么是 GPIO 引脚，以及开发版的引脚各有什么功能 如何使用 USB 转 TTL 串口板给 ESP32/ESP8266/51/STM32 等单片机刷固件 ST-Link/J-Link/DAPLink 调试编程器（仿真器）与 TTL 串口有何区别，JTAG 和 SWD 接口又是个啥？该用哪个？ 如何使用 C 语言为单片机编写程序？如何上传编译好的固件？如何调试？ … 总之兴趣驱动，不会的东西就 Google 一下或者问问 ChatGPT，玩起来~ ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:1:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#前言"},{"categories":["tech"],"content":" 一、常见基础公式 欧姆定律： $U = IR$ 电压 U 单位伏特 Volt，符号 $V$ 电流 I 单位安培 Ampere，符号 $A$ 电阻 R 单位欧姆 Ohm，符号 $\\Omega$ 电功率公式： $p= UI$ 功率 p 单位瓦特 Watt，符号 $W$，等同于 $V \\cdot A$ 的缩写 电能公式： $w = pT$ 其中 p 为电功率，单位前面说了就是 Watt T 为时间，单位秒 Second $w$ 电能的单位为焦耳 joule，等同于 $V \\cdot A \\cdot s$ 常见的电池通常会使用 $mA \\cdot h$ 或者 $w \\cdot h$ 来标记其电能容量。 $mA \\cdot h$ 乘上电压再转换下电流跟时间的单位为 A 跟 s，就得到焦耳数 $w \\cdot h$ 直接乘 3600（1 小时的秒数）就得到焦耳数 电容量公式： $C = Q/U$ 电容量 C 单位为法拉 Farad，符号为 $F$ 带电量 Q 的单位为库仑 Coulomb，符号为 $C$ 库仑的定义： $1C = 1A \\cdot s$ 1 库仑即 $6.24146 \\times 10^{18}$ 个电子所带的电荷量 电感 TODO（貌似用得比较少…待补充） ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:2:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#一常见基础公式"},{"categories":["tech"],"content":" 二、常用电子元件介绍常见电子元器件： 电阻 二极管 Diode 发光二极管 整流二极管 稳压二极管 三极管 MOSFET 场效应管 电压转换器（power converter）：整流器（rectifier）、逆变器（inverter）、斩波器 （chopper）及变频驱动器（VFD） 电容 电解电容 瓷片电容 独石电容 晶振 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#二常用电子元件介绍"},{"categories":["tech"],"content":" 1. 二极管 Diode https://learn.sparkfun.com/tutorials/diodes 二极管是一种只允许电流由单一方向流过，具有两个电极的元件，是现代电子产业的基石。可类比水流中的单向阀门，水只能从一端流向另一端，而不能逆流。 最初的二极管是真空电子二极管，很大、需要预热、功耗大，易破碎。后来美国人使用半导体材料发明了晶体二极管（或者叫半导体二极管）。目前常用的二极管都是晶体二极管，主要使用硅或者锗这类半导体材料。 晶体二极管的核心是 PN 结（p–n junction），要了解 PN 结，需要先介绍半导体的几个概念： 空穴：又称 Electron hole，物理学中指原子的共价键上流失一个电子，最后在共价键上留下的空位。 载流子：半导体中有两种载流子，即价带中带正电的空穴，和导带中带负电的电子。 P 型半导体：P 表示 Positive，指以带正电的空穴导电为主的半导体，也叫空穴半导体。 在纯净的硅晶体中掺入三价元素（如硼），使之取代晶格中硅原子的位置，就形成P型半导体。 N 型半导体：N 表示 Negative，指自由电子浓度远大于空穴浓度的杂质半导体。 例如，含有适量五价元素砷、磷、锑等的锗或硅等半导体。 懂了上面这些后，让我们考虑在一个 N 型半导体跟 P 型半导体形成的 PN 结中，电子显然只能从 N 极流向 P 极，因为只有 N 极才有足够的电子。相反电流只能从 P 级流向 N 极，因为只有 P 极才有足够的空穴。 如果电流要反向流动，那 PN 结的 P 极的电子会更多，而 N 级的空穴也会更多，电势差会更大，显然就会非常费劲。 二极管在导通状态下二示意图如下，其中也展示了二极管对应的符号与真实二极管的结构（带环的一侧为其 N 极）： 电阻拥有线性的伏安特性曲线，遵从欧姆定律。而二极管则完全不同，它伏安特性曲线 （Current-Voltage Graph）如下： 几个主要特征与相关名词介绍： 更详细的文章：PN Junction Diode 正向压降 Forward Voltage: 指使电流能够导通的最小电压 $V_F$ 「正向压降」被用于克服二极管的内部电场了，所以在电流通过二极管后，电压需要减去这个电压，这也是中文名「正向压降」的由来。 硅二极管的正向压降通常为 0.6v - 1v，锗二极管的正向压降通常为 0.3v 根据伏安特性曲线，实际上随着电流的变化，「正向压降」也是有小幅波动的，不过计算时一般都认为它是固定值。 击穿电压 Breakdown Voltage: 指使电流能否反向导通的最小电压，从图中标识看 $V_{BR}$ 为 -50v，显然它远大于不到 1v 的「正向压降」。 当电流能经过二极管反向导通时，我们称二极管被击穿（Breakdown） 二极管依据其设计目标，分类了许多不同类别： 普通二极管 整流器(rectifier) / 功率二极管（power diode） 依靠二极管只能单向导通的原理，可以使用它将交流电变成直流电。 能承受较大的正向电流和较高的反向电压 发光二极管（Light-Emitting Diodes, LEDs） LED 的正向压降取决于它的颜色，而且比较固定，通常红色约为 1.6v，绿色有 2v 和 3v 两种， 黄色和橙色约为 2.2v，蓝色约为 3.2v 稳压二极管 利用二极管在反向击穿状态，其电流可在很大范围内变化而电压基本不变的现象，制成的起稳压作用的二极管。 开关二极管 能够快速由导通变为截止或由截止变为导通的一种二极管。 检波二极管 TODO 阻尼二极管 具有较低有电压降和较高的工作频率，且能承受较高的反向击穿电压和较大的峰值电流。 还有二极管堆组： 整流桥堆(半桥、全桥) 菱形联接 等等… ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:1","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#1-二极管-diode"},{"categories":["tech"],"content":" 2. 三极管 triode / bipolar transistor三极管即双极型晶体管，缩写 BJT，前面介绍了二极管结构为单个 PN 结，而三极管的结构则为 PNP 或者 NPN 结构，具有电流放大作用，是电子电路的核心元件之一。 它的工作方式就像是一个一个液压阀门，通过小电流来顶开中间的通路，使大电流得以通过，一个 NPN 型放大器电路的示意图如下： b 与 e 之间的电压形成一个小电流，这个小电流越大，c 与 e 之间的电阻就越小。 就像是如下液压阀门，b 处的水压越大，液压阀门被推得越开，c 与 e 之间的水流就越大： 三极管不是凭空把电放大了，而是说： 小的电信号（小水流）把另一个通路的大电流的阀门打开了， 后面的器件能够感受到这个大电流， 所以是放大了。对电来说 实际有两个电源供电的 一个是小电源 （小信号、信号源） 一个是大电源。 咱们的收音机，实际就是天线，接收到空气中的小电流，你可以理解为毛毛雨。 这个毛毛雨到了三极管的一个脚上打开阀门， 电池供电通过另外两个脚流动，再打开一个后面的三极管， 一级级的这样不断打开，一般收音机最早的时候是三管收音机、六管收音机，就是这么个意思一直到这个水流大到能够推动喇叭就发声了。 一个极简三级放大收音机电路： 两种三极管的符号与识别： 三个电极介绍： C: 即 Collector 集电极 B: 即 Base 基极 E: 即 Emitter 发射极 可以看到 NPN 跟 PNP 三极管最大的区别，是在于电流流向： NPN 的 Base 基极是 P 对应正极，电流从 B 与 C 极 流向 E 极 PNP 的 Base 基极是 N 对应负极，电流从 E 极流向 B 与 C 两个电极 根据 B 极电流 $I_B$ 的变化，$V_{CE}$ 的变化曲线如图： 可以看到在 $I_{B}$ 一定的情况下，不论 $V_{CE}$ 在 2v 以上如何变化，$I_{C}$ 的电流都几乎是恒定的。换个角度看电压在 2v - 12v 之间时，$I_{B}$ 与 $I_{C}$ 几乎是完全的线性关系，不受电压波动的影响。 注意 12v 以上只是没有画出来，假使这个三极管最多只能承受 12v 电压，那更高的电压会击穿它， 你就能看到三极管冒火花了… $\\frac{I_{C}}{I_{B}}$ 之间的比率（常数）被称做三极管的电流增益（Current Gain），一般使用 $\\beta$ 表示。 因为实际场景中 $I_{B}$ 不太好判断，通常都是直接调整 $V_{BE}$，因此我们再换个角度，对比下 $I_{C}$ 与 $V_{BE}$： 通过上图可以发现三极管的另外两个特征： $V_{BE}$ 需要一个启动电压，大约在 0.7v 左右，低于 0.7v 时$I_C$ 的电流一直非常小。 在 $V_{BE}$ 超过 0.7v 后，任何此电压的小变化，都会导致 $I_{C}$ 的剧烈变化。 一个常见的单状态 NPN 放大器电路如下： 可以注意到，输入 $V_{in}$是一个很小的交流信号，过来之前加了一个电容隔绝掉其中参杂的直流信号。 其次因为 $V_{BE}$ 需要一个启动电压才能进入电流放大的工作区间，这里通过 $R1$ 与 $R2$ 为 $V_{BE}$ 提供了一个启动电压 DC Biasing Point. ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:2","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#2-三极管-triode--bipolar-transistor"},{"categories":["tech"],"content":" 3. MOSFET 金属氧化物场效应晶体管 https://elamazing.com/2021/03/31/mosfet/ MOSFET 与三极管的区别与选用：https://www.eet-china.com/mp/a17394.html CMOS 集成电路工艺 - 百科: https://www.zgbk.com/ecph/words?SiteID=1\u0026ID=124559\u0026Type=bkzyb MOSFET 晶体管一般简称 MOS 管，是电压控制元件（通过栅极电压控制源漏间导通电阻），而双极型晶体管（三极管）是电流控制元件（通过基极较小的电流控制较大的集电极电流）。 MOS 管在导通压降下，导通电阻小，栅极驱动不需要电流，损耗小，驱动电路简单，自带保护二极管， 热阻特性好，适合大功率并联，缺点开关速度不高，比较昂贵。 而功能与 MOS 管类似的三极管，特点是开关速度高，大型三极管的 IC 可以做的很大，缺点损耗大， 基极驱动电流大，驱动复杂。 一般来说低成本场合，普通应用优先考虑用三极管，不行的话才考虑 MOS 管。 场效应管能在很小电流和很低电压的条件下工作，功耗低，而且可以很方便地把很多场效应管集成在一块硅片上，因此场效应管在大规模集成电路中得到了广泛的应用。目前主流的数字集成电路，包括 CPU/GPU/RAM，基本都是通过光刻制造的 CMOS 集成电路（Complementary Metal-Oxide-Semiconductor Integrated Circuit），CMOS 就是基于 MOSFET 技术实现的。 MOSFET 管的结构、极性，用法等内容，待补充… TODO ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:3","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#3-mosfet-金属氧化物场效应晶体管"},{"categories":["tech"],"content":" 4. 电容 Capacitor电容是电能的容器，里面存储的是电荷，电容在电路中是储能、缓冲、减压、过滤器件。。 水要通过池塘、湖泊，首先需要灌满它才能过得去。所以这部分水（电能）可以被这些容器保存下来， 这是电容的储能作用，另外很明显，无论前面的水流多么湍急，到了湖泊就要先灌满它，湖泊开口再向下游流水，自然流水就缓慢一些，所以它也有缓冲的作用。大波浪到了湖泊变平稳，实际变成了小波浪，波的形状都变了，这就是过滤的作用，只允许特定的波通过。 再回顾下电容相关的公式： 电容量公式： $C = Q/U$ 电容量 C 单位为法拉 Farad，符号为 $F$ 带电量 Q 的单位为库仑 Coulomb，符号为 $C$ 库仑的定义： $1C = 1A \\cdot s$ 1 库仑即 $6.24146 \\times 10^{18}$ 个电子所带的电荷量 电容的类型： 瓷片电容 用陶瓷材料作介质，在陶瓷表面涂覆一层金属（银）薄膜，再经高温烧结后作为电极而成。 用途：通常用于高稳定振荡回路中，作为回路、旁路电容器及垫整电容器。但仅限于在工作频率较低的回路中作旁路或隔直流用，或对稳定性和损耗要求不高的场合〈包括高频在内〉。瓷片电容不宜使用在脉冲电路中，因为它们易于被脉冲电压击穿。 铝电解电容（有极性） 有极性铝电解电容器是将附有氧化膜的铝箔（正极）和浸有电解液的衬垫纸，与阴极（负极）箔叠片一起卷绕而成。 优点: 容量范围大，一般为110 000 μF，额定工作电压范围为6.3 V450 V。 缺点: 介质损耗、容量误差大（最大允许偏差+100%、–20%）耐高温性较差，存放时间长容易失效。 用途: 通常在直流电源电路或中、低频电路中起滤波、退耦、信号耦合及时间常数设定、隔直流等作用。 注意：因其具有极性，不能用于交流电路。 独石电容 独石电容是用钛酸钡为主的陶瓷材料烧结制成的多层叠片状超小型电容器。 优点: 性能可靠、耐高温、耐潮湿、容量大（容量范围1 pF ~ 1 μF）、漏电流小等 缺点: 工作电压低（耐压低于100 V） 用途: 广泛应用于谐振、旁路、耦合、滤波等。 常用的有CT4 （低频） 、CT42（低频）；CC4（高频）、CC42（高频）等系列。 钽电解电容 有两种制作工艺： 箔式钽电解电容器：内部采用卷绕芯子,负极为液体电解质，介质为氧化钽 粉烧结式： 阳极（正极）用颗粒很细的钽粉压块后烧结而成 优点: 介质损耗小、频率特性好、耐高温、漏电流小。 缺点: 生产成本高、耐压低 用途: 广泛应用于通信、航天、军工及家用电器上各种中 、低频电路和时间常数设置电路中。 等等 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:4","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#4-电容-capacitor"},{"categories":["tech"],"content":" 5. 电感 Inductance「电磁感应（Electromagnetic induction）」我们都学过，它是指放在变化磁通量中的导体，会产生电动势。 此电动势称为感应电动势或感生电动势，若将此导体闭合成一回路，则该电动势会驱使电子流动，形成感应电流（感生电流）。 简单的说就是磁场变化能产生电能，电流变化也会形成磁场。 电磁感应最为人所知的应用应该就是「发电机」、「电动马达」跟「变压器」了。「发电机」通过电磁感应将机械能转换为电能，而「电动马达」刚好相反，它通过电磁感应将电能转换为机械能。这个转换实际上都是依靠磁场与「电磁感应」实现的。 而我们这里提的电感这种元器件，其核心原理是楞次定律（Lenz’s law）： 由于磁通量的改变而产生的感应电流，此电流的流向为抗拒磁通量改变的方向。 将楞次定律应用在闭合回路的自感效应中，得到的结论是： 电路上所诱导出的电动势的方向，总是使得它所驱动的电流，会阻碍原先产生它（即电动势）的磁通量之变化。 具体而言，对于「电感」，当电流增加时它会将能量以磁场的形式暂时存储起来，等到电流减小时它又会将磁场的能量释放出来，这会产生抵抗电流变化的效果。 电感并不损耗能量，它只是临时将电流存储起来，待会儿再释放出来而已（这叫什么？削峰填谷，平滑算法）。 电感的结构通常是漆包铜线缠绕在一个永磁体上，因为需要有电流的变化才能工作，通常仅应用在交流电领域。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:5","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#5-电感-inductance"},{"categories":["tech"],"content":" 6. 电阻 足够深入的分析：电阻的定义到底是什么？ 我们对电阻最直观的理解，是中学时学过的： $$R = \\frac{V}{I}$$ 但是在简单的含有电阻 R + 一个电感或电容的直流电路中，电流是随时间变化的，并在最终达到一个稳态。 这时根据上面的公式计算，因为电压是固定的，我们发现电路中电阻 R 的阻值实际是随时间变化的。 这个问题在直流电路中并不明显，因为电路最终仍然会达到稳态，这时电阻就跟它的标称电阻差距不大了。 但是在交流电路中，因为电流始终是在震荡的，这个问题就会变得相当明显，以至于无法简单地使用「电阻」来表达一个电阻器的特性，为此引入了一个新概念叫「阻抗」。 在具有电阻、电感和电容的电路里，对电路中的电流所起的阻碍作用叫做阻抗。阻抗常用Z表示，是一个复数，实部称为电阻，虚部称为电抗，其中电容在电路中对交流电所起的阻碍作用称为容抗 ,电感在电路中对交流电所起的阻碍作用称为感抗，电容和电感在电路中对交流电引起的阻碍作用总称为阻抗。 阻抗的单位是欧姆。阻抗的概念不仅存在于电路中，在力学的振动系统中也有涉及。 如果仔细看看你买过的耳机的相关参数，会发现它就包含一个「阻抗」参数，知乎上就有相关讨论耳机是不是阻抗越高越好？. 对电阻更精确的理解是：电阻是电压对电流的变化率，它不一定是一个静态值（也就是说可能是非线性的，比如二极管的伏安特性曲线就不是直线）。 单片机的下拉电阻与上拉电阻用单片机设计电路时，一个重要的点就是下拉电阻与上拉电阻。 不太好直接解释，直接看视频吧，下面这两个视频解释得很清晰： 上拉电阻的通俗解释 下拉电阻的通俗解释 再补充一个博友的文章单片机的GPIO配置，详细解释了 GPIO 相关的配置原理。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:6","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#6-电阻"},{"categories":["tech"],"content":" 6. 电阻 足够深入的分析：电阻的定义到底是什么？ 我们对电阻最直观的理解，是中学时学过的： $$R = \\frac{V}{I}$$ 但是在简单的含有电阻 R + 一个电感或电容的直流电路中，电流是随时间变化的，并在最终达到一个稳态。 这时根据上面的公式计算，因为电压是固定的，我们发现电路中电阻 R 的阻值实际是随时间变化的。 这个问题在直流电路中并不明显，因为电路最终仍然会达到稳态，这时电阻就跟它的标称电阻差距不大了。 但是在交流电路中，因为电流始终是在震荡的，这个问题就会变得相当明显，以至于无法简单地使用「电阻」来表达一个电阻器的特性，为此引入了一个新概念叫「阻抗」。 在具有电阻、电感和电容的电路里，对电路中的电流所起的阻碍作用叫做阻抗。阻抗常用Z表示，是一个复数，实部称为电阻，虚部称为电抗，其中电容在电路中对交流电所起的阻碍作用称为容抗 ,电感在电路中对交流电所起的阻碍作用称为感抗，电容和电感在电路中对交流电引起的阻碍作用总称为阻抗。 阻抗的单位是欧姆。阻抗的概念不仅存在于电路中，在力学的振动系统中也有涉及。 如果仔细看看你买过的耳机的相关参数，会发现它就包含一个「阻抗」参数，知乎上就有相关讨论耳机是不是阻抗越高越好？. 对电阻更精确的理解是：电阻是电压对电流的变化率，它不一定是一个静态值（也就是说可能是非线性的，比如二极管的伏安特性曲线就不是直线）。 单片机的下拉电阻与上拉电阻用单片机设计电路时，一个重要的点就是下拉电阻与上拉电阻。 不太好直接解释，直接看视频吧，下面这两个视频解释得很清晰： 上拉电阻的通俗解释 下拉电阻的通俗解释 再补充一个博友的文章单片机的GPIO配置，详细解释了 GPIO 相关的配置原理。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:6","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#单片机的下拉电阻与上拉电阻"},{"categories":["tech"],"content":" 7. 晶振 (Xtal) 与振荡电路 秒懂单片机晶振电路原理 石英晶体或晶振，是利用石英晶体（又称水晶）的压电效应，用来产生高精度振荡频率的一种电子器件，属于被动器件（无源源件）。 晶体是指其中的原子、分子、或离子以规则、重复的模式朝各方向延伸的一种固体。晶体与几乎所有的弹性物质都具有自然共振频率，透过适当的传感器可加以利用。 石英晶体的优点是在温度变化时，影响震荡频率的弹性系数与尺寸变化轻微，因而在频率特性上表现稳定。 石英晶体谐振器的原理： 石英晶体上的电极对一颗被适当切割并安置的石英晶体施以电场时，晶体会产生形变。这与晶体受压力产生电势的现象刚好相反，因此被称做逆压电效应。 当外加电场移除时，石英晶体又会恢复原状并发出电场，因而在电极上产生电压，这是我们熟知的压电效应。 逆压电效应 + 压电效应 这两个特性造成石英晶体在电路中的行为，类似于某种电感器、电容器、与电阻器所组合成的 RLC 电路。组合中的电感电容谐振频率则反映了石英晶体的实体共振频率。 当外加交变电压的频率与晶片的固有频率（决定于晶片的尺寸与切割方法）相等时，机械振动的幅度将急剧增加，这种现象称为压电谐振。 可能有些初学者会对晶振的频率感到奇怪，12M、24M 之类的晶振较好理解，选用如 11.0592MHZ 的晶振给人一种奇怪的感觉，这个问题解释起来比较麻烦，如果初学者在练习串口编程的时候就会对此有所理解，这种晶振主要是可以方便和精确的设计串口或其它异步通讯时的波特率。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:7","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#7-晶振-xtal-与振荡电路"},{"categories":["tech"],"content":" 8. 地电路中每个器件上有电能量集聚，形成电势差，就相当于物体的高度差。假设没有一个参考基准点，就没法测量这个电势差了，因此规定电路的某个点就是作为基准面，也就是地（GND/Ground）了。 地/GND 并不需要是真正的地面，对于我主要关注的弱电电路板而言，电路的负极就是地。 同理可推出，如果需要将同一个电路板同时接入多个源电路，则必须将这多个电路板的负极连接在一起，这样它们的「GND」参考基准点才是一致的！ 静电破坏与防静电 https://zhuanlan.zhihu.com/p/570713171 弱电领域另外一个常见的接地应该就是静电接地了，这是为了确保人体/工作台与地面的电势差为零， 避免工作时静电放电导致元器件损坏。 人体感应的静电电压一般在 2KV-4KV 左右，通常是人体轻微运动或与绝缘摩擦引起的。这么高的电压，足够击穿很多电子元件了，所以电子厂都会强制员工穿戴防静电装置（有线防静电手环）。 静电接地通常要求接真正的地面，比如与建筑物接触紧密的金属门窗、水龙头等都算是「地」。个人用的话，据朋友介绍效果最好的方法是：穿拖鞋，并且一只脚踩地上哈哈~ ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:8","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#8-地"},{"categories":["tech"],"content":" 8. 地电路中每个器件上有电能量集聚，形成电势差，就相当于物体的高度差。假设没有一个参考基准点，就没法测量这个电势差了，因此规定电路的某个点就是作为基准面，也就是地（GND/Ground）了。 地/GND 并不需要是真正的地面，对于我主要关注的弱电电路板而言，电路的负极就是地。 同理可推出，如果需要将同一个电路板同时接入多个源电路，则必须将这多个电路板的负极连接在一起，这样它们的「GND」参考基准点才是一致的！ 静电破坏与防静电 https://zhuanlan.zhihu.com/p/570713171 弱电领域另外一个常见的接地应该就是静电接地了，这是为了确保人体/工作台与地面的电势差为零， 避免工作时静电放电导致元器件损坏。 人体感应的静电电压一般在 2KV-4KV 左右，通常是人体轻微运动或与绝缘摩擦引起的。这么高的电压，足够击穿很多电子元件了，所以电子厂都会强制员工穿戴防静电装置（有线防静电手环）。 静电接地通常要求接真正的地面，比如与建筑物接触紧密的金属门窗、水龙头等都算是「地」。个人用的话，据朋友介绍效果最好的方法是：穿拖鞋，并且一只脚踩地上哈哈~ ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:3:8","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#静电破坏与防静电"},{"categories":["tech"],"content":" 三、常见电路计算方式","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:4:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#三常见电路计算方式"},{"categories":["tech"],"content":" 1. 如何选用正确的电阻？这需要使用到我们中学学过的物理学欧姆定律公式： $$V = I \\cdot R$$ 首先针对电子电路领域的 hello world，即发光二极管 + 电阻： 我们可以根据 LED 灯的最大电流来估算电阻值，根据欧姆定律有 $$R = \\frac{V}{I}$$ 普通发光二极管的正常工作电流通常为 $2 \\text{mA}$ ~ $20 \\text{mA}$，电流越大它就越亮，正向压降有好几种，假设我们的为 $3.3v$。 因此电路允许的最大电流为 $0.02 \\text{A}$，如果电源电压为 3.7v，那电阻得到的电压大概为 $0.4 \\text{V}$，这样可计算得到 $R$ 为 $20 \\Omega$. 发光二极管在正常工作状态几乎没有电阻，因此可以直接将上面计算出的结果当作串联电阻的阻值。 因此为了使发光二极管正常工作，串联电阻应该略大于 $25 \\Omega$. ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:4:1","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#1-如何选用正确的电阻"},{"categories":["tech"],"content":" 2. 电路分析中的两个重要定律 KAL 基尔霍夫电流定律：所有进入某节点的电流的总和等于所有离开这节点的电流的总和 KVL 基尔霍夫电压定律：沿着闭合回路所有元件两端的电势差（电压）的代数和等于零 这两个定律感觉通过「能量守恒」去理解，会显得很直观，不论是电流还是电压，都不会无中生有，在整个电路上它始终是守恒的。 KVL + 节点电压法是分析电路的一种有效手段。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:4:2","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#2-电路分析中的两个重要定律"},{"categories":["tech"],"content":" 3. 隔直通交与隔交通直上面这个是常见的简单特性描述，但是不够准确，准确的说： 电容是隔断不变的电信号，通过变化的电信号。 电感是阻碍变化的电信号，通过不变的电信号。 显然直流电的电流也是可以变化的，比如刚过了整流桥的直流电就是一个脉动信号。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:4:3","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#3-隔直通交与隔交通直"},{"categories":["tech"],"content":" 4. 交直流叠加信号交流信号就很好，很真实，为什么还要有交流直流叠加信号，到最后还要把直流信号去掉，只保留交流信号，多麻烦。这是因为，任何器件如果想打开或者处于一定状态，多少都需要一定的能量驱动的，如果这个能量不足，让器件处于不稳定的状态，我们还原不了真实的信号，所以三极管放大加上静态偏置，实际上就是为了让他先工作在临近放大区，再来交流信号才能正确还原。 所谓的静态偏置，实际上就是挂上个电阻先给这个三极管的某个引脚加上直流电。再来的交流信号与直流叠加变成交直流混叠信号，来驱动三极管的b极。 犹如大坝的开口在5米处，但是交流信号（变化的信号）只有1米的波动，所以先把水位抬高到5米，这个波动才能送过去。 现在信号放大电路大部分被运算放大器替代，两个运放之间有一个隔直电容，这是因为运放不需要这种类似三极管的偏置，它不需要抬高水位，本身它建立的条件就是你来波动我就能正常反馈到后级，你这个时候如果叠加了直流信号，反而出问题了，因为你把水位抬高了，比较低的信号不能正常反馈到后级被这个直流信号掩盖了。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:4:4","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#4-交直流叠加信号"},{"categories":["tech"],"content":" 四、电子电路工具套装介绍玩硬件的话，工具套装是必不可少的，最先遇到的场景就是——很多的传感器都需要自己焊接排针。 常用工具的主要有这些： 万用电表（Multimeter） 面包板（Breadboard） 电烙铁（Soldering Iron） 玻纤洞洞板（Stripboard / Perfboard） 其他进阶玩耍时可能会用到的工具： 示波器（Oscilloscope） 可调直流稳压电源（Adjustable DC Power Supply） 频谱分析仪 电子元器件又主要分类两类： 插式元器件 传统电子元器件，都带有较长引脚，PCB 版需要为引脚预留通孔。 相关技术：through-hole technology 这种元件比较大个，都很适合手工焊接，焊接完成后还需要剪掉多余引脚。 片式元器件 SMD (surface-mount device) 一种新型元器件，也叫贴片元件。 比插式元器件要小很多，而且 PCB 板不需要预留插孔，更节省材料跟空间，广泛应用在各种小型化电子设备中。 相关技术：(SMT) Surface-mount technology 相关设备：激光打印钢网、贴片机（巨贵） 贴片元件手工焊接时不适合用电烙铁焊，因为它太小了，这样焊接会很费劲。 贴片元件最简单的手工焊接方法是使用针筒焊锡膏在 PCB 触点上涂好锡膏，可用牙签去掉多余锡膏，然后用镊子将贴片元件放上去，最后使用恒温焊台加热完成焊接。元件放歪点没关系，加热时它会因为液态锡的表面张力自动正位。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:5:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#四电子电路工具套装介绍"},{"categories":["tech"],"content":" 1. 电烙铁篇电烙铁主要考虑的是升温速度跟温度保持能力，便宜的电烙铁基本都有升温慢、焊接中途易失温等毛病。目前总结的电烙铁信息如下： 便携电烙铁：入门级别推荐 优缺点: 便携、价格低。但是升温相对焊台要慢一些，温控相对不够精确，而且无自动休眠， 空烧烙铁头容易氧化，再有就是它没有接地不防经典，焊接精密元件比较危险。 貌似主要推荐广东黄花 907 电烙铁，淘宝官方店买个刀头的 54 大洋 热风枪：主要用来拆焊，以及焊接贴片元件 + 芯片。 我买了一把德力西 2000W 的数显热风枪，不过貌似更多人推荐那种二合一焊台，直接控制热风枪跟电烙铁两个玩意儿。 焊台：进阶推荐，也可考虑一步到位… 优缺点: 发热很快、热容相对较大，自动休眠不会空烧、还有过流保护、单片机稳定温控。缺点是要贵一些，另外相对没那么便携。 相关流行产品 白菜白光 T12 恒温焊台，最早是网友基于日本白光公司 T12 烙铁头（日本工厂到期强制报废的洋垃圾）配上自制恒温控制电路完成的 DIY 焊台，因为相对高端焊台相当便宜所以冠以「白菜」之名。 淘宝上有一些卖这个的，质量见仁见智吧，我没买过。 日本白光 HAKKO 焊台：这个很多人推荐，说是质量好。不过贵，新手用可能有点奢侈了。 二合一焊台：焊台自带热风枪 + 高频电烙铁两件套，高手必备（一般拆机才会用到热风枪） 高频电烙铁使用的是跟电磁炉一样的高频涡流发热原理，电烙铁头自身发热，不需要任何发热芯， 发热很快、热容大、烙铁头更换便宜。高端烙铁头都是高频的。 反正就很高级也很贵啦。我现阶段买了它也是浪费钱，所以没了解具体型号啥的了 恒温加热台：功能跟热风枪差不多，但是体积大很多，而且更贵，新手不推荐买。 关于电烙铁头，貌似刀头是最推荐的，因为它用途最广泛，热容大，基本适用所有场景。 电烙铁，我最后买的第一把电烙铁是网友 DIY 的「L245 焊笔 玫瑰金」，铝合金 CNC 切割工艺，Type-C 供电，最高支持到 PD 120W，颜值很高，口碑也很好，价格是 148 大洋。使用起来还是比较 OK 的，热得很快，热容也 OK。不过毕竟是 DIY 的便宜焊笔，质量不稳，我有遇到过多次误休眠、未识别到焊芯、芯片系统崩溃等问题，都是靠断电重启解决的。 电烙铁的使用与保养 参考：http://www.cxg.cn/newshow1346.html 前面讲了，我毕竟买的是 150 一把的焊笔，C245 这个烙铁头也不便宜，直接当耗材随便折腾就太浪费了。有必要搞清楚怎么使用与保养电烙铁： 焊接作业前，先为高温海绵加水湿润，再挤掉部分水分。 如果使用非湿润的清洁海绵，会使烙铁头受损氧化，导致不沾锡。 另外我发现「镀铜钢丝球」确实比高温海绵好用多了，墙裂推荐！笔不干净了往钢丝球里插一插，立即光亮如新。 焊接作业中，每次都先在高温海绵上擦干净烙铁头上的旧锡，再进行焊接。 如果是用「镀铜钢丝球」，那就直接在钢丝球里插一插，立即光洁如新。 中途不使用时，如果无自动休眠功能，可以手动将温度调低至 200 度以下，避免空烧。空烧会降低烙铁头寿命。 焊接完毕后，将温度调至约 250 摄氏度，使用湿润的高温海绵清洁烙铁头，最后将烙铁头加上一层新锡作保护，这样可以保护烙铁头和空气隔离，烙铁头不会氧化变黑。 烙铁头已经氧化、不沾锡时应如何处理 先把温度调到 300 摄氏度，用清洁海绵清理烙铁头，并检查烙铁头状况。 如果烙铁头的镀锡层部分含有黑色氧化物时，可镀上新锡层，再用清洁海绵抹净烙铁头。如此重复清理，直到彻底去除氧化物，然后在镀上新锡层。 将温度调至 200 摄氏度左右貌似比较容易上锡，不易聚成球。 实测上锡再用海绵抹除，每次都能摸走一些黑色氧化物，非常有效。不过要清理干净还是需要一些耐心。 如果烙铁头变形或穿孔，必须替换新咀。 其他注意事项 勿大力焊接：只要让烙铁头充分接触焊点，热量就可传递，无需大力焊接。 尽量低温焊接：高温焊接会加速烙铁头氧化，降低烙铁头使用寿命。如烙铁头温度超过470℃，它的氧化速度是380℃的两倍。 经常保持烙铁头上锡：这可以减低烙铁头的氧化机会，使烙铁头更耐用。 保持烙铁头清洁与及时清理氧化物 小心放入烙铁架：如果烙铁头接触到烙铁架无法自动休眠，长时间空烧将会毁掉烙铁头。 选用活性低的助焊剂：最便宜的就是松香，更好一点的是无铅无酸无卤素助焊剂。 焊锡丝在焊接过程中为什么会爆锡？ https://zhuanlan.zhihu.com/p/584316437 建议直接看上面的文章，可能的原因大概是： 受潮 焊锡丝混有杂质，或者助焊剂含量过高 焊接操作时手上有汗或是洗过手后手没有完全干就开始焊接 烙铁温度过高 我最近买的两卷焊锡丝就有爆锡的问题，烙铁温度是设的很常规的 320 度甚至更低的 290 度，现在怀疑是不是这个无铅焊锡丝有问题。罪魁祸首找到了，是因为我的锡线架，它下方就是湿润的高温海绵…这显然很容易受潮… 如何使用电烙铁进行拆焊当你焊错焊反了元件，或者你需要修修改改电路板时，就需要先进行拆焊。 用电烙铁进行拆焊需要注意这些事项： 温度必须要高，起码 350 以上 烙铁尖必须留点锡在上面。如果烙铁尖不挂锡，焊接的时候会发现即使温度高，电路板的焊锡也很难融化 另一个方式是先加点有铅锡丝降低焊锡熔点，然后再用吸锡器或吸锡带来吸 如果用吸锡器，发现不撤烙铁头直接把吸锡器怼上去，效果是最好的 如果使用的是吸锡带，温度就必须更高，估计至少得 380 甚至更高 因为吸锡带一般是纯铜，导热性能很好。一般 320 度就很容易熔的锡，上了吸锡带后热量全被吸锡带传导走了，温度不高根本融化不了 或者直接上热风枪+镊子也行（我还没试过…）。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:5:1","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#1-电烙铁篇"},{"categories":["tech"],"content":" 1. 电烙铁篇电烙铁主要考虑的是升温速度跟温度保持能力，便宜的电烙铁基本都有升温慢、焊接中途易失温等毛病。目前总结的电烙铁信息如下： 便携电烙铁：入门级别推荐 优缺点: 便携、价格低。但是升温相对焊台要慢一些，温控相对不够精确，而且无自动休眠， 空烧烙铁头容易氧化，再有就是它没有接地不防经典，焊接精密元件比较危险。 貌似主要推荐广东黄花 907 电烙铁，淘宝官方店买个刀头的 54 大洋 热风枪：主要用来拆焊，以及焊接贴片元件 + 芯片。 我买了一把德力西 2000W 的数显热风枪，不过貌似更多人推荐那种二合一焊台，直接控制热风枪跟电烙铁两个玩意儿。 焊台：进阶推荐，也可考虑一步到位… 优缺点: 发热很快、热容相对较大，自动休眠不会空烧、还有过流保护、单片机稳定温控。缺点是要贵一些，另外相对没那么便携。 相关流行产品 白菜白光 T12 恒温焊台，最早是网友基于日本白光公司 T12 烙铁头（日本工厂到期强制报废的洋垃圾）配上自制恒温控制电路完成的 DIY 焊台，因为相对高端焊台相当便宜所以冠以「白菜」之名。 淘宝上有一些卖这个的，质量见仁见智吧，我没买过。 日本白光 HAKKO 焊台：这个很多人推荐，说是质量好。不过贵，新手用可能有点奢侈了。 二合一焊台：焊台自带热风枪 + 高频电烙铁两件套，高手必备（一般拆机才会用到热风枪） 高频电烙铁使用的是跟电磁炉一样的高频涡流发热原理，电烙铁头自身发热，不需要任何发热芯， 发热很快、热容大、烙铁头更换便宜。高端烙铁头都是高频的。 反正就很高级也很贵啦。我现阶段买了它也是浪费钱，所以没了解具体型号啥的了 恒温加热台：功能跟热风枪差不多，但是体积大很多，而且更贵，新手不推荐买。 关于电烙铁头，貌似刀头是最推荐的，因为它用途最广泛，热容大，基本适用所有场景。 电烙铁，我最后买的第一把电烙铁是网友 DIY 的「L245 焊笔 玫瑰金」，铝合金 CNC 切割工艺，Type-C 供电，最高支持到 PD 120W，颜值很高，口碑也很好，价格是 148 大洋。使用起来还是比较 OK 的，热得很快，热容也 OK。不过毕竟是 DIY 的便宜焊笔，质量不稳，我有遇到过多次误休眠、未识别到焊芯、芯片系统崩溃等问题，都是靠断电重启解决的。 电烙铁的使用与保养 参考：http://www.cxg.cn/newshow1346.html 前面讲了，我毕竟买的是 150 一把的焊笔，C245 这个烙铁头也不便宜，直接当耗材随便折腾就太浪费了。有必要搞清楚怎么使用与保养电烙铁： 焊接作业前，先为高温海绵加水湿润，再挤掉部分水分。 如果使用非湿润的清洁海绵，会使烙铁头受损氧化，导致不沾锡。 另外我发现「镀铜钢丝球」确实比高温海绵好用多了，墙裂推荐！笔不干净了往钢丝球里插一插，立即光亮如新。 焊接作业中，每次都先在高温海绵上擦干净烙铁头上的旧锡，再进行焊接。 如果是用「镀铜钢丝球」，那就直接在钢丝球里插一插，立即光洁如新。 中途不使用时，如果无自动休眠功能，可以手动将温度调低至 200 度以下，避免空烧。空烧会降低烙铁头寿命。 焊接完毕后，将温度调至约 250 摄氏度，使用湿润的高温海绵清洁烙铁头，最后将烙铁头加上一层新锡作保护，这样可以保护烙铁头和空气隔离，烙铁头不会氧化变黑。 烙铁头已经氧化、不沾锡时应如何处理 先把温度调到 300 摄氏度，用清洁海绵清理烙铁头，并检查烙铁头状况。 如果烙铁头的镀锡层部分含有黑色氧化物时，可镀上新锡层，再用清洁海绵抹净烙铁头。如此重复清理，直到彻底去除氧化物，然后在镀上新锡层。 将温度调至 200 摄氏度左右貌似比较容易上锡，不易聚成球。 实测上锡再用海绵抹除，每次都能摸走一些黑色氧化物，非常有效。不过要清理干净还是需要一些耐心。 如果烙铁头变形或穿孔，必须替换新咀。 其他注意事项 勿大力焊接：只要让烙铁头充分接触焊点，热量就可传递，无需大力焊接。 尽量低温焊接：高温焊接会加速烙铁头氧化，降低烙铁头使用寿命。如烙铁头温度超过470℃，它的氧化速度是380℃的两倍。 经常保持烙铁头上锡：这可以减低烙铁头的氧化机会，使烙铁头更耐用。 保持烙铁头清洁与及时清理氧化物 小心放入烙铁架：如果烙铁头接触到烙铁架无法自动休眠，长时间空烧将会毁掉烙铁头。 选用活性低的助焊剂：最便宜的就是松香，更好一点的是无铅无酸无卤素助焊剂。 焊锡丝在焊接过程中为什么会爆锡？ https://zhuanlan.zhihu.com/p/584316437 建议直接看上面的文章，可能的原因大概是： 受潮 焊锡丝混有杂质，或者助焊剂含量过高 焊接操作时手上有汗或是洗过手后手没有完全干就开始焊接 烙铁温度过高 我最近买的两卷焊锡丝就有爆锡的问题，烙铁温度是设的很常规的 320 度甚至更低的 290 度，现在怀疑是不是这个无铅焊锡丝有问题。罪魁祸首找到了，是因为我的锡线架，它下方就是湿润的高温海绵…这显然很容易受潮… 如何使用电烙铁进行拆焊当你焊错焊反了元件，或者你需要修修改改电路板时，就需要先进行拆焊。 用电烙铁进行拆焊需要注意这些事项： 温度必须要高，起码 350 以上 烙铁尖必须留点锡在上面。如果烙铁尖不挂锡，焊接的时候会发现即使温度高，电路板的焊锡也很难融化 另一个方式是先加点有铅锡丝降低焊锡熔点，然后再用吸锡器或吸锡带来吸 如果用吸锡器，发现不撤烙铁头直接把吸锡器怼上去，效果是最好的 如果使用的是吸锡带，温度就必须更高，估计至少得 380 甚至更高 因为吸锡带一般是纯铜，导热性能很好。一般 320 度就很容易熔的锡，上了吸锡带后热量全被吸锡带传导走了，温度不高根本融化不了 或者直接上热风枪+镊子也行（我还没试过…）。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:5:1","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#电烙铁的使用与保养"},{"categories":["tech"],"content":" 1. 电烙铁篇电烙铁主要考虑的是升温速度跟温度保持能力，便宜的电烙铁基本都有升温慢、焊接中途易失温等毛病。目前总结的电烙铁信息如下： 便携电烙铁：入门级别推荐 优缺点: 便携、价格低。但是升温相对焊台要慢一些，温控相对不够精确，而且无自动休眠， 空烧烙铁头容易氧化，再有就是它没有接地不防经典，焊接精密元件比较危险。 貌似主要推荐广东黄花 907 电烙铁，淘宝官方店买个刀头的 54 大洋 热风枪：主要用来拆焊，以及焊接贴片元件 + 芯片。 我买了一把德力西 2000W 的数显热风枪，不过貌似更多人推荐那种二合一焊台，直接控制热风枪跟电烙铁两个玩意儿。 焊台：进阶推荐，也可考虑一步到位… 优缺点: 发热很快、热容相对较大，自动休眠不会空烧、还有过流保护、单片机稳定温控。缺点是要贵一些，另外相对没那么便携。 相关流行产品 白菜白光 T12 恒温焊台，最早是网友基于日本白光公司 T12 烙铁头（日本工厂到期强制报废的洋垃圾）配上自制恒温控制电路完成的 DIY 焊台，因为相对高端焊台相当便宜所以冠以「白菜」之名。 淘宝上有一些卖这个的，质量见仁见智吧，我没买过。 日本白光 HAKKO 焊台：这个很多人推荐，说是质量好。不过贵，新手用可能有点奢侈了。 二合一焊台：焊台自带热风枪 + 高频电烙铁两件套，高手必备（一般拆机才会用到热风枪） 高频电烙铁使用的是跟电磁炉一样的高频涡流发热原理，电烙铁头自身发热，不需要任何发热芯， 发热很快、热容大、烙铁头更换便宜。高端烙铁头都是高频的。 反正就很高级也很贵啦。我现阶段买了它也是浪费钱，所以没了解具体型号啥的了 恒温加热台：功能跟热风枪差不多，但是体积大很多，而且更贵，新手不推荐买。 关于电烙铁头，貌似刀头是最推荐的，因为它用途最广泛，热容大，基本适用所有场景。 电烙铁，我最后买的第一把电烙铁是网友 DIY 的「L245 焊笔 玫瑰金」，铝合金 CNC 切割工艺，Type-C 供电，最高支持到 PD 120W，颜值很高，口碑也很好，价格是 148 大洋。使用起来还是比较 OK 的，热得很快，热容也 OK。不过毕竟是 DIY 的便宜焊笔，质量不稳，我有遇到过多次误休眠、未识别到焊芯、芯片系统崩溃等问题，都是靠断电重启解决的。 电烙铁的使用与保养 参考：http://www.cxg.cn/newshow1346.html 前面讲了，我毕竟买的是 150 一把的焊笔，C245 这个烙铁头也不便宜，直接当耗材随便折腾就太浪费了。有必要搞清楚怎么使用与保养电烙铁： 焊接作业前，先为高温海绵加水湿润，再挤掉部分水分。 如果使用非湿润的清洁海绵，会使烙铁头受损氧化，导致不沾锡。 另外我发现「镀铜钢丝球」确实比高温海绵好用多了，墙裂推荐！笔不干净了往钢丝球里插一插，立即光亮如新。 焊接作业中，每次都先在高温海绵上擦干净烙铁头上的旧锡，再进行焊接。 如果是用「镀铜钢丝球」，那就直接在钢丝球里插一插，立即光洁如新。 中途不使用时，如果无自动休眠功能，可以手动将温度调低至 200 度以下，避免空烧。空烧会降低烙铁头寿命。 焊接完毕后，将温度调至约 250 摄氏度，使用湿润的高温海绵清洁烙铁头，最后将烙铁头加上一层新锡作保护，这样可以保护烙铁头和空气隔离，烙铁头不会氧化变黑。 烙铁头已经氧化、不沾锡时应如何处理 先把温度调到 300 摄氏度，用清洁海绵清理烙铁头，并检查烙铁头状况。 如果烙铁头的镀锡层部分含有黑色氧化物时，可镀上新锡层，再用清洁海绵抹净烙铁头。如此重复清理，直到彻底去除氧化物，然后在镀上新锡层。 将温度调至 200 摄氏度左右貌似比较容易上锡，不易聚成球。 实测上锡再用海绵抹除，每次都能摸走一些黑色氧化物，非常有效。不过要清理干净还是需要一些耐心。 如果烙铁头变形或穿孔，必须替换新咀。 其他注意事项 勿大力焊接：只要让烙铁头充分接触焊点，热量就可传递，无需大力焊接。 尽量低温焊接：高温焊接会加速烙铁头氧化，降低烙铁头使用寿命。如烙铁头温度超过470℃，它的氧化速度是380℃的两倍。 经常保持烙铁头上锡：这可以减低烙铁头的氧化机会，使烙铁头更耐用。 保持烙铁头清洁与及时清理氧化物 小心放入烙铁架：如果烙铁头接触到烙铁架无法自动休眠，长时间空烧将会毁掉烙铁头。 选用活性低的助焊剂：最便宜的就是松香，更好一点的是无铅无酸无卤素助焊剂。 焊锡丝在焊接过程中为什么会爆锡？ https://zhuanlan.zhihu.com/p/584316437 建议直接看上面的文章，可能的原因大概是： 受潮 焊锡丝混有杂质，或者助焊剂含量过高 焊接操作时手上有汗或是洗过手后手没有完全干就开始焊接 烙铁温度过高 我最近买的两卷焊锡丝就有爆锡的问题，烙铁温度是设的很常规的 320 度甚至更低的 290 度，现在怀疑是不是这个无铅焊锡丝有问题。罪魁祸首找到了，是因为我的锡线架，它下方就是湿润的高温海绵…这显然很容易受潮… 如何使用电烙铁进行拆焊当你焊错焊反了元件，或者你需要修修改改电路板时，就需要先进行拆焊。 用电烙铁进行拆焊需要注意这些事项： 温度必须要高，起码 350 以上 烙铁尖必须留点锡在上面。如果烙铁尖不挂锡，焊接的时候会发现即使温度高，电路板的焊锡也很难融化 另一个方式是先加点有铅锡丝降低焊锡熔点，然后再用吸锡器或吸锡带来吸 如果用吸锡器，发现不撤烙铁头直接把吸锡器怼上去，效果是最好的 如果使用的是吸锡带，温度就必须更高，估计至少得 380 甚至更高 因为吸锡带一般是纯铜，导热性能很好。一般 320 度就很容易熔的锡，上了吸锡带后热量全被吸锡带传导走了，温度不高根本融化不了 或者直接上热风枪+镊子也行（我还没试过…）。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:5:1","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#焊锡丝在焊接过程中为什么会爆锡"},{"categories":["tech"],"content":" 1. 电烙铁篇电烙铁主要考虑的是升温速度跟温度保持能力，便宜的电烙铁基本都有升温慢、焊接中途易失温等毛病。目前总结的电烙铁信息如下： 便携电烙铁：入门级别推荐 优缺点: 便携、价格低。但是升温相对焊台要慢一些，温控相对不够精确，而且无自动休眠， 空烧烙铁头容易氧化，再有就是它没有接地不防经典，焊接精密元件比较危险。 貌似主要推荐广东黄花 907 电烙铁，淘宝官方店买个刀头的 54 大洋 热风枪：主要用来拆焊，以及焊接贴片元件 + 芯片。 我买了一把德力西 2000W 的数显热风枪，不过貌似更多人推荐那种二合一焊台，直接控制热风枪跟电烙铁两个玩意儿。 焊台：进阶推荐，也可考虑一步到位… 优缺点: 发热很快、热容相对较大，自动休眠不会空烧、还有过流保护、单片机稳定温控。缺点是要贵一些，另外相对没那么便携。 相关流行产品 白菜白光 T12 恒温焊台，最早是网友基于日本白光公司 T12 烙铁头（日本工厂到期强制报废的洋垃圾）配上自制恒温控制电路完成的 DIY 焊台，因为相对高端焊台相当便宜所以冠以「白菜」之名。 淘宝上有一些卖这个的，质量见仁见智吧，我没买过。 日本白光 HAKKO 焊台：这个很多人推荐，说是质量好。不过贵，新手用可能有点奢侈了。 二合一焊台：焊台自带热风枪 + 高频电烙铁两件套，高手必备（一般拆机才会用到热风枪） 高频电烙铁使用的是跟电磁炉一样的高频涡流发热原理，电烙铁头自身发热，不需要任何发热芯， 发热很快、热容大、烙铁头更换便宜。高端烙铁头都是高频的。 反正就很高级也很贵啦。我现阶段买了它也是浪费钱，所以没了解具体型号啥的了 恒温加热台：功能跟热风枪差不多，但是体积大很多，而且更贵，新手不推荐买。 关于电烙铁头，貌似刀头是最推荐的，因为它用途最广泛，热容大，基本适用所有场景。 电烙铁，我最后买的第一把电烙铁是网友 DIY 的「L245 焊笔 玫瑰金」，铝合金 CNC 切割工艺，Type-C 供电，最高支持到 PD 120W，颜值很高，口碑也很好，价格是 148 大洋。使用起来还是比较 OK 的，热得很快，热容也 OK。不过毕竟是 DIY 的便宜焊笔，质量不稳，我有遇到过多次误休眠、未识别到焊芯、芯片系统崩溃等问题，都是靠断电重启解决的。 电烙铁的使用与保养 参考：http://www.cxg.cn/newshow1346.html 前面讲了，我毕竟买的是 150 一把的焊笔，C245 这个烙铁头也不便宜，直接当耗材随便折腾就太浪费了。有必要搞清楚怎么使用与保养电烙铁： 焊接作业前，先为高温海绵加水湿润，再挤掉部分水分。 如果使用非湿润的清洁海绵，会使烙铁头受损氧化，导致不沾锡。 另外我发现「镀铜钢丝球」确实比高温海绵好用多了，墙裂推荐！笔不干净了往钢丝球里插一插，立即光亮如新。 焊接作业中，每次都先在高温海绵上擦干净烙铁头上的旧锡，再进行焊接。 如果是用「镀铜钢丝球」，那就直接在钢丝球里插一插，立即光洁如新。 中途不使用时，如果无自动休眠功能，可以手动将温度调低至 200 度以下，避免空烧。空烧会降低烙铁头寿命。 焊接完毕后，将温度调至约 250 摄氏度，使用湿润的高温海绵清洁烙铁头，最后将烙铁头加上一层新锡作保护，这样可以保护烙铁头和空气隔离，烙铁头不会氧化变黑。 烙铁头已经氧化、不沾锡时应如何处理 先把温度调到 300 摄氏度，用清洁海绵清理烙铁头，并检查烙铁头状况。 如果烙铁头的镀锡层部分含有黑色氧化物时，可镀上新锡层，再用清洁海绵抹净烙铁头。如此重复清理，直到彻底去除氧化物，然后在镀上新锡层。 将温度调至 200 摄氏度左右貌似比较容易上锡，不易聚成球。 实测上锡再用海绵抹除，每次都能摸走一些黑色氧化物，非常有效。不过要清理干净还是需要一些耐心。 如果烙铁头变形或穿孔，必须替换新咀。 其他注意事项 勿大力焊接：只要让烙铁头充分接触焊点，热量就可传递，无需大力焊接。 尽量低温焊接：高温焊接会加速烙铁头氧化，降低烙铁头使用寿命。如烙铁头温度超过470℃，它的氧化速度是380℃的两倍。 经常保持烙铁头上锡：这可以减低烙铁头的氧化机会，使烙铁头更耐用。 保持烙铁头清洁与及时清理氧化物 小心放入烙铁架：如果烙铁头接触到烙铁架无法自动休眠，长时间空烧将会毁掉烙铁头。 选用活性低的助焊剂：最便宜的就是松香，更好一点的是无铅无酸无卤素助焊剂。 焊锡丝在焊接过程中为什么会爆锡？ https://zhuanlan.zhihu.com/p/584316437 建议直接看上面的文章，可能的原因大概是： 受潮 焊锡丝混有杂质，或者助焊剂含量过高 焊接操作时手上有汗或是洗过手后手没有完全干就开始焊接 烙铁温度过高 我最近买的两卷焊锡丝就有爆锡的问题，烙铁温度是设的很常规的 320 度甚至更低的 290 度，现在怀疑是不是这个无铅焊锡丝有问题。罪魁祸首找到了，是因为我的锡线架，它下方就是湿润的高温海绵…这显然很容易受潮… 如何使用电烙铁进行拆焊当你焊错焊反了元件，或者你需要修修改改电路板时，就需要先进行拆焊。 用电烙铁进行拆焊需要注意这些事项： 温度必须要高，起码 350 以上 烙铁尖必须留点锡在上面。如果烙铁尖不挂锡，焊接的时候会发现即使温度高，电路板的焊锡也很难融化 另一个方式是先加点有铅锡丝降低焊锡熔点，然后再用吸锡器或吸锡带来吸 如果用吸锡器，发现不撤烙铁头直接把吸锡器怼上去，效果是最好的 如果使用的是吸锡带，温度就必须更高，估计至少得 380 甚至更高 因为吸锡带一般是纯铜，导热性能很好。一般 320 度就很容易熔的锡，上了吸锡带后热量全被吸锡带传导走了，温度不高根本融化不了 或者直接上热风枪+镊子也行（我还没试过…）。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:5:1","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#如何使用电烙铁进行拆焊"},{"categories":["tech"],"content":" 2. 其他工具与材料焊材（建议日常用贵一点的无铅锡丝，虽然熔点高些，但对身体好）： 焊锡丝：最常见的焊材，不过稍微要求一点焊接技术，可能需要大约半个小时熟悉下 常用 0.8mm 跟 1.0mm 的锡丝 个人玩建议买无铅的，虽然贵点熔点高一点，但更环保，对身体也好。 锡膏：新型焊接材料，由焊锡粉、助焊剂以及其它的表面活性剂等混合成的膏状物。 对于常用焊接场景，可以直接抹上锡膏，然后用热风枪一吹，或者用烙铁刀头拖焊，或者直接上发热板 / 恒温加热台，据说非常简单好用。 最常用的场景是复杂 PCB 板子，直接用定制的钢丝网覆盖 PCB 板子刷上锡膏、直接就把触点都刷上了，然后再用镊子手工贴上贴片元器件。不过这个有难度…已经是高手玩法了。最省心是花钱直接找 PCB 厂子给打印 + 焊接（钞能力）。 对于焊点不多的贴片，可以直接使用针筒式的锡膏挤上去，然后再用牙签或镊子去掉多余的锡膏， 用镊子把贴片元件放上去（有点歪没事，加热时焊锡的张力会使它自动回正），最后直接上热风枪或加热台就能焊接 ok 了。 同样建议买无铅的，虽然贵点熔点高一点，但更环保，对身体也好。 高温海绵：可以说是焊接必备了，一定要加水湿润后再使用。可以多备几片，脏了洗洗，洗不干净就换。 镀铜钢丝球：同样是用于清洁烙铁头的，前面讲焊接技术时已经说过了，这个确实比高温海绵好用很多。 助焊剂 Flux： 在焊接工艺中能帮助和促进焊接过程，同时具有保护作用、阻止氧化反应的化学物质。 高纯度松香：便宜常用，一般焊个传感器跟普通 PCB 板子完全够用。 如果板子要长期使用，那焊完需要用酒精浸泡清洗，避免助焊剂碳化导致绝缘性能下降。（如果只是练手的板子，那就无所谓了） 无铅无卤无酸助焊剂：高端助焊剂，免洗 无铅主要是为了环保，对身体好。 因为卤素离子很难清洗干净，助焊剂残留将导致绝缘性能下降，因此免洗助焊剂必须得无卤素。 无酸是为了避免助焊剂腐蚀电路板跟、引脚、烙铁头。 以及其他焊接相关工具： 吸锡器：主要用于电器拆焊 场景：一是焊错了或者锡多了，拆焊后重新焊接。二是拆焊其他电路 这玩意儿一个便宜的才十多块，入门阶段买一个也行。不过也有说拿电烙铁热一下然后一磕，焊锡就自己掉下去了，自己玩不一定需要这玩意儿。 吸锡带：拿来清理表贴焊盘上的残锡。就是一卷细铜丝编制的带子，融化的锡容易被它吸走 比吸锡器更便宜万用，缺点是需要更高的温度，可以考虑买一卷。 热风枪：貌似主要是拆焊用的，当然用来吹热缩管也很好用。 焊台夹具：焊线焊板子都挺实用，相当于长出来四只手。而且相比放桌面，它的散热速度低很多，更难失温。 尖嘴钳：焊接完一些非贴片元件，必须要把多余的引脚剪掉，尖嘴钳感觉挺需要的。 维修工作台（耐高温硅胶垫）：淘宝上一二十块钱一块，可以保护桌子、方便放一些小元器件。 还有就是跟焊接没啥关系，但是 DIY 常用的工具： 切割垫：如果需要做一些切割，这个应该也很有用，看许多网友都有，不过我暂时没搞清楚自己是否需要。 螺丝磁性收纳垫：其实跟焊接关系不大了，不过也列一下 螺丝刀 + 万能扳手 + 水口钳：这个好像跟焊接没啥关系，不过也可以列一下 尤其是电动螺丝刀，刀头一定要买好一点的，并且最好是标准有替代品的。我以前用电动螺丝刀就遇到过刀头硬度不行被十字螺丝刀头磨平了的情况… 水口钳推荐德力西 螺丝 + 螺母：螺丝刀跟扳手都有了，螺丝螺母不得买几套？ 其中有些特别的是自锁螺母，这种螺母自带尼龙自锁圈，即使没拧到位也能自锁。不过需要用比较大的力气才能拧进去，这是正常现象。 螺母防松的六种基本方法，你知道几个？（动图） 螺丝的型号，DIY 中常用的，M3即螺絲外徑為 3mm, M4 即螺絲外徑為 4mm，同理 M5 即 5mm 有時會註明螺絲牙距，如 M3x0.5，M4x0.70，M5x0.8，M6x1，但因為这是標準規範，通常不提 对结构强度要求不高的场景，也可以自己用 3D 打印机打印螺丝螺母。 螺丝更详细的中英术语对照：螺絲規格與定義 - 緯丞螺絲 游标卡尺 + 卷尺：最简单的是买数字的，不需要费心思读数…也推荐德力西的 3D 打印机、激光切割机等等其他 DIY 工具 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:5:2","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#2-其他工具与材料"},{"categories":["tech"],"content":" 五、后续学习路线有了电路基础后，首先可以买一些入门的焊接套件练练焊接技术，并搞明白它的原理。我在淘宝「电子爱好者之家」上买了几个焊接套件，如指尖陀螺、5v 升 12v 升压板、LED 摇摇棒、十二个实验洞洞板套件、高压发生器等。 边玩边学习相关知识是最有意思的，玩到一定阶段后，可以再考虑补一补基础知识。基础理论方面我查到这几本（为了我的英语能力，选择读英文的）： Practical Electronics for Inventors, Fourth Edition: 中文版名为《实用电子元器件与电路基础》，是评论区 @辛未羊 推荐的，感觉确实很适合我这种业余新手入门~ Foundations of Analog and Digital Electronic Circuits： 这本书比较受推荐，中文版《模拟和数字电子电路基础》，豆瓣评分 9.3，不过我看了下比上面那本要难，感觉适合后面进阶看。 学习基础的电路理论时可以仿真软件同步学习，如： Multisim（元器件仿真）、Proteus（单片机仿真） 这两个软件都非常流行，不过基本都仅支持 Windows 系统，我选择放弃。 EDA（Electronic Design Automation） 电路板原理图、PCB（Printed Circuit Board） 设计工具 立创 EDA: 国产 EDA，全平台支持，也提供 Web 版 KiCAD: 开源电路板设计工具，功能强大，支持插件，社区资源多。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:6:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#五后续学习路线"},{"categories":["tech"],"content":" 1. 单片机有一定电路基础后，就可以开始玩单片机了。 介绍：单片机的英文名叫 Microcontroller Unit，缩写为 MCU. 它是把 CPU、RAM、定时/计数器（timer/counter）、I/O 接口等都集成在一块集成电路芯片上的微型计算机。 应用：主要用于前端的无操作系统、以实时控制为主的环境，如电子钟表、电机控制等。在硬件爱好者手中可用于机器人前端控制，四轴飞行器前端控制，3D打印机前端控制等。 典型产品： Arduino: AVR 单片机为核心控制器的单片机应用开发板，是开源硬件，新手友好 STM32: 貌似是单片机从业人员的入行首选，使用 ARM Cortex-M 系列核心。 补充说明： 单片机非常简单，因为很接近底层，而且硬件配置极差，干不了太多的事。主要的优势就是稳定、开发也简单。 单片机跟硬件的绑定很严重，经常出现一套代码换一个单片机平台，就得完全重写。 单片机最简单的玩法当属 esphome，只需要会 yaml 配置语言就能开始用 ESP32/ESP8266/ESP32-C3 等 MCU 玩智能家居，不需要写任何代码，生态非常丰富，作为入门路径感觉很合适（文章开头就说了，我就是从这玩意儿入坑的硬件…） 但是 ESPHome 毕竟太简单，用的都是别人写好的现成模块，想实现点更自定义的功能就得自己学习单片机编程了。 我的单片机编程学习路径大概是： 8051: 最简单最经典的单片机 我的 8051 汇编学习笔记与代码：ryan4yin/learn-8051-asm STM32: 工业届应用最广泛的单片机，网上资料众多。 开发工具链很成熟完善，不过有点偏底层，适合用于学习底层知识。 我的 STM32 学习笔记与代码（持续更新中，使用 C 语言，后续打算试下 Rust）：ryan4yin/learn-stm32f103c8t6 ESP32: 包含 wifi 蓝牙功能的 IoT 单片机，在物联网领域应用非常广泛，硬件发烧友的最爱。 乐鑫官方的 ESP-IDF 完全开源，功能比较完善，封装层次比 STM32 HAL 更高，而且迭代很快，用起来更简单（不过相对地就对底层更缺乏掌控）。 我的 ESP32 学习笔记与代码（同样持续更新中，也是用的 C，后面也打算用 Rust 搞搞）：electrical-engineering/esp32 其他 买了矽速科技新出的 Maix Zero M0S/M1S，使用 RISC-V 架构的 MCU，貌似目前必须用芯片官方 （博流智能）的 SDK 写代码。点了个灯就一直吃灰了（ 单片机领域目前仍然是 ARM32 架构的天下，不过开源免费的 RISC-V 架构发展迅猛，有望与 ARM32 分庭抗礼。目前乐鑫基于 RISC-V 的 ESP32C3 就挺受欢迎的，还出了书，另外后续版本 ESP32C5 也已经被 ESP-IDF 支持了，发展很快。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:6:1","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#1-单片机"},{"categories":["tech"],"content":" 2. 嵌入式 Linux（Linux on Embedded System） 嵌入式系统（Embedded System），是指嵌入机械或电气系统内部、具有专一功能和实时计算性能的计算机系统。 单片机玩够了后，就可以开始玩嵌入式 Linux了。 介绍：嵌入式 Linux，即运行 Linux 操作系统的、性能比 MCU 更高的微型计算机，行业上最常用 ARM Cortex-A5X 系列芯片与 Linux 开发一些嵌入式设备。 应用：路由器、电视盒子、智能家居等。在硬件爱好者手里可以用来做计算机视觉控制小车、WiFi、蓝牙控制中枢等等。 典型产品 Raspberry Pi: 使用 ARM Cortex-A 系列 CPU 的微型计算机，社区庞大，生态丰富。 其他各种国产派，如基于瑞芯微 RK35XX 系列 SoC 的 OrangePi、RockPi、野火鲁班猫等，它们都比现在的树莓派便宜很多（2023 年的 4B 2G 全新要 1000+ 太恐怖了），性能也更高，生态差一些不过瑕不掩瑜。 STM32/IMX6ULL 也有相关产品 补充说明 嵌入式 Linux 代码的可移植性相对要好很多，因为硬件相关的逻辑都封装在驱动层了。 我目前的学习顺序与进度： 瑞芯微 RK3588s 系列国产派: 性能贼强，还自带 NPU(2TOPS * 3) 玩耍的笔记代码放在了这里（Python 与 C 语言）electrical-engineering/rk3588 也用它玩上了 NixOS: ryan4yin/nixos-rk3588 树莓派 4B: 玩耍笔记与代码：electrical-engineering/raspberrypi 其他 除了前面俩，还兜兜转转玩了很多新产品，笔记都写在这里面了：electrical-engineering MAIX-III AXera-Pi AX620A（爱芯派），1.8TOPS 算力（标称 3.6TOPS 的一半不能用于 AI） 这块板子的 NPU 感觉性能还可以，但是 CPU 跟 IO 都有点拉，跑个 pip3 list 都要卡老半天。毕竟 A7 内核，估计性能也就这样了，全靠交叉编译续命。 没啥开源资料，只适合用来玩玩 AI，Linux 系统没啥可玩性。 鲁班猫 0 无线版（LubanCat Zero W） 基于 RK3566，开放的资料非常全，包含 SoC 原厂的各种文档、SDK 驱动开发包、核心板封装库，还提供许多免费的在线文档，内容包含 Linux 内核编译部署、Linux 驱动开发、嵌入式 QT 开发等等 因为资料很全，用来学 Linux 内核驱动开发感觉是比较合适的。 矽速科技的 LicheePi 4A，国产高性能 RISC-V 开发版。 已经用它玩上了 NixOS:ryan4yin/nixos-licheepi4a 群星闪耀家的 Milk-V Mars，同样是国产高性能 RISC-V 开发版，不过比 LicheePi 4A 弱一些 （价格低好多哪）。 用的是赛昉家的 JH7100 芯片，用 Nick_Cao 老师的代码跑了 NixOS 玩。 吐槽：它家这名字是会取的，不论是「群星闪耀」还是「Milk-V」都很有意思 其他我感兴趣的资料（资料内容有一定的重叠）： 《Linux/Unix 系统编程手册》：讲解 Linux 的主要系统 API 野火嵌入式 Linux 系列教程： 基础使用 + 内核编程： 感觉跟《Linux/Unix 系统编程手册》内容是重复的，可以简单过一过 Linux 镜像构建与部署: 跟随此文档自己构建一个 Linux 镜像部署到板卡上，这样可以更好的理解 Linux 的启动过程 Linux 驱动开发入门 - 基于鲁班猫 RK356X 系列板卡: Linux 驱动开发入门教程。 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。内容跟野火的教程差不多，可以对照学习。 另外还有本 2018 年出的Linux Driver Development for Embedded Processors 2nd Edition 可当作参考书看，写得没上面那本好、内容也没那么新，但是看评价也不错，特点是有许多的 Lab 可做。 Linux Kernel Programming: A comprehensive guide to kernel internals: Linux 内核编程领域的新书，适合入门 Linux 内核，amazon 上评价挺好，先收藏一个 南京大学 计算机科学与技术系 计算机系统基础 课程实验 (PA) Understanding the Linux Kernel, 3rd Edition：Linux 内核技术进阶。 嵌入式 Linux 领域目前也仍然是 ARM 架构的天下，但是开源免费的 RISC-V 架构发展也很快，性能越来越强，生态越来越好，很值得期待。 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:6:2","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#2-嵌入式-linuxlinux-on-embedded-system"},{"categories":["tech"],"content":" 其他最近也整了点 FPGA 玩，学了点 Verilog 语言，浅尝辄止，做了点笔记： ee/fpga ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:7:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#其他"},{"categories":["tech"],"content":" 社区公众号收藏单纯一个人埋头自学未免太过枯燥，效率也不一定高，偶尔也可以逛逛各种社区、看看相关的技术博客、文章，是一个更丰富的信息源。 我收集的一些相关论坛、公众号、交流群总结在了这里，可供参考： 可以给我推荐几个相关的论坛或者微信公众号吗？ - 知乎 ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:8:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#社区公众号收藏"},{"categories":["tech"],"content":" 最后简单总结下上面这些都学了一遍的话，业余玩玩硬件应该就很够用了，期待我完成这个学习路线的那一天… ","date":"2023-01-31","objectID":"/posts/electrical-engineering-circuits-basics-1/:9:0","series":["EE 入门"],"tags":["电子电路","Electrical Engineering","MCU"],"title":"EE 入门（一） - 电子电路基础知识","uri":"/posts/electrical-engineering-circuits-basics-1/#最后简单总结下"},{"categories":["life","tech"],"content":" 闲言碎语是的又过去一年，又到了一年一度的传统节目——年终总结时间。 ","date":"2023-01-02","objectID":"/posts/2022-summary/:1:0","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#闲言碎语"},{"categories":["life","tech"],"content":" 2022 年流水账先简单过一下我 2022 年的流水账（有记录一个 /history，回顾起来就是方便）： 1 月 购入 Synthesizer V + 青溯 AI 声库，简单调了几首歌试试，效果非常棒。然后就一直放了一年没碰它…还试用了免费的 ACE 虚拟歌姬，合成效果确实很强，跟收费的 Synthesizer V 有的一拼。 在家过春节，给家里二楼装了空调、加湿器跟地垫。但是没买地暖垫，导致开了空调后地上的垫子冰凉。后面补买了地暖垫但是已经要上班了没体验上。 2 月跟 3 月 想学下区块链技术，结果发现课程一开始就讲加密哈希函数的基本性质，就决定先搞一波密码学， 结果就是输出了一个《写给开发人员的实用密码学》系列文章， 内容大部分是翻译的，少部分是我自己补充。 主要工作：跟推荐系统大佬一起将服务从 HTTP 切换到 gRPC，效果立竿见影，服务流量下降 50% ~ 60%，延迟下降 30% ~ 50%。 4 月份 读完了 Mastering Ethereum，对以太坊有了基本的了解。 读了《Go 程序设计语言（英文版）》 Go 程序设计语言（英文版） 2022-08-19 补图 很高兴通过了职级晋升，不再是 SRE 萌新了。 主要工作：使用 aws/karpenter 实现离线计算集群的弹性扩缩容，省了一波成本。 5 月份 主要是学完了《深入浅出 Kubernetes》这个极客时间专栏 通过《分布式协议与算法实战》等相关资料简单了解了下分布式共识算法的原理，记录了些笔记，8 月份的时候把笔记整理输出为了一篇博客分布式系统的一致性问题与共识算法 还读了许多社区的区块链相关资料，包括但不限于Web 3.0：穿越十年的讨论 - 知乎、《Web3 DApp 最佳编程实践指南》、dcbuild3r/blockchain-development-guide 因为 AI 发展迅猛，来了三分钟兴趣学了一点动手学深度学习 - Pytorch 版，但是进度条走了不到 15% 就不了了之了。 主要工作：研究跨云应用部署方案与跨云 kubernetes 网络方案，如 karmada/kubevela/istio， 以及 L4/L7 层的开源/商业网关方案 6 月份 读完了《在生命的尽头拥抱你-临终关怀医生手记》 读了一点买的新书：《语言学的邀请》跟《Intimate Relationship》 7 月份 主要工作：确定并实施网关架构优化的初步方案，使用 Go 语言写了一个 Nginx Gateway 控制器，迁移流量到新容器化网关省了一波成本。 8 月 读完了《在峡江的转弯处 - 陈行甲人生笔记》 陈行甲人生笔记 延续上个月对 Linux 系统的兴趣，快速过了一遍 The ANSI C Programming Language 以熟悉 C 的语法，之后开始阅读 Linux/Unix 系统编程手册（上册） 写了一个小项目 video2ascii-c 练手 C 语言。 The ANSI C Programming Language 因为今年搞网关 APISIX/Nginx 接触比较多，看了一点极客时间《OpenResty 从入门到实战》但是因为兴趣并不强烈，又不了了之了。 主要工作： 搞网关优化省了一波成本，但是期间也搞出一个严重故障… 承接了一个数据上报网关的需求，需要在网关层支持一些稍微复杂点的功能确保升级流程的稳定性。跟 APISIX 官方沟通后得到了比较好的解决方案custom plugin - set an upstream as a http fallback server 9 月 偶然发现手机桌面上有一个安装了好久但是一直没用过的 APP 英语流利说，顺手用它测了下自己的英文水平。然后就对英语感兴趣了，制定了英语学习计划并发布对应的博文Learn English Again，然后就开始坚持学英语，感觉整个过程都很顺利。 主要工作： 仍然是搞网关优化省成本，因为各种原因，再次输出一篇 Post Mortem 搞数据上报网关的需求 10 月 找了很多英语学习资料，通过每日的坚持学习，渐渐找到了自己的英语学习节奏，完善了学习规划。 《Linux/Unix 系统编程手册（上册）》阅读进度过半，但是业余时间就这么点，同时用来学习 Linux 跟英语实在有点吃力，这本书的阅读就慢慢放下了。 Linux/Unix 系统编程手册（上册） 通过友链漫游，发现了 0xFFFF 社区，内容质量很高，也在社区的 QQ 群里跟群友们聊了些有意思有价值的内容。 打游戏学英语超飒的重剑女仆 Noelle DEEMO 2 中丰富的对话内容 因为许多原因，中概股大跌，公司架构大调整，走了很多大佬，包括去年带我冲浪的算法部门前辈。 主要工作 搞数据上报网关的需求，一路踩坑，总算把数万 QPS 的流量全部迁移到新网关上了。 11 月 重新对搞 Homelab 产生了兴趣，买了三台 MINI 主机组了一个 Homelab，时隔一年多又开始折腾 Proxmox VE，做各种规划。 迭代了很多次后的个人 Homelab 文档：ryan4yin/knowledge/homelab 我的 Homelab 导航页 2022-11-12 因为业余时间沉迷搞 Homelab，英语打卡就变得断断续续了…但是词汇量测试的效果出乎意料， 进步速度喜人，阅读能力也能感觉到有明显提升。 月底搬家换了个新租房，床是挂天花板上的，房间就宽敞了很多，而且拉了独立的电信宽带，网速杠杠的。 11/25 去东莞松山湖跟高中同学聚会，然后跟几位同学打麻将打到半夜三点多… 还远远眺望了眼同学读博的地方——「中国散裂中子源」，感觉很高大上 主要工作：继续推进线上网关优化项目，以及调研 K8s / Istio 的新版本变化，为集群升级做预备工作。 12 月 从 Homelab 折腾到 HomeAssistant/ESPHome，然后就折腾 ESP32/ESP8266，结果很意外地就买了一堆硬件，入手了电烙铁热风枪万用表等各种仪器，ESP/51/STM32 都玩了个遍… 输出内容有两个代码仓库：learn-8051-asm 与 learn-stm32f103c8t6，以及一份 EE 笔记：Electrical Engineering 我的电子电路初体验 8051 汇编 - 数码管显示 2023 ChatGPT 横空出世，引发全网热潮。有技术大佬感慨，这个时刻竟然来临得如此之快，惊喜之余也有点猝不及防。我也把玩了一波，也用它帮助我学了许多硬件相关的东西，很有帮助。 个人猜测未来 ChatGPT 成熟后大概率能极大提升技术人员的工作效率，很可能间接影响到许多人的工作。 年底还入手了一台 3D 打印机 ELEGOO Neptune 3 Pro… 全国逐渐放开疫情管控，我得了新冠，然后康复… 这个月折腾硬件，英语漏打卡更严重了，但是词汇量仍然在稳步增长，阅读起来也是越来越顺畅。 主要工作： 线上网关优化项目基本落地，取得了预期收益，但是没达到之前设的激进目标。（旧网关仍留存极少部分流量，还需要时间去统一网关架构） 做 K8s 集群升级准备，然后月底公司大面积新冠，拖慢了这项工作的进度，即使后调了升级时间，仍然感觉有点虚… 最后是连续三年蝉联我年度歌手的天依同学，截图放这里纪念一下： 我的网易云年度歌手 ","date":"2023-01-02","objectID":"/posts/2022-summary/:2:0","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#2022-年流水账"},{"categories":["life","tech"],"content":" 2022 年 Highlights","date":"2023-01-02","objectID":"/posts/2022-summary/:3:0","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#2022-年-highlights"},{"categories":["life","tech"],"content":" 1. 英语英语也是我今年比较惊喜的一个部分，很长一段时间内，我都觉得英语的优先级并不高，一直没有把它的学习排上日程，水平也一直没啥显著提升。 但是从今年 9 月份开始到现在这四个月的英语学习中，我的进步相当明显，从去年大概 4700 词，到现在测试结果为 6583 词，涨了近 2000 词，月均接近 500 词（按这个速度，2023 年 10000 词的目标好像没啥难度了）。 词汇量测试结果按时间排序如下，使用的测试工具是Test Your Vocabulary ： 2023-01-02 词汇量测试结果：6583 词 2022-12-19 词汇量测试结果：6300 词 2022-11-17 词汇量测试结果：5600 词 2022-10-18 词汇量测试结果：5100 词 另外因为主要是靠读书来学英语，今年的英文阅读能力也有明显提升，跟 9 月份刚开始读的时候比， 阅读体验要流畅多了。一些英文原版书阅读成就： 在薄荷阅读上读完的第一本英语原版书 而口语、写作这些今年基本没练习，原地踏步。 ","date":"2023-01-02","objectID":"/posts/2022-summary/:3:1","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#1-英语"},{"categories":["life","tech"],"content":" 2. 业余技术今年业余搞的技术，感觉这些都是我比较满意的： Web3: 今年上半年花了不少时间去了解 Web3，但是仍然没敢说自己已经懂了它。水比较深，浅尝辄止。 电子电路（硬件）：点亮这个技能完全是个意外…但也挺惊喜的，毕竟我大学学的建筑声学，以前都没接触过硬件。 Go 语言：去年底定的目标是将 Go 语言应用在至少两个项目上，实际上只用在了一个项目上，完成度 50% 吧。 Linux: Linux 今年主要是复习了一遍 C 语言，然后看了半本《Linux/Unix 系统编程手册（上册）》，之后因为学英语就给放下了。 毕竟英语的成果很不错，这个结果我觉得也是预期内的。 博客：今年博客经营得尚可，数了下有 18 篇技术干货，四篇非技术文章。最主要是三月份翻译密码学的文章冲了一波内容量。虽然 12 月份又鸽掉了…总体还是满意的。 ","date":"2023-01-02","objectID":"/posts/2022-summary/:3:2","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#2-业余技术"},{"categories":["life","tech"],"content":" 3. 工作SRE 组 2022 年工作的主旋律其实就是省钱，我 2022 年的工作上有更多的挑战，不过因为得心应手很多，反倒没什么想特别着墨描述的了。 我上半年工作成果比较突出，下半年虽然工作成果差一些，但是业余的学习成果相当突出，总体很满意自己今年的成绩。 单纯从工作方面讲，我给自己的评价仍然是「良好」。 ","date":"2023-01-02","objectID":"/posts/2022-summary/:3:3","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#3-工作"},{"categories":["life","tech"],"content":" 4. 阅读2022 年一共读完了这些书： 《人间失格》 《月宫》 《Practical Cryptography for Developers》 《Mastering Ethereum》 《Go 程序设计语言（英文版）》 《深入浅出 Kubernetes - 张磊》 《在生命的尽头拥抱你-临终关怀医生手记》 《在峡江的转弯处 - 陈行甲人生笔记》 《The ANSI C Programming Language》 The Time Machine Learn Robotics With Raspberry Pi 学习使用树莓派控制智能小车，结合本书与网上资料，我制作了一台使用 Xbox One 手柄遥控的四驱小车，相当有意思~ Learn Robotics Programming, 2nd Edition 跟前面那本一样是讲树莓派小车的，不过这本书更深入，代码含量高很多。 快速翻了一遍，跳过了其中大部分代码，因为书中的小车不太符合我的需求。 The Unlikely Pilgrimage of Harold Fry 51 单片机自学笔记 看起来，去年定的一个月至少读一本书的目标，还是达成了滴~ ","date":"2023-01-02","objectID":"/posts/2022-summary/:3:4","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#4-阅读"},{"categories":["life","tech"],"content":" 2023 年的展望","date":"2023-01-02","objectID":"/posts/2022-summary/:4:0","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#2023-年的展望"},{"categories":["life","tech"],"content":" 技术侧2022 年的结果跟年初的展望区别仍然是挺大的，但是我个人挺满意。 这里再记一下 2023 年技术上的展望，看看今年能实现多少，又会点出多少意料之外的技能吧哈哈： 云原生 去年定的阅读 k8s 及相关生态的源码没任何进度，2023 年继续… 2019 年到现在，我的工作时长已经有三年半了，希望更多的东西能通过学习底层知识去知其所以然，而不是全靠网上找资料，人云亦云一知半解地去解决问题。 Linux 与网络 2023 年把《Linux/Unix 系统编程手册》这套书看完，并且过完0xFFFF - MIT6.S081 Operating System Engineering (Fall 2020) 这个课，对 Linux 内核与操作系统形成较深入的理解。 借着对硬件的兴趣学一学 Linux 驱动开发。 学习学习时下超流行的 eBPF 技术 3D 打印 2022 年底买了台打印机，那不必须得打印点自己设计的东西？ FreeCAD 学！Blender 可能跟 3D 打印没啥关系但是也要学！ 编程语言 今年 Go/C 两个语言的技能点感觉是点出来了，2023 年需要巩固下，用它们完成些更复杂的任务。 另外借着搞硬件的兴趣，把 Rust/C++ 两门语言也玩一玩 C++ 主要是用来玩 ESP32/ESP8266，rust 那可是时下最潮的系统级语言，2022 年虽然用 rust 写了点 demo 但离熟练还差很远。 其他 2022 年我给开源社区提交的代码贡献几乎没有，希望 2023 年能至少给三个开源项目提交一些代码贡献，这也是检验自己的代码水平。 制作一台自己的无人机或者穿越机（虽然还不太懂什么是穿越机…），并借此练习自己学习的软硬件知识。 更多地在公司内部、博客等地方分享自己所学的知识，提升所学知识的可复用性，同时也碰撞出更多的灵光，更深入地理解它们。 ","date":"2023-01-02","objectID":"/posts/2022-summary/:4:1","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#技术侧"},{"categories":["life","tech"],"content":" 生活侧2022 年初我写的生活上的展望，貌似只有「阅读」这一项达标了… 不过今年也仍旧记录下 2023 年的展望： 2022 年因为疫情以及自己懒，参与的户外活动相当少，2023 年希望能更多的做些户外运动，身体还是很重要的啊。 把轮滑水平练上去一点，轮滑鞋在 2022 年吃灰了几乎一整年… 音乐上，口琴、竹笛、midi 键盘、Synthesizer V / ACE Studio / Reaper，总要把其中一个练一练吧…（什么？学吉他？？不敢开新坑了，旧坑都还没填完呢…） 阅读：仍然跟去年保持一样的节奏就好，目标是一个月至少阅读一本书。 英语：英语的规划在Learn English Again 中已经做得比较详尽了，这里仅摘抄下目标。 2023 年达到 CEFR 的 C1 等级，报考并取得 BEC 高级证书 2023 年底词汇量超过 10000 ","date":"2023-01-02","objectID":"/posts/2022-summary/:4:2","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#生活侧"},{"categories":["life","tech"],"content":" 结语2021 年的年终总结文末，我给自己 2022 年的期许是「更上一层楼」，感觉确实应验了。 那么 2023 年，我希望自己能够「认识更多有趣的人，见识下更宽广的世界」~ 更多有趣的的 2022 年度总结：https://github.com/saveweb/review-2022 ","date":"2023-01-02","objectID":"/posts/2022-summary/:5:0","series":["年终总结"],"tags":["总结"],"title":"2022 年年终总结","uri":"/posts/2022-summary/#结语"},{"categories":["tech"],"content":" 本文介绍我使用 PVE 的一些心得（不保证正确 emmmm），可能需要一定使用经验才能顺畅阅读。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:0:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#"},{"categories":["tech"],"content":" 前言我在去年的文章 「QEMU/KVM 虚拟化环境的搭建与使用」 中介绍了如何使用 QEMU/KVM 作为桌面虚拟化软件，其功能对标开源免费的Oracle VM VirtualBox 以及收费但是用户众多的VMware Workstation Pro. 虽然我们也可以远程使用 QEMU/KVM，但是使用门槛比较高。而且如果要管理多台服务器，各种命令也比较繁琐。我们显然需要更易用的软件来管理服务器场景下的虚拟化。 而这篇文章介绍的 Proxmox Virtual Environment（后续简称 PVE），就是一个基于 QEMU/KVM 的虚拟机集群管理平台。 PVE 以 Debian + QEMU/KVM + LXC 为基础进行了深度定制，提供了一套比较完善的 Web UI，基本上 95% 的操作都可以直接通过它的 Web UI 完成，但是仍然有些功能只需要使用它的 CLI 完成，或者说需要手动修改一些配置文件。 PVE 完全基于 Linux 世界的各种开源技术，存储技术使用了 LVM（也支持 Ceph/iSCSI/NFS），也支持通过 cloudinit 预配置网络、磁盘扩容、设置 hostname（这其实是 libvirtd 的功能）。它的文档也比较齐全，而且写得清晰易懂，还包含许多它底层的 QEMU/KVM/CEPH/Cloudinit 等开源技术的内容， 对学习 Linux 虚拟化技术也有些帮助。（这里必须喷下 VMware 的文档，真的是写得烂得一批，不知所云） 总的来说，PVE 没有 vSphere Hypervisor 跟 Windows Hyper-V 那么成熟、完善、稳定，但是基于 QEMU/KVM 且能够免费使用，很适合 Linux/开源/虚拟化 爱好者折腾。 你可能还听说过 OpenStack，不过这个玩意儿我没接触过，所以这里略过了它。 因为这些原因，我选择了 PVE 作为我的 Homelab 系统。 先贴一张我当前 Homelab 的 PVE 控制台截图，然后就进入正文。 我的 PVE 集群 如果你想了解我的 PVE 集群都跑了些啥，可以瞅一瞅homelab - ryan4yin/knowledge. ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:1:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#前言"},{"categories":["tech"],"content":" 一、安装 PVE 系统建议直接使用 ventoy 制作一个 U 盘启动盘，把官网下载的 PVE ISO 镜像拷贝进去，即可使用它进行系统安装。安装过程中需要注意的点有： 如果你有多台机器，每台机器需要使用不同的主机名称（hostname），否则后面组建 PVE 集群时会有麻烦。 建议使用机器型号 + 数字编号作为机器的 hostname 为每台 PVE 节点配置静态 IP，避免 IP 变更。 系统安装好后即可按照提示直接访问其 Web UI，会提示 HTTPS 证书无效，忽略即可。另外还会有一个烦人的 PVE 订阅提示，也可直接忽略（7.2 及以上版本，暂时没找到怎么禁用掉这个提示）。 此外对于国内环境，建议使用如下命令配置国内镜像源（提升软件安装速度）： shell # 设置 debian 的阿里镜像源 cp /etc/apt/sources.list /etc/apt/sources.list.bak sed -i \"s@\\(deb\\|security\\).debian.org@mirrors.aliyun.com@g\" /etc/apt/sources.list # 设置 pve 国内镜像源 # https://mirrors.bfsu.edu.cn/help/proxmox/ echo 'deb https://mirrors.bfsu.edu.cn/proxmox/debian buster pve-no-subscription' \u003e /etc/apt/sources.list.d/pve-no-subscription.list ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:2:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#一安装-pve-系统"},{"categories":["tech"],"content":" 组建 PVE 集群 如果你仅使用单机 PVE，可忽略这一节内容。 将多台 PVE 节点组成一个集群，可以获得很多新玩法，比如虚拟机在多节点间的热迁移。 注意 CPU 架构差别较大很可能会导致无法热迁移，建议使用同品牌、同代的 CPU，最好是 CPU 型号完全一致。比如都是 Intel 的 12 代 CPU，或者都是 AMD 的 5 代 CPU。 这个也还挺简单的，首先随便登入一台机器的 Web Console，点击「Datacenter」=\u003e「Cluster」=\u003e「Create Cluster」即可创建一个 PVE 集群。 接着复制「Join Information」中的内容，在其他每台 PVE 节点的 Web Console 页面中，点击「Datacenter」=\u003e「Cluster」=\u003e「Join Cluster」，然后粘贴前面复制的「Join Information」，再输入前面节点的密码，等待大约一分钟，然后刷新页面，PVE 集群即组建完成。 PVE 集群配置 PVE 集群的所有节点是完全平等的，集群组建完成后，登录其中任意一个节点的 Web Console 都可以管理集群中所有节点的资源。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:2:1","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#组建-pve-集群"},{"categories":["tech"],"content":" 二、PVE 控制台的使用PVE 控制台的使用还挺简单的，多试试基本就会用了。这里不做详细介绍，主要说明下创建虚拟机时一些重要的参数： CPU 将 CPU 类型设置为 host 可以提高性能，适合比较吃性能或者对实时性要求高的虚拟机如 windows/openwrt 对于虚拟机核数，建议将 sockets 设为 1（即 CPU 插槽数，一般物理服务器才会有 2 及以上的 CPU 插槽），cores 设为你想分配给该虚拟机的 CPU 核数 仅针对多物理 CPU 场景（多 sockets）才需要启用 NUMA（个人猜测，可能有错） 磁盘、网卡 磁盘驱动建议用 virtio SCSI、网卡驱动建议用 VirtIO(paravirtualized)，它的性能更高。 Linux 虚拟机原生支持 virtio 半虚拟化，而 windows 想要完全开启半虚拟化，需要手动安装驱动，详见Windows_VirtIO_Drivers - Proxmox WIKI， 简单的说就是要下个 iso 挂载到 windows 主机中，并安装其中的驱动。 如果硬盘是 SSD，虚拟机磁盘可以启用 SSD Emulation，对于 IO 性能要求高的场景还可以为磁盘勾选 IO Thread 功能 显示器 默认使用 std 类型，兼容性最好，但是是纯 CPU 模拟的，比较耗 CPU。 如果你有需要显卡加速的桌面虚拟机，但是又不想搞复杂的显卡直通，可以选择VirGL GPU(virtio-gl) 类型（注意不是 VirtIO-GPU(virtio)，这个驱动没有显卡加速能力），它能以较小的性能损耗将虚拟机中的 3D/2D 运算 offload 到 host GPU，而且避免复杂的驱动配置，只需要在 PVE 中执行。但是目前它仅支持 Linux 4.4+ 的 Guest 主机，并且要求 mesa (\u003e=11.2) compiled with the option gallium-drivers=virgl（我感觉这功能目前还有点鸡肋）。 要使用 VirGL GPU(virtio-gl)，还需要在 PVE 主机上安装额外的依赖：apt install libgl1 libegl1，安装好后即可使用。 详见QEMU Graphic card - Proxmox VE 其他选项 调整启动项顺序，对于 cloud image 建议只启用 scsi0 这个选项 虚拟机模板（Template）与克隆（Clone） 建议首先使用 ubuntu/opensuse cloud image 配置好基础环境（比如安装好 vim/curl/qemu-guest-agent），然后转换为 template，其他所有 Linux 虚拟机都可以直接 clone 一个，改个新名字，再改改 cloudinit 配置跟磁盘大小，就能直接启动使用了。相当方便。 仅 Full Clone 的虚拟机才可以在 PVE 集群节点间随意迁移，因此如果你需要虚拟机迁移功能， 请不要使用 Link Clone. BIOS 通常都建议使用默认的 SeaBIOS，仅 Windows 等场景才建议换成 OMVF(UEFI) OMVF 的分辨率、Secure Boot 等参数，都可以在启动时按 ESC 进入 UEFI 配置界面来调整。 上面这些内容，官方有详细文档，能读英文的话可以直接看Qemu/KVM Virtual Machines - Proxmox WIKI. ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:3:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#二pve-控制台的使用"},{"categories":["tech"],"content":" 1. 使用 cloudinit 自动配置网卡、SSH密钥、存储空间 完全参照官方文档Cloud-Init_Support - PVE Docs 注意：下面的几种镜像都分别有自己的坑点，仅 Ubuntu/OpenSUSE 测试通过，其他发行版的 Cloud 镜像都有各种毛病… 一般配 Linux 虚拟机，我们当然希望能在虚拟机启动时，就自动配置好 IP 地址、SSH 密钥、文件系统自动扩容，这样能免去很多手工操作。cloudinit 就是一个能帮你自动完成这些功能的工具，AWS、阿里云等各大云服务厂商都支持这种配置方式，好消息是 PVE 也支持。 下面简单介绍下如何使用 cloudinit 来自动化配置 Linux 虚拟机。 首先 cloudinit 必须使用特殊的系统镜像，下面是几个知名发行版的 Cloud 系统镜像： Ubuntu Cloud Images (RELEASED): 提供 img 格式的裸镜像（PVE 也支持此格式） 请下载带有 .img 结尾的镜像，其中以 kvm.img 结尾的镜像会更精简一点，而名称中不包含 kvm 的镜像会稍微大一点，但是带了所有常用的内核模块。（如果你不理解前者精简了啥，请选择后者——也就是稍大的这个镜像文件。） OpenSUSE Cloud Images 请下载带有 NoCloud 或者 OpenStack 字样的镜像。 对于其他镜像，可以考虑手动通过 iso 来制作一个 cloudinit 镜像，参考openstack - create ubuntu cloud images from iso 注：Debian Cloud Images 的镜像无法使用，其他 ubuntu/opensuse 的 cloud 镜像也各有问题…在后面的常见问题中有简单描述这些问题。 这里评论区有些新内容，指出 cloud image 的各种毛病可能的解决方案，想深入了解请移步评论区。 上述镜像和我们普通虚拟机使用的 ISO 镜像的区别，一是镜像格式不同，二是都自带了cloud-init/cloud-utils-growpart 等用于自动化配置虚拟机的相关工具。 其名字中的 NoCloud 表示支持 cloudinit NoCloud 数据源——即使用 seed.iso 提供 user-data/meta-data/network-config 配置，PVE 就是使用的这种模式。而 Openstack 镜像通常也都支持 NoCloud 模式，所以一般也是可以使用的。 以 ubuntu 的 cloudimg 镜像为例，下载好镜像后，首先创建虚拟机，并以导入的磁盘为该虚拟机的硬盘，命令如下： 如下操作也可在 Web UI 上操作，这里仅以命令行为例。 shell # 创建新虚拟机 qm create 9000 --name ubuntu-bionic-template --memory 2048 --net0 virtio,bridge=vmbr0 # 将下载好的 img/qcow2 镜像导入为新虚拟机的硬盘 qm importdisk 9000 ubuntu-20.10-server-cloudimg-amd64.img local-lvm # 通过 scsi 方式，将导入的硬盘挂载到虚拟机上 qm set 9000 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-9000-disk-0 # qcow2 镜像默认仅 2G 大小，需要手动扩容到 32G，否则虚拟机启动会报错 qm resize 9000 scsi0 32G 然后创建挂载 cloud-init 的 seed.iso，修改启动项以及其他： shell # 创建一个 cloud-init 需要使用的 CDROM 盘(sr0) qm set 9000 --ide2 local-lvm:cloudinit # 设置系统引导盘 qm set 9000 --boot c --bootdisk scsi0 # 设置 serial0 为显示终端，很多云镜像都需要这个。 qm set 9000 --serial0 socket --vga serial0 上面的工作都完成后，还需要做一些后续配置 手动设置 cloud-init 参数，重新生成 cloudinit image，启动虚拟机，并通过 ssh 登入远程终端 cloud image 基本都没有默认密码，并且禁用了 SSH 密码登录。必须通过 cloud-init 参数添加私钥、设置账号、密码、私钥。 检查 qemu-guest-agent，如果未自带，一定要手动安装它！ ubuntu 需要通过 sudo apt install qemu-guest-agent 手动安装它 安装所需的基础环境，如 docker/docker-compose/vim/git/python3 关闭虚拟机，然后将虚拟机设为模板 接下来就可以从这个模板虚拟机，克隆各类新虚拟机了~ 保险起见，改完配置后记得点下 Regenerate Image 其他 cloudinit 相关文档： 配置 Cloud-Init 工具 - 华为云 canonical/cloud-init - github Run Amazon Linux 2 as a virtual machine on premises ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:3:1","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#1-使用-cloudinit-自动配置网卡ssh密钥存储空间"},{"categories":["tech"],"content":" 2. 虚拟机硬盘扩容CentOS/Ubuntu/Debian 提供的 Cloud 镜像，都自带了 cloud-utils-growpart 这个组件，可以实现在扩容物理硬盘时，自动调整 Linux 的分区大小。 因此需要扩容虚拟机时，直接通过 UI 面板/命令行扩容虚拟机的硬盘，然后重启虚拟机即可，Linux的分区会在系统启动阶段被 cloud-utils-growpart 自动扩容。 PVE 可通过如下命令进行磁盘扩容： shell # 将 id 为 9000 的虚拟机的 scsi0 磁盘，扩容到 32G # 请自行修改虚拟机 ID 与磁盘大小，注意仅支持扩容！不能缩容。 qm resize 9000 scsi0 32G 而其他非 Cloud 镜像，则需要在扩容磁盘后再进入虚拟机手动扩容分区跟磁盘，具体命令就不介绍了，请自行查阅相关文档吧。 因为这个方便的特性，也为了减少虚拟化的开销，Cloud 镜像默认是不使用 LVM 逻辑分区的。LVM逻辑分区虽然方便，但是它对物理机的作用更大些。虚拟机因为本身就能动态扩容“物理硬盘”的大小， 基本不用不到 LVM。 还有一点，就是虚拟机通常只需要一个根分区就行，尤其是归 openstack/kubernetes 管的虚拟机。只有在使用分布式存储之类的场景下，数据需要独立存储，才需要用到额外的分区(/data 之类的)。一般只有物理机，才需要像网上很多文章提的那样，为 /boot / /home 去单独分区。而且现在大家都用 SSD 了，物理机这样做分区的都少了，比如我个人电脑，就是一个 / 分区打天下。。。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:3:2","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#2-虚拟机硬盘扩容"},{"categories":["tech"],"content":" 三、常见问题","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#三常见问题"},{"categories":["tech"],"content":" 1. 导入已有的 qcow2 镜像 这一步必须要命令行操作，WebUI 界面不支持。 首先在页面上新建一台新虚拟机，记录下虚拟机 ID。 假设你创建的虚拟机 id 为 201，现在通过 scp/rsync 等手段将 qcow2 传输到 PVE 节点上，然后命令行使用如下命令导入 qcow2/img 镜像： shell # 命令格式 qm importdisk \u003cvmid\u003e \u003csource\u003e \u003cstorage\u003e # 示例 qm importdisk 201 vm-201-disk-1.qcow2 local-lvm 导入完成后，在虚拟机的 WebUI 界面中，会看到底下多了一个「未使用的磁盘 0」。 接着删除掉默认的磁盘（分离+删除，要两步），再挂载这个「未使用的磁盘 0」。 挂载完成后直接启动是不行的，还需要在设置中将新磁盘添加到启动项中，这样就能正常启动了。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:1","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#1-导入已有的-qcow2-镜像"},{"categories":["tech"],"content":" 2. 点击 shutdown 后 PVE 系统卡住PVE 的 shutdown 功能依赖 qemu-guest-agent，对于还没有安装 qemu-guest-agent 的任何主机，或者已经卡死无响应的虚拟机，千万不要点 shutdown 按钮，因为一定会卡住很久，最后失败！ shutdown 卡住的解决办法：手动在下方的「Tasks」面板中双击卡住的「Shutdown」操作，然后点击「stop」停止该操作。 该如何关闭这类没有 qemu-guest-agent 或者已经卡死无响应的主机？答案是使用 stop！ ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:2","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#2-点击-shutdown-后-pve-系统卡住"},{"categories":["tech"],"content":" 3. can’t lock file ‘/var/lock/qemu-server/lock-xxx.conf’ – got timeoutPVE 虚拟机卡在 BIOS 系统引导这一步，无法启动，也无法 stop！ 解决方法：手动删除掉 lockfile: /var/lock/qemu-server/lock-xxx.conf 因为虚拟机还卡在 BIOS 引导这一步，删除掉 lockfile 再关闭虚拟机并不会导致数据丢失。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:3","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#3-cant-lock-file-varlockqemu-serverlock-xxxconf--got-timeout"},{"categories":["tech"],"content":" 4. PVE 集群有一个节点宕机，如何解除关联？将多个节点组成一个 PVE Cluster 是很自然的一个选择，它能提供虚拟机热迁移、统一管理面板等非常方便的功能。但是这会带来集群级别的高可用问题。 根据官方文档 Cluster_Manager - Proxmox，如果你需要容忍一定数量的节点宕机，PVE Cluster 至少需要三台主机（这跟 Etcd 一样，大概是 Raft 共识算法的要求），并且所有节点的 PVE 版本要完全一致。 那么如果个别节点出了问题，无法修复，该如何将它踢出集群呢？ 如果在线节点占比超过 50%，节点删除的流程如下： 首先通过访问节点的 shell 界面，通过命令 pvecm nodes 确认集群的所有节点 将需要移除的节点彻底关机，并且确保它不会以当前配置再次启动（也就是说关机后需要清空硬盘， 避免数据混乱） 如果被删除节点已宕机，则可跳过 关机 步骤 通过命令 pvecm delnode xxx 将问题节点移除集群 重置旧节点硬盘，并重新装机，用做其他用途。 如果你的集群只有 2 个节点，或者有超过 3 个节点但是宕机节点数不低于 50%，那出于数据一致性要求 Raft 算法会禁止更新集群数据，上面的流程就走不通了。如果你直接走上面的流程，它会报错cluster not ready - no quorum? 这时需要首先修改配置，使剩下的节点重新达成一致。其实就是修改选主节点时的投票数。 对于 2 个节点但挂掉 1 个的情况，首先执行如下指令允许当前节点自己选主： shell # 设置只需要 1 票就能当前主节点 # 潜在问题是可能有些集群元数据只在损坏节点上有，这么改会导致这些数据丢失，从而造成一些问题。 # 安全起见，建议在修复集群后，再重启一遍节点... pvecm expected 1 现在 quorum 就已经恢复了，可以走前面给出的节点移除流程。 如果节点已经删除，但是 Web GUI 上仍然显示有被删除的节点，可以在集群的所有剩余节点上，手动删除掉 /etc/pve/nodes/node-name/ 文件夹，即可从集群中彻底删除该节点的数据，注意千万别删错了，不然就尴尬了… 如果 corosync 完全无法启动，上面给出的命令也会修改选主投票参数也会失败，这时可以直接手动修改 /etc/corosync/corosync.conf 删除掉有问题的节点对应的配置，调低 expected 投票数，使 corosync 能正常启动，再执行前述操作。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:4","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#4-pve-集群有一个节点宕机如何解除关联"},{"categories":["tech"],"content":" 5. cloud image 的坑 ubuntu cloud image 的坑 ubuntu 启动时会报错 no such device: root，但是过一会就会正常启动。 这是 ubuntu cloud image 的 bug: https://bugs.launchpad.net/cloud-images/+bug/1726476 ubuntu 启动后很快就会进入登录界面，但是 root 密码可能还没改好，登录会报密码错误，等待一会再尝试登录就 OK 了 ubuntu 的默认网卡名称是 ens3，不是 eth0，注意修改 network_config 的网卡名称，否则网络配置不会生效 以 kvm 结尾的 Ubuntu Cloud Image 无法识别到 USB 设备，将 USB 端口映射到该虚拟机中没有任何作用。 kvm 使用了精简版的 linux 内核，去掉了 USB 等在云上用不到的驱动，建议改用无 kvm 结尾的镜像。 「Ubuntu Cloud Image 无法识别到 USB 设备」的排查记录现象： 在尝试使用 PVE 将 USB 接口直通到 Ubuntu Cloud Image 启动的虚拟机作为 NAS 系统时，发现lsblk 根本无法找到我的 USB 硬盘 换成我笔记本接硬盘盒，能够正常识别并挂载硬盘 使用 lsusb 不会报错，但是也看不到任何内容 使用 lspci 能找到 USB 对应的 PCI 设备 进一步使用 cat /proc/modules | grep usb 与 lsmod | grep usb 均查不到任何 usb 相关的内核模块 而在我笔记本上 lsmod | grep usb 能够输出 usb_storage usb_core 等多项内核模块。 再用 modprobe usb 会提示modprobe: FATAL: Module usb not found in directory /lib/modules/5.15.0-1021-kvm 问题原因很明显了，Ubuntu 根本没有为 cloud image 预置 usb 内核模块，所以才有这个问题… 进一步搜索发现这个帖子：What’s the difference between ubuntu’s amd64-disk-kvm.img and the regular amd64.img cloud images?， 解答了我的疑惑。 原因是，我使用了 ubuntu 为 cloud 环境做了精简的 kvm 内核，非常轻量，但是缺少 usb 等常用内核模块。 对于 NAS 外接存储这个场景，我应该使用不以 kvm 结尾的 ubuntu cloud image，换了个基础镜像后问题就解决了~ opensuse cloud image 的坑 opensuse leap 15 只支持 network_config v1，对 v2 的支持有 bug，gateway4 不会生效 debian cloud image 的坑debian 的 cloud 镜像根本没法用，建议避免使用它。 debian 启动时会彻底卡住，或者直接报错 kernel panic 原因是添加了 spice 图形卡，换成 vnc 就正常了 Debian Cloud Images 中的 nocloud 镜像不会在启动时运行 cloudinit，cloudinit 完全不生效 不知道是啥坑，没解决 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:5","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#5-cloud-image-的坑"},{"categories":["tech"],"content":" 5. cloud image 的坑 ubuntu cloud image 的坑 ubuntu 启动时会报错 no such device: root，但是过一会就会正常启动。 这是 ubuntu cloud image 的 bug: https://bugs.launchpad.net/cloud-images/+bug/1726476 ubuntu 启动后很快就会进入登录界面，但是 root 密码可能还没改好，登录会报密码错误，等待一会再尝试登录就 OK 了 ubuntu 的默认网卡名称是 ens3，不是 eth0，注意修改 network_config 的网卡名称，否则网络配置不会生效 以 kvm 结尾的 Ubuntu Cloud Image 无法识别到 USB 设备，将 USB 端口映射到该虚拟机中没有任何作用。 kvm 使用了精简版的 linux 内核，去掉了 USB 等在云上用不到的驱动，建议改用无 kvm 结尾的镜像。 「Ubuntu Cloud Image 无法识别到 USB 设备」的排查记录现象： 在尝试使用 PVE 将 USB 接口直通到 Ubuntu Cloud Image 启动的虚拟机作为 NAS 系统时，发现lsblk 根本无法找到我的 USB 硬盘 换成我笔记本接硬盘盒，能够正常识别并挂载硬盘 使用 lsusb 不会报错，但是也看不到任何内容 使用 lspci 能找到 USB 对应的 PCI 设备 进一步使用 cat /proc/modules | grep usb 与 lsmod | grep usb 均查不到任何 usb 相关的内核模块 而在我笔记本上 lsmod | grep usb 能够输出 usb_storage usb_core 等多项内核模块。 再用 modprobe usb 会提示modprobe: FATAL: Module usb not found in directory /lib/modules/5.15.0-1021-kvm 问题原因很明显了，Ubuntu 根本没有为 cloud image 预置 usb 内核模块，所以才有这个问题… 进一步搜索发现这个帖子：What’s the difference between ubuntu’s amd64-disk-kvm.img and the regular amd64.img cloud images?， 解答了我的疑惑。 原因是，我使用了 ubuntu 为 cloud 环境做了精简的 kvm 内核，非常轻量，但是缺少 usb 等常用内核模块。 对于 NAS 外接存储这个场景，我应该使用不以 kvm 结尾的 ubuntu cloud image，换了个基础镜像后问题就解决了~ opensuse cloud image 的坑 opensuse leap 15 只支持 network_config v1，对 v2 的支持有 bug，gateway4 不会生效 debian cloud image 的坑debian 的 cloud 镜像根本没法用，建议避免使用它。 debian 启动时会彻底卡住，或者直接报错 kernel panic 原因是添加了 spice 图形卡，换成 vnc 就正常了 Debian Cloud Images 中的 nocloud 镜像不会在启动时运行 cloudinit，cloudinit 完全不生效 不知道是啥坑，没解决 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:5","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#ubuntu-cloud-image-的坑"},{"categories":["tech"],"content":" 5. cloud image 的坑 ubuntu cloud image 的坑 ubuntu 启动时会报错 no such device: root，但是过一会就会正常启动。 这是 ubuntu cloud image 的 bug: https://bugs.launchpad.net/cloud-images/+bug/1726476 ubuntu 启动后很快就会进入登录界面，但是 root 密码可能还没改好，登录会报密码错误，等待一会再尝试登录就 OK 了 ubuntu 的默认网卡名称是 ens3，不是 eth0，注意修改 network_config 的网卡名称，否则网络配置不会生效 以 kvm 结尾的 Ubuntu Cloud Image 无法识别到 USB 设备，将 USB 端口映射到该虚拟机中没有任何作用。 kvm 使用了精简版的 linux 内核，去掉了 USB 等在云上用不到的驱动，建议改用无 kvm 结尾的镜像。 「Ubuntu Cloud Image 无法识别到 USB 设备」的排查记录现象： 在尝试使用 PVE 将 USB 接口直通到 Ubuntu Cloud Image 启动的虚拟机作为 NAS 系统时，发现lsblk 根本无法找到我的 USB 硬盘 换成我笔记本接硬盘盒，能够正常识别并挂载硬盘 使用 lsusb 不会报错，但是也看不到任何内容 使用 lspci 能找到 USB 对应的 PCI 设备 进一步使用 cat /proc/modules | grep usb 与 lsmod | grep usb 均查不到任何 usb 相关的内核模块 而在我笔记本上 lsmod | grep usb 能够输出 usb_storage usb_core 等多项内核模块。 再用 modprobe usb 会提示modprobe: FATAL: Module usb not found in directory /lib/modules/5.15.0-1021-kvm 问题原因很明显了，Ubuntu 根本没有为 cloud image 预置 usb 内核模块，所以才有这个问题… 进一步搜索发现这个帖子：What’s the difference between ubuntu’s amd64-disk-kvm.img and the regular amd64.img cloud images?， 解答了我的疑惑。 原因是，我使用了 ubuntu 为 cloud 环境做了精简的 kvm 内核，非常轻量，但是缺少 usb 等常用内核模块。 对于 NAS 外接存储这个场景，我应该使用不以 kvm 结尾的 ubuntu cloud image，换了个基础镜像后问题就解决了~ opensuse cloud image 的坑 opensuse leap 15 只支持 network_config v1，对 v2 的支持有 bug，gateway4 不会生效 debian cloud image 的坑debian 的 cloud 镜像根本没法用，建议避免使用它。 debian 启动时会彻底卡住，或者直接报错 kernel panic 原因是添加了 spice 图形卡，换成 vnc 就正常了 Debian Cloud Images 中的 nocloud 镜像不会在启动时运行 cloudinit，cloudinit 完全不生效 不知道是啥坑，没解决 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:5","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#ubuntu-cloud-image-无法识别到-usb-设备的排查记录"},{"categories":["tech"],"content":" 5. cloud image 的坑 ubuntu cloud image 的坑 ubuntu 启动时会报错 no such device: root，但是过一会就会正常启动。 这是 ubuntu cloud image 的 bug: https://bugs.launchpad.net/cloud-images/+bug/1726476 ubuntu 启动后很快就会进入登录界面，但是 root 密码可能还没改好，登录会报密码错误，等待一会再尝试登录就 OK 了 ubuntu 的默认网卡名称是 ens3，不是 eth0，注意修改 network_config 的网卡名称，否则网络配置不会生效 以 kvm 结尾的 Ubuntu Cloud Image 无法识别到 USB 设备，将 USB 端口映射到该虚拟机中没有任何作用。 kvm 使用了精简版的 linux 内核，去掉了 USB 等在云上用不到的驱动，建议改用无 kvm 结尾的镜像。 「Ubuntu Cloud Image 无法识别到 USB 设备」的排查记录现象： 在尝试使用 PVE 将 USB 接口直通到 Ubuntu Cloud Image 启动的虚拟机作为 NAS 系统时，发现lsblk 根本无法找到我的 USB 硬盘 换成我笔记本接硬盘盒，能够正常识别并挂载硬盘 使用 lsusb 不会报错，但是也看不到任何内容 使用 lspci 能找到 USB 对应的 PCI 设备 进一步使用 cat /proc/modules | grep usb 与 lsmod | grep usb 均查不到任何 usb 相关的内核模块 而在我笔记本上 lsmod | grep usb 能够输出 usb_storage usb_core 等多项内核模块。 再用 modprobe usb 会提示modprobe: FATAL: Module usb not found in directory /lib/modules/5.15.0-1021-kvm 问题原因很明显了，Ubuntu 根本没有为 cloud image 预置 usb 内核模块，所以才有这个问题… 进一步搜索发现这个帖子：What’s the difference between ubuntu’s amd64-disk-kvm.img and the regular amd64.img cloud images?， 解答了我的疑惑。 原因是，我使用了 ubuntu 为 cloud 环境做了精简的 kvm 内核，非常轻量，但是缺少 usb 等常用内核模块。 对于 NAS 外接存储这个场景，我应该使用不以 kvm 结尾的 ubuntu cloud image，换了个基础镜像后问题就解决了~ opensuse cloud image 的坑 opensuse leap 15 只支持 network_config v1，对 v2 的支持有 bug，gateway4 不会生效 debian cloud image 的坑debian 的 cloud 镜像根本没法用，建议避免使用它。 debian 启动时会彻底卡住，或者直接报错 kernel panic 原因是添加了 spice 图形卡，换成 vnc 就正常了 Debian Cloud Images 中的 nocloud 镜像不会在启动时运行 cloudinit，cloudinit 完全不生效 不知道是啥坑，没解决 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:5","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#opensuse-cloud-image-的坑"},{"categories":["tech"],"content":" 5. cloud image 的坑 ubuntu cloud image 的坑 ubuntu 启动时会报错 no such device: root，但是过一会就会正常启动。 这是 ubuntu cloud image 的 bug: https://bugs.launchpad.net/cloud-images/+bug/1726476 ubuntu 启动后很快就会进入登录界面，但是 root 密码可能还没改好，登录会报密码错误，等待一会再尝试登录就 OK 了 ubuntu 的默认网卡名称是 ens3，不是 eth0，注意修改 network_config 的网卡名称，否则网络配置不会生效 以 kvm 结尾的 Ubuntu Cloud Image 无法识别到 USB 设备，将 USB 端口映射到该虚拟机中没有任何作用。 kvm 使用了精简版的 linux 内核，去掉了 USB 等在云上用不到的驱动，建议改用无 kvm 结尾的镜像。 「Ubuntu Cloud Image 无法识别到 USB 设备」的排查记录现象： 在尝试使用 PVE 将 USB 接口直通到 Ubuntu Cloud Image 启动的虚拟机作为 NAS 系统时，发现lsblk 根本无法找到我的 USB 硬盘 换成我笔记本接硬盘盒，能够正常识别并挂载硬盘 使用 lsusb 不会报错，但是也看不到任何内容 使用 lspci 能找到 USB 对应的 PCI 设备 进一步使用 cat /proc/modules | grep usb 与 lsmod | grep usb 均查不到任何 usb 相关的内核模块 而在我笔记本上 lsmod | grep usb 能够输出 usb_storage usb_core 等多项内核模块。 再用 modprobe usb 会提示modprobe: FATAL: Module usb not found in directory /lib/modules/5.15.0-1021-kvm 问题原因很明显了，Ubuntu 根本没有为 cloud image 预置 usb 内核模块，所以才有这个问题… 进一步搜索发现这个帖子：What’s the difference between ubuntu’s amd64-disk-kvm.img and the regular amd64.img cloud images?， 解答了我的疑惑。 原因是，我使用了 ubuntu 为 cloud 环境做了精简的 kvm 内核，非常轻量，但是缺少 usb 等常用内核模块。 对于 NAS 外接存储这个场景，我应该使用不以 kvm 结尾的 ubuntu cloud image，换了个基础镜像后问题就解决了~ opensuse cloud image 的坑 opensuse leap 15 只支持 network_config v1，对 v2 的支持有 bug，gateway4 不会生效 debian cloud image 的坑debian 的 cloud 镜像根本没法用，建议避免使用它。 debian 启动时会彻底卡住，或者直接报错 kernel panic 原因是添加了 spice 图形卡，换成 vnc 就正常了 Debian Cloud Images 中的 nocloud 镜像不会在启动时运行 cloudinit，cloudinit 完全不生效 不知道是啥坑，没解决 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:5","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#debian-cloud-image-的坑"},{"categories":["tech"],"content":" 6. 克隆创建的虚拟机，卡在 Booting from Hard Disk... 状态被用做模板的虚拟机可以正常启动，但是克隆的虚拟机就卡住了。 可能的原因： 磁盘有问题，出这个问题的 cloud image 是 ubuntu-20.10-server-cloudimg-amd64.img，我更换成 ubuntu-20.10-server-cloudimg-amd64-disk-kvm.img 就没问题了。 磁盘镜像均下载自 https://cloud-images.ubuntu.com/releases/groovy/release-20201210/ BIOS 不匹配：将 BIOS 从 SeaBIOS 切换到 OVMF(UEFI) 如果仍然无法启动，请进入 OVMF 的 BIOS 界面关闭「Secure Boot」后再重启看看 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:6","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#6-克隆创建的虚拟机卡在-booting-from-hard-disk-状态"},{"categories":["tech"],"content":" 7. 虚拟机启动时 cloudinit 报错 failed to start OpenBSD Secure Shell server有如下几种可能： 可能性一：虚拟机名称包含非法字符 pve 的 cloudinit 配置会在启动时尝试将虚拟机 hostname 修改为与虚拟机一致，但是又没有对虚拟机名称做合法性校验… 当你使用的虚拟机名称包含了非法字符时就会出这个问题，比如ubuntu-22.10-cloudimage-template，其中的 . 就是非法的， . 在 DNS 中用于划分不同的域！ 解决方法：克隆个新虚拟机并改用合法名称，再删除旧虚拟机，问题就解决了。 可能性二：磁盘空间不足 qcow 镜像转换成的虚拟机磁盘很小，只有 2G，如果不扩容，启动时就会出各种奇怪的问题。 解决方法：通过 Web UI 扩容磁盘大小，建议至少给 32G。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:7","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#7-虚拟机启动时-cloudinit-报错-failed-to-start-openbsd-secure-shell-server"},{"categories":["tech"],"content":" 8. 修改 Linux 虚拟机的 Hostname如前所述，pve 的 cloudinit 配置会在启动时尝试将虚拟机 hostname 修改为与虚拟机一致，这导致手动修改无法生效无效。 解决方法：从旧的虚拟机克隆一个新虚拟机，将新虚拟机名称设为你期望的 hostname，然后删除旧虚拟机，启动新克隆的虚拟机，即完成了 hostname 重命名。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:8","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#8-修改-linux-虚拟机的-hostname"},{"categories":["tech"],"content":" 9. 虚拟机迁移时报错 Host key verification failed 社区相关帖子：https://forum.proxmox.com/threads/host-key-verification-failed-when-migrate.41666/ 这通常是因为节点增删，或者不小心动到了 ~/.ssh/known_hosts 文件，导致的问题。 可以通过手动在每台节点上执行如下命令解决： shell ssh -o 'HostKeyAlias=\u003cTarget node Name\u003e' root@\u003cTarget node IP\u003e 注意将上述命令中的 Target node Name\u003e 改为节点名称，将 \u003cTarget node IP\u003e 改为节点 IP 地址。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:9","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#9-虚拟机迁移时报错-host-key-verification-failed"},{"categories":["tech"],"content":" 10. PVE 的 vm 不支持 vmx/svm 虚拟化指令集在 Linux 虚拟机中运行如下命令： shell egrep '(vmx|svm)' --color=always /proc/cpuinfo 有输出则说明此虚拟机本身也支持 vmx/svm 虚拟化指令集（vmx 是 intel 指令集，svm 是 amd 的指令集）。 如果没有任何输出，说明此虚拟机不支持嵌套虚拟机，无法在其内部运行 Hyper-V 或者 kvm 虚拟化程序。 一般来说 PVE 宿主机默认就会启用嵌套虚拟化功能，可通过如下指令验证： shell # intel 用这个命令，输出 Y 则表示启用了嵌套虚拟化 cat /sys/module/kvm_intel/parameters/nested # amd 用如下指令，输出 1 则表示启用了嵌套虚拟化 cat /sys/module/kvm_amd/parameters/nested 如果输出不是 Y/1，则需要手动启用嵌套虚拟化功能。 如果是 intel cpu，需要使用如下命令启用嵌套虚拟化功能： shell ## 1. 关闭所有虚拟机，并卸载 kvm_intel 内核模块 sudo modprobe -r kvm_intel ## 2. 启用嵌套虚拟化功能 sudo modprobe kvm_intel nested=1 ## 3. 保存配置，使嵌套虚拟化功能在重启后自动启用 cat \u003c\u003cEOF | sudo tee /etc/modprobe.d/kvm.conf options kvm_intel nested=1 EOF 如果是 amd cpu，则应使用如下命令启用嵌套虚拟化功能： shell ## 1. 关闭所有虚拟机，并卸载 kvm_intel 内核模块 sudo modprobe -r kvm_amd ## 2. 启用嵌套虚拟化功能 sudo modprobe kvm_amd nested=1 ## 3. 保存配置，使嵌套虚拟化功能在重启后自动启用 cat \u003c\u003cEOF | sudo tee /etc/modprobe.d/kvm.conf options kvm_amd nested=1 EOF 上面这么一堆操作后，宿主机就已经启用了嵌套虚拟化，但是虚拟机内部却仍然不一定能有虚拟化指令集。 根本原因是 PVE 默认使用 kvm64 这种虚拟化的 CPU 类型，它不支持 vmx/svm 指令集！将虚拟机的 CPU 类型改为 host，然后重启虚拟机，问题就解决了。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:10","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#10-pve-的-vm-不支持-vmxsvm-虚拟化指令集"},{"categories":["tech"],"content":" 11. 如何在多台主机间同步 iso 镜像、backup 文件PVE 自动创建的备份，默认都只会保存到本机的 local 分区中，那万一机器出了问题，很可能备份就一起丢了。为了确保数据安全，就需要做多机备份，或者将数据统一备份到另一个 NAS 系统。 我考虑了如下几个备份方案： proxmox-backup-server：proxmox 官方推出的一个备份工具，使用 rust 编写。 它的主要好处在于，支持直接在 proxmox-ve 中将其添加为 cluster 级别的 storage，然后就可以通过 PVE 的定时备份任务，直接将数据备份到 proxmox-backup-server 中。但是我遇到这么几个问题，导致我放弃了它: 一是我想直接把数据通过 SMB 协议备份到 Windows Server 远程存储中，但是将 SMB 挂载磁盘用做 proxmox-backup-server 的 Datastore 会出问题，备份时 pbs 会创建一些特殊的临时文件，可能要用到 SMB 挂载插件不支持的特性，导致操作会失败。 二是我的 proxmox-backup-server 跟 Windows Server 都跑在 proxmox 虚拟机里面，那它就不能备份它自己，一备份就会卡住。 cronab + rclone/rsync: 极简方案，使用 crontab 跑定时脚本，用 rclone/rsync 同步数据。流程大致如下： 首先在 PVE DataCenter =\u003e Backup 中创建一个定期备份任务，将所有 vm 都备份到 local 存储中，它实际就存储位置为宿主机的 /var/lib/vz/dump。 通过 crontab 定时任务跑脚本，使用 rclone 将每个节点的 /var/lib/vz/ 中的文件全部通过 SMB 协议同步到 HDD 中。crontab 的运行时间设置在 PVE 完成后为最佳。并且将同步指标上传到 victoria-metrics 监控系统，如果备份功能失效，监控系统将通过短信或邮件告警。 /var/lib/vz/ 中除了备份文件外还保存了 iso 镜像等文件，这里也一起备份了。 restic: 一个更专业的远程增量备份工具，通过 rclone 支持几乎所有常见协议的远程存储（s3/ssh/smb 等），支持多种备份策略、版本策略、保留策略， 支持加密备份。 restic 看着确实挺棒，但是感觉有点复杂了，很多功能我都不需要。PVE 自带的备份功能已经提供了备份的「保留策略」，我这里实际只需要一个数据同步工具。因此没选择它。 如上文所述，一番研究后我抛弃了 proxmox-backup-server 与 restic，最终选择了最简单的 cronab + rclone 方案，简单实用又符合我自己的需求（仅我个人的选择，建议结合需求自行抉择）。 同步脚本也很简单，首先通过 rclone config 手动将所有 PVE 节点加入为 rclone 的 remote，再将我的 smb 远程存储加进来（也可以手动改 ~/.config/rclone/rclone.conf）。 这个方案最大的缺点是，所有备份都需要保存在每台节点的 local 卷中，所以有必要给 local 分配较大的磁盘空间，不然机器多的话很快就满了… rclone 配置好后，我写了个几行的 shell 脚本做备份同步： shell # 我的三台 pve 节点，对应的 rclone remote 名称 declare -a pve_nodes=( \"pve-um560\" \"pve-gtr5\" \"pve-s500plus\" ) # crontab 执行任务，需要指定下配置文件的绝对路径 for node in \"${pve_nodes[@]}\"; do rclone sync --progress \\ --config=/home/ryan/.config/rclone/rclone.conf \\ ${node}:/var/lib/vz \\ smb-downloads:/Downloads/proxmox-backup/${node}/ done # TODO 上传监控指标到监控系统，用于监控任务是否成功。 然后手动执行 /bin/bash /home/ryan/rclone-sync-to-nas.sh \u003e /home/ryan/rclone-sync.log 看看是否运行正常。 运行没问题后，再添加这么一个每天晚上 5 点（UTC 21 点）多执行的定时任务进行同步，就完成了： shell # 为了均衡负载，建议分钟值随便填个奇数。 17 21 * * * /bin/bash /home/ryan/rclone-sync-to-nas.sh \u003e /home/ryan/rclone-sync.log 可以把运行时间调整到 1 分钟后确认下效果，如果要看实时日志可以用tail -f /home/ryan/rclone-sync.log 查看。 如果任务未执行，可以通过 sudo systmctl status cron 查看任务执行日志，排查问题。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:11","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#backup"},{"categories":["tech"],"content":" 12. 使用 cloud image 创建的虚拟机扩容磁盘并重启后，文件系统未自动扩容这个我遇到过几次，都是因为磁盘容量用尽，导致 cloudinit 扩容脚本运行失败，只要手动回收些空间，再重启系统，就能自动扩容。 我试了只要能确保系统还剩余 100M 左右的存储空间就能正常扩容了，更低的还没试过。 如果数据实在不能清，也可以考虑手动扩容，有两种方法： 直接使用 growpart /dev/vda 1 进行扩容。第一个参数是磁盘路径，第二个参数是分区号，这里是 1，表示扩容第一个分区。此命令会同时扩容分区和文件系统。 用 fdisk 先删除分区，再重新创建分区，实现修改分区表扩容。然后还需要用 resize2fs 扩容文件系统。细节请自行网上搜索文档。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:4:12","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#12-使用-cloud-image-创建的虚拟机扩容磁盘并重启后文件系统未自动扩容"},{"categories":["tech"],"content":" 四、PVE 网络配置","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:5:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#四pve-网络配置"},{"categories":["tech"],"content":" 1. 桥接多张物理网卡示例如下，主要就是在 vmbr0 网桥的 Bridge Ports 里面： 桥接多张物理网卡 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:5:1","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#1-桥接多张物理网卡"},{"categories":["tech"],"content":" 2. 手动添加 USB 物理网卡 参考官方文档:SysAdmin - Network Configuration 我遇到这个问题的场景是：我的 mini 主机（GTR5）只有两个 2.5G 网卡，不太够用。而家里的路由器剩下的都是千兆网口，路由器也难以拓展网卡。网上搜了下 2.5G 交换机又发现价格 429 起步，所以决定买两张 USB 2.5GbE 网卡插在这台小主机上作为便宜的网口拓展方案。 现在网卡有了，有两种方式可以让 PVE 识别到这张网卡： 好像 PVE 偶尔也能自动识别到网卡，就是比较慢… 方法一：直接重启机器，然后就能在 Web UI 的 Network 配置中见到这张 USB 网卡了。之后直接把该网卡加入到 vmbr 网桥的 Bridge Ports 中并应用配置，就大功告成了。 方法二：不重启机器实现添加 USB 网卡。如果机器不能重启，就可以走这个流程： 首先，使用 ip link 命令打印出当前的所有网络接口 将 2.5GbE 网卡插到 USB3.0 端口上，Linux 将自动识别到它 现在再使用 ip link 命令查看所有网络接口，找到新增的接口名称（通常在输出内容最末尾）。 在我的环境中新的 USB 网卡名称为 enx00e04c680178 在配置文件 /etc/network/interfaces 的末尾新增一行：iface enx00e04c680178 inet manual（注意替换网卡名称） 现在直接刷新 Web UI 页面， USB 网卡就会出现了。之后直接把该网卡加入到 vmbr 网桥的Bridge Ports 中并应用配置，就大功告成了。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:5:2","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#2-手动添加-usb-物理网卡"},{"categories":["tech"],"content":" 3. 配置 WiFi 网卡如果主机自带了 WiFi 网卡，启动后 Proxmox VE 能识别到该网卡，但是无法通过 Web UI 修改它的任何配置。 那么本着物尽其用的精神，该如何利用上这张 WiFi 网卡呢？ 根据 PVE 官方文档 WLAN - Proxmox VE Docs，并不建议在 PVE 上使用 WLAN，它存在如下问题： WiFi 自身必须是一个 Linux Bridge 设备，无法被桥接到 vmbr0 等网桥上。因为大多数 Access Point 都会直接拒绝掉未授权的源地址发过来的数据包… 与有线连接相比，WiFi 的延迟要高得多，而且延迟波动较大。 因此仅建议在不得已的情况下，才使用 WiFi 网卡. 如果要配置 WLAN 网卡的话，官方建议直接参考 Debian 的官方文档进行配置：How to use a WiFi interface - Debian，不过这里也找到一篇中文博客： proxmox中使用ax210连接无线网络 - 佛西博客 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:5:3","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#3-配置-wifi-网卡"},{"categories":["tech"],"content":" 五、提升 PVE 的安全性","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:6:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#五提升-pve-的安全性"},{"categories":["tech"],"content":" 1. 配置 ACME 证书并使其自动更新对于个人使用而言，不配置证书好像也 ok，虽然访问 Web UI 时浏览器会提示不安全，但也不影响使用。 如果你拥有自己的域名，同时也期望更高的安全性，根据Certificate Management - 官方文档，pve 可借助 acme.sh 进行证书的申请与自动更新。 TODO ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:6:1","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#1-配置-acme-证书并使其自动更新"},{"categories":["tech"],"content":" 2. SSH 禁用密码登录pve 的 ssh 默认是启用了密码登录的，为了安全性，建议上传 ssh 密钥改用密钥登录，并禁用密码登录功能。 详见Linux 主机安全设置.md - ryan4yin ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:6:2","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#2-ssh-禁用密码登录"},{"categories":["tech"],"content":" 3. 用户管理PVE 支持对接多种授权协议，对于个人使用而言，直接使用 Linux PAM 是最简单的。 即使是在内网，为了安全性，也建议设置复杂密码，同时所有虚拟机也建议仅启用密钥登录，所有 Web 页面都建议设置复杂密码。（特别是家里没有访客网络的时候…） ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:6:3","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#3-用户管理"},{"categories":["tech"],"content":" 六、PCI(e) 直通（显卡、硬盘、USB 设备等）QEMU/KVM 的 PCI(e) 直通功能可以让虚拟机独占指定的 PCI(e) 设备，越过宿主机控制器直接与该 PCI(e) 设备通信。 相比使用 QEMU/KVM 提供的 virtio 半虚拟化硬件，PCI(e) 直通有如下优势： 大大提升虚拟机与 PCI(e) 设备的 IO 性能（更低的延迟，更高的速度，更低的资源占用）。 可以利用上 QEMU/KVM 本身不支持的硬件特性，比如 PCI 直通最常见的使用场景——显卡直通。 那么最常见的 PCI(e) 直通需求有： 显卡直通，实现在内部 windows 主机中用宿主机显卡看影视、玩游戏、剪视频 硬盘或 USB 直通，以提升硬盘或 USB 的 IO 性能。 首先列举下相关的文档： PCI(e) Passthrough - Proxmox WIKI. GPU OVMF PCI Passthrough (recommended) - Proxmox WIKI QEMU/Guest graphics acceleration - Arch WIKI TODO 实操内容待补充… ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:7:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#六pcie-直通显卡硬盘usb-设备等"},{"categories":["tech"],"content":" 拓展 - cloudinit 高级配置PVE 使用 CDROM 只读盘(/dev/sr0)来进行 cloud-init 的配置。在虚拟机启动后，/dev/sr0 将被卸载。 可挂载上该只读盘，查看其中的初始化配置内容： shell $ mkdir cloud-config $ mount /dev/sr0 cloud-config mount: /dev/sr0 is write-protected, mounting read-only $ ls cloud-config meta-data network-config user-data 查看其中内容，会发现 user-data 有很多参数都被硬编码了，没有通过 PVE Web Console 暴露出来，导致我们无法自定义这些配置。 比如它硬编码了 manage_etc_hosts: true，强制每次都使用虚拟机的名称作为 hostname. 如果确认有修改这些配置的需求，完全可以修改掉 PVE 代码里的硬编码参数。通过全文搜索即可找到硬编码参数的位置，以 manage_etc_hosts 为例： shell # 在 /usr/share 中全文搜索 manage_etc_hosts 这个关键字 grep -r manage_etc_hosts /usr/share 直接就搜索到了硬编码位置是 /usr/share/perl5/PVE/QemuServer/Cloudinit.pm，修改对应的 cloudinit 配置模板，然后重启节点（重启才能重新加载对应的 ruby 程序），即可实现对该硬编码参数的修改。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:8:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#拓展---cloudinit-高级配置"},{"categories":["tech"],"content":" 拓展 - 自动化配置与监控告警自动化配置相关工具： Telmate/terraform-provider-proxmox: 用户最多，但是只支持管理虚拟机资源 danitso/terraform-provider-proxmox: stars 少，但是可以管理 PVE 的大部分资源，包括节点、用户、资源池、TLS证书等等 代码更顺眼，但是作者忙，没时间合并 pr，导致 Bug 更多一些，而且很久没更新了… ryan4yin/pulumi-proxmox: 我维护的一个 proxmox 自动配置工具（很久没更新了…） Python SDK 监控告警： prometheus pve expoter: 通过 prometheus+grafana 监控 PVE 集群 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:9:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#拓展---自动化配置与监控告警"},{"categories":["tech"],"content":" 拓展 - PVE 运行在 ARM 开发版上PVE 官方目前还未推出 ARM 支持，但是社区已有方案： pimox7 安装Arm版本的Proxmox VE - 佛西博客 Proxmox-Arm64 proxmox 社区比较活跃，建议多在社区内看看相关进展。 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:10:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#拓展---pve-运行在-arm-开发版上"},{"categories":["tech"],"content":" 拓展 - 其他 QEMU/KVM 相关的虚拟化平台PVE 毕竟是一个商业系统，虽然目前可以免费用，但是以后就不一定了。 如果你担心 PVE 以后会不提供免费使用的功能，或者单纯想折腾折腾的技术，还可以试试下面这些虚拟化平台： webvirtcloud: 其前身是 webvirtmgr，一个完全开源的 QEMU/KVM Web UI，额外提供了用户管理功能。 kubevirt: 基于 Kubernetes 进行虚拟化管理 rancher/harvester: Rancher 开源的基于 Kubernetes 的超融合平台(HCI) 其底层使用 kubevirt 提供虚拟化能力，通过 longhorn 提供分布式存储能力。 HCI 超融合 = 计算虚拟化 + 网络虚拟化 + 分布式存储，它和传统的虚拟化软件最大的不同是： 分布式存储。 企业级场景下一般至少得 10GbE 网络 + SSD 才能 hold 住 HCI 超融合架构。 超融合对存储的一些要求： 软件定义 – 解除硬件绑定，可通过升级拓展更丰富的功能，自动化能力高 全分布式架构 - 扩展性好，消除单点故障风险 高可靠性 - 智能的故障恢复功能，丰富的数据保护手段 高性能 – 支持多种存储介质，充分挖掘和利用新式硬件的性能 高度融合 – 架构简单并易于管理 超融合架构可以降低私有云的构建与维护难度，让私有云的使用维护和公有云一样简单。 超融合架构下，虚拟机的计算和存储是完全高可用的：计算资源能智能动态更换，存储也是分布式存储，底层计算和存储也可以很简单的扩缩容。 我打算有时间在 PVE 集群里跑个 rancher/harvester 玩玩 emmmm ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:11:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#拓展---其他-qemukvm-相关的虚拟化平台"},{"categories":["tech"],"content":" 参考 KVM 虚拟化环境搭建 - ProxmoxVE KVM 虚拟化环境搭建 - WebVirtMgr Proxmox Virtual Environment - Proxmox WIKI QEMU - Arch Linux WIKI 佛西博客 - PVE 相关: 这位博主写了很多 pve 相关的内容，而且比较有深度 ","date":"2022-11-27","objectID":"/posts/proxmox-virtual-environment-instruction/:12:0","series":[],"tags":["虚拟化","Visualization","KVM","QEMU","libvirt","Proxmox"],"title":"Proxmox Virtual Environment 使用指南","uri":"/posts/proxmox-virtual-environment-instruction/#参考"},{"categories":["life"],"content":"我最近理解到一个事实——许多知识或者技能，都是可以通过正确的学习方法，加上短时间大量的练习，就能达到 60 分及格标准的。而这个及格水平，相对于完全没有进行过这样训练的其他人而言， 可能就已经很惊艳了。 如果你总是半途而废，可能只是你受了快餐式短期快乐的诱惑而放弃，或者你潜意识觉得它并不重要从而无法坚持。 ","date":"2022-10-05","objectID":"/posts/deliberate-practice/:0:0","series":null,"tags":["学习","方法论","舒适区","延迟满足"],"title":"刻意练习","uri":"/posts/deliberate-practice/#"},{"categories":["life"],"content":" 佐证之一 - 英语口语受限于国内英语教育的方法，很多的同学朋友口语发音都比较糟糕。但我实践发现，纠正自己的口语发音，达到到 60 分水平，并不是一件很难的事。真正的问题在于，绝大多数人从没有去查过相关资料、并进行大量的练习。 我大三之前发音也惨不忍睹，然后因为个人兴趣与需求想把英语学好，就查了很多资料，跟着恶魔奶爸给出的英语口语学习方法，花了大概一个月的时间专门练了一波，效果立竿见影。 ","date":"2022-10-05","objectID":"/posts/deliberate-practice/:1:0","series":null,"tags":["学习","方法论","舒适区","延迟满足"],"title":"刻意练习","uri":"/posts/deliberate-practice/#佐证之一---英语口语"},{"categories":["life"],"content":" 佐证之二 - 练字我堂弟之前分享过他练字的经历给我。 他大学之前的字跟我一样丑不拉几。大学期间因为无聊，就专门查了很多资料，定下了练字计划。统共就练了两三个月，现在他写的字的美观程度，已经吊打我这个菜鸡。 ","date":"2022-10-05","objectID":"/posts/deliberate-practice/:2:0","series":null,"tags":["学习","方法论","舒适区","延迟满足"],"title":"刻意练习","uri":"/posts/deliberate-practice/#佐证之二---练字"},{"categories":["life"],"content":" 佐证之三 - 减肥我有一个同村的朋友，也是我小学同学。他从初中开始就胖得不行，一直到结婚工作，体重都没减下来。 但是在去年因为婚姻变故以及一些其他原因，比较失意，突然就打算减肥，就开始坚持跑步。 跑多远忘了，可能慢慢加量到每天十公里吧。他晚饭不吃，饿了就疯狂喝水。一开始因为体重太高，一趟下来膝关节直接跑到浮肿。 就这样坚持了大概三个月，直接瘦了 50 斤，整个人清爽帅气太多了。 ","date":"2022-10-05","objectID":"/posts/deliberate-practice/:3:0","series":null,"tags":["学习","方法论","舒适区","延迟满足"],"title":"刻意练习","uri":"/posts/deliberate-practice/#佐证之三---减肥"},{"categories":["life"],"content":" 结语经常听人说要「踏出舒适区」、「延迟满足」，其实是一个道理。 即使掌握了正确的练习方法，如果因为坚持了三天发现没效果，就坚持不下去了，那无论如何都不会有好的效果。可如果把这个时间放大十倍——三十天，视目标的难度，就会开始出现比较明显的效果了。 如果你对自己比较狠（高强度练习），再把坚持的时间再放大到三个月，你的技能水平就能获得相当显著的提升！就像我那位减肥的朋友、我练字的堂弟一样！ 三个月的时间，对于学习一项技能而言，真的是很短的一段时间，但是却有能力使你入门一项终生受益的技能。对未来的自己好一点，多投资投资自己，诸君共勉。 ","date":"2022-10-05","objectID":"/posts/deliberate-practice/:4:0","series":null,"tags":["学习","方法论","舒适区","延迟满足"],"title":"刻意练习","uri":"/posts/deliberate-practice/#结语"},{"categories":["life"],"content":" 参考 Deliberate Practice: Achieve Mastery in Anything - Youtube ","date":"2022-10-05","objectID":"/posts/deliberate-practice/:5:0","series":null,"tags":["学习","方法论","舒适区","延迟满足"],"title":"刻意练习","uri":"/posts/deliberate-practice/#参考"},{"categories":["life"],"content":" 一、缘起与学习目标工作了三年多了，我的英语阅读水平大致一直处在「能较流畅地阅读各类技术文档，但是阅读与理解速度不够快」的程度。工作以来没有专门去学过英语，不过会有意识地尽量去阅读英文技术文档、英文技术书籍，或者在 Youtube 上找一些国际技术会议视频学习（如 KubeCon、IstioCon），所以这三年多我的英语水平提升应该是有一个缓慢的提升。 这周五的时候（是的就是 2022-09-02），偶然发现自己手机里还装了个英语流利说 APP，安装了两年多但一直没碰过它 emmmm 突然来了兴趣就打算试用一波，由此开始了我的重学英语之旅… 首先肯定是要测试一下自己的英文水平，确定英语学习的起点。流利说 APP 把英语分成 7 个 Level， 它评价我属于 Lv.5，大致对应 CEFR 评级 B2。五个评级维度中我的发音是最好的，口语是最差的。而词汇量大概在 5000 这个档位（我觉得如果算上我懂的计算机名词可能会更高些 emmmm）。 测出的结果跟我的自我感觉基本吻合。之前有跟 AWS 工程师做过几次英语沟通，发现我的口语勉强可以支撑日常技术沟通，但是感觉很费劲，原因显然是口语基本没怎么练过。而我的发音之所以还不错， 主要是大三的时候专门跟着奶爸推荐的《赖世雄美语音标》、《American Spoken English》等资料练习过，我当时还写过篇文章讲这个——学英语啊学英语。 先不提细节，总之我就是这么着，突然对学英语又来了兴趣~ 当天就深入探索了下英语流利说这个 APP，又花 49 元买了一个月的「懂你英语 A+ 个性化学习计划」，这个计划还给分配微信学习社群跟班主任，每天坚持 30 到 60 分钟。第一天体验这个学习计划，感觉确实跟我比较契合，而且也不枯燥，打算坚持一个月试试。 最后，为了更有目的性地提升自己的英语，我总结了下我此次「重学英语」的目的，以及当前的水平 （按优先级排序）： 能流畅地用英语交流：流利说测试显示我这项技能很差，亟待提升。 流畅阅读各类英文资料：我目前可以无障碍阅读大多数编程相关的英文博客跟文档，但是阅读速度不够快，有些长句经常要读好几次才能理解大意。另外词汇量偏低，非技术类的资料我读起来非常吃力。 无障碍看美剧：我的听力水平跟词汇水平大概差不多，看美剧还是得有个英文字幕，还需要提升。 写英文博客：高中毕业后就没学过语法，也没怎么练过写作，词汇量又低，我的写作能力显然还有很大的提升空间。 ","date":"2022-09-04","objectID":"/posts/learn-english-again/:0:1","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#一缘起与学习目标"},{"categories":["life"],"content":" 二、英语自学路线完全指南因为最近对英语来了电，就打算深入了解下市面上常见的英语学习路径，争取找到最佳学习路线。 1. 不花钱的自学路线首先是不花钱的自学路线，这个我在大三时有过实践，当时打算考研，专门练过一波英语，大致路线是这样的： 恶魔奶爸总结的 练习英语口语的办法，本人实测效果显著 其中介绍的 ESLPod、EnglishPod 等材料，不仅可以提升语感，也能帮助提升听力。 如果是进阶的话，可以去 Youtube 上看一些流行的 Talk Show，这类视频很有意思、语速快信息量又大，是很合适的进阶学习材料 以看视频的方式学英语，一个要点就是「重复」或者说「精听」。 「一点英语」就是在这上面下了不少功夫，要选那种第一遍听有难度的视频，最好内容还比较有趣 （比如 Talk Show 就比较有意思）。 可行的学习方法：第一遍带字幕看视频，但是尽量别看字幕；第二遍带字幕反复精看听不懂的部分，第三遍再完全关掉字幕验证学习效果。 还有就是很多人推荐的阅读英文原版书 一开始可能会读得很慢，这个时候最重要的就是坚持！阅读量上来了，速度自然就提升了。 让自己更容易坚持的阅读方法： 要找一些你感兴趣的、难度适中的原版书（比如感兴趣的小说、专业相关的技术书等）。因为阅读的首要点是要能感受到乐趣，否则会很枯燥，很难坚持。 然后要做好规划并坚持执行，比如每天读 10 页。 当然你也可以走传统的背单词、词组、例句这样的路线，这条路线的好处是能快速提升自己的英语能力，但是相对的也很难坚持。 常见的有新东方的词汇录播课（网上能找到免费的资料分享），以前听过新东方朱伟老师的《恋练有词》，感觉挺不错的。 不背单词、百词斩这类 APP 也可以试试，不过我个人体验是基本坚持不了多久… 每日英语听力 跟欧路词典是同一家公司做的，很良心。 不过因为我已经买了其他课程了，这个 APP 目前用得比较少。 每日英语听写 Daily English Dictation 1-400 这是 Youtube 上相当火的一个英语听写系列视频，做得非常棒，坚持听完 400 期，听力就神功大成了！ 个人感觉起码得有个 CET4 水平，才能比较舒服地跟这个课程。 一些对话、角色设定、世界观设定较多的游戏也是学英语的好材料，比如「Genshin Impact」、「DEEMO 2」，或者一些 Galgame 都可。娱乐跟英语两不误 emmm Genshin Impact 里超飒的重剑女仆 Noelle DEEMO 2 中丰富的对话内容 DEEMO 2 的一些设定 上面主要是针对 CET4 四级以上水平的英语学习者，如果基础比较差，可以考虑按照恶魔奶爸推荐的零基础英语学习路线进行自学： 英语：壹章 - 恶魔奶爸：针对零基础/基础超级不好的同学 英语：贰章 - 恶魔奶爸：针对处于英语初级阶段的同学 英语：叁章 - 恶魔奶爸：针对大学四六级水平的同学 （就跟我上面列的差不多了） 2. 花钱抄捷径而对于我这种上班族或者其他有闲钱的学生，而且不求快速提升英语能力的人而言，如果愿意花点小钱，也是有捷径可以抄的。 首先就是线下课程跟线上视频课程（比如新东方、山姆英语），这个没时间参加，时间安排也不够灵活，而且可能会比较枯燥，直接 Pass. 其次就是英语学习类 APP 了，这个市面上还有挺多的，我试用了一波后感觉很适合我，并且筛选出了下面几个比较中意的 APP：欧路词典、知米阅读、开言英语以及一点英语。 并且使用上述几款软件提供的英语能力测试，完整评估了下我的英语水平。下图从左至右依次是一点英语、开言英语，以及英语流利说的整体英语水平跟口语水平两项测试结果（结果偏高，仅供参考）： 各 APP 的英语水平测评结果（一点英语的结果明显偏高） 流利说英语评级对照表 下面详细介绍下我比较推荐的英语学习类 APP 跟课程： 欧路词典 最牛逼的词典软件，非常适合用于日常查词。 最强的一点在于它能导入各种离线词典，详见恶魔奶爸有分享的词典文章， 我用到的有如下几本： 中英双解词典 柯林斯双解 Collins Cobuild Advanced Learner’s English Dictionary：包含中文翻译、英英释义跟句子示例，还提供 1-5 的单词使用频率标识 Microsoft Bing Dictionary：是网友抓取制作的必应大词典，跟 Collins Cobuild 类似，包含级别（CET4/CET6/GRE）、中文翻译、英英释义跟句子翻译 英英词典 LDOCE朗文当代5 英英版: 光体积就 1G，包含了所有单词例句的录音，是非常详尽的一本英英学习词典。收词量相当大，短语、用法完备 词组搭配、同义词词典 牛津搭配词典第二版 Collins Thesaurus Collins COBUILD English Usage WordNet 3.0 USE THE RIGHT WORD: 同义近义词用法解释，很详细 Merriam-Webster’s Vocabulary Builder: 通过词根词缀来关联起各种相关单词 英语常用词疑难用法手册 薄荷阅读：199 元 / 100 天 阅读原版书学习英语 用它辅助阅读原版书的好处有： 会提前帮你选出生词进行预习 提供配套的有声书，可以听书也可以看书 随时双击查词，查词非常方便 学习完成后还有提供配套的讲义，对各项单词、词组、疑难句进行详细解释，这个感觉会非常有用！ 缺点 每逢购物活动就开始打广告，让人有点烦。 相比后面的竞品，价格有点太高了… 知米阅读：199 元，365 天畅读所有书单 阅读原版书学习英语 跟薄荷阅读几乎一样的模式，功能大同小异。 但是它的年费会员仅为薄荷阅读 VIP 的 1/5，而且还能畅读所有书单！ 个人感觉它的小程序设计感比薄荷阅读更好 一点英语： 折后 1998 元 270 天 如果坚持打卡 270 天，期末还返 1000 现金 + 500 续课券（实测大半年下来意外太多，基本不可能拿到、但是万一就成功了呢…） 通过「演讲/动漫/喜剧/音乐现场」等各种视频内容学习英语单词，同时练习听力跟口语 相比其他 APP，它的课程会有趣很多，不那么枯燥。我觉得用它练习听力非常合适。 缺陷： 它的视频语料有部分跟单词有点对不上，有时候会遇上，不过无伤大雅。 它背单词的功能跟「不背单词」或者「欧路词典」差距较大。其中英解释比较简陋、记忆算法强度没不背单词高，也没有词根词缀功能，此外例句偶尔会有些错误。 开言英语 通过外教录制的视频 + 程序控制实现了一定的可交互性，感觉课程比英语流利说做得要有趣些。 开言英语的 WeMeet 英语角体验真的超棒，而且还免费！ 第一次参加时，一起聊天的两位朋友，一位在万豪酒店工作了差不多 10 年，另一位是产品经理，在腾讯、搜狐、美团等公司工作了差不多 15 年，聊得非常开心，很有收获！ 价格 外教系统课是 2598 一年，提供海量外教视频互动课跟音频课 还有个外教视频课，互动性会更高，价格是 1598 一年 《潘吉 Jenny 告诉你》这个口碑在，我觉得开言英语的外教课程是可以考虑的 新东方在线 在线录播课、直播课，从以前学过的考研英语看，还是很专业、很成体系的。 对 BEC、雅思、托福都有对应的课程可选，价格对工作党而言问题不大。 VoiceTube：价格还不太清楚 应该是最早做「看视频学英语」功能的 APP 之一，它最初发布于 2013/09，而前面介绍的「一点英语」最初发布于 2018 年（而且 UI 布局也跟 VoiceTube 很类似）。 获得了 Facebook 2016 年年度最佳 APP 奖项 但是（因为版权原因）它基本都是直接内嵌的 Youtube 视频，它需要同时安装 Youtube（同时还需要一些使你能访问 Youtube 的特殊手段）。 另外一些我用过但是不推荐的 APP 也简单说下问题所在： 英语流利说 懂你英语 A+ 课程质量确实不错，但是全都是 AI 语音，虽然很标准，但是太机械了，不带啥感情，我学了才 6 天就开始厌倦了。个人不推荐选它 百词斩 更适合用于趣味性背单词 对很多学生党（比如大学期间的我）而言这些 APP 一年下来好几千的费用确实是难以承担，但是对我这样的工作党而言，不失为一个很好的学习手段。 上述这些软件最大的优势是「互动」，这会使它比单纯自学有趣很多，更容易坚持。 除薄荷英语外，其他三个 APP 都有提供长短不一的会员体验时间，有兴趣的话可以自己试用下~ 总结下，目前已买课程： 「知米阅读」199 元，365 天畅读所有书单。看的第一本书是「Time Machine」。阅读体验很不错，而且比薄荷阅读便宜太多了。 「一点英语」看视频/听歌学英语真的非常棒！已经花 1998 元买了 270 天的会员 争取每天打卡，赚到 1000 现金返还目前（2022-11-07）进度是 62 天，已经漏打卡 5 天… 部分原因是漏打卡第一天后，争取每天打卡的执念就没那么强烈了，偶尔哪一天太累了或者别的事没搞完，就直接不打卡了… 「薄荷英语」花了 199 元买了《一个人的朝圣》这个原版书，书已经看完了。内容不错，就是跟知米阅读比太贵了。 「英语流利说」花了 49 元，买了个 30 天的懂你英语 A+ 体验课，但是因为前述原因（AI 语音太死板，听多了烦）就用了一周就没继续用了。 ","date":"2022-09-04","objectID":"/posts/learn-english-again/:0:2","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#二英语自学路线完全指南"},{"categories":["life"],"content":" 二、英语自学路线完全指南因为最近对英语来了电，就打算深入了解下市面上常见的英语学习路径，争取找到最佳学习路线。 1. 不花钱的自学路线首先是不花钱的自学路线，这个我在大三时有过实践，当时打算考研，专门练过一波英语，大致路线是这样的： 恶魔奶爸总结的 练习英语口语的办法，本人实测效果显著 其中介绍的 ESLPod、EnglishPod 等材料，不仅可以提升语感，也能帮助提升听力。 如果是进阶的话，可以去 Youtube 上看一些流行的 Talk Show，这类视频很有意思、语速快信息量又大，是很合适的进阶学习材料 以看视频的方式学英语，一个要点就是「重复」或者说「精听」。 「一点英语」就是在这上面下了不少功夫，要选那种第一遍听有难度的视频，最好内容还比较有趣 （比如 Talk Show 就比较有意思）。 可行的学习方法：第一遍带字幕看视频，但是尽量别看字幕；第二遍带字幕反复精看听不懂的部分，第三遍再完全关掉字幕验证学习效果。 还有就是很多人推荐的阅读英文原版书 一开始可能会读得很慢，这个时候最重要的就是坚持！阅读量上来了，速度自然就提升了。 让自己更容易坚持的阅读方法： 要找一些你感兴趣的、难度适中的原版书（比如感兴趣的小说、专业相关的技术书等）。因为阅读的首要点是要能感受到乐趣，否则会很枯燥，很难坚持。 然后要做好规划并坚持执行，比如每天读 10 页。 当然你也可以走传统的背单词、词组、例句这样的路线，这条路线的好处是能快速提升自己的英语能力，但是相对的也很难坚持。 常见的有新东方的词汇录播课（网上能找到免费的资料分享），以前听过新东方朱伟老师的《恋练有词》，感觉挺不错的。 不背单词、百词斩这类 APP 也可以试试，不过我个人体验是基本坚持不了多久… 每日英语听力 跟欧路词典是同一家公司做的，很良心。 不过因为我已经买了其他课程了，这个 APP 目前用得比较少。 每日英语听写 Daily English Dictation 1-400 这是 Youtube 上相当火的一个英语听写系列视频，做得非常棒，坚持听完 400 期，听力就神功大成了！ 个人感觉起码得有个 CET4 水平，才能比较舒服地跟这个课程。 一些对话、角色设定、世界观设定较多的游戏也是学英语的好材料，比如「Genshin Impact」、「DEEMO 2」，或者一些 Galgame 都可。娱乐跟英语两不误 emmm Genshin Impact 里超飒的重剑女仆 Noelle DEEMO 2 中丰富的对话内容 DEEMO 2 的一些设定 上面主要是针对 CET4 四级以上水平的英语学习者，如果基础比较差，可以考虑按照恶魔奶爸推荐的零基础英语学习路线进行自学： 英语：壹章 - 恶魔奶爸：针对零基础/基础超级不好的同学 英语：贰章 - 恶魔奶爸：针对处于英语初级阶段的同学 英语：叁章 - 恶魔奶爸：针对大学四六级水平的同学 （就跟我上面列的差不多了） 2. 花钱抄捷径而对于我这种上班族或者其他有闲钱的学生，而且不求快速提升英语能力的人而言，如果愿意花点小钱，也是有捷径可以抄的。 首先就是线下课程跟线上视频课程（比如新东方、山姆英语），这个没时间参加，时间安排也不够灵活，而且可能会比较枯燥，直接 Pass. 其次就是英语学习类 APP 了，这个市面上还有挺多的，我试用了一波后感觉很适合我，并且筛选出了下面几个比较中意的 APP：欧路词典、知米阅读、开言英语以及一点英语。 并且使用上述几款软件提供的英语能力测试，完整评估了下我的英语水平。下图从左至右依次是一点英语、开言英语，以及英语流利说的整体英语水平跟口语水平两项测试结果（结果偏高，仅供参考）： 各 APP 的英语水平测评结果（一点英语的结果明显偏高） 流利说英语评级对照表 下面详细介绍下我比较推荐的英语学习类 APP 跟课程： 欧路词典 最牛逼的词典软件，非常适合用于日常查词。 最强的一点在于它能导入各种离线词典，详见恶魔奶爸有分享的词典文章， 我用到的有如下几本： 中英双解词典 柯林斯双解 Collins Cobuild Advanced Learner’s English Dictionary：包含中文翻译、英英释义跟句子示例，还提供 1-5 的单词使用频率标识 Microsoft Bing Dictionary：是网友抓取制作的必应大词典，跟 Collins Cobuild 类似，包含级别（CET4/CET6/GRE）、中文翻译、英英释义跟句子翻译 英英词典 LDOCE朗文当代5 英英版: 光体积就 1G，包含了所有单词例句的录音，是非常详尽的一本英英学习词典。收词量相当大，短语、用法完备 词组搭配、同义词词典 牛津搭配词典第二版 Collins Thesaurus Collins COBUILD English Usage WordNet 3.0 USE THE RIGHT WORD: 同义近义词用法解释，很详细 Merriam-Webster’s Vocabulary Builder: 通过词根词缀来关联起各种相关单词 英语常用词疑难用法手册 薄荷阅读：199 元 / 100 天 阅读原版书学习英语 用它辅助阅读原版书的好处有： 会提前帮你选出生词进行预习 提供配套的有声书，可以听书也可以看书 随时双击查词，查词非常方便 学习完成后还有提供配套的讲义，对各项单词、词组、疑难句进行详细解释，这个感觉会非常有用！ 缺点 每逢购物活动就开始打广告，让人有点烦。 相比后面的竞品，价格有点太高了… 知米阅读：199 元，365 天畅读所有书单 阅读原版书学习英语 跟薄荷阅读几乎一样的模式，功能大同小异。 但是它的年费会员仅为薄荷阅读 VIP 的 1/5，而且还能畅读所有书单！ 个人感觉它的小程序设计感比薄荷阅读更好 一点英语： 折后 1998 元 270 天 如果坚持打卡 270 天，期末还返 1000 现金 + 500 续课券（实测大半年下来意外太多，基本不可能拿到、但是万一就成功了呢…） 通过「演讲/动漫/喜剧/音乐现场」等各种视频内容学习英语单词，同时练习听力跟口语 相比其他 APP，它的课程会有趣很多，不那么枯燥。我觉得用它练习听力非常合适。 缺陷： 它的视频语料有部分跟单词有点对不上，有时候会遇上，不过无伤大雅。 它背单词的功能跟「不背单词」或者「欧路词典」差距较大。其中英解释比较简陋、记忆算法强度没不背单词高，也没有词根词缀功能，此外例句偶尔会有些错误。 开言英语 通过外教录制的视频 + 程序控制实现了一定的可交互性，感觉课程比英语流利说做得要有趣些。 开言英语的 WeMeet 英语角体验真的超棒，而且还免费！ 第一次参加时，一起聊天的两位朋友，一位在万豪酒店工作了差不多 10 年，另一位是产品经理，在腾讯、搜狐、美团等公司工作了差不多 15 年，聊得非常开心，很有收获！ 价格 外教系统课是 2598 一年，提供海量外教视频互动课跟音频课 还有个外教视频课，互动性会更高，价格是 1598 一年 《潘吉 Jenny 告诉你》这个口碑在，我觉得开言英语的外教课程是可以考虑的 新东方在线 在线录播课、直播课，从以前学过的考研英语看，还是很专业、很成体系的。 对 BEC、雅思、托福都有对应的课程可选，价格对工作党而言问题不大。 VoiceTube：价格还不太清楚 应该是最早做「看视频学英语」功能的 APP 之一，它最初发布于 2013/09，而前面介绍的「一点英语」最初发布于 2018 年（而且 UI 布局也跟 VoiceTube 很类似）。 获得了 Facebook 2016 年年度最佳 APP 奖项 但是（因为版权原因）它基本都是直接内嵌的 Youtube 视频，它需要同时安装 Youtube（同时还需要一些使你能访问 Youtube 的特殊手段）。 另外一些我用过但是不推荐的 APP 也简单说下问题所在： 英语流利说 懂你英语 A+ 课程质量确实不错，但是全都是 AI 语音，虽然很标准，但是太机械了，不带啥感情，我学了才 6 天就开始厌倦了。个人不推荐选它 百词斩 更适合用于趣味性背单词 对很多学生党（比如大学期间的我）而言这些 APP 一年下来好几千的费用确实是难以承担，但是对我这样的工作党而言，不失为一个很好的学习手段。 上述这些软件最大的优势是「互动」，这会使它比单纯自学有趣很多，更容易坚持。 除薄荷英语外，其他三个 APP 都有提供长短不一的会员体验时间，有兴趣的话可以自己试用下~ 总结下，目前已买课程： 「知米阅读」199 元，365 天畅读所有书单。看的第一本书是「Time Machine」。阅读体验很不错，而且比薄荷阅读便宜太多了。 「一点英语」看视频/听歌学英语真的非常棒！已经花 1998 元买了 270 天的会员 争取每天打卡，赚到 1000 现金返还目前（2022-11-07）进度是 62 天，已经漏打卡 5 天… 部分原因是漏打卡第一天后，争取每天打卡的执念就没那么强烈了，偶尔哪一天太累了或者别的事没搞完，就直接不打卡了… 「薄荷英语」花了 199 元买了《一个人的朝圣》这个原版书，书已经看完了。内容不错，就是跟知米阅读比太贵了。 「英语流利说」花了 49 元，买了个 30 天的懂你英语 A+ 体验课，但是因为前述原因（AI 语音太死板，听多了烦）就用了一周就没继续用了。 ","date":"2022-09-04","objectID":"/posts/learn-english-again/:0:2","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#1-不花钱的自学路线"},{"categories":["life"],"content":" 二、英语自学路线完全指南因为最近对英语来了电，就打算深入了解下市面上常见的英语学习路径，争取找到最佳学习路线。 1. 不花钱的自学路线首先是不花钱的自学路线，这个我在大三时有过实践，当时打算考研，专门练过一波英语，大致路线是这样的： 恶魔奶爸总结的 练习英语口语的办法，本人实测效果显著 其中介绍的 ESLPod、EnglishPod 等材料，不仅可以提升语感，也能帮助提升听力。 如果是进阶的话，可以去 Youtube 上看一些流行的 Talk Show，这类视频很有意思、语速快信息量又大，是很合适的进阶学习材料 以看视频的方式学英语，一个要点就是「重复」或者说「精听」。 「一点英语」就是在这上面下了不少功夫，要选那种第一遍听有难度的视频，最好内容还比较有趣 （比如 Talk Show 就比较有意思）。 可行的学习方法：第一遍带字幕看视频，但是尽量别看字幕；第二遍带字幕反复精看听不懂的部分，第三遍再完全关掉字幕验证学习效果。 还有就是很多人推荐的阅读英文原版书 一开始可能会读得很慢，这个时候最重要的就是坚持！阅读量上来了，速度自然就提升了。 让自己更容易坚持的阅读方法： 要找一些你感兴趣的、难度适中的原版书（比如感兴趣的小说、专业相关的技术书等）。因为阅读的首要点是要能感受到乐趣，否则会很枯燥，很难坚持。 然后要做好规划并坚持执行，比如每天读 10 页。 当然你也可以走传统的背单词、词组、例句这样的路线，这条路线的好处是能快速提升自己的英语能力，但是相对的也很难坚持。 常见的有新东方的词汇录播课（网上能找到免费的资料分享），以前听过新东方朱伟老师的《恋练有词》，感觉挺不错的。 不背单词、百词斩这类 APP 也可以试试，不过我个人体验是基本坚持不了多久… 每日英语听力 跟欧路词典是同一家公司做的，很良心。 不过因为我已经买了其他课程了，这个 APP 目前用得比较少。 每日英语听写 Daily English Dictation 1-400 这是 Youtube 上相当火的一个英语听写系列视频，做得非常棒，坚持听完 400 期，听力就神功大成了！ 个人感觉起码得有个 CET4 水平，才能比较舒服地跟这个课程。 一些对话、角色设定、世界观设定较多的游戏也是学英语的好材料，比如「Genshin Impact」、「DEEMO 2」，或者一些 Galgame 都可。娱乐跟英语两不误 emmm Genshin Impact 里超飒的重剑女仆 Noelle DEEMO 2 中丰富的对话内容 DEEMO 2 的一些设定 上面主要是针对 CET4 四级以上水平的英语学习者，如果基础比较差，可以考虑按照恶魔奶爸推荐的零基础英语学习路线进行自学： 英语：壹章 - 恶魔奶爸：针对零基础/基础超级不好的同学 英语：贰章 - 恶魔奶爸：针对处于英语初级阶段的同学 英语：叁章 - 恶魔奶爸：针对大学四六级水平的同学 （就跟我上面列的差不多了） 2. 花钱抄捷径而对于我这种上班族或者其他有闲钱的学生，而且不求快速提升英语能力的人而言，如果愿意花点小钱，也是有捷径可以抄的。 首先就是线下课程跟线上视频课程（比如新东方、山姆英语），这个没时间参加，时间安排也不够灵活，而且可能会比较枯燥，直接 Pass. 其次就是英语学习类 APP 了，这个市面上还有挺多的，我试用了一波后感觉很适合我，并且筛选出了下面几个比较中意的 APP：欧路词典、知米阅读、开言英语以及一点英语。 并且使用上述几款软件提供的英语能力测试，完整评估了下我的英语水平。下图从左至右依次是一点英语、开言英语，以及英语流利说的整体英语水平跟口语水平两项测试结果（结果偏高，仅供参考）： 各 APP 的英语水平测评结果（一点英语的结果明显偏高） 流利说英语评级对照表 下面详细介绍下我比较推荐的英语学习类 APP 跟课程： 欧路词典 最牛逼的词典软件，非常适合用于日常查词。 最强的一点在于它能导入各种离线词典，详见恶魔奶爸有分享的词典文章， 我用到的有如下几本： 中英双解词典 柯林斯双解 Collins Cobuild Advanced Learner’s English Dictionary：包含中文翻译、英英释义跟句子示例，还提供 1-5 的单词使用频率标识 Microsoft Bing Dictionary：是网友抓取制作的必应大词典，跟 Collins Cobuild 类似，包含级别（CET4/CET6/GRE）、中文翻译、英英释义跟句子翻译 英英词典 LDOCE朗文当代5 英英版: 光体积就 1G，包含了所有单词例句的录音，是非常详尽的一本英英学习词典。收词量相当大，短语、用法完备 词组搭配、同义词词典 牛津搭配词典第二版 Collins Thesaurus Collins COBUILD English Usage WordNet 3.0 USE THE RIGHT WORD: 同义近义词用法解释，很详细 Merriam-Webster’s Vocabulary Builder: 通过词根词缀来关联起各种相关单词 英语常用词疑难用法手册 薄荷阅读：199 元 / 100 天 阅读原版书学习英语 用它辅助阅读原版书的好处有： 会提前帮你选出生词进行预习 提供配套的有声书，可以听书也可以看书 随时双击查词，查词非常方便 学习完成后还有提供配套的讲义，对各项单词、词组、疑难句进行详细解释，这个感觉会非常有用！ 缺点 每逢购物活动就开始打广告，让人有点烦。 相比后面的竞品，价格有点太高了… 知米阅读：199 元，365 天畅读所有书单 阅读原版书学习英语 跟薄荷阅读几乎一样的模式，功能大同小异。 但是它的年费会员仅为薄荷阅读 VIP 的 1/5，而且还能畅读所有书单！ 个人感觉它的小程序设计感比薄荷阅读更好 一点英语： 折后 1998 元 270 天 如果坚持打卡 270 天，期末还返 1000 现金 + 500 续课券（实测大半年下来意外太多，基本不可能拿到、但是万一就成功了呢…） 通过「演讲/动漫/喜剧/音乐现场」等各种视频内容学习英语单词，同时练习听力跟口语 相比其他 APP，它的课程会有趣很多，不那么枯燥。我觉得用它练习听力非常合适。 缺陷： 它的视频语料有部分跟单词有点对不上，有时候会遇上，不过无伤大雅。 它背单词的功能跟「不背单词」或者「欧路词典」差距较大。其中英解释比较简陋、记忆算法强度没不背单词高，也没有词根词缀功能，此外例句偶尔会有些错误。 开言英语 通过外教录制的视频 + 程序控制实现了一定的可交互性，感觉课程比英语流利说做得要有趣些。 开言英语的 WeMeet 英语角体验真的超棒，而且还免费！ 第一次参加时，一起聊天的两位朋友，一位在万豪酒店工作了差不多 10 年，另一位是产品经理，在腾讯、搜狐、美团等公司工作了差不多 15 年，聊得非常开心，很有收获！ 价格 外教系统课是 2598 一年，提供海量外教视频互动课跟音频课 还有个外教视频课，互动性会更高，价格是 1598 一年 《潘吉 Jenny 告诉你》这个口碑在，我觉得开言英语的外教课程是可以考虑的 新东方在线 在线录播课、直播课，从以前学过的考研英语看，还是很专业、很成体系的。 对 BEC、雅思、托福都有对应的课程可选，价格对工作党而言问题不大。 VoiceTube：价格还不太清楚 应该是最早做「看视频学英语」功能的 APP 之一，它最初发布于 2013/09，而前面介绍的「一点英语」最初发布于 2018 年（而且 UI 布局也跟 VoiceTube 很类似）。 获得了 Facebook 2016 年年度最佳 APP 奖项 但是（因为版权原因）它基本都是直接内嵌的 Youtube 视频，它需要同时安装 Youtube（同时还需要一些使你能访问 Youtube 的特殊手段）。 另外一些我用过但是不推荐的 APP 也简单说下问题所在： 英语流利说 懂你英语 A+ 课程质量确实不错，但是全都是 AI 语音，虽然很标准，但是太机械了，不带啥感情，我学了才 6 天就开始厌倦了。个人不推荐选它 百词斩 更适合用于趣味性背单词 对很多学生党（比如大学期间的我）而言这些 APP 一年下来好几千的费用确实是难以承担，但是对我这样的工作党而言，不失为一个很好的学习手段。 上述这些软件最大的优势是「互动」，这会使它比单纯自学有趣很多，更容易坚持。 除薄荷英语外，其他三个 APP 都有提供长短不一的会员体验时间，有兴趣的话可以自己试用下~ 总结下，目前已买课程： 「知米阅读」199 元，365 天畅读所有书单。看的第一本书是「Time Machine」。阅读体验很不错，而且比薄荷阅读便宜太多了。 「一点英语」看视频/听歌学英语真的非常棒！已经花 1998 元买了 270 天的会员 争取每天打卡，赚到 1000 现金返还目前（2022-11-07）进度是 62 天，已经漏打卡 5 天… 部分原因是漏打卡第一天后，争取每天打卡的执念就没那么强烈了，偶尔哪一天太累了或者别的事没搞完，就直接不打卡了… 「薄荷英语」花了 199 元买了《一个人的朝圣》这个原版书，书已经看完了。内容不错，就是跟知米阅读比太贵了。 「英语流利说」花了 49 元，买了个 30 天的懂你英语 A+ 体验课，但是因为前述原因（AI 语音太死板，听多了烦）就用了一周就没继续用了。 ","date":"2022-09-04","objectID":"/posts/learn-english-again/:0:2","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#2-花钱抄捷径"},{"categories":["life"],"content":" 三、如何练习英语口语？首先，上面已经提过了，「开言英语」的线上英语角 WeMeet 非常的棒。这个功能是免费的，但是需要交 5 块钱押金，我觉得这 5 块钱的押金大大提升了这个英语角的队友质量，能确保大家都是有强烈意愿去用英语聊天的。实际的体验也非常棒，不仅能练习口语，还能交到各行各业喜欢英语的朋友。 其次，如果你的上网工具能使你用上 Discord，也有一些英语学习的 Discord Server 非常值得推荐， 你能在上面跟各种母语、各个国家、各种口音的朋友一起用英语聊天，体验还不错，还能交到有意思的国外朋友。 英语学习 Discord Server 地址: https://discord.gg/english https://discord.gg/c-e 加入 Server 后要往下翻很多页才能到 English Practice 部分，目前分别有 8 个「Beginner English」聊天室跟 8 个「Intermeidate/Advanced English」聊天室，赶快加入开聊吧~ 此外，如果你想走高效的氪金路线，也有很多线上真人上外教平台供你选择，主要有如下几个： Lingoda: 被认为是最佳成人英语学习平台之一，它有部分课还提供了学费返还机制，可以了解下。 Cambly: twitter 上推友推荐的，最流行的英语学习平台之一，真人外教一对一教学 据说打折时很便宜，优惠价只要人民币 30 元 30 分钟 Preply: 我用的词汇测试站点，现在就属于这家公司的产品。据说比较专业，提供 4-5 人小组教学跟一对一教学两种学习模式 貌似比较贵：$15-20 per hour 你还可以在这个站点上面应聘汉语教学（有 20%-30% 的抽成，150 美刀才能提现） Verbling: 一样也是一个老牌语言学习站点 据说价格比较优惠，还支持免费试课。 跟 Preply 一样你也可以在这个站点上面应聘汉语教学作为兼职。 ITalk: 国内创办的比较大的语言交换平台，据说是最 casual 的学习平台，所以不太适合自我驱动力不强的朋友。 ","date":"2022-09-04","objectID":"/posts/learn-english-again/:1:0","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#三如何练习英语口语"},{"categories":["life"],"content":" 四、总而言之嗯，毕竟是花了钱的，这次的大目标就是用近一年的时间，全面提升英语水平。 听说考 BEC 高级对英语水平提升帮助很大，为了提供一点压力与动力，就定一个硬性目标： 2023 年达到 CEFR 的 C1 等级，报考并取得 BEC 高级证书 2023 年底词汇量超过 10000 ","date":"2022-09-04","objectID":"/posts/learn-english-again/:2:0","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#四总而言之"},{"categories":["life"],"content":" 其他资料文末再分享几篇我收藏的英语学习相关的资料，个人感觉读了有些帮助： 编程的技术资料好多都是全英文的该怎么学习？ 关键点总结：你要能接受自己不能 100% 看懂资料 如何做到流畅阅读英文资料和听懂国外公开课？ 提升阅读速度过程中一个最基本的因素就是阅读量，跳出舒适区，刻意练习。 如何有效提高英语写作？ 关键点总结：提高英文写作水平的第一步是「大量阅读」 YouTube 上有哪些质量不错的英文教学视频和英文视频？ 这些视频很多都能在 Bilibili 上搜到 通过「听写」学习英语，效果怎么样？ 英语学习过程中如何有效地提升词汇功底？ 关键点总结：学习一门语言文字的最短最佳途径，是掌握它的词根（root），也就是那些其他单词借以形成的原生词 知乎「英语学习」话题下的精华回答 ","date":"2022-09-04","objectID":"/posts/learn-english-again/:3:0","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#其他资料"},{"categories":["life"],"content":" 学习成果记录 - 持续更新先简单记录下学习时间线： 2022/9 - 2022/12 平均每天大概花 30 分钟学习英语（偶尔兴之所至可能会花 1 小时甚至更多，也偶尔会断几天）。 主要学习方法就是通过「薄荷阅读」「知米阅读」读英文原版书，以及通过「一点英语」看视频练听力、记单词。 2023/1: 基本算是休息了一个月，没怎么学。 未完待续 ","date":"2022-09-04","objectID":"/posts/learn-english-again/:4:0","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#学习成果记录---持续更新"},{"categories":["life"],"content":" 1. 词汇量词汇量测试结果按时间排序如下，使用的测试工具是Test Your Vocabulary ： 2022 年 9 月初，测试结果：4700 词（无截图） 2022-10-18 词汇量测试结果：5100 词 2022-10-18 词汇量测试结果：5100 词 2022-11-17 词汇量测试结果：5600 词 2022-12-19 词汇量测试结果： 6300 词 2023-01-02 词汇量测试结果：6583 词 2023 年 1 月份暂停了学习，词汇量基本没涨，算是休息了一个月。 ","date":"2022-09-04","objectID":"/posts/learn-english-again/:4:1","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#1-词汇量"},{"categories":["life"],"content":" 2. 口语交流能力2022 年只是参与了两次「开言英语」的线上英语角 WeMeet，另外探索了下学习方法，但是时间原因未持续推进。 计划在 2023 年进行专门练习，未完待续… ","date":"2022-09-04","objectID":"/posts/learn-english-again/:4:2","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#2-口语交流能力"},{"categories":["life"],"content":" 3. 其他进展 写作能力 计划在 2023 年进行专门练习，未完待续… 阅读能力 2022 年的 4 个月里，阅读是我提升词汇量的主要手段之一，看统计读了有 15 万词，词汇量跟阅读速度都有明显提升。 在薄荷阅读上读完的第一本英语原版书 ","date":"2022-09-04","objectID":"/posts/learn-english-again/:4:3","series":null,"tags":["英语","语言学习"],"title":"Learn English Again","uri":"/posts/learn-english-again/#3-其他进展"},{"categories":["tech"],"content":" 个人笔记，不保证正确！ 谈到分布式数据库，不论是 Etcd/Zookeeper 这样的中心化分布式数据库，还是 Ethereum 区块链这样的去中心化数据库，都避免不了两个关键词：「一致性」跟「共识」。 本文是笔者学习「一致性」和「共识」以及相关的理论知识时记录的笔记，这些知识能帮助我们了解 Etcd/Zookeeper/Consul/MySQL/PostgreSQL/DynamoDB/Cassandra/MongoDB/CockroachDB/TiDB 等一众数据库的区别，理解各数据库的优势与局限性，搞懂数据库隔离级别的含义以及应该如何设置， 并使我们能在各种应用场景中选择出适用的数据库。 如果你对区块链感兴趣，那这篇文章也能帮助你了解区块链这样的去中心化数据库，跟业界流行的分布式数据库在技术上有何区别，又有哪些共同点，具体是如何实现。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:0:0","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#"},{"categories":["tech"],"content":" 一、一致性 - Consistency「一致性」本身是一个比较模糊的定义，视使用场景的不同，存在许多不同的含义。由于数据库仍然是一个新兴领域，目前存在许多不同的一致性模型，其中的一些术语描述的一致性之间可能还有重叠关系，这些关系甚至会困扰专业的数据库开发人员。 但是究其根本，实际上在谈论一致性时，我们是在谈论事务一致性跟数据一致性，下面我们分别介绍下这两个一致性。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:0","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#一一致性---consistency"},{"categories":["tech"],"content":" 1. 事务一致性 - Transactions Consistency「事务一致性」指的是数据库中事务的一致性，它是 ACID 理论中最不起眼的特性，也并不是本文的重点。但是这里就写这么一句话也说不过去，所以下面就仔细介绍下事务与 ACID 理论。 事务与 ACID 理论事务是一种「要么全部完成，要么完全不做（All or Nothing）」的指令运行机制。 ACID 理论定义，拥有如下四个特性的「数据库指令序列」，就被称为事务： 原子性 Atomicity：事务是一个不可分割的工作单元，事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在某个中间状态。 比如 A 转账 100 元给 B，要么转账失败，要么转账成功，不可能卡在 A 被扣除了 100 元，而 B 还没收到 100 元的中间状态。 原子性在单机数据库上已得到妥善解决，但是在分布式数据库上它成为一项新的挑战。要在分布式架构下支持原子性并不容易，所以不少 NoSQL 产品都选择绕过这个问题，聚焦到那些对原子性不敏感的细分场景。 一致性 Consistency：也叫数据的「正确性 Correctness」或者完整性，指事务对数据库状态的变更必须满足所有预定义的规则，包括「约束 constraints」、「级联 cascades」、「触发器 triggers」以及这些规则的任何组合。 比如如果用户为某个字段设置了约束条件 unique，那么事务对该表的所有修改都必须保证此约束成立，否则它将会失败。 是存在感最低的特性 隔离性 Isolation：并发执行的多个事务之间是完全隔离的，它们的执行效果跟按事务的开始顺序串行执行完全一致。 事务中最复杂的特性 持久性 Durability：事务执行完毕后，结果就保存不变了。这个最好理解。 ACID 是传统的单机数据库的核心特性，比如 MySQL/PostgreSQL. ACID 中最复杂的特性 - 隔离性完全地实现 ACID 得到的数据库，性能是非常差的。因此在关系数据库中，设计者通常会选择牺牲相对不重要的「隔离性」来获取更好的性能。 而一旦隔离不够彻底，就可能会遇到一些事务之间互相影响的异常情况，这些异常被分为如下几种： 脏写 Dirty writes：即事务 T1 跟事务 T2 同时在原数据的基础上更新同一个数据，导致结果不符合预期。 案例：两个事务同时尝试从账户中扣款 1000 元，但是它们读到的初始状态都是 5000 元，于是都尝试将账户修改为 4000 元，结果就是少扣了 1000 元。 最简单的解决方法：针对 UPDATE table SET field = field - 1000 WHERE id = 1 这类数据增删改的逻辑，需要对被更新的行加一把「行写锁」，使其他需要写此数据的事务等待。 脏读 Dirty reads：事务 T1 读取了事务 T2 未提交的数据。这个数据不一定准确，被称为脏数据，因为假如事务 T2 回滚了，T1 拿到的就是一个错误的数据 案例：假设小明小红在一个银行账户存了 5000 元，小明小红在用这同一个账户消费 1000 元，这中间小明付款的事务读取到账户已经被小红的事务修改为了 4000 元，于是它把余额修改为 3000 元，然后付款成功。但是在小明的付款事务成功后，小红的付款失败回滚了，余额又从 3000 被修改回 5000 元。小明就完成了 0 元购的壮举。 最简单的解决方法：事务 T2 写数据时对被修改的行加「行写锁」，T2 结束后再释放锁，这样事务 T1 的读取就会被阻塞，直到锁释放。 不可重复读 Non-repeatable reads：事务 T1 读取数据后，紧接着事务 T2 就更新了数据并提交，事务 T1 再次读取的时候发现数据不一致了 案例： 小明在京东上抢购商品，抢购事务启动时事务读到还剩 36 件商品，于是继续执行抢购逻辑，之后事务因为某种原因需要再读一次商品数量，结果发现商品数量已经变成 0 了，抢购失败。 更麻烦的是，不可重复读导致 SELECT 跟 UPDATE 之间也可能出现数据变更，如果你在事务中先通过 SELECT field INTO myvar FROM mytable WHERE uid = 1 读到余额，再在此基础上通过UPDATE 去更新余额，很可能导致数据变得一团糟！ 正确的做法是使用 UPDATE mytable SET field = field - 1000 WHERE id = 1，因为每一条 SQL 命令本身都是原子的，这个 SQL 不会有问题。 最简单的解决办法：事务 T1 读数据时，也加一把「行」锁，直到不再需要读该数据了，再释放锁。 幻读 Phantom reads：事务 T1 在多次批量读数据时，事务 T2 往其中执行了插入/删除操作， 导致 T1 读到的是旧数据的一个残影，而非当前真实的数据状态。 最简单的解决办法：事务 T1 在批量读数据时，先加一把范围锁，在事务 T1 结束读取之后，再释放这把锁。这能同时解决「幻读」跟「不可重复读」的问题。 根据隔离程度，ANSI SQL-92 标准中将「隔离性」细分为四个等级（避免「脏写」是数据库的必备要求，因此未记录在下面的四个等级中）： 串行化 Serializable：也就是完全的隔离，只要事务之间存在互相影响的可能，就（通过锁机制）强制它们串行执行。 可重复读 Repeatable read：可避免脏读、不可重复读的发生，但是解决不了幻读的问题。 读已提交 Read committed：只能避免脏读 读未提交 Read uncommitted：最低级别，完全放弃隔离性 MySQL 默认的隔离级别为「可重复读 Repeatable Read」，PostgreSQL 和 Oracle 默认隔离级别为「读已提交 Read committed」。 为什么 MySQL/PostgreSQL/Oracle 的默认隔离级别是这样设置的呢？该如何选择正确的隔离级别呢？ 我们针对普通的高并发业务场景做个简单分析： 首先，「脏读」是必须避免的，它会使事务读到错误的数据！最低的「读未提交」级别直接排除 「串行化」的性能太差，也直接排除 只要 SQL 用得对，「不可重复读」问题对业务逻辑的正确性通常并无影响，所以是可以容忍的。 因此一般「读已提交」是最佳的隔离级别，这也是 PostgreSQL/Oracle 将其设为默认隔离级别的原因。 那么为什么 MySQL 这么特立独行，将默认隔离级别提高到了「可重复读」呢？为啥阿里这种大的互联网公司又会把 MySQL 默认的隔离级别改成「读已提交」？ 根据网上查到的资料，这是 MySQL 的历史问题导致的。MySQL 5.0 之前只支持 statement 这种 binlog 格式，此格式在「读已提交」的隔离级别下会出现诸多问题，最明显的就是可能会导致主从数据库的数据不一致。 除了设置默认的隔离级别外，MySQL 还禁止在使用 statement 格式的 binlog 时，使用 READ COMMITTED 作为事务隔离级别，尝试修改隔离级别会报错Transaction level 'READ-COMMITTED' in InnoDB is not safe for binlog mode 'STATEMENT' 而互联网公司将隔离级别改为「读已提交」的原因也很好理解，正如前文所述「读已提交」是最佳的隔离级别，这样修改能够提升数据库的性能。 「隔离性」的本质其实就是事务的并发控制，不同的隔离级别代表了对并发事务的隔离程度，主要的实现手段是「多版本并发控制 MVCC」与「锁」。锁机制前面已经简单介绍过了，而 MVCC 其实就是为每个事务创建一个特定隔离级别的快照，这样读写不会互相阻塞，性能就提升了。（MVCC 暂时也是超纲知识，后面再研究吧 emmmm） ANSI SQL-92 对异常现象的分析仍然太过简单了，1995 年新发布的论文A Critique of ANSI SQL Isolation Levels 丰富和细化了 SQL-92 的内容，定义了六种隔离级别和八种异常现象（有大佬强烈建议通读此论文，重点是文中的快照隔离（Snapshot Isolation, SI）级别）。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:1","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#1-事务一致性---transactions-consistency"},{"categories":["tech"],"content":" 1. 事务一致性 - Transactions Consistency「事务一致性」指的是数据库中事务的一致性，它是 ACID 理论中最不起眼的特性，也并不是本文的重点。但是这里就写这么一句话也说不过去，所以下面就仔细介绍下事务与 ACID 理论。 事务与 ACID 理论事务是一种「要么全部完成，要么完全不做（All or Nothing）」的指令运行机制。 ACID 理论定义，拥有如下四个特性的「数据库指令序列」，就被称为事务： 原子性 Atomicity：事务是一个不可分割的工作单元，事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在某个中间状态。 比如 A 转账 100 元给 B，要么转账失败，要么转账成功，不可能卡在 A 被扣除了 100 元，而 B 还没收到 100 元的中间状态。 原子性在单机数据库上已得到妥善解决，但是在分布式数据库上它成为一项新的挑战。要在分布式架构下支持原子性并不容易，所以不少 NoSQL 产品都选择绕过这个问题，聚焦到那些对原子性不敏感的细分场景。 一致性 Consistency：也叫数据的「正确性 Correctness」或者完整性，指事务对数据库状态的变更必须满足所有预定义的规则，包括「约束 constraints」、「级联 cascades」、「触发器 triggers」以及这些规则的任何组合。 比如如果用户为某个字段设置了约束条件 unique，那么事务对该表的所有修改都必须保证此约束成立，否则它将会失败。 是存在感最低的特性 隔离性 Isolation：并发执行的多个事务之间是完全隔离的，它们的执行效果跟按事务的开始顺序串行执行完全一致。 事务中最复杂的特性 持久性 Durability：事务执行完毕后，结果就保存不变了。这个最好理解。 ACID 是传统的单机数据库的核心特性，比如 MySQL/PostgreSQL. ACID 中最复杂的特性 - 隔离性完全地实现 ACID 得到的数据库，性能是非常差的。因此在关系数据库中，设计者通常会选择牺牲相对不重要的「隔离性」来获取更好的性能。 而一旦隔离不够彻底，就可能会遇到一些事务之间互相影响的异常情况，这些异常被分为如下几种： 脏写 Dirty writes：即事务 T1 跟事务 T2 同时在原数据的基础上更新同一个数据，导致结果不符合预期。 案例：两个事务同时尝试从账户中扣款 1000 元，但是它们读到的初始状态都是 5000 元，于是都尝试将账户修改为 4000 元，结果就是少扣了 1000 元。 最简单的解决方法：针对 UPDATE table SET field = field - 1000 WHERE id = 1 这类数据增删改的逻辑，需要对被更新的行加一把「行写锁」，使其他需要写此数据的事务等待。 脏读 Dirty reads：事务 T1 读取了事务 T2 未提交的数据。这个数据不一定准确，被称为脏数据，因为假如事务 T2 回滚了，T1 拿到的就是一个错误的数据 案例：假设小明小红在一个银行账户存了 5000 元，小明小红在用这同一个账户消费 1000 元，这中间小明付款的事务读取到账户已经被小红的事务修改为了 4000 元，于是它把余额修改为 3000 元，然后付款成功。但是在小明的付款事务成功后，小红的付款失败回滚了，余额又从 3000 被修改回 5000 元。小明就完成了 0 元购的壮举。 最简单的解决方法：事务 T2 写数据时对被修改的行加「行写锁」，T2 结束后再释放锁，这样事务 T1 的读取就会被阻塞，直到锁释放。 不可重复读 Non-repeatable reads：事务 T1 读取数据后，紧接着事务 T2 就更新了数据并提交，事务 T1 再次读取的时候发现数据不一致了 案例： 小明在京东上抢购商品，抢购事务启动时事务读到还剩 36 件商品，于是继续执行抢购逻辑，之后事务因为某种原因需要再读一次商品数量，结果发现商品数量已经变成 0 了，抢购失败。 更麻烦的是，不可重复读导致 SELECT 跟 UPDATE 之间也可能出现数据变更，如果你在事务中先通过 SELECT field INTO myvar FROM mytable WHERE uid = 1 读到余额，再在此基础上通过UPDATE 去更新余额，很可能导致数据变得一团糟！ 正确的做法是使用 UPDATE mytable SET field = field - 1000 WHERE id = 1，因为每一条 SQL 命令本身都是原子的，这个 SQL 不会有问题。 最简单的解决办法：事务 T1 读数据时，也加一把「行」锁，直到不再需要读该数据了，再释放锁。 幻读 Phantom reads：事务 T1 在多次批量读数据时，事务 T2 往其中执行了插入/删除操作， 导致 T1 读到的是旧数据的一个残影，而非当前真实的数据状态。 最简单的解决办法：事务 T1 在批量读数据时，先加一把范围锁，在事务 T1 结束读取之后，再释放这把锁。这能同时解决「幻读」跟「不可重复读」的问题。 根据隔离程度，ANSI SQL-92 标准中将「隔离性」细分为四个等级（避免「脏写」是数据库的必备要求，因此未记录在下面的四个等级中）： 串行化 Serializable：也就是完全的隔离，只要事务之间存在互相影响的可能，就（通过锁机制）强制它们串行执行。 可重复读 Repeatable read：可避免脏读、不可重复读的发生，但是解决不了幻读的问题。 读已提交 Read committed：只能避免脏读 读未提交 Read uncommitted：最低级别，完全放弃隔离性 MySQL 默认的隔离级别为「可重复读 Repeatable Read」，PostgreSQL 和 Oracle 默认隔离级别为「读已提交 Read committed」。 为什么 MySQL/PostgreSQL/Oracle 的默认隔离级别是这样设置的呢？该如何选择正确的隔离级别呢？ 我们针对普通的高并发业务场景做个简单分析： 首先，「脏读」是必须避免的，它会使事务读到错误的数据！最低的「读未提交」级别直接排除 「串行化」的性能太差，也直接排除 只要 SQL 用得对，「不可重复读」问题对业务逻辑的正确性通常并无影响，所以是可以容忍的。 因此一般「读已提交」是最佳的隔离级别，这也是 PostgreSQL/Oracle 将其设为默认隔离级别的原因。 那么为什么 MySQL 这么特立独行，将默认隔离级别提高到了「可重复读」呢？为啥阿里这种大的互联网公司又会把 MySQL 默认的隔离级别改成「读已提交」？ 根据网上查到的资料，这是 MySQL 的历史问题导致的。MySQL 5.0 之前只支持 statement 这种 binlog 格式，此格式在「读已提交」的隔离级别下会出现诸多问题，最明显的就是可能会导致主从数据库的数据不一致。 除了设置默认的隔离级别外，MySQL 还禁止在使用 statement 格式的 binlog 时，使用 READ COMMITTED 作为事务隔离级别，尝试修改隔离级别会报错Transaction level 'READ-COMMITTED' in InnoDB is not safe for binlog mode 'STATEMENT' 而互联网公司将隔离级别改为「读已提交」的原因也很好理解，正如前文所述「读已提交」是最佳的隔离级别，这样修改能够提升数据库的性能。 「隔离性」的本质其实就是事务的并发控制，不同的隔离级别代表了对并发事务的隔离程度，主要的实现手段是「多版本并发控制 MVCC」与「锁」。锁机制前面已经简单介绍过了，而 MVCC 其实就是为每个事务创建一个特定隔离级别的快照，这样读写不会互相阻塞，性能就提升了。（MVCC 暂时也是超纲知识，后面再研究吧 emmmm） ANSI SQL-92 对异常现象的分析仍然太过简单了，1995 年新发布的论文A Critique of ANSI SQL Isolation Levels 丰富和细化了 SQL-92 的内容，定义了六种隔离级别和八种异常现象（有大佬强烈建议通读此论文，重点是文中的快照隔离（Snapshot Isolation, SI）级别）。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:1","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#事务与-acid-理论"},{"categories":["tech"],"content":" 1. 事务一致性 - Transactions Consistency「事务一致性」指的是数据库中事务的一致性，它是 ACID 理论中最不起眼的特性，也并不是本文的重点。但是这里就写这么一句话也说不过去，所以下面就仔细介绍下事务与 ACID 理论。 事务与 ACID 理论事务是一种「要么全部完成，要么完全不做（All or Nothing）」的指令运行机制。 ACID 理论定义，拥有如下四个特性的「数据库指令序列」，就被称为事务： 原子性 Atomicity：事务是一个不可分割的工作单元，事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在某个中间状态。 比如 A 转账 100 元给 B，要么转账失败，要么转账成功，不可能卡在 A 被扣除了 100 元，而 B 还没收到 100 元的中间状态。 原子性在单机数据库上已得到妥善解决，但是在分布式数据库上它成为一项新的挑战。要在分布式架构下支持原子性并不容易，所以不少 NoSQL 产品都选择绕过这个问题，聚焦到那些对原子性不敏感的细分场景。 一致性 Consistency：也叫数据的「正确性 Correctness」或者完整性，指事务对数据库状态的变更必须满足所有预定义的规则，包括「约束 constraints」、「级联 cascades」、「触发器 triggers」以及这些规则的任何组合。 比如如果用户为某个字段设置了约束条件 unique，那么事务对该表的所有修改都必须保证此约束成立，否则它将会失败。 是存在感最低的特性 隔离性 Isolation：并发执行的多个事务之间是完全隔离的，它们的执行效果跟按事务的开始顺序串行执行完全一致。 事务中最复杂的特性 持久性 Durability：事务执行完毕后，结果就保存不变了。这个最好理解。 ACID 是传统的单机数据库的核心特性，比如 MySQL/PostgreSQL. ACID 中最复杂的特性 - 隔离性完全地实现 ACID 得到的数据库，性能是非常差的。因此在关系数据库中，设计者通常会选择牺牲相对不重要的「隔离性」来获取更好的性能。 而一旦隔离不够彻底，就可能会遇到一些事务之间互相影响的异常情况，这些异常被分为如下几种： 脏写 Dirty writes：即事务 T1 跟事务 T2 同时在原数据的基础上更新同一个数据，导致结果不符合预期。 案例：两个事务同时尝试从账户中扣款 1000 元，但是它们读到的初始状态都是 5000 元，于是都尝试将账户修改为 4000 元，结果就是少扣了 1000 元。 最简单的解决方法：针对 UPDATE table SET field = field - 1000 WHERE id = 1 这类数据增删改的逻辑，需要对被更新的行加一把「行写锁」，使其他需要写此数据的事务等待。 脏读 Dirty reads：事务 T1 读取了事务 T2 未提交的数据。这个数据不一定准确，被称为脏数据，因为假如事务 T2 回滚了，T1 拿到的就是一个错误的数据 案例：假设小明小红在一个银行账户存了 5000 元，小明小红在用这同一个账户消费 1000 元，这中间小明付款的事务读取到账户已经被小红的事务修改为了 4000 元，于是它把余额修改为 3000 元，然后付款成功。但是在小明的付款事务成功后，小红的付款失败回滚了，余额又从 3000 被修改回 5000 元。小明就完成了 0 元购的壮举。 最简单的解决方法：事务 T2 写数据时对被修改的行加「行写锁」，T2 结束后再释放锁，这样事务 T1 的读取就会被阻塞，直到锁释放。 不可重复读 Non-repeatable reads：事务 T1 读取数据后，紧接着事务 T2 就更新了数据并提交，事务 T1 再次读取的时候发现数据不一致了 案例： 小明在京东上抢购商品，抢购事务启动时事务读到还剩 36 件商品，于是继续执行抢购逻辑，之后事务因为某种原因需要再读一次商品数量，结果发现商品数量已经变成 0 了，抢购失败。 更麻烦的是，不可重复读导致 SELECT 跟 UPDATE 之间也可能出现数据变更，如果你在事务中先通过 SELECT field INTO myvar FROM mytable WHERE uid = 1 读到余额，再在此基础上通过UPDATE 去更新余额，很可能导致数据变得一团糟！ 正确的做法是使用 UPDATE mytable SET field = field - 1000 WHERE id = 1，因为每一条 SQL 命令本身都是原子的，这个 SQL 不会有问题。 最简单的解决办法：事务 T1 读数据时，也加一把「行」锁，直到不再需要读该数据了，再释放锁。 幻读 Phantom reads：事务 T1 在多次批量读数据时，事务 T2 往其中执行了插入/删除操作， 导致 T1 读到的是旧数据的一个残影，而非当前真实的数据状态。 最简单的解决办法：事务 T1 在批量读数据时，先加一把范围锁，在事务 T1 结束读取之后，再释放这把锁。这能同时解决「幻读」跟「不可重复读」的问题。 根据隔离程度，ANSI SQL-92 标准中将「隔离性」细分为四个等级（避免「脏写」是数据库的必备要求，因此未记录在下面的四个等级中）： 串行化 Serializable：也就是完全的隔离，只要事务之间存在互相影响的可能，就（通过锁机制）强制它们串行执行。 可重复读 Repeatable read：可避免脏读、不可重复读的发生，但是解决不了幻读的问题。 读已提交 Read committed：只能避免脏读 读未提交 Read uncommitted：最低级别，完全放弃隔离性 MySQL 默认的隔离级别为「可重复读 Repeatable Read」，PostgreSQL 和 Oracle 默认隔离级别为「读已提交 Read committed」。 为什么 MySQL/PostgreSQL/Oracle 的默认隔离级别是这样设置的呢？该如何选择正确的隔离级别呢？ 我们针对普通的高并发业务场景做个简单分析： 首先，「脏读」是必须避免的，它会使事务读到错误的数据！最低的「读未提交」级别直接排除 「串行化」的性能太差，也直接排除 只要 SQL 用得对，「不可重复读」问题对业务逻辑的正确性通常并无影响，所以是可以容忍的。 因此一般「读已提交」是最佳的隔离级别，这也是 PostgreSQL/Oracle 将其设为默认隔离级别的原因。 那么为什么 MySQL 这么特立独行，将默认隔离级别提高到了「可重复读」呢？为啥阿里这种大的互联网公司又会把 MySQL 默认的隔离级别改成「读已提交」？ 根据网上查到的资料，这是 MySQL 的历史问题导致的。MySQL 5.0 之前只支持 statement 这种 binlog 格式，此格式在「读已提交」的隔离级别下会出现诸多问题，最明显的就是可能会导致主从数据库的数据不一致。 除了设置默认的隔离级别外，MySQL 还禁止在使用 statement 格式的 binlog 时，使用 READ COMMITTED 作为事务隔离级别，尝试修改隔离级别会报错Transaction level 'READ-COMMITTED' in InnoDB is not safe for binlog mode 'STATEMENT' 而互联网公司将隔离级别改为「读已提交」的原因也很好理解，正如前文所述「读已提交」是最佳的隔离级别，这样修改能够提升数据库的性能。 「隔离性」的本质其实就是事务的并发控制，不同的隔离级别代表了对并发事务的隔离程度，主要的实现手段是「多版本并发控制 MVCC」与「锁」。锁机制前面已经简单介绍过了，而 MVCC 其实就是为每个事务创建一个特定隔离级别的快照，这样读写不会互相阻塞，性能就提升了。（MVCC 暂时也是超纲知识，后面再研究吧 emmmm） ANSI SQL-92 对异常现象的分析仍然太过简单了，1995 年新发布的论文A Critique of ANSI SQL Isolation Levels 丰富和细化了 SQL-92 的内容，定义了六种隔离级别和八种异常现象（有大佬强烈建议通读此论文，重点是文中的快照隔离（Snapshot Isolation, SI）级别）。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:1","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#acid-中最复杂的特性---隔离性"},{"categories":["tech"],"content":" 2. 数据一致性 Data Consistency「数据一致性」是指对数据库的每一次读操作都应该读到最新写入的数据，或者直接报错。 对单机数据库而言「数据一致性」往往不是问题，因为它通常只有一份保存在磁盘或内存中的数据。但是在分布式系统中，为了数据安全性或者为了性能，往往每一份数据都在多个节点上存有其副本，这就引出了数据副本们的一致性问题。因此，我们通常谈论的「数据一致性」就是指分布式系统的「数据一致性」。 CAP 原则是分布式系统领域一个著名的理论，它告诉我们在分布式系统中如下三种属性不可能全部达成，因此也被称作「CAP 不可能三角」： 数据一致性 Data Consistency：客户端的每次读操作，不管访问系统的哪个节点，要么读到的都是同一份最新写入的数据，要么读取失败 强调数据完全正确 可用性 Availability：任何来自客户端的请求，不管访问哪个非故障节点，都能得到响应数据，但不保证是同一份最新数据 强调的是服务可用，但不保证数据正确 分区容错性 Partition Tolerance：即使节点之间出现了任意数量的消息丢失或者高延迟，系统仍能正常运行 就是说网络丢包或延迟会导致系统被分成多个 Partition，系统能够容忍这种情况 为了保证分区容错性 P，考虑当分布式系统因为网络问题被割裂成多个分区时，每个分区只有如下两种选择，A 跟 C 必须牺牲掉其中之一： 取消操作并拒绝提供服务，这降低了可用性，但是能确保数据一致性 继续处理请求，这确保了可用性，但是数据一致性就无法保证了 如果系统的多个分区都在同时提供服务，导致数据不一致并且存在冲突无法合并，这就被称为分布式系统的「脑裂」，显然任何分布式系统都不会希望发生「脑裂」。 因为分布式系统与单机系统不同，它涉及到多节点间的网络通讯和交互，但是只要有网络交互就一定会有延迟和数据丢失，节点间的分区故障是很有可能发生的。因此为了正常运行，P 是分布式系统必须保证的特性，在出现分区故障时，为了 P 只能牺牲掉 A 或者 C。 工程上是要 AP 还是 CP，得视情况而定： Etcd/Zookeeper/Consul: 它们通常被用于存储系统运行的关键元信息，每次读，都要能读取到最新数据。因此它们实现了 CP，牺牲了 A DynamoDB/Cassandra/MongoDB：不要求数据一致性，一段时间内用旧的缓存问题也不大，但是要求可用性，因此应该实现 AP，牺牲掉 C 数据一致性模型分布式系统中，多副本数据上的一组读写策略，被称为「（数据）一致性模型 Consistency Model」。一致性模型数量很多，让人难以分辨。为了便于理解，我们先从状态视角出发区分一下强一致与弱一致的概念，在这个的基础上再从操作视角去理解这众多的一致性模型。 1. 状态视角 - 强一致与弱一致我们首先把整个分布式系统看作一个白盒，从状态视角看，任何变更操作后，分布式系统的多个数据副本只有如下三种状态： 在某些条件下，各副本状态不一致的现象只是暂时的，后续还会转换到一致的状态，这被称为「弱一致」； 这通常是使用异步复制来同步各副本的状态。 相对的说，如果系统各副本不存在「不一致」这种状态，只要变更操作成功数据就一定完全一致，那它就被称为「强一致」。 这要求所有副本之间的数据更新必须完全同步，就必须使用全同步复制。 永远不会一致：这在分布式系统中就是 bug 了，也被称为「脑裂」。 上面描述的是整个系统的客观、实际状态，但对于绝大部分用户而言分布式系统更多的是一个黑盒，因此更流行的是基于「黑盒」的分类方式，它根据系统的对外状态将系统分成两种类型： 强一致：指对系统的任何节点/进程，写操作完成后，任何用户对任何节点的后续访问都能读到新的值。就好像系统只存在一个副本一样。 最常用算法是 Raft/Paxos，它们的写操作只要求超过半数节点写入成功，因此写入完成时，内部状态实际是不一致的，但是对它进行读写，效果跟「全同步复制」没有区别。 弱一致：指对系统的任何节点/进程，写操作完成后，后续的任何访问可能会拿到的值是不确定的，但经过一段时间后，后续的任何访问都能读到新的值。 弱一致是非常模糊的定义。如果我们把最终所有用户都能访问到新的值被称为「系统收敛」， 系统收敛的用时可以有明确边界，也可以没有。系统收敛前的访问行为可以有明确规范，也可以不存在规范。一切都看具体系统的实现。 如果系统能够在有限时间内收敛，那它就是「最终一致」，否则可以认为它是「不一致」。 为了实际需要，数据库专家对系统收敛之前的读写效果进行各种限制，对系统的收敛时间进行各种限制，得到了许多一致性模型。 2. 操作视角 - 多种一致性模型从每个客户端的操作角度看，有四种一致性模型： 写后读一致性 Read after Write Consistency：也被称作「读自己所写一致性」，即自己写完数据版本 N 后，后续读到的版本一定不小于版本 N。 它解决的问题：A 发了个抖音视频，刷新页面后却莫名其妙消失了（旧版本），几分钟后才重新刷出来。 实现方式之一：为写入者单独添加一个读取规则，他的读都由已更新其写入数据的副本来处理。 单调读一致性 Monotonic Read Consistency：保证多个读操作的顺序，即客户端一旦读到某个数据版本 N，后续不会读到比 N 更低的版本。 它解决的问题是：A 删除了一个抖音视频，可多次刷新，偶尔刷不到视频，偶尔又能刷到被删除视频（旧版本），几分钟后才彻底被删除。 实现方式之一：为每个用户的读都创建一个副本映射，后续的读都由一个固定的副本处理，避免随机切换副本而读到更老的值。 单调写一致性 Monotonic Write Consistency：保证多个写操作的顺序，即客户端对同一数据的两次写入操作，一定按其被提交的顺序被执行。 读后写一致性 Write after Read Consistency：读后写一致性，保证一个客户端读到数据版本 N 后（可能是其他客户端写入的），随后对同一数据的写操作必须要在版本号大于等于 N 的副本上执行。 上述四个一致性模型都只从每个客户端自身的角度定义规则，比较片面，因此它们都是「弱一致模型」。 而不考虑客户端，直接从所有数据库用户的操作视角看，有如下几种一致性模型： 线性一致性 Linearizability：线性一致性利用了事件的提交顺序，它保证任何读操作得到的数据，其顺序跟读/写事件的提交顺序一致。 简单的说它要求整个系统表现得像只存在一个副本，所有操作的执行结果就跟这些事件按提交顺序完全串行执行一样。这实际也是在说所有并发事件都是原子的，一旦互相之间存在冲突，就一定得按顺序执行，因此也有人称它为「原子一致性」。 线性一致性，完全等价于系统对外状态的「强一致性」 线性一致性的系统是完全确定性的 实现方式：需要一个所有节点都一致的「全局时钟」，这样才可以对所有事件进行全局排序。 大多数分布式数据库如 TiDB/Etcd 都是通过 NTP 等协议进行单点授时与同步实现的全局时钟。 有全球化部署需要的 Google Spanner 是使用 GPS + 原子钟实现的全局时钟 TrueTime，全局误差可以控制在 7ms 以内。 局限性：根据爱因斯坦相对论，「时间是相对的」，实际上并不存在绝对的时间，因此线性一致性只在经典物理学范围内适用。 顺序一致性 Sequentially Consistent： 顺序一致性最早是 Leslie Lamport 用来描述多核 CPU 的行为的，在分布式系统领域用得较少。 顺序一致性的要求有两点： 从单个进程（副本）的角度看，所有指令的执行顺序跟代码逻辑的顺序完全一致。 从所有的处理器（整个分布式系统）角度看，写操作不必立即对所有用户可见，但是所有副本必须以相同的顺序接收这些写操作。 顺序一致性和线性一致性都是要找到一个满足「写后读」的一组操作历史，差异在于线性一致性要求严格的时间序，而顺序一致性只要求满足代码的逻辑顺序，而其他代码逻辑未定义的事件顺序（比如多副本上各事件之间的顺序），具体是什么样的顺序无所谓，只要所有副本看到的事件顺序都相同就行。 顺序一致性并不能提供「确定性」，相同的两次操作仍然可能得到不同的事件顺序。 实现方式：因为不要求严格的全局时间序，它就不需要一个全局时钟了，但实际上为了满足全局的确定性，仍然需要一些复杂的操作。 因果一致性 Causal Consistency：线性一致性的全局时钟有其局限性，而因果一致性基于写事件的「偏序关系」提出了「逻辑时钟」的概念，并保证读顺序与逻辑时钟上的写事件顺序一致。 写事件的「偏序关系」关系是指，至少部分事件（比如一个节点内部的事件）是可以使用本地时钟直接排序的，而节点之间发生通讯时，接收方的事件一定晚于调用方的事件。基于这一点可以实现一个「逻辑时钟」，但逻辑时钟的缺点在于，如果某两个事件不存在相关性，那逻辑时钟给出的顺序就没有任何意义。 多数观点认为，因果一致性弱于线性一致性，但在并发性能上具有优势，也足以处理多数的异常现象，所以因果一致性也在工业界得到了应用。 CockroachDB 和 YugabyteDB 都在设计中采用了逻辑混合时钟（Hybrid Logical Clocks），这个方案源自 Lamport 的逻辑时钟，也取得了不错的效果 前缀一致性 Consistent Prefix：副本之间的同步过程中，会存在一些副本接收数据的顺序并不一致。「前缀一致性」是说所有用户读到的数据顺序的前缀永远是一致的。 「前缀」是指程序在执行写操作时，需要显式声明其「前缀」事件，这样每个事件就都存","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:2","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#2-数据一致性-data-consistency"},{"categories":["tech"],"content":" 2. 数据一致性 Data Consistency「数据一致性」是指对数据库的每一次读操作都应该读到最新写入的数据，或者直接报错。 对单机数据库而言「数据一致性」往往不是问题，因为它通常只有一份保存在磁盘或内存中的数据。但是在分布式系统中，为了数据安全性或者为了性能，往往每一份数据都在多个节点上存有其副本，这就引出了数据副本们的一致性问题。因此，我们通常谈论的「数据一致性」就是指分布式系统的「数据一致性」。 CAP 原则是分布式系统领域一个著名的理论，它告诉我们在分布式系统中如下三种属性不可能全部达成，因此也被称作「CAP 不可能三角」： 数据一致性 Data Consistency：客户端的每次读操作，不管访问系统的哪个节点，要么读到的都是同一份最新写入的数据，要么读取失败 强调数据完全正确 可用性 Availability：任何来自客户端的请求，不管访问哪个非故障节点，都能得到响应数据，但不保证是同一份最新数据 强调的是服务可用，但不保证数据正确 分区容错性 Partition Tolerance：即使节点之间出现了任意数量的消息丢失或者高延迟，系统仍能正常运行 就是说网络丢包或延迟会导致系统被分成多个 Partition，系统能够容忍这种情况 为了保证分区容错性 P，考虑当分布式系统因为网络问题被割裂成多个分区时，每个分区只有如下两种选择，A 跟 C 必须牺牲掉其中之一： 取消操作并拒绝提供服务，这降低了可用性，但是能确保数据一致性 继续处理请求，这确保了可用性，但是数据一致性就无法保证了 如果系统的多个分区都在同时提供服务，导致数据不一致并且存在冲突无法合并，这就被称为分布式系统的「脑裂」，显然任何分布式系统都不会希望发生「脑裂」。 因为分布式系统与单机系统不同，它涉及到多节点间的网络通讯和交互，但是只要有网络交互就一定会有延迟和数据丢失，节点间的分区故障是很有可能发生的。因此为了正常运行，P 是分布式系统必须保证的特性，在出现分区故障时，为了 P 只能牺牲掉 A 或者 C。 工程上是要 AP 还是 CP，得视情况而定： Etcd/Zookeeper/Consul: 它们通常被用于存储系统运行的关键元信息，每次读，都要能读取到最新数据。因此它们实现了 CP，牺牲了 A DynamoDB/Cassandra/MongoDB：不要求数据一致性，一段时间内用旧的缓存问题也不大，但是要求可用性，因此应该实现 AP，牺牲掉 C 数据一致性模型分布式系统中，多副本数据上的一组读写策略，被称为「（数据）一致性模型 Consistency Model」。一致性模型数量很多，让人难以分辨。为了便于理解，我们先从状态视角出发区分一下强一致与弱一致的概念，在这个的基础上再从操作视角去理解这众多的一致性模型。 1. 状态视角 - 强一致与弱一致我们首先把整个分布式系统看作一个白盒，从状态视角看，任何变更操作后，分布式系统的多个数据副本只有如下三种状态： 在某些条件下，各副本状态不一致的现象只是暂时的，后续还会转换到一致的状态，这被称为「弱一致」； 这通常是使用异步复制来同步各副本的状态。 相对的说，如果系统各副本不存在「不一致」这种状态，只要变更操作成功数据就一定完全一致，那它就被称为「强一致」。 这要求所有副本之间的数据更新必须完全同步，就必须使用全同步复制。 永远不会一致：这在分布式系统中就是 bug 了，也被称为「脑裂」。 上面描述的是整个系统的客观、实际状态，但对于绝大部分用户而言分布式系统更多的是一个黑盒，因此更流行的是基于「黑盒」的分类方式，它根据系统的对外状态将系统分成两种类型： 强一致：指对系统的任何节点/进程，写操作完成后，任何用户对任何节点的后续访问都能读到新的值。就好像系统只存在一个副本一样。 最常用算法是 Raft/Paxos，它们的写操作只要求超过半数节点写入成功，因此写入完成时，内部状态实际是不一致的，但是对它进行读写，效果跟「全同步复制」没有区别。 弱一致：指对系统的任何节点/进程，写操作完成后，后续的任何访问可能会拿到的值是不确定的，但经过一段时间后，后续的任何访问都能读到新的值。 弱一致是非常模糊的定义。如果我们把最终所有用户都能访问到新的值被称为「系统收敛」， 系统收敛的用时可以有明确边界，也可以没有。系统收敛前的访问行为可以有明确规范，也可以不存在规范。一切都看具体系统的实现。 如果系统能够在有限时间内收敛，那它就是「最终一致」，否则可以认为它是「不一致」。 为了实际需要，数据库专家对系统收敛之前的读写效果进行各种限制，对系统的收敛时间进行各种限制，得到了许多一致性模型。 2. 操作视角 - 多种一致性模型从每个客户端的操作角度看，有四种一致性模型： 写后读一致性 Read after Write Consistency：也被称作「读自己所写一致性」，即自己写完数据版本 N 后，后续读到的版本一定不小于版本 N。 它解决的问题：A 发了个抖音视频，刷新页面后却莫名其妙消失了（旧版本），几分钟后才重新刷出来。 实现方式之一：为写入者单独添加一个读取规则，他的读都由已更新其写入数据的副本来处理。 单调读一致性 Monotonic Read Consistency：保证多个读操作的顺序，即客户端一旦读到某个数据版本 N，后续不会读到比 N 更低的版本。 它解决的问题是：A 删除了一个抖音视频，可多次刷新，偶尔刷不到视频，偶尔又能刷到被删除视频（旧版本），几分钟后才彻底被删除。 实现方式之一：为每个用户的读都创建一个副本映射，后续的读都由一个固定的副本处理，避免随机切换副本而读到更老的值。 单调写一致性 Monotonic Write Consistency：保证多个写操作的顺序，即客户端对同一数据的两次写入操作，一定按其被提交的顺序被执行。 读后写一致性 Write after Read Consistency：读后写一致性，保证一个客户端读到数据版本 N 后（可能是其他客户端写入的），随后对同一数据的写操作必须要在版本号大于等于 N 的副本上执行。 上述四个一致性模型都只从每个客户端自身的角度定义规则，比较片面，因此它们都是「弱一致模型」。 而不考虑客户端，直接从所有数据库用户的操作视角看，有如下几种一致性模型： 线性一致性 Linearizability：线性一致性利用了事件的提交顺序，它保证任何读操作得到的数据，其顺序跟读/写事件的提交顺序一致。 简单的说它要求整个系统表现得像只存在一个副本，所有操作的执行结果就跟这些事件按提交顺序完全串行执行一样。这实际也是在说所有并发事件都是原子的，一旦互相之间存在冲突，就一定得按顺序执行，因此也有人称它为「原子一致性」。 线性一致性，完全等价于系统对外状态的「强一致性」 线性一致性的系统是完全确定性的 实现方式：需要一个所有节点都一致的「全局时钟」，这样才可以对所有事件进行全局排序。 大多数分布式数据库如 TiDB/Etcd 都是通过 NTP 等协议进行单点授时与同步实现的全局时钟。 有全球化部署需要的 Google Spanner 是使用 GPS + 原子钟实现的全局时钟 TrueTime，全局误差可以控制在 7ms 以内。 局限性：根据爱因斯坦相对论，「时间是相对的」，实际上并不存在绝对的时间，因此线性一致性只在经典物理学范围内适用。 顺序一致性 Sequentially Consistent： 顺序一致性最早是 Leslie Lamport 用来描述多核 CPU 的行为的，在分布式系统领域用得较少。 顺序一致性的要求有两点： 从单个进程（副本）的角度看，所有指令的执行顺序跟代码逻辑的顺序完全一致。 从所有的处理器（整个分布式系统）角度看，写操作不必立即对所有用户可见，但是所有副本必须以相同的顺序接收这些写操作。 顺序一致性和线性一致性都是要找到一个满足「写后读」的一组操作历史，差异在于线性一致性要求严格的时间序，而顺序一致性只要求满足代码的逻辑顺序，而其他代码逻辑未定义的事件顺序（比如多副本上各事件之间的顺序），具体是什么样的顺序无所谓，只要所有副本看到的事件顺序都相同就行。 顺序一致性并不能提供「确定性」，相同的两次操作仍然可能得到不同的事件顺序。 实现方式：因为不要求严格的全局时间序，它就不需要一个全局时钟了，但实际上为了满足全局的确定性，仍然需要一些复杂的操作。 因果一致性 Causal Consistency：线性一致性的全局时钟有其局限性，而因果一致性基于写事件的「偏序关系」提出了「逻辑时钟」的概念，并保证读顺序与逻辑时钟上的写事件顺序一致。 写事件的「偏序关系」关系是指，至少部分事件（比如一个节点内部的事件）是可以使用本地时钟直接排序的，而节点之间发生通讯时，接收方的事件一定晚于调用方的事件。基于这一点可以实现一个「逻辑时钟」，但逻辑时钟的缺点在于，如果某两个事件不存在相关性，那逻辑时钟给出的顺序就没有任何意义。 多数观点认为，因果一致性弱于线性一致性，但在并发性能上具有优势，也足以处理多数的异常现象，所以因果一致性也在工业界得到了应用。 CockroachDB 和 YugabyteDB 都在设计中采用了逻辑混合时钟（Hybrid Logical Clocks），这个方案源自 Lamport 的逻辑时钟，也取得了不错的效果 前缀一致性 Consistent Prefix：副本之间的同步过程中，会存在一些副本接收数据的顺序并不一致。「前缀一致性」是说所有用户读到的数据顺序的前缀永远是一致的。 「前缀」是指程序在执行写操作时，需要显式声明其「前缀」事件，这样每个事件就都存","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:2","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#数据一致性模型"},{"categories":["tech"],"content":" 2. 数据一致性 Data Consistency「数据一致性」是指对数据库的每一次读操作都应该读到最新写入的数据，或者直接报错。 对单机数据库而言「数据一致性」往往不是问题，因为它通常只有一份保存在磁盘或内存中的数据。但是在分布式系统中，为了数据安全性或者为了性能，往往每一份数据都在多个节点上存有其副本，这就引出了数据副本们的一致性问题。因此，我们通常谈论的「数据一致性」就是指分布式系统的「数据一致性」。 CAP 原则是分布式系统领域一个著名的理论，它告诉我们在分布式系统中如下三种属性不可能全部达成，因此也被称作「CAP 不可能三角」： 数据一致性 Data Consistency：客户端的每次读操作，不管访问系统的哪个节点，要么读到的都是同一份最新写入的数据，要么读取失败 强调数据完全正确 可用性 Availability：任何来自客户端的请求，不管访问哪个非故障节点，都能得到响应数据，但不保证是同一份最新数据 强调的是服务可用，但不保证数据正确 分区容错性 Partition Tolerance：即使节点之间出现了任意数量的消息丢失或者高延迟，系统仍能正常运行 就是说网络丢包或延迟会导致系统被分成多个 Partition，系统能够容忍这种情况 为了保证分区容错性 P，考虑当分布式系统因为网络问题被割裂成多个分区时，每个分区只有如下两种选择，A 跟 C 必须牺牲掉其中之一： 取消操作并拒绝提供服务，这降低了可用性，但是能确保数据一致性 继续处理请求，这确保了可用性，但是数据一致性就无法保证了 如果系统的多个分区都在同时提供服务，导致数据不一致并且存在冲突无法合并，这就被称为分布式系统的「脑裂」，显然任何分布式系统都不会希望发生「脑裂」。 因为分布式系统与单机系统不同，它涉及到多节点间的网络通讯和交互，但是只要有网络交互就一定会有延迟和数据丢失，节点间的分区故障是很有可能发生的。因此为了正常运行，P 是分布式系统必须保证的特性，在出现分区故障时，为了 P 只能牺牲掉 A 或者 C。 工程上是要 AP 还是 CP，得视情况而定： Etcd/Zookeeper/Consul: 它们通常被用于存储系统运行的关键元信息，每次读，都要能读取到最新数据。因此它们实现了 CP，牺牲了 A DynamoDB/Cassandra/MongoDB：不要求数据一致性，一段时间内用旧的缓存问题也不大，但是要求可用性，因此应该实现 AP，牺牲掉 C 数据一致性模型分布式系统中，多副本数据上的一组读写策略，被称为「（数据）一致性模型 Consistency Model」。一致性模型数量很多，让人难以分辨。为了便于理解，我们先从状态视角出发区分一下强一致与弱一致的概念，在这个的基础上再从操作视角去理解这众多的一致性模型。 1. 状态视角 - 强一致与弱一致我们首先把整个分布式系统看作一个白盒，从状态视角看，任何变更操作后，分布式系统的多个数据副本只有如下三种状态： 在某些条件下，各副本状态不一致的现象只是暂时的，后续还会转换到一致的状态，这被称为「弱一致」； 这通常是使用异步复制来同步各副本的状态。 相对的说，如果系统各副本不存在「不一致」这种状态，只要变更操作成功数据就一定完全一致，那它就被称为「强一致」。 这要求所有副本之间的数据更新必须完全同步，就必须使用全同步复制。 永远不会一致：这在分布式系统中就是 bug 了，也被称为「脑裂」。 上面描述的是整个系统的客观、实际状态，但对于绝大部分用户而言分布式系统更多的是一个黑盒，因此更流行的是基于「黑盒」的分类方式，它根据系统的对外状态将系统分成两种类型： 强一致：指对系统的任何节点/进程，写操作完成后，任何用户对任何节点的后续访问都能读到新的值。就好像系统只存在一个副本一样。 最常用算法是 Raft/Paxos，它们的写操作只要求超过半数节点写入成功，因此写入完成时，内部状态实际是不一致的，但是对它进行读写，效果跟「全同步复制」没有区别。 弱一致：指对系统的任何节点/进程，写操作完成后，后续的任何访问可能会拿到的值是不确定的，但经过一段时间后，后续的任何访问都能读到新的值。 弱一致是非常模糊的定义。如果我们把最终所有用户都能访问到新的值被称为「系统收敛」， 系统收敛的用时可以有明确边界，也可以没有。系统收敛前的访问行为可以有明确规范，也可以不存在规范。一切都看具体系统的实现。 如果系统能够在有限时间内收敛，那它就是「最终一致」，否则可以认为它是「不一致」。 为了实际需要，数据库专家对系统收敛之前的读写效果进行各种限制，对系统的收敛时间进行各种限制，得到了许多一致性模型。 2. 操作视角 - 多种一致性模型从每个客户端的操作角度看，有四种一致性模型： 写后读一致性 Read after Write Consistency：也被称作「读自己所写一致性」，即自己写完数据版本 N 后，后续读到的版本一定不小于版本 N。 它解决的问题：A 发了个抖音视频，刷新页面后却莫名其妙消失了（旧版本），几分钟后才重新刷出来。 实现方式之一：为写入者单独添加一个读取规则，他的读都由已更新其写入数据的副本来处理。 单调读一致性 Monotonic Read Consistency：保证多个读操作的顺序，即客户端一旦读到某个数据版本 N，后续不会读到比 N 更低的版本。 它解决的问题是：A 删除了一个抖音视频，可多次刷新，偶尔刷不到视频，偶尔又能刷到被删除视频（旧版本），几分钟后才彻底被删除。 实现方式之一：为每个用户的读都创建一个副本映射，后续的读都由一个固定的副本处理，避免随机切换副本而读到更老的值。 单调写一致性 Monotonic Write Consistency：保证多个写操作的顺序，即客户端对同一数据的两次写入操作，一定按其被提交的顺序被执行。 读后写一致性 Write after Read Consistency：读后写一致性，保证一个客户端读到数据版本 N 后（可能是其他客户端写入的），随后对同一数据的写操作必须要在版本号大于等于 N 的副本上执行。 上述四个一致性模型都只从每个客户端自身的角度定义规则，比较片面，因此它们都是「弱一致模型」。 而不考虑客户端，直接从所有数据库用户的操作视角看，有如下几种一致性模型： 线性一致性 Linearizability：线性一致性利用了事件的提交顺序，它保证任何读操作得到的数据，其顺序跟读/写事件的提交顺序一致。 简单的说它要求整个系统表现得像只存在一个副本，所有操作的执行结果就跟这些事件按提交顺序完全串行执行一样。这实际也是在说所有并发事件都是原子的，一旦互相之间存在冲突，就一定得按顺序执行，因此也有人称它为「原子一致性」。 线性一致性，完全等价于系统对外状态的「强一致性」 线性一致性的系统是完全确定性的 实现方式：需要一个所有节点都一致的「全局时钟」，这样才可以对所有事件进行全局排序。 大多数分布式数据库如 TiDB/Etcd 都是通过 NTP 等协议进行单点授时与同步实现的全局时钟。 有全球化部署需要的 Google Spanner 是使用 GPS + 原子钟实现的全局时钟 TrueTime，全局误差可以控制在 7ms 以内。 局限性：根据爱因斯坦相对论，「时间是相对的」，实际上并不存在绝对的时间，因此线性一致性只在经典物理学范围内适用。 顺序一致性 Sequentially Consistent： 顺序一致性最早是 Leslie Lamport 用来描述多核 CPU 的行为的，在分布式系统领域用得较少。 顺序一致性的要求有两点： 从单个进程（副本）的角度看，所有指令的执行顺序跟代码逻辑的顺序完全一致。 从所有的处理器（整个分布式系统）角度看，写操作不必立即对所有用户可见，但是所有副本必须以相同的顺序接收这些写操作。 顺序一致性和线性一致性都是要找到一个满足「写后读」的一组操作历史，差异在于线性一致性要求严格的时间序，而顺序一致性只要求满足代码的逻辑顺序，而其他代码逻辑未定义的事件顺序（比如多副本上各事件之间的顺序），具体是什么样的顺序无所谓，只要所有副本看到的事件顺序都相同就行。 顺序一致性并不能提供「确定性」，相同的两次操作仍然可能得到不同的事件顺序。 实现方式：因为不要求严格的全局时间序，它就不需要一个全局时钟了，但实际上为了满足全局的确定性，仍然需要一些复杂的操作。 因果一致性 Causal Consistency：线性一致性的全局时钟有其局限性，而因果一致性基于写事件的「偏序关系」提出了「逻辑时钟」的概念，并保证读顺序与逻辑时钟上的写事件顺序一致。 写事件的「偏序关系」关系是指，至少部分事件（比如一个节点内部的事件）是可以使用本地时钟直接排序的，而节点之间发生通讯时，接收方的事件一定晚于调用方的事件。基于这一点可以实现一个「逻辑时钟」，但逻辑时钟的缺点在于，如果某两个事件不存在相关性，那逻辑时钟给出的顺序就没有任何意义。 多数观点认为，因果一致性弱于线性一致性，但在并发性能上具有优势，也足以处理多数的异常现象，所以因果一致性也在工业界得到了应用。 CockroachDB 和 YugabyteDB 都在设计中采用了逻辑混合时钟（Hybrid Logical Clocks），这个方案源自 Lamport 的逻辑时钟，也取得了不错的效果 前缀一致性 Consistent Prefix：副本之间的同步过程中，会存在一些副本接收数据的顺序并不一致。「前缀一致性」是说所有用户读到的数据顺序的前缀永远是一致的。 「前缀」是指程序在执行写操作时，需要显式声明其「前缀」事件，这样每个事件就都存","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:2","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#1-状态视角---强一致与弱一致"},{"categories":["tech"],"content":" 2. 数据一致性 Data Consistency「数据一致性」是指对数据库的每一次读操作都应该读到最新写入的数据，或者直接报错。 对单机数据库而言「数据一致性」往往不是问题，因为它通常只有一份保存在磁盘或内存中的数据。但是在分布式系统中，为了数据安全性或者为了性能，往往每一份数据都在多个节点上存有其副本，这就引出了数据副本们的一致性问题。因此，我们通常谈论的「数据一致性」就是指分布式系统的「数据一致性」。 CAP 原则是分布式系统领域一个著名的理论，它告诉我们在分布式系统中如下三种属性不可能全部达成，因此也被称作「CAP 不可能三角」： 数据一致性 Data Consistency：客户端的每次读操作，不管访问系统的哪个节点，要么读到的都是同一份最新写入的数据，要么读取失败 强调数据完全正确 可用性 Availability：任何来自客户端的请求，不管访问哪个非故障节点，都能得到响应数据，但不保证是同一份最新数据 强调的是服务可用，但不保证数据正确 分区容错性 Partition Tolerance：即使节点之间出现了任意数量的消息丢失或者高延迟，系统仍能正常运行 就是说网络丢包或延迟会导致系统被分成多个 Partition，系统能够容忍这种情况 为了保证分区容错性 P，考虑当分布式系统因为网络问题被割裂成多个分区时，每个分区只有如下两种选择，A 跟 C 必须牺牲掉其中之一： 取消操作并拒绝提供服务，这降低了可用性，但是能确保数据一致性 继续处理请求，这确保了可用性，但是数据一致性就无法保证了 如果系统的多个分区都在同时提供服务，导致数据不一致并且存在冲突无法合并，这就被称为分布式系统的「脑裂」，显然任何分布式系统都不会希望发生「脑裂」。 因为分布式系统与单机系统不同，它涉及到多节点间的网络通讯和交互，但是只要有网络交互就一定会有延迟和数据丢失，节点间的分区故障是很有可能发生的。因此为了正常运行，P 是分布式系统必须保证的特性，在出现分区故障时，为了 P 只能牺牲掉 A 或者 C。 工程上是要 AP 还是 CP，得视情况而定： Etcd/Zookeeper/Consul: 它们通常被用于存储系统运行的关键元信息，每次读，都要能读取到最新数据。因此它们实现了 CP，牺牲了 A DynamoDB/Cassandra/MongoDB：不要求数据一致性，一段时间内用旧的缓存问题也不大，但是要求可用性，因此应该实现 AP，牺牲掉 C 数据一致性模型分布式系统中，多副本数据上的一组读写策略，被称为「（数据）一致性模型 Consistency Model」。一致性模型数量很多，让人难以分辨。为了便于理解，我们先从状态视角出发区分一下强一致与弱一致的概念，在这个的基础上再从操作视角去理解这众多的一致性模型。 1. 状态视角 - 强一致与弱一致我们首先把整个分布式系统看作一个白盒，从状态视角看，任何变更操作后，分布式系统的多个数据副本只有如下三种状态： 在某些条件下，各副本状态不一致的现象只是暂时的，后续还会转换到一致的状态，这被称为「弱一致」； 这通常是使用异步复制来同步各副本的状态。 相对的说，如果系统各副本不存在「不一致」这种状态，只要变更操作成功数据就一定完全一致，那它就被称为「强一致」。 这要求所有副本之间的数据更新必须完全同步，就必须使用全同步复制。 永远不会一致：这在分布式系统中就是 bug 了，也被称为「脑裂」。 上面描述的是整个系统的客观、实际状态，但对于绝大部分用户而言分布式系统更多的是一个黑盒，因此更流行的是基于「黑盒」的分类方式，它根据系统的对外状态将系统分成两种类型： 强一致：指对系统的任何节点/进程，写操作完成后，任何用户对任何节点的后续访问都能读到新的值。就好像系统只存在一个副本一样。 最常用算法是 Raft/Paxos，它们的写操作只要求超过半数节点写入成功，因此写入完成时，内部状态实际是不一致的，但是对它进行读写，效果跟「全同步复制」没有区别。 弱一致：指对系统的任何节点/进程，写操作完成后，后续的任何访问可能会拿到的值是不确定的，但经过一段时间后，后续的任何访问都能读到新的值。 弱一致是非常模糊的定义。如果我们把最终所有用户都能访问到新的值被称为「系统收敛」， 系统收敛的用时可以有明确边界，也可以没有。系统收敛前的访问行为可以有明确规范，也可以不存在规范。一切都看具体系统的实现。 如果系统能够在有限时间内收敛，那它就是「最终一致」，否则可以认为它是「不一致」。 为了实际需要，数据库专家对系统收敛之前的读写效果进行各种限制，对系统的收敛时间进行各种限制，得到了许多一致性模型。 2. 操作视角 - 多种一致性模型从每个客户端的操作角度看，有四种一致性模型： 写后读一致性 Read after Write Consistency：也被称作「读自己所写一致性」，即自己写完数据版本 N 后，后续读到的版本一定不小于版本 N。 它解决的问题：A 发了个抖音视频，刷新页面后却莫名其妙消失了（旧版本），几分钟后才重新刷出来。 实现方式之一：为写入者单独添加一个读取规则，他的读都由已更新其写入数据的副本来处理。 单调读一致性 Monotonic Read Consistency：保证多个读操作的顺序，即客户端一旦读到某个数据版本 N，后续不会读到比 N 更低的版本。 它解决的问题是：A 删除了一个抖音视频，可多次刷新，偶尔刷不到视频，偶尔又能刷到被删除视频（旧版本），几分钟后才彻底被删除。 实现方式之一：为每个用户的读都创建一个副本映射，后续的读都由一个固定的副本处理，避免随机切换副本而读到更老的值。 单调写一致性 Monotonic Write Consistency：保证多个写操作的顺序，即客户端对同一数据的两次写入操作，一定按其被提交的顺序被执行。 读后写一致性 Write after Read Consistency：读后写一致性，保证一个客户端读到数据版本 N 后（可能是其他客户端写入的），随后对同一数据的写操作必须要在版本号大于等于 N 的副本上执行。 上述四个一致性模型都只从每个客户端自身的角度定义规则，比较片面，因此它们都是「弱一致模型」。 而不考虑客户端，直接从所有数据库用户的操作视角看，有如下几种一致性模型： 线性一致性 Linearizability：线性一致性利用了事件的提交顺序，它保证任何读操作得到的数据，其顺序跟读/写事件的提交顺序一致。 简单的说它要求整个系统表现得像只存在一个副本，所有操作的执行结果就跟这些事件按提交顺序完全串行执行一样。这实际也是在说所有并发事件都是原子的，一旦互相之间存在冲突，就一定得按顺序执行，因此也有人称它为「原子一致性」。 线性一致性，完全等价于系统对外状态的「强一致性」 线性一致性的系统是完全确定性的 实现方式：需要一个所有节点都一致的「全局时钟」，这样才可以对所有事件进行全局排序。 大多数分布式数据库如 TiDB/Etcd 都是通过 NTP 等协议进行单点授时与同步实现的全局时钟。 有全球化部署需要的 Google Spanner 是使用 GPS + 原子钟实现的全局时钟 TrueTime，全局误差可以控制在 7ms 以内。 局限性：根据爱因斯坦相对论，「时间是相对的」，实际上并不存在绝对的时间，因此线性一致性只在经典物理学范围内适用。 顺序一致性 Sequentially Consistent： 顺序一致性最早是 Leslie Lamport 用来描述多核 CPU 的行为的，在分布式系统领域用得较少。 顺序一致性的要求有两点： 从单个进程（副本）的角度看，所有指令的执行顺序跟代码逻辑的顺序完全一致。 从所有的处理器（整个分布式系统）角度看，写操作不必立即对所有用户可见，但是所有副本必须以相同的顺序接收这些写操作。 顺序一致性和线性一致性都是要找到一个满足「写后读」的一组操作历史，差异在于线性一致性要求严格的时间序，而顺序一致性只要求满足代码的逻辑顺序，而其他代码逻辑未定义的事件顺序（比如多副本上各事件之间的顺序），具体是什么样的顺序无所谓，只要所有副本看到的事件顺序都相同就行。 顺序一致性并不能提供「确定性」，相同的两次操作仍然可能得到不同的事件顺序。 实现方式：因为不要求严格的全局时间序，它就不需要一个全局时钟了，但实际上为了满足全局的确定性，仍然需要一些复杂的操作。 因果一致性 Causal Consistency：线性一致性的全局时钟有其局限性，而因果一致性基于写事件的「偏序关系」提出了「逻辑时钟」的概念，并保证读顺序与逻辑时钟上的写事件顺序一致。 写事件的「偏序关系」关系是指，至少部分事件（比如一个节点内部的事件）是可以使用本地时钟直接排序的，而节点之间发生通讯时，接收方的事件一定晚于调用方的事件。基于这一点可以实现一个「逻辑时钟」，但逻辑时钟的缺点在于，如果某两个事件不存在相关性，那逻辑时钟给出的顺序就没有任何意义。 多数观点认为，因果一致性弱于线性一致性，但在并发性能上具有优势，也足以处理多数的异常现象，所以因果一致性也在工业界得到了应用。 CockroachDB 和 YugabyteDB 都在设计中采用了逻辑混合时钟（Hybrid Logical Clocks），这个方案源自 Lamport 的逻辑时钟，也取得了不错的效果 前缀一致性 Consistent Prefix：副本之间的同步过程中，会存在一些副本接收数据的顺序并不一致。「前缀一致性」是说所有用户读到的数据顺序的前缀永远是一致的。 「前缀」是指程序在执行写操作时，需要显式声明其「前缀」事件，这样每个事件就都存","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:1:2","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#2-操作视角---多种一致性模型"},{"categories":["tech"],"content":" 二、分布式系统的 BASE 与最终一致性BASE 理论： 基本可用 Basically Available：当分布式系统在出现不可预知的故障时，允许损失部分功能的可用性，保障核心功能的可用性 四种实现基本可用的手段：流量削峰、延迟响应、体验降级、过载保护 软状态 Soft state：在柔性事务中，允许系统存在中间状态，且这个中间状态不会影响系统整体可用性。比如，数据库读写分离，写库同步到读库（主库同步到从库）会有一个延时，其实就是一种柔性状态。 最终一致性 Eventually consistent：前面已经说得很详细了，它指对系统的任何节点/进程， 写操作完成后，后续的任何访问可能会拿到的值是不确定的，但经过有限的一段时间后，后续的任何访问都能读到新的值。 ACID 与 BASE 实质上是分布式系统实现中的的两个极端： ACID 理论就如它的含义「酸」一样，是 CAP 原则中一致性的边界——最强的一致性，是牺牲掉 A 后达到 CP 的极致。 BASE 翻译过来就是「碱」，它是 CAP 原则中可用性的边界——最高的可用性，最弱的一致性，通过牺牲掉 C 来达到 AP 的极致。 根据 CAP 理论，如果在分布式系统中实现了一致性，可用性必然受到影响。比如，如果出现一个节点故障，则整个分布式事务的执行都是失败的。实际上，绝大部分场景对一致性要求没那么高，短暂的不一致是能接受的，另外，也基于可用性和并发性能的考虑，建议在开发实现分布式系统时，如果不是必须，尽量不要实现事务，可以考虑采用最终一致性。 最终一致性的实现手段： 读时修复：在读取数据时，检测数据的不一致，进行修复 写时修复：在写入数据时，检测数据的不一致，进行修复 异步修复：这个是最常用的方式，通过定时对账，检测副本数据的一致性并修复 在实现最终一致性的时候，还推荐同时实现自定义写一致性级别（比如 All、Quorum、One、Any），许多分布式数据库的最终一致性级别都是可调的。 但是随着 TiDB 等分布式关系数据库的兴起，分布式领域的 BASE 理论实际上正在被 ACID 赶超，ACID 焕发又一春了。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:2:0","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#二分布式系统的-base-与最终一致性"},{"categories":["tech"],"content":" 三、共识算法共识算法，也被称为一致性协议，是指在分布式系统中多个节点之间对某个提案 Proposal（例如多个事务请求，先执行谁？）达成一致看法的一套流程。 提案的含义在分布式系统中十分宽泛，如多个事件发生的顺序、某个键对应的值、谁是主节点……等等。可以认为任何可以达成一致的信息都是一个提案。 对于分布式系统来讲，各个节点通常都是相同的确定性状态机模型（又称为状态机复制问题，State-Machine Replication），从相同初始状态开始接收相同顺序的指令，则可以保证相同的结果状态。因此，系统中多个节点最关键的是对多个事件的顺序进行共识，即排序。 共识算法是达成数据一致性的一种手段，而且是数据强一致性的必要非充分条件。比如直接使用 Raft 算法，但是允许读取集群的任何节点，只能得到数据的最终一致性，还需要其他手段才能确保强一致性。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:0","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#三共识算法"},{"categories":["tech"],"content":" 拜占庭将军问题与拜占庭容错拜占庭错误是 1982 年兰伯特在《拜占庭将军问题》中提出的一个错误模型，描述了在少数节点不仅存在故障，还存在恶意行为的场景下，能否达成共识这样一个问题，论文描述如下： 9 位拜占庭将军分别率领一支军队要共同围困一座城市，因为这座城市很强大，如果不协调统一将军们的行动策略，部分军队进攻、部分军队撤退会造成围困失败，因此各位将军必须通过投票来达成一致策略，要么一起进攻，要么一起撤退。 因为各位将军分别占据城市的一角，他们只能通过信使互相联系。在协调过程中每位将军都将自己投票“进攻”还是“撤退”的消息通过信使分别通知其他所有将军，这样一来每位将军根据自己的投票和其他将军送过来的投票，就可以知道投票结果，从而决定是进攻还是撤退。 而问题的复杂性就在于：将军中可能出现叛徒，他们不仅可以投票给错误的决策，还可能会选择性地发送投票。假设 9 位将军中有 1 名叛徒，8 位忠诚的将军中出现了 4 人投“进攻”，4 人投“撤退 ”，这时候叛徒可能故意给 4 名投“进攻”的将军投“进攻”，而给另外 4 名投“撤退”的将军投“撤退 ”。这样在 4 名投“进攻”的将军看来，投票是 5 人投“进攻”，从而发动进攻；而另外 4 名将军看来是 5 人投“撤退”，从而撤退。这样，一致性就遭到了破坏。 还有一种情况，因为将军之间需要通过信使交流，即便所有的将军都是忠诚的，派出去的信使也可能被敌军截杀，甚至被间谍替换，也就是说将军之间进行交流的信息通道是不能保证可靠性的。所以在没有收到对应将军消息的时候，将军们会默认投一个票，例如“进攻”。 更一般地，在已知有 N 个将军谋反的情况下，其余 M 个忠诚的将军在不受叛徒的影响下能否达成共识？有什么样的前提条件，该如何达成共识？这就是拜占庭将军问题。 如果一个共识算法在一定条件下能够解决拜占庭将军问题，那我们就称这个算法是「拜占庭容错 Byzantine Fault Tolerance（BFT）」算法。反之如果一个共识算法无法接受任何一个节点作恶，那它就被称为「非拜占庭容错 Crash Fault Tolerance (CFT)」算法。 可以通过简单穷举发现，二忠一叛是无法达成共识的，这个结论结合反证法可证明，拜占庭容错算法要求叛徒的比例必须低于 1/3。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:1","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#拜占庭将军问题与拜占庭容错"},{"categories":["tech"],"content":" 常用共识算法对于「非拜占庭容错 Crash Fault Tolerance (CFT)」的情况，已经存在不少经典的算法，包括 Paxos（1990 年）、Raft（2014 年）及其变种等。这类容错算法往往性能比较好，处理较快，容忍不超过一半的故障节点。 对于「拜占庭容错 Byzantine Fault Tolerance（BFT）」的情况，目前有 PBFT（Practical Byzantine Fault Tolerance，1999 年）为代表的确定性系列算法、PoW（1999 年）为代表的概率算法等算法可选。确定性算法一旦达成共识就不可逆转，即共识是最终结果；而概率类算法的共识结果则是临时的，随着时间推移或某种强化，共识结果被推翻的概率越来越小，最终成为事实上结果。拜占庭类容错算法往往性能较差，容忍不超过 1/3 的故障节点。 此外，XFT（Cross Fault Tolerance，2015 年）等最近提出的改进算法可以提供类似 CFT 的处理响应速度，并能在大多数节点正常工作时提供 BFT 保障。Algorand 算法（2017 年）基于 PBFT 进行改进，通过引入可验证随机函数解决了提案选择的问题，理论上可以在容忍拜占庭错误的前提下实现更好的性能（1000+ TPS）。 注：实践中，对客户端来说要拿到共识结果需要自行验证，典型地，可访问足够多个服务节点来比对结果，确保获取结果的准确性。 常见共识算法列举如下： 拜占庭容错 一致性 性能 可用性（能容忍多大比例的节点出现故障） 两阶段提交 2PC 否 强一致性 低 低 TCC(try-confirm-cancel) 否 最终一致性 低 低 Paxos 否 强一致性 中 中 ZAB 否 最终一致性 中 中 Raft 否 强一致性 中 中 Gossip 否 最终一致性 高 高 Quorum NWR 否 强一致性 中 中 PBFT 是 N/A 低 中 PoW 是 N/A 低 中 PoS 是 N/A 低 中 PoH 是 N/A 中 中 注：这里虽然列出了 PoW/PoS/PoH 等应用在区块链中的一致性算法，但是它们跟 PBFT 等其他拜占庭容错算法存在很大的区别，后面会给出介绍。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:2","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#常用共识算法"},{"categories":["tech"],"content":" 不同共识算法的应用场景在不可信环境中，因为可能存在恶意行为，就需要使用支持拜占庭容错的共识算法如 PoW/PoS，使系统在存在部分节点作恶的情况下仍然能达成共识。这就是区块链使用 PoW/PoS 算法而不是 Paxos/Raft 算法的原因。 而在企业内网等场景下，可以认为是可信环境，基本不会出现恶意节点或者可以通过 mTLS 等手段进行节点身份认证，这种场景下系统具有故障容错能力就够了，就没必要做到拜占庭容错，因此常用 Raft/Paxos 等算法。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:3","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#不同共识算法的应用场景"},{"categories":["tech"],"content":" 非拜占庭错误共识算法 Paxos 与 Raft受限于篇幅与笔者精力，这部分暂时跳过…后面可能会写篇新的文章专门介绍 Paxos/Raft 算法。 ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:4","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#非拜占庭错误共识算法-paxos-与-raft"},{"categories":["tech"],"content":" 能容忍拜占庭错误的 PoW 概率共识算法PoW 即 Proof of Work 工作量证明，指系统为达到某一目标而设置的度量方法。简单理解就是一份证明，用来确认你做过一定量的工作。监测工作的整个过程通常是极为低效的，而通过对工作的结果进行认证来证明完成了相应的工作量，这个认证流程是非常简单高效的，这就是 PoW 的优势所在。 在 1993 年，Cynthia Dwork 和 Moni Naor 设计了一个系统用于反垃圾邮件、避免资源被滥用， 这是 PoW 算法的雏形。其核心思想如下： The main idea is to require a user to compute a moderately hard but not intractable function in order to gain access to the resource, thus preventing frivolous use. 翻译成中文： 其主要思想是要求用户计算一个中等难度但不难处理的函数，以获得对资源的访问，从而防止（系统资源被）滥用。 在 1999 年，Markus Jakobsson 与 Ari Juels 第一次从各种协议中提炼出 Proofs of Work 这个概念。 POW 系统中一定有两个角色，工作者和验证者，他们需要具有以下特点： 工作者需要完成的工作必须有一定的量，这个量由工作验证者给出。 验证者可以迅速的检验工作量是否达标。 工作者无法自己\"创造工作\"，必须由验证者发布工作。 工作者无法找到很快完成工作的办法。 说到这里，我们对 PoW 应该有足够的理解了，它就是让工作者消耗一定的资源作为使用系统的成本。对于正常的用户而言这部分被消耗的资源是完全可以接受的，但是对于恶意攻击者而言，它如果想滥用系统的资源或者发送海量的垃圾邮件，就需要消耗海量的计算资源作为成本，这样就极大地提升了攻击成本。 再总结下，PoW 算法的核心是它为信息发送加入了成本，降低了信息传递的速率。 把比特币区块链转换成拜占庭将军问题来看，它的思路是这样的： 限制一段时间内提案的个数，只有拥有对应权限的节点（将军）可以发起提案。 这是通过 PoW 工作量证明实现的，比特币区块链要求节点进行海量的哈希计算作为获得提案权限的代价，算法难度每隔两周调整一次以保证整个系统找到正确 Hash 值的平均用时大约为 10 分钟。 由强一致性放宽至最终一致性。 对应一次提案的结果不需要全部的节点马上跟进，只需要在节点能搜寻到的全网络中的所有链条中，选取最长的链条进行后续拓展就可以。 使用非对称加密算法对节点间的消息传递提供签名技术支持，每个节点（将军）都有属于自己的秘钥 （公钥私钥），唯一标识节点身份。 使用非对称加密算法传递消息，能够保证消息传递的私密性。而且消息签名不可篡改，这避免了消息被恶意节点伪造。 我们前面有给出一个结论：拜占庭容错算法要求叛徒的比例必须低于 1/3。 但是区块链与拜占庭将军问题的区别很大，举例如下： 区块链允许任何节点随时加入或离开区块链，而拜占庭将军问题是预设了节点数，而且不考虑节点的添加或删除。 比特币区块链的 PoW 算法只能保证整个系统找到正确 Hash 值的平均用时大约为 10 分钟，那肯定就存在性能更好的节点用时更短，性能更差的节点用时更长，甚至某些节点运气好几秒钟就算出了结果，这都是完全可能的。而越早算出这个 Hash 值的节点，它的提案（区块）成为最长链条的概率就越大。 PoW 由强一致性放宽至最终一致性，系统总会选取最长的链进行后续拓展，那如果某个链条一开始不长，但是它的拓展速度足够快，它就能成为最长的链条。而拜占庭将军问题不允许任何分支，只存在一个结果！ 只是受限于算力，随着时间的推移，短的链条追上最长链条的概率会越来越小。 总之因为区块链这样的特点，它会产生一些跟拜占庭容错算法不同的结果： 攻击者拥有的节点数量占比是毫无意义的，核心是算力，也就对应着区块链中的提案权。 即使攻击者拥有了 99% 的节点，但是它的总体算力很弱的话，它的提案（区块）成为最长链条的概率也会很低。 区块链的 51% 攻击：因为「系统总是选取最长链条进行后续拓展」这个原则，只有某个攻击者拥有了超过 50% 算力的情况下，它才拥有绝对性的优势，使它的区块在一定时间后一定能成为最长的链条，并且始终维持这样一个优势，从而达成攻击目的。 至于 PoW 算法的具体实现，以及它的替代算法 PoS/PoH 等新兴算法的原理与实现，将在后续的区块链系列文章中详细介绍，尽请期待… ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:3:5","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#能容忍拜占庭错误的-pow-概率共识算法"},{"categories":["tech"],"content":" 参考 《Designing Data-Intensive Applications - The Big Ideas Behind Reliable, Scalable, and Maintainable Systems (Martin Kleppmann)》 极客时间《分布式数据库 30 讲》 极客时间《分布式协议与算法实战》 分布式存储系统的一致性是什么？- OceanBase 线性一致性和 Raft - PingCAP 一致性模型笔记 条分缕析分布式：浅析强弱一致性 - 张铁蕾 Why you should pick strong consistency, whenever possible - Google Spanner 拜占庭将军问题与区块链 区块链协议安全系列— —当拜占庭将军犯错时，区块链共识还安全么？（上集） Eventually Consistent - Revisited ","date":"2022-08-07","objectID":"/posts/consistency-and-consensus-algorithm/:4:0","series":null,"tags":["共识","一致性","分布式","数据库","区块链"],"title":"分布式数据库的一致性问题与共识算法","uri":"/posts/consistency-and-consensus-algorithm/#参考"},{"categories":["tech"],"content":"我在之前的文章写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 中，介绍了如何使用 openssl 生成与管理各种用途的数字证书，也简单介绍了如何通过 certbot 等工具与 ACME 证书申请与管理协议，进行数字证书的申请与自动更新（autorenew）。 这篇文章要介绍的 cert-manager，跟 certbot 这类工具有点类似，区别在于它是工作在 Kubernetes 中的。 cert-manager 是一个证书的自动化管理工具，用于在 Kubernetes 集群中自动化地颁发与管理各种来源、各种用途的数字证书。它将确保证书有效，并在合适的时间自动更新证书。 多的就不说了，证书相关的内容请参见我的写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 或者其他资料，现在直接进入正题。 注：cert-manager 的管理对象是「证书」，如果你仅需要使用非对称加密的公私钥对进行 JWT 签名、数据加解密，可以考虑直接使用secrets 管理工具 Vault. ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:0:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#"},{"categories":["tech"],"content":" 一、部署 https://cert-manager.io/docs/installation/helm/ 官方提供了多种部署方式，使用 helm3 安装的方法如下： shell # 添加 cert-manager 的 helm 仓库 helm repo add jetstack https://charts.jetstack.io helm repo update # 查看版本号 helm search repo jetstack/cert-manager -l | head # 下载并解压 chart，目的是方便 gitops 版本管理 helm pull jetstack/cert-manager --untar --version 1.8.2 helm install \\ cert-manager ./cert-manager \\ --namespace cert-manager \\ --create-namespace \\ # 下面这个参数会导致使用 helm 卸载的时候，会删除所有 CRDs，可能导致所有 CRDs 资源全部丢失！要格外注意 --set installCRDs=true ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:1:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#deploy"},{"categories":["tech"],"content":" 二、创建 Issuercert-manager 支持多种 issuer，你甚至可以通过它的标准 API 创建自己的 Issuer。 但是总的来说不外乎三种： 由权威 CA 机构签名的「公网受信任证书」: 这类证书会被浏览器、小程序等第三方应用/服务商信任 本地签名证书: 即由本地 CA 证书签名的数字证书 自签名证书: 即使用证书的私钥为证书自己签名 下面介绍下如何申请公网证书以及本地签名证书。 ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#create-issuer"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档Entrust’s ACME implementation GlobalSign: 官方文档GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥导出功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面分别演示如何使用 AWS Route53 跟 AliDNS，通过 DNS 验证方式申请一个 Let’s Encrypt 证书。 （其他 DNS 提供商的配置方式请直接看官方文档） 1.1 使用 AWS Route53 创建一个证书签发者「Certificate Issuer」 非 AWS Route53 用户可忽略这一节 https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1.1 通过 IAM 授权 cert-manager 调用 AWS Route53 API 这里介绍一种不需要创建 ACCESS_KEY_ID/ACCESS_SECRET，直接使用 AWS EKS 官方的免密认证的方法。会更复杂一点，但是更安全可维护。 首先需要为 EKS 集群创建 OIDC provider，参见aws-iam-and-kubernetes， 这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy， 可以命名为 \u003cClusterName\u003eCertManagerRoute53Access（注意替换掉 \u003cClusterName\u003e）： json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [\"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\"], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： shell aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： shell export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/\u003cClusterName\u003eCertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: yaml # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: shell helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.1.2 创建一个使用 AWS Route53 进行验证的 ACME Issuer在 xxx 名字空间创建一个 Iusser： yaml apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod-aws namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 # server:","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#create-public-cert"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档Entrust’s ACME implementation GlobalSign: 官方文档GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥导出功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面分别演示如何使用 AWS Route53 跟 AliDNS，通过 DNS 验证方式申请一个 Let’s Encrypt 证书。 （其他 DNS 提供商的配置方式请直接看官方文档） 1.1 使用 AWS Route53 创建一个证书签发者「Certificate Issuer」 非 AWS Route53 用户可忽略这一节 https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1.1 通过 IAM 授权 cert-manager 调用 AWS Route53 API 这里介绍一种不需要创建 ACCESS_KEY_ID/ACCESS_SECRET，直接使用 AWS EKS 官方的免密认证的方法。会更复杂一点，但是更安全可维护。 首先需要为 EKS 集群创建 OIDC provider，参见aws-iam-and-kubernetes， 这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy， 可以命名为 CertManagerRoute53Access（注意替换掉 ）： json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [\"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\"], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： shell aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： shell export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: yaml # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: shell helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.1.2 创建一个使用 AWS Route53 进行验证的 ACME Issuer在 xxx 名字空间创建一个 Iusser： yaml apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod-aws namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 # server:","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#1-1-aws-route53"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档Entrust’s ACME implementation GlobalSign: 官方文档GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥导出功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面分别演示如何使用 AWS Route53 跟 AliDNS，通过 DNS 验证方式申请一个 Let’s Encrypt 证书。 （其他 DNS 提供商的配置方式请直接看官方文档） 1.1 使用 AWS Route53 创建一个证书签发者「Certificate Issuer」 非 AWS Route53 用户可忽略这一节 https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1.1 通过 IAM 授权 cert-manager 调用 AWS Route53 API 这里介绍一种不需要创建 ACCESS_KEY_ID/ACCESS_SECRET，直接使用 AWS EKS 官方的免密认证的方法。会更复杂一点，但是更安全可维护。 首先需要为 EKS 集群创建 OIDC provider，参见aws-iam-and-kubernetes， 这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy， 可以命名为 CertManagerRoute53Access（注意替换掉 ）： json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [\"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\"], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： shell aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： shell export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: yaml # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: shell helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.1.2 创建一个使用 AWS Route53 进行验证的 ACME Issuer在 xxx 名字空间创建一个 Iusser： yaml apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod-aws namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 # server:","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#1-1-1-iam-cert-manager-aws-route53-api"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档Entrust’s ACME implementation GlobalSign: 官方文档GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥导出功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面分别演示如何使用 AWS Route53 跟 AliDNS，通过 DNS 验证方式申请一个 Let’s Encrypt 证书。 （其他 DNS 提供商的配置方式请直接看官方文档） 1.1 使用 AWS Route53 创建一个证书签发者「Certificate Issuer」 非 AWS Route53 用户可忽略这一节 https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1.1 通过 IAM 授权 cert-manager 调用 AWS Route53 API 这里介绍一种不需要创建 ACCESS_KEY_ID/ACCESS_SECRET，直接使用 AWS EKS 官方的免密认证的方法。会更复杂一点，但是更安全可维护。 首先需要为 EKS 集群创建 OIDC provider，参见aws-iam-and-kubernetes， 这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy， 可以命名为 CertManagerRoute53Access（注意替换掉 ）： json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [\"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\"], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： shell aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： shell export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: yaml # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: shell helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.1.2 创建一个使用 AWS Route53 进行验证的 ACME Issuer在 xxx 名字空间创建一个 Iusser： yaml apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod-aws namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 # server:","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#1-1-2-aws-route53-acme-issuer"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档Entrust’s ACME implementation GlobalSign: 官方文档GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥导出功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面分别演示如何使用 AWS Route53 跟 AliDNS，通过 DNS 验证方式申请一个 Let’s Encrypt 证书。 （其他 DNS 提供商的配置方式请直接看官方文档） 1.1 使用 AWS Route53 创建一个证书签发者「Certificate Issuer」 非 AWS Route53 用户可忽略这一节 https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1.1 通过 IAM 授权 cert-manager 调用 AWS Route53 API 这里介绍一种不需要创建 ACCESS_KEY_ID/ACCESS_SECRET，直接使用 AWS EKS 官方的免密认证的方法。会更复杂一点，但是更安全可维护。 首先需要为 EKS 集群创建 OIDC provider，参见aws-iam-and-kubernetes， 这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy， 可以命名为 CertManagerRoute53Access（注意替换掉 ）： json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [\"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\"], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： shell aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： shell export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: yaml # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: shell helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.1.2 创建一个使用 AWS Route53 进行验证的 ACME Issuer在 xxx 名字空间创建一个 Iusser： yaml apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod-aws namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 # server:","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#1-2-alidns-certificate-issuer"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档Entrust’s ACME implementation GlobalSign: 官方文档GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥导出功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面分别演示如何使用 AWS Route53 跟 AliDNS，通过 DNS 验证方式申请一个 Let’s Encrypt 证书。 （其他 DNS 提供商的配置方式请直接看官方文档） 1.1 使用 AWS Route53 创建一个证书签发者「Certificate Issuer」 非 AWS Route53 用户可忽略这一节 https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1.1 通过 IAM 授权 cert-manager 调用 AWS Route53 API 这里介绍一种不需要创建 ACCESS_KEY_ID/ACCESS_SECRET，直接使用 AWS EKS 官方的免密认证的方法。会更复杂一点，但是更安全可维护。 首先需要为 EKS 集群创建 OIDC provider，参见aws-iam-and-kubernetes， 这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy， 可以命名为 CertManagerRoute53Access（注意替换掉 ）： json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [\"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\"], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： shell aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： shell export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: yaml # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: shell helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.1.2 创建一个使用 AWS Route53 进行验证的 ACME Issuer在 xxx 名字空间创建一个 Iusser： yaml apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod-aws namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 # server:","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#111-通过-iam-授权-cert-manager-调用-aws-route53-api"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档Entrust’s ACME implementation GlobalSign: 官方文档GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥导出功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面分别演示如何使用 AWS Route53 跟 AliDNS，通过 DNS 验证方式申请一个 Let’s Encrypt 证书。 （其他 DNS 提供商的配置方式请直接看官方文档） 1.1 使用 AWS Route53 创建一个证书签发者「Certificate Issuer」 非 AWS Route53 用户可忽略这一节 https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1.1 通过 IAM 授权 cert-manager 调用 AWS Route53 API 这里介绍一种不需要创建 ACCESS_KEY_ID/ACCESS_SECRET，直接使用 AWS EKS 官方的免密认证的方法。会更复杂一点，但是更安全可维护。 首先需要为 EKS 集群创建 OIDC provider，参见aws-iam-and-kubernetes， 这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy， 可以命名为 CertManagerRoute53Access（注意替换掉 ）： json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [\"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\"], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： shell aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： shell export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: yaml # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: shell helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.1.2 创建一个使用 AWS Route53 进行验证的 ACME Issuer在 xxx 名字空间创建一个 Iusser： yaml apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod-aws namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 # server:","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#1-1-2-alidns-acme-issuer"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档Entrust’s ACME implementation GlobalSign: 官方文档GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥导出功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面分别演示如何使用 AWS Route53 跟 AliDNS，通过 DNS 验证方式申请一个 Let’s Encrypt 证书。 （其他 DNS 提供商的配置方式请直接看官方文档） 1.1 使用 AWS Route53 创建一个证书签发者「Certificate Issuer」 非 AWS Route53 用户可忽略这一节 https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1.1 通过 IAM 授权 cert-manager 调用 AWS Route53 API 这里介绍一种不需要创建 ACCESS_KEY_ID/ACCESS_SECRET，直接使用 AWS EKS 官方的免密认证的方法。会更复杂一点，但是更安全可维护。 首先需要为 EKS 集群创建 OIDC provider，参见aws-iam-and-kubernetes， 这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy， 可以命名为 CertManagerRoute53Access（注意替换掉 ）： json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [\"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\"], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： shell aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： shell export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: yaml # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: shell helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.1.2 创建一个使用 AWS Route53 进行验证的 ACME Issuer在 xxx 名字空间创建一个 Iusser： yaml apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod-aws namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 # server:","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#1-3-acme-certificate"},{"categories":["tech"],"content":" 1. 通过权威机构创建公网受信证书通过权威机构创建的公网受信证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 cert-manager 支持两种申请公网受信证书的方式： ACME（Automated Certificate Management Environment (ACME) Certificate Authority server）证书自动化申请与管理协议。 venafi-as-a-service: venafi 是一个证书的集中化管理平台，它也提供了 cert-manager 插件，可用于自动化申请 DigiCert/Entrust/GlobalSign/Let’s Encrypt 四种类型的公网受信证书。 这里主要介绍使用 ACMEv2 协议申请公网证书，支持使用此开放协议申请证书的权威机构有： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 貌似也是一个比较有名的 SSL 证书服务 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它提供了一个额外的 Dashboard 查看与管理所有申请的证书，这是比较方便的地方。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档Digicert - Third-party ACME client automation Google Public Authority(Google Trust Services): Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign，OCSP 服务器在国内速度也很快。 详见acme.sh/wiki/Google-Public-CA 此功能目前（2022-08-10）仍处于 beta 状态，需要提表单申请才能获得使用 官方地址：https://pki.goog/ Entrust: 官方文档Entrust’s ACME implementation GlobalSign: 官方文档GlobalSign ACME Service 这里也顺便介绍下收费证书服务对证书的分级，以及该如何选用： Domain Validated（DV）证书 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥导出功能！ Organization Validated (OV) 证书 是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 ACME 支持 HTTP01 跟 DNS01 两种域名验证方式，其中 DNS01 是最简便的方法。 下面分别演示如何使用 AWS Route53 跟 AliDNS，通过 DNS 验证方式申请一个 Let’s Encrypt 证书。 （其他 DNS 提供商的配置方式请直接看官方文档） 1.1 使用 AWS Route53 创建一个证书签发者「Certificate Issuer」 非 AWS Route53 用户可忽略这一节 https://cert-manager.io/docs/configuration/acme/dns01/route53/ 1.1.1 通过 IAM 授权 cert-manager 调用 AWS Route53 API 这里介绍一种不需要创建 ACCESS_KEY_ID/ACCESS_SECRET，直接使用 AWS EKS 官方的免密认证的方法。会更复杂一点，但是更安全可维护。 首先需要为 EKS 集群创建 OIDC provider，参见aws-iam-and-kubernetes， 这里不再赘述。 cert-manager 需要查询与更新 Route53 记录的权限，因此需要使用如下配置创建一个 IAM Policy， 可以命名为 CertManagerRoute53Access（注意替换掉 ）： json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [\"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\"], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } 比如使用 awscli 创建此 policy： shell aws iam create-policy \\ --policy-name XxxCertManagerRoute53Access \\ --policy-document file://cert-manager-route53-access.json 然后通过上述配置创建一个 IAM Role 并自动给 cert-manager 所在的 EKS 集群添加信任关系： shell export CLUSTER_NAME=\"xxx\" export AWS_ACCOUNT_ID=\"112233445566\" # 使用 eksctl 自动创建对应的 role 并添加信任关系 # 需要先安装好 eksctl eksctl create iamserviceaccount \\ --cluster \"${CLUSTER_NAME}\" --name cert-manager --namespace cert-manager \\ --role-name \"${CLUSTER_NAME}-cert-manager-route53-role\" \\ --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/CertManagerRoute53Access\" \\ --role-only \\ --approve 之后需要为 cert-manager 的 ServiceAccount 添加注解来绑定上面刚创建好的 IAM Role，首先创建如下 helm values 文件 cert-manager-values.yaml: yaml # 如果把这个改成 false，也会导致 cert-manager 的所有 CRDs 及相关资源被删除！ installCRDs: true serviceAccount: annotations: # 注意修改这里的 ${AWS_ACCOUNT_ID} 以及 ${CLUSTER_NAME} eks.amazonaws.com/role-arn: \u003e- arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-cert-manager-route53-role securityContext: enabled: true # 根据官方文档，还得修改下这个，允许 cert-manager 读取 ServiceAccount Token，从而获得授权 fsGroup: 1001 然后重新部署 cert-manager: shell helm upgrade -i cert-manager ./cert-manager -n cert-manager -f cert-manager-values.yaml 这样就完成了授权。 1.1.2 创建一个使用 AWS Route53 进行验证的 ACME Issuer在 xxx 名字空间创建一个 Iusser： yaml apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod-aws namespace: xxx spec: acme: # 用于接受域名过期提醒的邮件地址 email: user@example.com # ACME 服务器，比如 let's encrypt、Digicert 等 # let's encrypt 的测试 URL，可用于测试配置正确性 # server:","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#csi-driver"},{"categories":["tech"],"content":" 2. 通过私有 CA 颁发证书Private CA 是一种企业自己生成的 CA 证书，通常企业用它来构建自己的 PKI 基础设施。 在 TLS 协议这个应用场景下，Private CA 颁发的证书仅适合在企业内部使用，必须在客户端安装上这个 CA 证书，才能正常访问由它签名的数字证书加密的 Web API 或者站点。Private CA 签名的数字证书在公网上是不被信任的！ cert-manager 提供的 Private CA 服务有： Vault: 鼎鼎大名了，Vault 是一个密码即服务工具，可以部署在 K8s 集群中，提供许多密码、证书相关的功能。 开源免费 AWS Certificate Manager Private CA: 跟 Vault 的 CA 功能是一致的，区别是它是托管的，由 AWS 负责维护。 每个 Private CA 证书：$400/month 每个签发的证书（仅读取了私钥及证书内容后才会收费）：按梯度一次性收费，0-1000 个以内是 $0.75 每个 其他的自己看文档… 这个因为暂时用不上，所以还没研究，之后有研究再给补上。 TO BE DONE. ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:2:2","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#2-通过私有-ca-颁发证书"},{"categories":["tech"],"content":" 三、cert-manager 与 istio/ingress 等网关集成cert-manager 提供的 Certificate 资源，会将生成好的公私钥存放在 Secret 中，而 Istio/Ingress 都支持这种格式的 Secret，所以使用还是挺简单的。 以 Istio Gateway 为例，直接在 Gateway 资源上指定 Secret 名称即可： yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: example-gateway spec: selector: istio: ingressgateway servers: - port: number: 8080 name: http protocol: HTTP hosts: - product.example.com tls: httpsRedirect: true # sends 301 redirect for http requests - port: number: 8443 name: https protocol: HTTPS tls: mode: SIMPLE # enables HTTPS on this port credentialName: tls-example.com # This should match the Certificate secretName hosts: - product.example.com # This should match a DNS name in the Certificate --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: product spec: hosts: - product.example.com gateways: - example-gateway http: - route: - destination: host: product port: number: 8080 --- apiVersion: v1 kind: Service metadata: labels: app: product name: product namespace: prod spec: ports: - name: grpc port: 9090 protocol: TCP targetPort: 9090 - name: http port: 8080 protocol: TCP targetPort: 8080 selector: app: product sessionAffinity: None type: ClusterIP --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: product spec: host: product # 定义了两个 subset subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2 --- # 其他 deployment 等配置 之后再配合 VirtualService 等资源，就可以将 Istio 跟 cert-manager 结合起来啦。 ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:3:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#cert-manager-and-gateway"},{"categories":["tech"],"content":" 四、将 cert-manager 证书挂载到自定义网关中 注意，千万别使用 subPath 挂载，根据官方文档， 这种方式挂载的 Secret 文件不会自动更新！ 既然证书被存放在 Secret 中，自然可以直接当成数据卷挂载到 Pods 中，示例如下： yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: tls-example.com mountPath: \"/certs/example.com\" readOnly: true volumes: - name: tls-example.com secret: secretName: tls-example.com optional: false # default setting; \"mysecret\" must exist 对于 nginx 而言，可以简单地搞个 sidecar 监控下，有配置变更就 reload 下 nginx，实现证书自动更新。 或者可以考虑直接写个 k8s informer 监控 secret 的变更，有变更就直接 reload 所有 nginx 实例，总之实现的方式有很多种。 ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:4:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#cert-manager-istio-ingress"},{"categories":["tech"],"content":" 五、监控告警证书的过期时间是一个很重要的指标，证书过期了，网站就无法正常访问了。虽然正常情况下 cert-manager 应该能够自动更新证书，但是万一出现了问题，又没有及时发现，那就麻烦了。 因此，建议对证书的过期时间进行监控，当证书的过期时间小于一定阈值时，及时发出告警。 cert-manager 提供了 Prometheus 监控指标，可以直接使用 Prometheus 等工具进行监控告警。 官方文档是这个：https://cert-manager.io/docs/usage/prometheus-metrics/#scraping-metrics 文档中没详细列出所有的指标，可以直接接入到 Prometheus 中，然后通过 Grafana 查看。 比如要设置证书过期时间的告警，可以使用如下 PromQL： promql (certmanager_certificate_expiration_timestamp_seconds - time())/3600/24 \u003c 20 上面这个 PromQL 表示，如果证书的过期时间小于 20 天，就会触发告警。 ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:5:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#monitoring-and-alerting"},{"categories":["tech"],"content":" 六、注意事项服务端 TLS 协议的配置有许多的优化点，有些配置对性能的提升是很明显的，建议自行网上搜索相关资料，这里仅列出部分相关信息。 ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:6:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#attention"},{"categories":["tech"],"content":" OCSP 证书验证协议会大幅拖慢 HTTPS 协议的响应速度 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态， 这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。因为 OCSP 信息会带有 CA 证书的签名及有效期，客户端可以直接通过签名验证 OCSP 信息的真实性与有效性，这样就避免了客户端访问 OCSP 服务器带来的开销。 而另一个方法，就是选用 ocsp 服务器在目标用户区域速度快的 CA 机构签发证书。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: conf $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling，如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布全球，也不存在性能问题。这种情况下开个 OCSP Stapling 反而是浪费流量，因为每次 TLS 握手都得发送一个 OCSP 状态信息。 我测试发现只有 www.digicert.com/www.douban.com 等少数站点启用了 ocsp stapling，www.baidu.com/www.google.com/www.zhihu.com 都未启用 ocsp stapling. ","date":"2022-07-31","objectID":"/posts/kubernetes-cert-management/:6:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["数字证书","证书","TLS","Kubernetes","cert-manager"],"title":"Kubernetes 中的证书管理工具 - cert-manager","uri":"/posts/kubernetes-cert-management/#ocsp-证书验证协议会大幅拖慢-https-协议的响应速度"},{"categories":["life"],"content":" 我并不知道何时才是死期， 却日日问自己， 会不会有幽灵来牵我的手， 引我一路向西？ 可他们看到的光明又在哪里？ 会不会也为我亮起？ 这一切快来吧，我已等不及！ ——《红色地带的沉思》 by 临终患者 Patricia 我最近在看一本书，《在生命的尽头拥抱你-临终关怀医生手记》，它的英文原名即本文的标题。李白在《春夜宴从弟桃花园序》中写「而浮生若梦，为欢几何？」，仿照下文法，本文的标题「Death Is But a Dream」大概可以直译成「临终若梦」，跟此书的中文版名称有些不同的韵味在。 这书讲的是死亡的过程——临终梦境。随着年龄渐长，老一辈们慢慢老去，我们都不可避免地会越来越多地接触到死亡。这世间轮替更迭，爱恨情仇是不变的主题，但是死亡始终是配角，实在是因为每人一生都只有唯一一次机会去真正体味死亡，难有实感。而且死亡往往代表着终结，我们做为生者，当然更向往书写生者的世界。 救人一命胜造七级浮屠，珍爱生命是我们从小到大被教育的思想。但是这样的思想却也造成了许多悲剧，全世界有许多的患者痛苦不堪地活在世上，求死不能，最终在病魔的折磨下凄惨离世，如果我们能谨慎承认「死亡」的价值，从这个角度看也是拯救了许多的患者与家庭。 而关于死亡，在我亲身经历的几次长辈葬礼中，我发现父辈们对死亡大都看得很开，他们说「人总有一死，老人家过世了我们当子女的肯定要送最后一程，但是不需要想太多，魂归天地罢了。」，我佩服这种豁达。 但是说到临终梦境，我是真的没什么了解。我从来没跟长辈交流过生命末期的梦境，脑海中也挖掘不出相关的记忆。送走我爷爷的时候，看着爷爷因为呼吸困难而大口喘气，堂哥跪在我前面，双眼泛红隐含泪光，但我完全没有实感——一切都显得那么不真实。听着爷爷被痰堵塞气管、艰难的呼吸声，我甚至感到害怕，想要逃离。 爷爷过世后，奶奶就是一个人生活了，一个人起居、一个人给菜苗松土，然后在菜地里不小心跌了一跤，就随着爷爷去了。 2023/1/7 补充：跟堂弟印照记忆后发现，这里我的记忆或许已经不对了… 我后来在爷爷奶奶房里一个壁橱上，找到三四枚铜钱，还沾着泥土，有些锈蚀痕迹。我把其中一枚通宝红线串好，贴身带了好几年，心情不好的时候就凝视着这枚铜钱黯然神伤，心情好的时候也要捂着它入睡。 我还喜欢上了戏曲，缠着同学读了她的《中国戏剧史》。大学的时候又喜欢上越剧，吴侬软语。又因为初中时学过点竹笛，喜欢上了传统乐曲，我对一些经典老歌也情有独钟。在很多同学跟同事的眼中，我的音乐品味是很「独特」的，这或许都源自爷爷奶奶的熏陶。实际上我小的时候并不喜欢戏剧，我跟爷爷奶奶去看庙戏的目的，通常都只是为了吃一碗凉粉，或者为了去玩耍、看个热闹。偶尔去爷爷奶奶家玩，也只是觉得他们太孤单了，跟他们随便聊聊天，实际上这么多年，跟爷爷奶奶看过的戏曲，我就没听懂过几句台词。 那是多少年前了呢？只知道是很多年前了，不仔细回忆回忆、掐指算算，都搞不清具体过了多少年月。这么多个日日夜夜里，我幼稚过、热血过，也迷茫过、颓废过，倒也不算庸庸碌碌，我还是知足的，这种心态貌似是被称作现充 emmm 高三时曾经看过一本超级喜欢的励志书，后来我高考完于徽州求学、深圳打工，这么多年一直将它带在身边，书名叫《这一生再也不会有的奇遇》。书的扉页只有一句话：「当明天再也不是无限，你还会像今天一样度过你的人生吗？」 《这一生再也不会有的奇遇》已陪我度过了七八个春夏秋冬 当明天再也不是无限 写到这儿，我又想起我高中时还看过一本书《刺猬的优雅》，它同样陪伴我度过了四年大学岁月，后来又辗转到了深圳，但现在倒是不在身边。书中有几句话我印象深刻，放在这篇文章里也挺应景的： 话又说回来，不能因为有想死的心，往后就要像烂菜帮一样的过日子，甚至应该完全相反。 重要的不是死亡，而是在死亡的那一刻我们在做什么。 我在做什么呢？ 我曾遇到一个人，而且我正准备爱上他。 随意写下这些文字，脑子里各种想法恣意流淌，我打算提前写写我的临终遗言，把一切都准备好。 但在死亡到来之前，我仍要精彩的活！ ","date":"2022-05-24","objectID":"/posts/death-is-but-a-dream/:0:0","series":null,"tags":["临终体验","临终关怀","死亡","临终","梦境","哲学"],"title":"Death Is But a Dream","uri":"/posts/death-is-but-a-dream/#"},{"categories":["life"],"content":" 参考 《在生命的尽头拥抱你-临终关怀医生手记》 Death Is But A Dream (2021) Official trailer I See Dead People: Dreams and Visions of the Dying | Dr. Christopher Kerr | TEDxBuffalo End-of-Life Experiences - Dr. Christopher Kerr 有谁看过《刺猬的优雅》这本书吗？如何？ - 於清樂 有什么人生中值得一读的小说或名著？（最好现代作家的书）？ - 於清樂 ","date":"2022-05-24","objectID":"/posts/death-is-but-a-dream/:1:0","series":null,"tags":["临终体验","临终关怀","死亡","临终","梦境","哲学"],"title":"Death Is But a Dream","uri":"/posts/death-is-but-a-dream/#参考"},{"categories":["tech"],"content":" 个人笔记，不一定正确… 当前文章完成度 - 70% ","date":"2022-05-13","objectID":"/posts/about-nat/:0:0","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#"},{"categories":["tech"],"content":" 前言NAT，即 Network Address Translation，是 IPv4 网络中非常重要的一个功能，用于执行 IP 地址与端口的转换。 IPv4 的设计者没预料到因特网技术的发展会如此之快，在设计时只使用了 32bits 的地址空间，随着因特网的飞速发展，它很快就变得不够用了。后来虽然设计了新的 IPv6 协议，但是它与 IPv4 不兼容，需要新的硬件设备以及各种网络程序支持，无法快速普及。 NAT 就是在 IPv6 普及前，临时解决 IPv4 地址空间不够用而开发的技术，通俗地讲 NAT 就是用来给 IPv4 续命的。它解决 IPv4 地址短缺问题的方法是： 每个家庭、组织、企业，在内部都使用局域网通讯，不占用公网 IPv4 资源 在局域网与上层网络的交界处（路由器），使用 NAT 技术进行 IP/port 转换，使用户能正常访问上层网络 在曾经 IPv4 地址还不是特别短缺的时候，普通家庭的网络架构通常是：「家庭局域网」=\u003e「NAT 网关 （家庭路由器）」=\u003e「因特网」。 但是互联网主要发展于欧美，因此许多欧美的组织与机构在初期被分配了大量的 IPv4 资源，而后入场的中国分配到的 IPv4 地址就不太能匹配上我们的人口。因此相比欧美，中国的 IPv4 地址是非常短缺的，即使使用上述这样的网络架构——也就是给每个家庭（或组织）分配一个 IPv4 地址——都有点捉襟见肘了。于是中国电信等运营商不得不再加一层 NAT，让多个家庭共用同一个 IP 地址，这时网络架构会变成这样：「家庭局域网」=\u003e「家庭 NAT 网关」=\u003e「运营商广域网」=\u003e「运营商 NAT 网关」=\u003e「公共因特网」。由于此架构通过两层 NAT 网关串联了三个不同的 IPv4 网络，它也被形象地称为NAT444 技术，详见电信级NAT - 维基百科。 据 v2ex 上传闻国内运营商甚至开始尝试使用 NAT4444 了… 目的是给用户分配一个假公网 IP 地址… 不过 IPv6 也正在变得越来越流行，看 v2ex 上最近（2022-08）就很多人在聊，一些城市在试点 ipv4 over ipv6 隧道技术，底层完全换成 IPv6 协议加 IPoE 拨号了，带来的问题是没法桥接，详见电信又一新动作：上网业务不再使用 PPPoE 新装宽带无法改桥接 - v2ex。 总的来说，NAT 是一项非常成功的技术，它成功帮 IPv4 续命了几十年，甚至到如今 2022 年，全球网络仍然是 IPv4 的天下。 ","date":"2022-05-13","objectID":"/posts/about-nat/:1:0","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#前言"},{"categories":["tech"],"content":" NAT 如何工作NAT 的工作方式，使用图例描述是这样的： NAT 示例 从外部网络看一个 NAT 网关（一个启用了 NAT 的路由器），它只是拥有一个 IPv4 地址的普通设备， 所有从局域网发送到公网的流量，其 IP 地址都是这个路由器的 WAN IP 地址，在上图中，这个 IP 地址是 138.76.29.7. 本质上，NAT 网关隐藏了家庭网络的细节，从外部网络上看，整个家庭网络就像一台普通的网络设备。 下面我们会学习到，上述这个 NAT 工作方式实际上是 NAPT，它同时使用 L3/L4 的信息进行地址转换工作。 ","date":"2022-05-13","objectID":"/posts/about-nat/:2:0","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#nat-如何工作"},{"categories":["tech"],"content":" NAT 的地址映射方式NAT 的具体实现有许多的变种，不存在统一的规范，但是大体上能分为两种模型：「一对一 NAT」与「一对多 NAT」，下面分别进行介绍。 ","date":"2022-05-13","objectID":"/posts/about-nat/:3:0","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#nat-的地址映射方式"},{"categories":["tech"],"content":" 1. 一对一 NAT一对一 NAT，这种类型的 NAT 在 RFC2663 中被称为 Basic NAT。它在技术上比较简单，只利用网络层的信息，对 IP 地址进行转换。 简单的说，Basic NAT 要求每个内网 IP 都需要绑定一个唯一的公网 IP，才能连通外部网络。 其主要应用场景是，公网用户需要访问到内网主机。 Basic NAT 有三种类型：「静态 NAT」、「动态 NAT」以及「NAT Server」。 现在的很多家庭路由器都自带一个被称为 DMZ 主机的功能，它是「Demilitarized Zone」的缩写，意为隔离区。它允许将某一台内网主机设置为 DMZ 主机（或者叫隔离区主机，仅此主机可供外网访问），所有从外部进来的流量，都会被通过 Basic NAT 修改为对应的内网 IP 地址，然后直接发送到该主机。路由器的这种 DMZ 技术就是「静态 NAT」，因为 DMZ 主机对应的内网 IP 需要手动配置，不会动态变化。 DMZ 主机拓扑结构 而「动态 NAT」则需要一个公网 IP 地址池，每次用户需要访问公网时，动态 NAT 会给它分配一个动态公网 IP 并自动配置相应的 NAT 规则，使用完再回收。 第三种是「NAT Server」，云服务商提供的「公网 IP」就是通过「NAT Server」实现的，在云服务器中使用 ip addr ls 查看你会发现，该主机上实际只配了局域网 IP 地址，但是它却能正常使用公网 IP 通信，原因就是云服务商在「NAT Server」上为这些服务器配置了 IP 转发规则。为一台云服务器绑定一个公网 IP，实际上就是请求「NAT Server」从公网 IP 地址池中取出一个，并配置对应的 NAT 规则到这台云服务器的局域网 IP。 示例如下，其中的 Internet Gateway 实际上就是个一对一 NAT Server： AWS VPC 中的 NAT 网关以及 Internet 网关 云服务 VPC 中的公有子网，实际上就是一个 DMZ(Demilitarized Zone) 隔离区，是不安全的。而私有子网则是安全区，公网无法直接访问到其中的主机。 而「动态 NAT」则需要路由器维护一个公网 IP 地址池，内网服务器需要访问公网时，动态 NAT 就从地址池中拿出一个公网 IP 给它使用，用完再回收。这种场景需要一个公网 IP 地址池，每当内部有服务需要请求外网时，就动态地为它分配一个公网 IP 地址，使用完再回收。 Basic NAT 的好处是，它仅工作在 L3 网络层，网络层上的协议都可以正常使用（比如 P2P），不需要啥「内网穿越」技术。 ","date":"2022-05-13","objectID":"/posts/about-nat/:3:1","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#1-一对一-nat"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#2-一对多-nat---napt"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#rfc3489-定义的-nat-类型四种"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#1-full-cone-nat"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#2-address-restricted-cone-nat"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#3-port-restricted-cone-nat"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#4-symmetric-nat"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#5-linux-中的-napt"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#rfc5389-定义的-nat-类型九种"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#1-映射规则"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#2-过滤规则"},{"categories":["tech"],"content":" 2. 一对多 NAT - NAPT一对多 NAT，也被称为 NAPT（network address and port translation），同样在RFC2663 中被定义。Easy IP 是 NAPT 的一个特殊形式。 NAPT 的主要应用场景是，内网用户需要访问到公网主机。绝大多数的家庭网络、办公网络都是 NAPT 类型的。原因应该很好理解——家庭网络或办公网络都包含许多联网设备，但是这类网络通常只有一个或数个公网 IP，使用一对一 NAT 的话公网 IP 显然是不够用的，所以需要使用一对多 NAT. NAPT 通过同时利用 L3 的 IP 信息，以及 L4 传输层的端口信息，来为局域网设备提供透明的、配置方便的、支持超高并发连接的外部网络通信，示意图如下： NAPT 的端口分配与转换规则（Mapping Behavior）以及对外来流量的过滤规则（Filtering Behavior）都存在许多不同的实现，没有统一的规范与标准，但是存在两种分类规范，这种分类方法主要用在 NAT 穿越技术中。 RFC3489 定义的 NAT 类型（四种）在 RFC3489 中将 NAPT 分为四种类型，这也是应用最为广泛的 NAT 分类方法，如下图： 下面我们逐一介绍这四种不同的 NAPT 类型。 从这里开始，下文中的 NAT 特指 NAPT，如果涉及「一对一 NAT」会使用它的全名。 1. Full-cone NATFull-cone NAT 的特点如下： 数据包流出：一旦内部地址（iAddr:iPort）映射到外部地址（eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：任意主机发送到 eAddr:ePort 的数据包，都能通过 NAT 到达 iAddr:iPort. 也就是不对外部进来的数据做任何限制，全部放行。 cone 圆锥，个人理解是一个比喻，任意发送进来的数据（多），都能通过 NAT 到达这个内部地址 （一），就像一个圆锥。 允许任意主机发送到 eAddr:ePort 的数据到达内部地址是很危险的行为，因为内部主机不一定配置了合适的安全策略。因此 Full-cone NAT 比较少见，就算路由器等 NAT 设备支持 Full-cone NAT， 通常也不会是默认选项。我们会在后面更详细地介绍它。 2. (Address-)Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部主机（nAddr:any），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 跟 Full-cone NAT 的区别在于，它限制了外部主机的 IP 地址。只有主动连接过的主机，才能发送数据到 NAT 内部。这提升了一些安全性。 3. Port-Restricted cone NAT 数据包流出：（跟 Full-cone NAT 完全一致）一旦内部地址（iAddr:iPort）映射到外部地址 （eAddr:ePort），所有发自 iAddr:iPort 的数据包都经由 eAddr:ePort 向外发送。 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 与 Address-Restricted cone NAT 的区别在于，它同时限制了外部主机的 IP 与端口，可以说是更进一步地提升了安全性。 4. Symmetric NAT 数据包流出：同一个内部地址（iAddr:iPort）与不同外部主机（nAddr:nPort）的通信，会随机使用不同的 NAT 外部端口（eAddr:randomPort）。也就是说内部地址与 NAT 外部地址的关系也是一对多！ 为每个连接都随机选择一个不同的 NAT 端口，这实际是进一步强化了 NAT 内网的安全性。但这也是 NAT 穿越最大的难点——它导致 Symmetric NAT 的端口难以预测！ 数据包流入：只有内部地址（iAddr:iPort）主动连接过的外部程序（nAddr:nPort），发送到 eAddr:ePort 的数据包，才能通过 NAT 到达 iAddr:iPort. 这个数据流入规则，与 Port-Restricted cone NAT 是完全一致的。 对称 NAT 是最安全的一种 NAT 结构，限制最为严格，应该也是应用最广泛的 NAT 结构。但是它导致所有的 TCP 连接都只能由从内部主动发起，外部发起的 TCP 连接请求会直接被 NAT 拒绝，因此它也是 P2P 玩家最头疼的一种 NAT 类型。解决方案是通过 UDP 迂回实现连接的建立，我们会在后面讨论这个问题。 5. Linux 中的 NAPTLinux 的网络栈中，可通过 iptables/netfilter 的 SNAT/MASQUERADE 实现 NAPT 网关，这种方式只能实现一个 Symmetric NAT. 也就是说绝大多数基于 Linux 实现的家庭局域网、Docker 虚拟网络、Kubernetes 虚拟网络、云服务的虚拟网络，都是 Symmetric NAT. 只有一些有 Full-cone NAT 需求的网吧、ISP 的 LSN(Large Scale NAT) 网关等组织，会使用非 Linux 内核的企业级路由器提供 Full-cone NAT 能力，这些设备可能是基于 FPGA 等专用芯片设计的。 想要将 Symmetric NAT 内的主机提供给外部访问，只能通过端口映射、一对一 NAT 等方式实现，后面会详细介绍这些方法。 RFC5389 定义的 NAT 类型（九种）RFC3489 这个早期 RFC 存在一些问题，问题之一就是它对 NAT 归类过于笼统，很多 NAPT 网关都无法很好的匹配上其中某个类别。 于是后来，RFC3489 被废弃并由 RFC5389 来替代，在 RFC5389 中，将 Mapping Behavior（映射规则）和 Filtering Behavior（过滤规则）分开来，定义了 3 种 Mapping Behavior（映射规则）和 3 种 Filtering Behavior（过滤规则），一共有 9 种组合。 1. 映射规则三种映射规则如图所示，假设一个内网主机 HostX 的内网 IP 地址为 X，端口号为 x，经 NAT 映射后的外网 IP 地址为 M，端口号为 m。为方便描述，将内网的 Endpoint 记为 Endpoint(X,x)，映射后外网的 Endpoint 记为 Endpoint(M,m)。内网 Endpoint(X,x) 发往外网 HostD1 的 IP 地址和端口号记为目的 Endpoint(D1,d1)；发往外网 HostD2 的 IP 地址和端口号记为目的Endpoint(D2,d2)。 NAT 映射规则 EIM(Endpoint-Independent Mapping) 外部地址无关映射 对于一个内网 Endpoint(X,x)，其映射的外网 Endpoint(M,m) 是固定的。即从相同的Endpoint(X,x) 发送到任何外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 ADM(Address-Dependent Mapping) 外部地址相关映射：对于一个内网 Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成 Endpoint(M1,m1)；发往目的Endpoint(D2,d2) 的报文，Endpoint(X,x) 被映射成 Endpoint(M2,m2)。只要D1=D2，不管d1 和d2是多少，都有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部 IP 地址和任何外部端口的报文在 NAT 设备上使用相同的映射。 APDM（Address and Port-Dependent Mapping）外部地址和端口相关映射：对于一个内网Endpoint(X,x)，发往目的 Endpoint(D1,d1) 的报文，Endpoint(X,x) 被映射成Endpoint(M1,m1)；发往目的 Endpoint(D2,d2) 的报文， Endpoint(X,x) 被映射成Endpoint(M2,m2)。只有当D1=D2，且d1=d2，才有 Endpoint(M1,m1)=Endpoint(M2,m2)。即从相同的 Endpoint(X,x) 发送到相同外部IP地址和相同外部端口的报文在NAT设备上使用相同的映射。 2. 过滤规则 NAT 过滤规则 EIF（Endpoint-Independent Filtering）外部地址无关过滤：对于一个内网Endpoint(X,x)，只要它曾经向外网发送过数据，外网主机就可以获取到它经 NAT 映射后的外网Endpoint(M,m) 。那么只要是发给 Endpoint(M,m) 的报文，不管来源于 D1 还是 D2，都能被转换并发往内网，其他报文被过滤掉。 ADF（Addr","date":"2022-05-13","objectID":"/posts/about-nat/:3:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#3-rfc3489-与-rfc5389-的-nat-类型定义关系"},{"categories":["tech"],"content":" NAT 的弊端 IP 会话的保持时效变短：NAT 需要维护一个会话列表，如果会话静默时间超过一个阈值，将会被从列表中移除。 为了避免这种情况，就需要定期发送心跳包来维持 NAT 会话。俗称心跳保活 IP 跟踪机制失效：一对多 NAT 使得多个局域网主机共用一个公网 IP，这导致基于公网 IP 进行流量分析的逻辑失去意义。 比如很多站点都加了基于 IP 的访问频率限制，这会造成局域网内多个用户之间的服务抢占与排队。 NAT 的工作机制依赖于修改IP包头的信息，这会妨碍一些安全协议的工作。 因为 NAT 篡改了 IP 地址、传输层端口号和校验和，这会导致 IP 层的认证协议彻底不能工作， 因为认证目的就是要保证这些信息在传输过程中没有变化。 对于一些隧道协议，NAT 的存在也导致了额外的问题，因为隧道协议通常用外层地址标识隧道实体，穿过 NAT 的隧道会有 IP 复用关系，在另一端需要小心处理。 ICMP 是一种网络控制协议，它的工作原理也是在两个主机之间传递差错和控制消息，因为IP的对应关系被重新映射，ICMP 也要进行复用和解复用处理，很多情况下因为 ICMP 报文载荷无法提供足够的信息，解复用会失败。 IP 分片机制是在信息源端或网络路径上，需要发送的 IP 报文尺寸大于路径实际能承载最大尺寸时，IP 协议层会将一个报文分成多个片断发送，然后在接收端重组这些片断恢复原始报文。IP 这样的分片机制会导致传输层的信息只包括在第一个分片中，NAT难以识别后续分片与关联表的对应关系，因此需要特殊处理。 ","date":"2022-05-13","objectID":"/posts/about-nat/:4:0","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#nat-的弊端"},{"categories":["tech"],"content":" NAT 穿越 - NAT Traversal天下苦 NAT 久矣，尤其是对各种 P2P 玩家，如 NAS 玩家、P2P 游戏玩家，以及需要搭建 VPN 虚拟私有网络的网络管理员而言。在常见的联机游戏、BitTorrent 文件共享协议、P2P 聊天等点对点通讯场景中，通讯双方客户端通常都运行在家庭局域网中，也就是说中间隔着两层家庭路由器的 NAT，路由器的默认配置都是安全优先的，存在很多安全限制，直接进行 P2P 通讯大概率会失败。 为了穿越这些 NAT 网关进行 P2P 通讯，就需要借助NAT 穿越技术。 这里讨论的前提是，你的网络只有单层 NAT，如果外部还存在公寓 NAT、ISP 广域网 NAT，那下面介绍的 NAT 提升技术实际上就没啥意义了。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:0","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#nat-穿越---nat-traversal"},{"categories":["tech"],"content":" 1. 「DMZ 主机」或者「定向 DNAT 转发」最简单的方法是 DMZ 主机功能，前面已经介绍过了，DMZ 可以直接给内网服务器绑定路由器的外部 IP，从该 IP 进来的所有流量都会直接被发送给这台内网服务器。被指定的 DMZ 主机，其 NAT 类型将从 NAPT 变成一对一 NAT，而一对一 NAT 对 P2P 通讯而言是透明的，这样就可以愉快地玩耍了。 在 Linux 路由器上实现类似 DMZ 的功能，只需要两行 iptables 命令，这可以称作「定向 DNAT 转发」： shell iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE # 普通的SNAT iptables -t nat -A PREROUTING -i eth0 -j DNAT --to-destination 192.168.1.3 # 将入站流量DNAT转发到内网主机192.168.1.3 这两项技术的缺点是只能将一台主机提供给外网访问，而且将整台主机开放到公网实际上是很危险的， 如果不懂网络很容易被黑客入侵。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:1","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#1-dmz-主机或者定向-dnat-转发"},{"categories":["tech"],"content":" 2. 静态端口转发退一步，可以直接用静态端口转发功能，就是在路由器上手动设置某个端口号的所有 TCP/UDP 流量， 都直接 NAT 转发到到内网的指定地址。也就是往 NAT 的转发表中手动添加内容，示意图： NAPT tables 设置好端口转发后，只要使用的是被设定的端口，NAT 对 P2P 通信而言将完全透明。绝大多数路由器都支持这项功能，NAS 发烧友们想玩 P2P 下载分享，基本都是这么搞的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#2-静态端口转发"},{"categories":["tech"],"content":" 3. UPnP 动态端口转发 最流行的 UPnP 实现是 https://github.com/miniupnp/miniupnp 静态端口转发对用户的技术要求较高，我作为一个网络小白，希望有一个傻瓜式的开关能让我愉快地玩耍 Xbox/PS5 联机游戏，该怎么办呢？你需要的只是在路由器上启用 UPnP(Universal Plug and Play) 协议，启用它后，内网游戏设备就可以通过 UPnP 向路由器动态申请一个端口号供其使用，UPnP 会自动配置对应的端口转发规则。 现在新出的路由器基本都支持 UPnP 功能，它是最简单有效的 NAT 提升方式。 UPnP 解决了「静态端口转发」需要手动配置的问题，在启用了 UPnP 后，对所有支持 UPnP 的内网程序而言，NAT 类型将提升到 Full-cone NAT. ","date":"2022-05-13","objectID":"/posts/about-nat/:5:3","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#3-upnp-动态端口转发"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持， 那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞 （NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大 （具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#4-nat-穿越协议---stunturnice"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持， 那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞 （NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大 （具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#stunturnice-的-nat-类型检测"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持， 那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞 （NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大 （具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#stunturnice-协议如何实现-nat-打洞"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持， 那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞 （NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大 （具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#1-a-与-b-在同一局域网中"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持， 那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞 （NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大 （具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#2-a-与-b-分别在不同的局域网中"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持， 那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞 （NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大 （具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#3-a-与-b-之间隔着三层以上的-nat"},{"categories":["tech"],"content":" 4. NAT 穿越协议 - STUN/TURN/ICE如果很不幸前面提到的「DMZ 主机」/「静态端口转发」/「UPnP」 三项技术，你的路由器都不支持， 那你就只能借助 NAT 穿越协议了。 目前有如下几个 NAT 穿越协议标准： RFC3489 Classic STUN Classic STUN 是一个早期的 STUN 规范，它定义了一整套完整的 NAT 穿越方案，但是因为存在许多问题，已经被废弃。 RFC5389 - Simple Traversal of UDP Through NATs (STUN) RFC5389 所定义的 STUN 协议是对 Classic STUN 的改进，它的定位不再是一个完整的 NAT 穿越解决方案，而是作为其他协议（例如SIP、FTP、DNS）处理 NAT 穿越问题的一个工具。 其可以用于检查网络中NAT设备的存在，并确定两个通信端点被NAT设备分配的IP地址和端口号。然后，通过ICE（Interactive Connectivity Establishment），自动创建一条能够进行NAT穿越的数据通道。 STUN 支持除 Symmetric NAT 之外的另外三种 NAT 类型 RFC5766 - Traversal Using Relays around NAT (TURN) TURN 在 STUN 协议之上添加了一个中继，以确保在无法实现 NAT 穿越的情况下，可以 fallback 到直接使用中继服务器进行通信。 这个中继的原理类似反向代理，单纯负责数据的转发 在美国有一项数据表示在进行 P2P 穿越的时候，穿越成功的概率为 70%，但是在国内这个成功率 50% 可能都到不了。因此就有必要使用 TURN 协议，这样才能保证在穿越失败的情况下，用户仍然能正常通信。 RFC8445 - Interactive Connectivity Establishment (ICE) 一个 NAT 穿越的协商协议，它统一了 STUN 与 TURN 两种协议，会尝试遍历所有可能的连接方案。 总的来说，标准的 NAT 穿越协议优先使用打洞 （NAT Hole Pounching）技术，如果打洞失败，就使用中继服务器技术兜底，确保能成功穿越。 STUN/TURN/ICE 的 NAT 类型检测RFC5389 定义了对 NAT 映射类型以及过滤类型的检测方法。 TBD STUN/TURN/ICE 协议如何实现 NAT 打洞首先 P2P 双方如果只隔着 0-1 层 NAT，那是不需要使用 NAT 打洞技术的，可以直连或者反向连接。 下面就讨论下 P2P 双方隔着 2 层及以上 NAT 的场景下，如何利用 UDP 协议实现 NAT 打洞。 一个完整的 NAT 打洞方案，需要包含如下功能： A 跟 B 需要知道对方的公网 IP 以及监听的端口号 解决方法：需要一个公网中介来介绍双方认识（交换 IP/port） NAT 连通性测试，需要借助公网主机，检测双方中间网络的类型 针对不同的 NAT 类型，存在哪些穿越手段？以何种顺序进行穿越尝试？ NAT 打洞可以使用 UDP/TCP 两种 L4 协议，但是 TCP 面向连接的特性使它在这个场景中限制性更大 （具体限制见参考文章，我有空再补充），因此各种 NAT 穿越协议通常都基于 UDP 实现。 此外，因为 NAT 的具体行为是非标准化的，路由器的防火墙策略也存在很大变动空间，再有就是 RF3489 的这种 NAT 分类方法不够精确，这些因素导致 NAT 穿透能否成功通常都是谈概率。 1. A 与 B 在同一局域网中这是最简单的情况，最佳方案是直接走内网通讯，不经过 NAT. 第二个方案是，这两个同一局域网内的客户端不走内网，仍然通过 NAT 通讯。这种通讯方式被称作「回环 NAT(Loopback NAT)」或者「发夹 NAT(Hairpin NAT)」。对于不支持或未启用「Hairpin NAT」的网关设备而言，这样的通讯尝试将会失败！ 2. A 与 B 分别在不同的局域网中这样实际上 A 与 B 中间就隔了两个 NAT 网关，这是最普遍的一种情况。 STUN/TURN 的 NAT 穿透流程大致如下： 首先，A 跟 B 两个程序启动时，需要把自己的内外网 IP 及端口信息上报到一台中介服务器 S 现在假设 A 想要跟 B 建立一个 P2P 连接，首先他们需要从 S 获得对方的 ID A 将 B 的 ID 发送给中介服务器 S，请求与 B 建立 P2P 连接 中介服务器将 B 的内外网 IP 及端口信息发送给 A，同时将 A 的网络信息发送给 B A 尝试请求 B 的公网地址 B_public_ip:B_public_port 这肯定会失败，但是会在 A 的 NAT 网关上留下记录：A 曾经请求过这个地址，那之后这个地址发到 A 的 NAT 网关的流量就可以进来了。 B 尝试请求 A 的公网地址 A_public_ip:A_public_port 同样这肯定也会失败，但是会在 B 的 NAT 网关上流量记录：B 曾经请求过这个地址，那之后这个地址发到 B 的 NAT 网关的流量就可以进来了 中间的两层 NAT 网关均形成 NAT 穿越记录，穿越完成。 现在 A 尝试请求 B 的公网地址 B_public_ip:B_public_port，由于 B 的 NAT 已有记录，流量顺利通过 NAT 到达程序 B B 发送给 A 的数据也同样，可以顺利到达 A 上述流程中的关键点在于，如何查出内网服务器被 NAT 分配的外部 IP 及端口，只要有了这两个信息，就可以通过 STUN 中介服务器交换这个信息，然后完成连接的建立了。家庭服务器通常都只有一个公网 IP，所以基本可以认为 IP 是固定的，因此最关键的问题就是「如何知道 NAT 为会话分配的端口地址」。 对端口的限制严格程度跟 NAPT 的类型有关，Full-cone 跟 Restricted cone 对端口都没有任何限制，所以上述流程肯定可以成功； TBD 一个穿越 Symmetric NATs 的 STUN 草案：Symmetric NAT Traversal using STUN 在使用 STUN/TURN 进行 NAT 穿越时，支持的的 NAT 类型如下表。行与列分别代表双方的 NAT 类型，✅ 表示支持 UDP 穿越，❌ 表示 TURN 无法进行 UDP 穿越： NAT 类型 Full Cone Restricted Port-Restricted Symmetric Full Cone ✅ ✅ ✅ ✅ Restricted ✅ ✅ ✅ ✅ Port-Restricted ✅ ✅ ✅ ❌ Symmetric ✅ ✅ ❌ ❌ 这种场景下 TURN 协议给出的解决方案是，fallback 到中继服务器策略作为兜底方案，保证连接能成功，但是这会给中继服务器带来很大压力，延迟等参数将不可避免地变差。 3. A 与 B 之间隔着三层以上的 NAT这种情况较为常见的有： ISP 为了节约使用公网 IP，给用户分配了个广域网 IP，中间就多了个广域网 NAT 大城市的各种租房公寓通常只会从 ISP 购买一两根宽带，二次分销给整栋楼的租客共用，这就造成中间多了一层公寓的 NAT 这是最复杂的一种情况，基本上就没什么 NAT 穿透的希望了，只能走下面介绍的兜底策略——服务器中继。 TBD 待续 4. 特殊穿越方案 - 服务器中继Relay 服务器中继是兼容性最佳，但是性能最差的方案，因为这个方案下，所有的 P2P 连接都需要经过中继服务器转发，在使用人数众多时这会给中继服务器造成很大的压力。 因此这个方案通常是用于兜底的。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:4","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#4-特殊穿越方案---服务器中继"},{"categories":["tech"],"content":" 特定协议的自穿越技术在所有方法中最复杂也最可靠的就是自己解决自己的问题。比如 IKE 和 IPsec 技术，在设计时就考虑了到如何穿越 NAT 的问题。因为这个协议是一个自加密的协议并且具有报文防修改的鉴别能力，其他通用方法爱莫能助。因为实际应用的 NAT 网关基本都是 NAPT 方式，所有通过传输层协议承载的报文可以顺利通过 NAT。IKE 和 IPsec 采用的方案就是用 UDP 在报文外面再加一层封装，而内部的报文就不再受到影响。IKE 中还专门增加了 NAT 网关是否存在的检查能力以及绕开 NAT 网关检测 IKE 协议的方法。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:5","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#特定协议的自穿越技术"},{"categories":["tech"],"content":" NAT ALG(Application Level Gateway)NAT ALG 是一种解决应用层协议（例如DNS、FTP）报文穿越 NAT 的技术，已经被 NAT 设备产商广泛采用，是 NAT 设备的必备功能。 TLDR 一句话介绍：NAT ALG 通过识别协议，直接修改报文数据部分（payload）的 IP 地址和端口信息，解决某些应用协议的报文穿越 NAT 问题。NAT ALG 工作在 L3-L7 层。 NAT ALG 的原理是利用带有 ALG 功能的 NAT 设备对特定应用层协议的支持，当设备检测到新的连接请求时，先根据传输层端口信息判断是否为已知应用类型。如果判断为已知应用，则调用该应用协议的 ALG 功能对报文的深层内容进行检查。若发现任何形式表达的 IP 地址和端口信息，NAT 都会将这些信息同步进行转换，并为这个新的连接建立一个附加的转换表项。当报文到达外网侧的目的主机时，应用层协议中携带的信息就是 NAT 设备转换后的IP地址和端口号，这样，就可以解决某些应用协议的报文穿越 NAT 问题。 目前支持NAT ALG功能的协议包括：DNS、FTP、SIP、PPTP和RTSP。NAT ALG 在对这些特定应用层协议进行 NAT 转换时，通过 NAT 的状态信息来改变封装在 IP 报文数据部分的特定数据，最终使应用层协议可以跨越不同范围运行。 ","date":"2022-05-13","objectID":"/posts/about-nat/:5:6","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#nat-algapplication-level-gateway"},{"categories":["tech"],"content":" 使用 Go 实验 NAT 穿透Go 可用的 NAT 穿越库有： coturn: 貌似是最流行的 STUN/TURN/ICE server go-stun: 一个简洁的 stun client 实现，大概适合用于学习？ pion/turn: 一个 STUN/TURN/ICE client/client 实现 pion/ice: 一个 ice 实现 TBD 待完善 ","date":"2022-05-13","objectID":"/posts/about-nat/:6:0","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#使用-go-实验-nat-穿透"},{"categories":["tech"],"content":" 虚拟网络、Overlay 与 Underlay虚拟网络就是在物理网络之上，构建的逻辑网络，也被称作 overlay 网络。比如 AWS VPC、Docker 容器网络、QEMU 的默认网络，都是虚拟网络。 而 underlay 网络，则是指承载 overlay 网络的底层网络。我个人理解，它是一个相对的概念，物理网络一定是 underlay 网络，但是虚拟网络之上如果还承载了其他虚拟网络（套娃），那它相对上层网络而言，也是一个 underlay 网络。 overlay 本质上就是一种隧道技术，将原生态的二层数据帧报文进行封装后通过隧道进行传输。常见的 overlay 网络协议主要是 vxlan 以及新一代的 geneve，它俩都是使用 UDP 包来封装链路层的以太网帧。 vxlan 在 2014 年标准化，而 geneve 在 2020 年底才通过草案阶段，目前尚未形成最终标准。但是目前 linux/cilium 都已经支持了 geneve. geneve 相对 vxlan 最大的变化，是它更灵活——它的 header 长度是可变的。 目前所有 overlay 的跨主机容器网络方案，几乎都是基于 vxlan 实现的（例外：cilium 也支持 geneve）。 vxlan/geneve 的详细介绍，参见Linux 中的虚拟网络接口 - vxlan/geneve 顺带再提一嘴，cilium/calico/kube-ovn 等 overlay 容器网络，都是 SDN 软件定义网络。 ","date":"2022-05-13","objectID":"/posts/about-nat/:7:0","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#虚拟网络overlay-与-underlay"},{"categories":["tech"],"content":" 相关工具有一些专门用于跨网搭建私有虚拟网络的工具，由于家庭网络设备前面通常都有至少一层 NAT（家庭路由器），因此这些工具都重度依赖 NAT 穿越技术。如果 NAT 层数太多无法穿越，它们会 fallback 到代理模式，也就是由一台公网服务器进行流量中继，但是这会对中继服务器造成很大压力，延迟跟带宽通常都会差很多。 如下是两个比较流行的 VPN 搭建工具： zerotier: 在 P2P 网络之上搭建的 SDN overlay 网络，使用自定义协议。 tailscales: 基于 wireguard 协议快速搭建私有虚拟网络 VPN ","date":"2022-05-13","objectID":"/posts/about-nat/:7:1","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#相关工具"},{"categories":["tech"],"content":" VPN 协议主流的 VPN 协议有：PPTP、L2TP、IPSec、OpenVPN、SSTP，以及最新潮的 Wireguard. TBD ","date":"2022-05-13","objectID":"/posts/about-nat/:7:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#vpn-协议"},{"categories":["tech"],"content":" 拓展知识","date":"2022-05-13","objectID":"/posts/about-nat/:8:0","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#拓展知识"},{"categories":["tech"],"content":" Symmetric NAT 允许的最大并发 TCP 连接数是多少？TCP 并发连接数受许多参数的限制，以 Linux 服务器为例，默认参数无法满足需要，通常都会手动修改它的参数，放宽文件描述符限制以及 TCP 连接队列、缓存相关的限制。 单纯从网络协议层面分析，对于一个仅有一个公网 IP 的 Symmetric NAT 网关，它与某个外部站点http://x.x.x.x:xx 要建立连接。考虑到 TCP 连接的定义实际上是一个四元组(srcIP, srcPort, dstIP, dstPort)，其中就只有 NAT 网关自己的 srcPort 是唯一的变量了，而端口号是一个 16bits 数字，取值范围为 0 - 65535。此外低于 1024 的数字是操作系统的保留端口， 因此 NAT 一般只会使用 1024-65535 这个区间的端口号，也就是说这个 NAT 网关最多只能与该站点建立 64512 个连接。 那么对于不同的协议 NAT 是如何处理的呢？NAT 肯定可以通过协议特征区分不同协议的流量，因此不同协议通过 NAT 建立的并发连接不会相互影响。 对于家庭网络而言 64512 个连接已经完全够用了，但是对于数据中心或者云上的 VPC 而言，就不一定够用了。举个例子，在AWS NAT 网关的文档中就有提到，AWS NAT 网关最高支持与每个不同的地址建立 55000 个并发连接。destination 的 IP 地址、端口号、(TCP/UDP/ICMP) 任一个发生改变，都可以再建立其他 55000 个并发连接。如果超过这个限制，就会发生「ErrorPortAllocation」错误。如果在 AWS 上遇到这个错误，那就说明你们的云上网络规划有问题了。 当然除了端口限制外，受限于 NAT 硬件、以太网协议以及其他影响，NAT 网关肯定还有包处理速率、带宽的限制，这个就略过不提了。 ","date":"2022-05-13","objectID":"/posts/about-nat/:8:1","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#symmetric-nat-允许的最大并发-tcp-连接数是多少"},{"categories":["tech"],"content":" AWS VPC 与 NATAWS VPC(virtual private cloud) 是一个逻辑隔离的虚拟私有网络，云服务架构的最佳实践之一就是通过 VPC 搭建云上私有网络，提升网络安全性。 AWS VPC 提供两种网关类型： NAT 网关 支持三种协议：TCP, UDP, ICMP 支持 IPv4 与 IPv6 两种 IP 协议 支持 5 Gbps 带宽，可自动扩展到 45 Gbps 可通过划分子网并在多个子网中添加 NAT 网关的方式，获得超过 45Gbps 的带宽 最高支持与每个不同的地址建立 55000 个并发连接 NAT 网关从属于 VPC 的子网 每个 NAT 网关只能绑定一个 IP 可通过划分子网并在多个子网中添加 NAT 网关的方式获得多个 IP 可达到 100w packets 每秒的处理速度，并能自动扩展到 400w packets 每秒 同样，需要更高的处理速度，请添加更多 NAT 网关 按处理数据量收费 默认路由到 NAT 子网，被称为「私有子网」（或者没默认路由，那就是无法访问公网的私有子网），连接只能由内网程序主动发起。 NAT 网关为流量执行「Symmetric NAT」 IGW 因特网网关 IGW 是一个高度可用的逻辑组件，不会限制 VPC 的总带宽、处理能力。 IGW 实例直接关联 VPC，不从属于任何可用区或子网 IGW 实质上是一个 NAT 设备，为绑定了公网 IP 地址的 ENI/EC2 实例，执行「一对一 NAT」 默认路由到 IGW 的子网，被称为「公有子网」 ","date":"2022-05-13","objectID":"/posts/about-nat/:8:2","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#aws-vpc-与-nat"},{"categories":["tech"],"content":" 参考 What Is Network Address Translation (NAT)? - Huawei Docs What Is STUN? - Huawei Docs NetEngine AR V300R019 配置指南-IP业务 - NAT 穿越 - 华为文档 P2P技术详解(一)：NAT详解——详细原理、P2P简介 P2P技术详解(二)：P2P中的NAT穿越(打洞)方案详解 P2P技术详解(三)：P2P中的NAT穿越(打洞)方案详解(进阶分析篇) Connect to the internet using an internet gateway - AWS VPC Internet Gateway 从DNAT到netfilter内核子系统，浅谈Linux的Full Cone NAT实现 Network address translation - wikipedia WebRTC NAT Traversal Methods: A Case for Embedded TURN WireGuard 教程：使用 DNS-SD 进行 NAT-to-NAT 穿透 - 云原生实验室 ","date":"2022-05-13","objectID":"/posts/about-nat/:9:0","series":["计算机网络相关"],"tags":["NAT","网络","NAT 穿越","内网穿透","虚拟网络","P2P","WebRTC"],"title":"NAT 网关、NAT 穿越以及虚拟网络","uri":"/posts/about-nat/#参考"},{"categories":["tech"],"content":" FinOps 是一种不断发展的云财务管理学科和文化实践，通过帮助工程、财务、技术和业务团队在数据驱动的预算分配上进行协作，使成本预算能够产生最大的业务价值。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:0:0","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#"},{"categories":["tech"],"content":" 云计算成本管控随着越来越多的企业上云，云计算的成本管控也越来越受关注。在讨论 Kubernetes 成本之前，先简单聊下如何管控云计算成本，有一个新名词被用于形容这项工作——FinOps. 传统的数据中心的成本是比较固定的，所有的成本变动通常都伴随着硬件更替。而在云上环境就很不一样了，由于云服务的按量收费特性，以及五花八门的计费规则，开发人员稍有不慎，云成本就可能会出现意料之外的变化。另一方面由于计费的复杂性，业务扩容对成本的影响也变得难以预测。 目前的主流云服务商（AWS/GCP/Alicloud/…）基本都提供基于资源标签的成本查询方法，也支持将成本导出并使用 SQL 进行细致分析。因此其实要做到快速高效的云成本分析与管控，主要就涉及到如下几个点： 契合需求的标签规范: 从公司业务侧需求出发，制定出合理的、多维度的 （Department/Team/Product/…）、有扩展空间的标签规范，这样才能按业务侧需要进行成本分析。 资源标签的准确率: 随着公司业务的发展，标签规范的迭代，标签的准确率总是会上下波动。而标签准确率越高，我们对云计算成本的管控能力就越强。 但是也存在许多特殊的云上资源，云服务商目前并未提供良好的成本分析手段，Kubernetes 集群成本就是其中之一。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:1:0","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#云计算成本管控"},{"categories":["tech"],"content":" Kubernetes 成本分析的难点目前许多企业应该都面临着这样的场景：所有的服务都运行在一或多个 Kubernetes 集群上，其中包含多条业务线、多个产品、多个业务团队的服务，甚至除了业务服务，可能还包含 CICD、数据分析、机器学习等多种其他工作负载。而这些 Kubernetes 集群通常都由一个独立的 SRE 部门管理。 但是 Kubernetes 集群本身并不提供成本拆分的能力，我们只能查到集群的整体成本、每个节点组的成本等这样粗粒度的成本信息，缺乏细粒度的成本分析能力。此外，Kubernetes 集群是一个非常动态的运行环境，其节点的数量、节点规格、Pod 所在的节点/Zone/Region，都可能会随着时间动态变动，这为成本分析带来了更大的困难。 这就导致我们很难回答这些问题：每条业务线、每个产品、每个业务团队、或者每个服务分别花了多少钱？是否存在资源浪费？有何优化手段？ 而 FinOps for Kubernetes，就是通过工程化分析、可视化成本分析等手段，来回答这些成本问题，分析与管控 Kubernetes 的成本。 接下来我会先介绍下云上 Kubernetes 成本分析的思路与手段，最后再介绍如何使用 Kubecost 分析 Kubernetes 集群的成本。 要做好 Kubernetes 成本工作，有如下三个要点： 理解 Kubernetes 成本的构成，搞懂准确分析 Kubernetes 成本有哪些难点 寻找优化 Kubernetes 集群、业务服务的手段 确定 Kubernetes 集群的成本拆分手段，建立能快速高效地分析与管控集群成本的流程 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:2:0","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubernetes-成本分析的难点"},{"categories":["tech"],"content":" Kubernetes 成本的构成以 AWS EKS 为例，它的成本有这些组成部分： AWS EKS 本身有 $0.1 per hour 的固定费用，这个很低 EKS 的所有节点会收对应的 EC2 实例运行费用、EBS 数据卷费用 EKS 中使用的 PV 会带来 EBS 数据卷的费用 跨区流量传输费用 所有节点之间的通讯（主要是服务之间的互相访问），如果跨了可用区，会收跨区流量传输费用 EKS 中的服务访问其他 AWS 服务如 RDS/ElastiCache，如果是跨可用区，会收取跨区流量费用 如果使用了 Istio IngressGateway 或 traefik 等网关层代理 Pod，那这些 Pod 与服务实例之间，有可能会产生跨区流量 NAT 网关费用 EKS 中的容器如果要访问因特网，就需要通过 NAT 网关，产生 NAT 费用 如果 VPC 未配置 endpoints 使访问 AWS 服务（dynamodb/s3 等）时直接走 AWS 内部网络，这些流量会经过 VPC 的 NAT 网关，从而产生 NAT 网关费用 对于托管版 NAT 网关，费用又包含两部分：公网流量费 + NAT 数据处理费用。其中数据处理费用可通过自建 NAT 实例来缩减。 服务如果要对外提供访问，最佳实践是通过 aws-load-balancer-controller 绑定 AWS ALB, 这里会产生 ALB 费用 监控系统成本 Kubernetes 的监控系统是不可或缺的 如果你使用的是 Datadog/NewRelic 等云服务，会造成云服务的成本；如果是自建 Prometheus， 会造成 Prometheus 的运行成本，以及 Pull 指标造成的跨区流量成本 总结下，其实就是三部分成本：计算、存储、网络。其中计算与存储成本是相对固定的，而网络成本就比较动态，跟是否跨区、是否通过 NAT 等诸多因素有关。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:3:0","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubernetes-成本的构成"},{"categories":["tech"],"content":" Kubernetes 资源分配的方式Kubernetes 提供了三种资源分配的方式，即服务质量 QoS，不同的分配方式，成本的计算难度也有区别： Guaranteed resource allocation(保证资源分配): 即将 requests 与 limits 设置为相等，确保预留所有所需资源 最保守的策略，服务性能最可靠，但是成本也最高 这种方式分配的资源，拆分起来是最方便的，因为它的计算成本是静态的 Burstable resource allocation(突发性能): 将 requests 设置得比 limits 低，这样相差的这一部分就是服务的可 Burst 资源量。 最佳实践，选择合适的 requests 与 limits，可达成性能与可靠性之间的平衡 这种资源，它 requests 的计算成本是静态的，Burstable 部分的计算成本是动态的 Best effort resource allocation(尽力而为): 只设置 limits，不设置 requests，让 Pod 可以调度到任何可调度的节点上 下策，这个选项会导致服务的性能无法保证，通常只在开发测试等资源受限的环境使用 这种方式分配的资源，完全依赖监控指标进行成本拆分 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:4:0","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubernetes-资源分配的方式"},{"categories":["tech"],"content":" 最佳实践要做到统一分析、拆分 Kubernetes 与其他云资源的成本，如下是一些最佳实践： 按产品或者业务线来划分名字空间，不允许跨名字空间互相访问。 如果存在多个产品或业务线共用的服务，可以在每个产品的名字空间分别部署一个副本，并把它们当成不同的服务来处理。 这样名字空间就是成本划分的一个维度，我们还可以在名字空间上为每个产品设置资源上限与预警。 按产品或业务线来划分节点组，通过节点组的标签来进行成本划分 这是第二个维度，但是节点组划分得太细，可能会导致资源利用不够充分。 这个方案仅供参考，不一定好用 为 Kubernetes 服务设计与其他云资源一致的成本标签，添加到 Pod 的 label 中，通过 kubecost 等手段，基于 label 进行更细致的成本分析 标签一致的好处是可以统一分析 Kubernetes 与其他云资源的成本 定期（比如每周一） check 云成本变化，定位并解决成本异常 建立自动化的成本异常检测与告警机制（部分云服务有提供类似的服务，也可自建），收到告警即触发成本异常分析任务 始终将资源标签准确率维持在较高数值，准确率低于一定数值即自动告警，触发标签修正任务 将成本上升的压力与成本下降的效益覆盖到开发人员，授权他们跟踪服务的 Kubernetes 利用率与成本，以激励开发人员与 SRE 合作管控服务成本。 成本优化实践： 多种工作负载混合部署，提升资源利用率。但是需要合理规划避免资源竞争 调节集群伸缩组件，在保障 SLA 的前提下提升资源利用率 比如 aws 就可以考虑在一些场景下用 karpenter 来做扩缩容、引入AWS Node Termination Handler 提升 Spot 实例的 SLA 尽量使用竞价实例，AWS 的竞价实例单价平均优惠超过 50% 合理地购买 Saving Plans 与 Reserved Instances，达成成本节约。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:5:0","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#最佳实践"},{"categories":["tech"],"content":" 多云环境上述讨论的绝大部分策略，都适用于多云环境。在这种涉及多个云服务提供商的场景，最重要的一点是：搭建平台无关的成本分析与管控平台。而其核心仍然是文章最前面提到的两点，只需要补充两个字 一致： 一致的资源标签规范: 从公司业务侧需求出发，制定出跨平台一致的标签规范，这样才能统一分析多云成本。 资源标签的准确率: 随着公司业务的发展，标签规范的迭代，标签的准确率总是会上下波动。而标签准确率越高，我们对云计算成本的管控能力就越强。 这样就可以把不同云服务商的数据转换成统一的格式，然后在自有的成本平台上进行统一的分析了。 搭建一个这样的成本分析平台其实并不难，许多大公司都是这么干的，小公司也可以从一个最小的平台开始做起，再慢慢完善功能。 以我现有的经验看，其实主要就包含这么几个部分： 成本数据转换模块：将来自不同云的成本数据，转换成与云服务无关的格式，方便统一处理 折扣模块：处理不同资源的折扣 比如 CDN 在用量高的时候通常会有很高的折扣比例 还有 SavingPlans/CommitmentDiscounts 也需要特殊的处理 标签修整模块 随着标签体系的发展，总会有些标签的变更，不方便直接在资源上执行，就需要在成本计算这里进行修正、增补或者删除 成本拆分模块 有些资源的成本是共用的，就需要结合其他来源的数据进行成本拆分，比如 Kubernetes 集群的成本 成本报表：将最终的数据制作成符合各类人员需求的可视化图表，按需求还可以考虑添加交互式特征 可使用 Grafana/Google DataStudio 等报表工具 此外这样一个跨云的成本管控平台也不一定需要完全自己来做，已经有很多公司看到了这块的前景，做出了现成的方案，可以看看 Gartner 的如下报告： Cloud Management Tooling Reviews and Ratings - Gartner 多云场景下其实要考虑的还有很多，目前多云网络（multicloud networking）、多云财务 （multicloud finops）、多云应用管理（multicloud application management）领域的需求越来越强劲，相关产品也越来越多，有需要可以自行研究。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:6:0","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#多云环境"},{"categories":["tech"],"content":" Kubernetes 成本分析前面讨论的内容都很「虚」，下面来点更「务实」的：Kubernetes 成本分析实战。 目前据我所知，主要有如下两个相关的开源工具： Kubecost/Opencost: kubecost 应该是目前最优秀的开源成本分析工具了，self-hosted 是免费的，支持按 deployment/service/label 等多个维度进行成本拆分，而且支持拆分网络成本。收费版提供更丰富的功能以及更长的数据存储时间。 kubecost 的核心部分已捐献给 CNCF，并改名为 opencost. crane: 腾讯开源的一款 Kubernetes 成本优化工具，支持成本报表以及 EHPA 两个功能，（截止 2022-05-04）才刚开源几个月，目前还比较简陋。 腾讯推出国内首个云原生成本优化开源项目 Crane 腾讯云在国内上线了 crane 的闭源版本「容器服务成本大师」，如果你使用的是腾讯云，可以体验看看（感觉跟 kubecost 很像） 其中 kubecost 是最成熟的一个，我们接下来以 kubecost 为例介绍下如何分析 Kubernetes 成本。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:0","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubernetes-成本分析"},{"categories":["tech"],"content":" 安装 kubecostkubecost 有两种推荐的安装方法： 使用 helm 安装免费版 包含如下组件： frontend 前端 UI 面板 cost-model 核心组件，提供基础的成本拆分能力 postgres 长期存储，仅企业版支持 kubecost-network-costs 一个 daemonset，提供网络指标用于计算网络成本（貌似未开源） cluster-controller 提供集群「大小调整（RightSizing）」以及「定时关闭集群」的能力 只保留 15 天的指标，无 SSO/SAML 登录支持，无 alerts/notification, 不可保存 reporters 报表 每个 kubecost 只可管理一个集群 只安装 Apache License 开源的 cost-model，它仅提供基础的成本拆分功能以及 API，无 UI 面板、长期存储、网络成本拆分、SAML 接入及其他商业功能。 开源的 cost-model 直接使用此配置文件即可部署：https://github.com/kubecost/cost-model/blob/master/kubernetes/exporter/exporter.yaml 而如果要部署带 UI 的商业版，需要首先访问https://www.kubecost.com/install#show-instructions 获取到 kubecostToken，然后使用 helm 进行部署。 首先下载并编辑 values.yaml 配置文件：https://github.com/kubecost/cost-analyzer-helm-chart/blob/develop/cost-analyzer/values.yaml， 示例如下： yaml # kubecost-values.yaml # 通过 http://kubecost.com/install 获取 token，用于跟踪商业授权状态 kubecostToken: \"xxx\" global: # 自动部署 prometheus + nodeExporter，也可以直接对接外部 prometheus prometheus: enabled: true # 如果 enable=false，则使用如下地址连接外部 prometheus fqdn: http://cost-analyzer-prometheus-server.default.svc # 自动部署 grafana，也可对接外部 grafana 面板 grafana: enabled: true # 如果 enable=false，则使用如下地址连接外部 grafana domainName: cost-analyzer-grafana.default.svc scheme: \"http\" # http or https, for the domain name above. proxy: true # If true, the kubecost frontend will route to your grafana through its service endpoint # grafana 子 chart 的配置 ## 更好的选择是单独部署 grafana，不使用 kubecost 的 subchart grafana: image: repository: grafana/grafana # 建议替换成私有镜像仓库地址 tag: 8.3.2 # prometheus 子 chart 的配置 ## 更好的选择是单独部署 prometheus，不使用 kubecost 的 subchart prometheus: server: persistentVolume: enabled: true size: 32Gi # 这个大小得视情况调整，集群较大的话 32Gi 肯定不够 retention: 15d # p8s 指标保留时长 nodeExporter: enabled: true ## If true, node-exporter pods share the host network namespace hostNetwork: true ## If true, node-exporter pods share the host PID namespace hostPID: true ## node-exporter container name name: node-exporter ## node-exporter container image image: repository: quay.io/prometheus/node-exporter # 替换成 quay 仓库避免 docker 仓库拉取限制 tag: v0.18.1 pullPolicy: IfNotPresent ## Monitors ConfigMap changes and POSTs to a URL ## Ref: https://github.com/jimmidyson/configmap-reload ## configmapReload: prometheus: ## If false, the configmap-reload container will not be deployed enabled: true ## configmap-reload container name name: configmap-reload ## configmap-reload container image image: repository: jimmidyson/configmap-reload # 建议替换成私有仓库避免 docker 仓库拉取限制 tag: v0.7.1 persistentVolume: enabled: true size: 32Gi # 同前所述 # storageClass: \"-\" # # 配置 ingress 入口，供外部访问 ingress: enabled: false # className: nginx annotations: # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \"true\" paths: [\"/\"] # There's no need to route specifically to the pods-- we have an nginx deployed that handles routing pathType: ImplementationSpecific hosts: - cost-analyzer.local nodeSelector: {} # 提升网络安全性的配置 networkPolicy: enabled: false denyEgress: true # create a network policy that denies egress from kubecost sameNamespace: true # Set to true if cost analyser and prometheus are on the same namespace # namespace: kubecost # Namespace where prometheus is installed # 分析网络成本，需要额外部署一个 daemonset networkCosts: enabled: false config: {} # 详见 values.yaml 内容 serviceAccount: create: true annotations: # 如果是 aws 上的集群，可以通过 serviceAccount 授权访问 ec2 pricing API 及 cur 数据 # 也可以直接为服务提供 AccessKeyID/Secret 进行授权 # 与 AWS 的集成会在后面详细介绍 eks.amazonaws.com/role-arn: arn:aws:iam:112233445566:role/KubecostRole # 注意替换这个 role-arn # 如下配置也可通过 Kubecost product UI 调整 # 但是此处的配置优先级更高，如果在这里配置了默认值，容器重启后就会使用此默认值，UI 上的修改将失效 kubecostProductConfigs: {} 然后部署： shell # 添加 repo helm repo add kubecost https://kubecost.github.io/cost-analyzer/ # 查看版本号 helm search repo kubecost/cost-analyzer -l | head # 下载并解压某个 chart helm pull kubecost/cost-analyzer --untar --version 1.92.0 # 使用自定义 values 配置安装或更新本地的 chart helm upgrade --create-namespace --install kubecost ./cost-analyzer -n kubecost -f kubecost-values.yaml 通过 port-forward 访问： shell kubectl po","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:1","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#安装-kubecost"},{"categories":["tech"],"content":" kubecost 的成本统计原理 1. CPU/RAM/GPU/Storage 成本分析Kubecost 通过 AWS/GCP 等云服务商 API 动态获取各 region/zone 的上述四项资源的每小时成本：CPU-hour, GPU-hour, Storage Gb-hour 与 RAM Gb-hour，或者通过 json 文件静态配置这几项资源的成本。OD 按需实例的资源价格通常比较固定，而 AWS Spot 实例的成本波动会比较大，可以通过 SpotCPU/SpotRAM 这两个参数来设置 spot 的默认价格，也可以为 kubecost 提供权限使它动态获取这两项资源的价格。 kubecost 根据每个容器的资源请求 requests 以及资源用量监控进行成本分配，对于未配置 requests 的资源将仅按实际用量监控进行成本分配。 kubecost 的成本统计粒度为 container，而 deployment/service/namespace/label 只是按不同的维度进行成本聚合而已。 2. 网络成本的分析 https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/network-allocation.md 对提供线上服务的云上 Kubernetes 集群而言，网络成本很可能等于甚至超过计算成本。这里面最贵的，是跨区/跨域传输的流量成本，以及 NAT 网关成本。NAT 网关成本可以通过自建 NAT 实例来部分缩减（这里仅考察了 AWS 云服务，其他云服务商的收费模式可能存在区别）。使用单个可用区风险比较高，资源池也可能不够用，因此我们通常会使用多个可用区，这就导致跨区流量成本激增。 kubecost 也支持使用 Pod network 监控指标对整个集群的流量成本进行拆分，kubecost 会部署一个绑定 hostNetwork 的 daemonset 来采集需要的网络指标，提供给 prometheus 拉取，再进行进一步的分析。 kubecost 将网络流量分成如下几类： in-zone: 免费流量 in-region: 跨区流量，国外的云服务商基本都会对跨区流量收费 cross-region: 跨域流量 更多的待研究，看 kubecost 官方文档吧。 另外还看到 kubecost 有忽略 s3 流量（因为不收费）的 issue:https://github.com/kubecost/cost-model/issues/517 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:2","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubecost-的成本统计原理"},{"categories":["tech"],"content":" kubecost 的成本统计原理 1. CPU/RAM/GPU/Storage 成本分析Kubecost 通过 AWS/GCP 等云服务商 API 动态获取各 region/zone 的上述四项资源的每小时成本：CPU-hour, GPU-hour, Storage Gb-hour 与 RAM Gb-hour，或者通过 json 文件静态配置这几项资源的成本。OD 按需实例的资源价格通常比较固定，而 AWS Spot 实例的成本波动会比较大，可以通过 SpotCPU/SpotRAM 这两个参数来设置 spot 的默认价格，也可以为 kubecost 提供权限使它动态获取这两项资源的价格。 kubecost 根据每个容器的资源请求 requests 以及资源用量监控进行成本分配，对于未配置 requests 的资源将仅按实际用量监控进行成本分配。 kubecost 的成本统计粒度为 container，而 deployment/service/namespace/label 只是按不同的维度进行成本聚合而已。 2. 网络成本的分析 https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/network-allocation.md 对提供线上服务的云上 Kubernetes 集群而言，网络成本很可能等于甚至超过计算成本。这里面最贵的，是跨区/跨域传输的流量成本，以及 NAT 网关成本。NAT 网关成本可以通过自建 NAT 实例来部分缩减（这里仅考察了 AWS 云服务，其他云服务商的收费模式可能存在区别）。使用单个可用区风险比较高，资源池也可能不够用，因此我们通常会使用多个可用区，这就导致跨区流量成本激增。 kubecost 也支持使用 Pod network 监控指标对整个集群的流量成本进行拆分，kubecost 会部署一个绑定 hostNetwork 的 daemonset 来采集需要的网络指标，提供给 prometheus 拉取，再进行进一步的分析。 kubecost 将网络流量分成如下几类： in-zone: 免费流量 in-region: 跨区流量，国外的云服务商基本都会对跨区流量收费 cross-region: 跨域流量 更多的待研究，看 kubecost 官方文档吧。 另外还看到 kubecost 有忽略 s3 流量（因为不收费）的 issue:https://github.com/kubecost/cost-model/issues/517 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:2","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#1-cpuramgpustorage-成本分析"},{"categories":["tech"],"content":" kubecost 的成本统计原理 1. CPU/RAM/GPU/Storage 成本分析Kubecost 通过 AWS/GCP 等云服务商 API 动态获取各 region/zone 的上述四项资源的每小时成本：CPU-hour, GPU-hour, Storage Gb-hour 与 RAM Gb-hour，或者通过 json 文件静态配置这几项资源的成本。OD 按需实例的资源价格通常比较固定，而 AWS Spot 实例的成本波动会比较大，可以通过 SpotCPU/SpotRAM 这两个参数来设置 spot 的默认价格，也可以为 kubecost 提供权限使它动态获取这两项资源的价格。 kubecost 根据每个容器的资源请求 requests 以及资源用量监控进行成本分配，对于未配置 requests 的资源将仅按实际用量监控进行成本分配。 kubecost 的成本统计粒度为 container，而 deployment/service/namespace/label 只是按不同的维度进行成本聚合而已。 2. 网络成本的分析 https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/network-allocation.md 对提供线上服务的云上 Kubernetes 集群而言，网络成本很可能等于甚至超过计算成本。这里面最贵的，是跨区/跨域传输的流量成本，以及 NAT 网关成本。NAT 网关成本可以通过自建 NAT 实例来部分缩减（这里仅考察了 AWS 云服务，其他云服务商的收费模式可能存在区别）。使用单个可用区风险比较高，资源池也可能不够用，因此我们通常会使用多个可用区，这就导致跨区流量成本激增。 kubecost 也支持使用 Pod network 监控指标对整个集群的流量成本进行拆分，kubecost 会部署一个绑定 hostNetwork 的 daemonset 来采集需要的网络指标，提供给 prometheus 拉取，再进行进一步的分析。 kubecost 将网络流量分成如下几类： in-zone: 免费流量 in-region: 跨区流量，国外的云服务商基本都会对跨区流量收费 cross-region: 跨域流量 更多的待研究，看 kubecost 官方文档吧。 另外还看到 kubecost 有忽略 s3 流量（因为不收费）的 issue:https://github.com/kubecost/cost-model/issues/517 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:2","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#2-网络成本的分析"},{"categories":["tech"],"content":" kubecost API https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/apis.md 成本拆分文档：https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/cost-allocation.md 成本拆分 API 文档：https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/allocation.md 查询成本拆分结果的 API 示例： python import requests resp = requests.get(\"http://localhost:9090/model/allocation\", params={ \"window\": \"2022-05-05T00:00:00Z,2022-05-06T00:00:00Z\", \"aggregate\": \"namespace,label:app\", # 以这几个纬度进行成本聚合 \"external\": True, # 拆分集群外部的成本（比如 s3/rds/es 等），需要通过其他手段提供外部资源的成本 \"accumulate\": True, # 累加指定 window 的所有成本 \"shareIdle\": False, # 将空闲成本拆分到所有资源上 \"idleByNode\": False, # 基于节点进行空闲资源的统计 \"shareTenancyCosts\": True, # 在集群的多个租户之间共享集群管理成本、节点数据卷成本。这部分成本将被添加到 `sharedCost` 字段中 \"shareNamespaces\": \"kube-system,kubecost,istio-system,monitoring\", # 将这些名字空间的成本设为共享成本 \"shareLabels\": \"\", \"shareCost\": None, \"shareSplit\": \"weighted\", # 共享成本的拆分方法，weight 加权拆分，even 均分 }) resp_json = resp.json() print(resp_json['code']) result = resp_json['data'] print(result[0]) 查询结果中有这几种特殊成本类别： __idle__: 未被占用的空闲资源消耗的成本 __unallocated_: 不含有 aggregate 对应维度的成本，比如按 label:app 进行聚合，不含有app 这个 label 的 pod 成本就会被分类到此标签 __unmounted__: 未挂载 PV 的成本 此外如果使用 kubecost 可视化面板，可能还会看到一个 other 类别，这是为了方便可视化，把成本太低的一些指标聚合展示了。 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:3","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubecost-api"},{"categories":["tech"],"content":" Kubecost 与 AWS 集成 https://github.com/kubecost/docs/blob/b7e9d25994ce3df6b3936a06023588f2249554e5/aws-cloud-integrations.md https://github.com/kubecost/docs/blob/main/aws-node-price-reconcilitation-methodology.md TBD ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:7:4","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#kubecost-与-aws-集成"},{"categories":["tech"],"content":" 参考 kubecost: kubecost 应该是目前最优秀的开源成本分析工具了，self-hosted 是免费的，也提供收费的云上版本，值得研究。 文档：https://github.com/kubecost/docs crane: 腾讯开源的一款 Kubernetes 成本优化工具，支持成本报表以及 EHPA 两个功能，才刚开源几个月，目前还比较简陋。 Calculating Container Costs - FinOps CPU利用率从10%提升至60%：中型企业云原生成本优化实战指南 - 星汉未来(Galaxy-Future) 资源利用率分析和优化建议 - 腾讯云容器服务 ","date":"2022-05-04","objectID":"/posts/finops-for-kubernetes/:8:0","series":["云原生相关"],"tags":["云原生","Kubernetes","FinOps","成本分析","Kubecost","MultiCloud","多云","多云财务管控"],"title":"FinOps for Kubernetes - 如何拆分 Kubernetes 成本","uri":"/posts/finops-for-kubernetes/#参考"},{"categories":["life","tech"],"content":"最近在学区块链技术，跟群友讨论时，一位群友抛出了他的观点：「所以智能合约，本质上依然是一个特殊的协议吧，只是套上了一个看起来高大上的词语而本质依然属于一种通信协议的东西，这么一想如果拆解开来实际上也应该没什么特别的。」 是啊，这样说的话，区块链技术确实挺简单的，没啥新的东西。底层就是各种现代密码学算法跟通讯协议而已，这些都是经过了几十年发展，已经很成熟的技术或者概念了。但是中本聪把这些旧技术组合到一起，搞了个比特币，没几年就引发了加密货币狂潮，就连 GPU 都因为加密货币的发展价格一路狂飙。2015 年以太坊往区块链上加了个功能：可以运行任何图灵完备的计算机程序（合约），编译成 EVM 字节码即可丢到以太坊区块链上运行。运行程序这样一件事本身有什么特殊的么？是台计算机都可以跑程序，但是以太坊第一个提出在区块链上跑图灵完备的程序，这导致以太坊成为了目前世界上第二大区块链系统，并且形成了一个庞大的开发者社区，目前网络上有多不胜数的以太坊开发教程及资料。 区块链技术仅仅只是以新的方式，组合使用了一系列成熟的工具而已，但是却引发了世界性的金融变革，甚至以太坊还在这之上研究出了更多的妙用，提出了 Web3.0 的概念。 现有技术的新用法，也完全可以形成一场革命性的变革，甚至这个新用法可以非常简单，关键在于你能否发现这样一种用法，并且意识到它可能存在的价值。这个是非常非常难的，微软曾经没看懂开源跟云计算的威力，诺基亚曾经觉得安卓就是垃圾不足为惧，很多搞金融的曾经觉得加密货币就是个笑话。恰如很多现在丢了工作的传统运维，可能以前也是觉得容器跟云计算没啥特别的，影响不到自己吧。 在没被新技术骑脸之前，一般人是很难感知到它对自己的影响的。 所以作为一名普通的技术人，我们也只能时时关注自己核心领域内各项新技术的发展，评估它们的潜力与价值，尽力看清它们的本质。甚至偶尔也要拓展自己的视野去了解下其他关联领域的变化，这样才能降低自己被时代抛弃的概率。 每个新兴领域都不缺乏时代的弄潮儿，而我们普通人，只能未雨绸缪，尽量打造好自己的技术小船，不致被时代的大潮倾翻。 最后回到正题，我这个周末才刚学了两天区块链，还没搞明白智能合约是个啥，更遑论看清「智能合约的潜在价值」了，但是既然区块链是目前的一个风口，这么多人鼓吹，我觉得是值得花点时间搞清楚它导致是个啥的。区块链、Web3.0 DAO NFT DeFi 这样的新概念，单单着眼于别人的鼓吹或者贬低，只会是雾里看花一知半解。只有自己搞懂它，才有机会把握住它的本质。 路途漫漫，诸君共勉。 ","date":"2022-03-28","objectID":"/posts/revolution-and-innovation/:0:0","series":null,"tags":["创新","变革"],"title":"变革与创新","uri":"/posts/revolution-and-innovation/#"},{"categories":["tech"],"content":" 本文大致是一篇原创文章，但是参考了许多网络资料，都列在文末的「参考」中了。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:0:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#"},{"categories":["tech"],"content":" 更新记录 2021-01-17: 完成 TLS 协议简介、数字证书介绍、数字证书的申请或生成方法、mTLS 介绍、TLS 协议的破解手段 2022-03-13 ~ 2022-03-14: 重新整理补充，改写为《写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议》，整合进我的实用密码学系列文章中 补充 PKI 公钥基础架构及 X509 证书标准介绍 TODO: 补充 ALPN 应用层协议协商的介绍 补充 TLS 协议的逆向手段 基于cfssl 详细介绍 PKI 的各项组件 基于 PKI 的应用服务间身份识别技术：SPIFF ID SPIFF ID 是云原生领域的标准，服务网格项目 Istio 就使用了 SPIFF ID 作为安全命名 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:1:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#更新记录"},{"categories":["tech"],"content":" 零、前言现代人的日常生活中，HTTPS 协议几乎无处不在，我们每天浏览网页时、用手机刷京东淘宝时、甚至每天秀自己绿色的健康码时，都在使用 HTTPS 协议。 作为一个开发人员，我想你应该多多少少有了解一点 HTTPS 协议。你可能知道 HTTPS 是一种加密传输协议，能保证数据传输的保密性。如果你拥有部署 HTTPS 服务的经验，那你或许还懂如何申请权威 HTTPS 证书，并配置在 Nginx 等 Web 程序上。 但是你是否清楚 HTTPS 是由 HTTP + TLS 两种协议组合而成的呢？更进一步你是否有抓包了解过 TLS 协议的完整流程？是否清楚它加解密的底层原理？是否清楚 Nginx 等 Web 服务器的 HTTPS 配置中一堆密码学参数的真正含义？是否知道 TLS 协议有哪些弱点、存在哪些攻击手段、如何防范？ 我们在《写给开发人员的实用密码学》的前七篇文章中已经学习了许多的密码学概念与算法，接下来我们就利用这些知识，深度剖析下 HTTPS 协议中的数字证书以及 TLS 协议。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:2:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#preface"},{"categories":["tech"],"content":" 一、数字证书与 PKI 公钥基础架构我们在前面已经学习了「对称密码算法」与「非对称密码算法」两个密码学体系，这里做个简单的总结。 对称密码算法（如 AES/ChaCha20）: 计算速度快、安全强度高，但是缺乏安全交换密钥的手段、密钥的保存和管理也很困难。 非对称密码算法（如 RSA/ECC）: 计算速度慢，但是它解决了上述对称密码算法最大的两个缺陷， 一是给出了安全的密钥交换算法 DHE/ECDHE，二呢它的公钥是可公开的，这降低了密钥的保存与管理难度。 但是非对称密码算法仍然存在一些问题: 公钥该如何分发？比如 Alice 跟 Bob 交换公钥时，如何确定收到的确实是对方的公钥，也就是说如何确认公钥的真实性、完整性、认证其来源身份？ 前面我们已经学习过，DH/ECDH 密钥交换协议可以防范嗅探攻击（窃听），但是无法抵挡中间人攻击（中继）。 如果 Alice 的私钥泄漏了，她该如何作废自己旧的公钥？ 数字证书与公钥基础架构就是为了解决上述问题而设计的。 首先简单介绍下公钥基础架构（Public Key Infrastructure），它是一组由硬件、软件、参与者、管理政策与流程组成的基础架构，其目的在于创造、管理、分配、使用、存储以及撤销数字证书。PKI 是一个总称，而并非指单独的某一个规范或标准，因此显然数字证书的规范（X.509）、存储格式（PKCS 系列标准、DER、PEM）、TLS 协议等都是 PKI 的一部分。 我们下面从公钥证书开始逐步介绍 PKI 中的各种概念及架构。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#pki"},{"categories":["tech"],"content":" 1. 公钥证书前面我们介绍了公钥密码系统存在的一个问题是「在分发公钥时，难以确认公钥的真实性、完整性及其来源身份」。PKI 通过「数字证书」+「证书认证机构」来解决这个问题，下面先简单介绍下「数字证书」。 数字证书指的其实就是公钥证书（也可直接简称为证书）。在现代网络通讯中通行的公钥证书标准名为 X.509 v3, 由RFC5280 定义。X.509 v3 格式被广泛应用在 TLS/SSL 等众多加密通讯协议中，它规定公钥证书应该包含如下内容: 证书 序列号（Serial Number）: 用以识别每一张证书，在作废证书的时候会用到它 版本: 证书的规范版本 公钥（Public Key）: 我们的核心目的就是分发公钥，因此显然要把公钥放在证书里面 公钥指纹: 即公钥的 Hash 值，当前大部分证书都使用 SHA256 计算此指纹 公钥用途（Key Usage + Extended Key Usage）: 记录了此证书可用于哪些用途——数字签名、身份认证等 主体（Subject）: 即姓名、组织、邮箱、地址等证书拥有者的个人信息。 有了这个我们就能确认证书的拥有者了 证书有效期的开始时间、结束时间（Not Before + Not After）: 为了确保安全性，每个证书都会记录一个自身的有效期 证书一旦签发并公开，随着科技的发展、时间的推移，其公钥的安全性会慢慢减弱 因此每个证书都应该包含一个合理的有效期，证书的拥有者应该在有效期截止前更换自身的证书以确保安全性 签发者（Issuer）: 签发此证书的「签发者」信息 其他拓展信息 数字签名（Signature）: 我们还需要对上面整个证书计算一个数字签名，来确保这些数据的真实性、完整性，确保证书未被恶意篡改/伪造 此数字签名由「证书签发者（Issuer）」使用其私钥+证书内容计算得出 数字签名算法（Signature Algorithm）: 证书所使用的签名算法，常用的有 RSA-SHA-256 与ECDSA-SHA-256 每个证书都有唯一的 ID，这样在私钥泄漏的情况下，我们可以通过公钥基础设施的 OCSP（Online Certificate Status Protocol）协议吊销某个证书。 吊销证书的操作还是比较罕见的，毕竟私钥泄漏并不容易遇到，因此这里就略过不提了，有需要的可以自行搜索。 使用 Firefox 查看网站 https://www.google.com 的证书信息如下： Google 证书内容 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#public-key-certificate"},{"categories":["tech"],"content":" 2. 证书信任链前面介绍证书内容时，提到了每个证书都包含「签发者（Issuer）」信息，并且还包含「签发者」使用「证书内容」与「签发者私钥」生成的数字签名。 那么在证书交换时，如何验证证书的真实性、完整性及来源身份呢？根据「数字签名」算法的原理，显然需要使用「签发者公钥」来验证「被签发证书」中的签名。 仍然辛苦 Alice 与 Bob 来演示下这个流程: 假设现在 Alice 生成了自己的公私钥对，她想将公钥发送给远在千里之外的 Bob，以便与 Bob 进行加密通讯 但是如果 Alice 直接发送公钥给 Bob，Bob 并无法验证其来源是 Alice，也无法验证该公钥是否被中间人篡改 PKI 引入了一个可信赖的第三者（Trusted third party，TTP）来解决这个问题。在 Alice 与 Bob 的案例中，就是说还有个第三者 Eve，他使用自己的私钥为自己的公钥证书签了名，生成了一个「自签名证书」，并且已经提前将这个「自签名证书」分发（比如当面交付、预置在操作系统中等手段）给了 Alice 跟 Bob. 现在 Alice 首先使用自己的公钥以及个人信息制作了自己的公钥证书，但是这个证书还缺乏一个 Issuer 属性以及数字签名，我们把它叫做「证书签名请求（Certificate Signing Request, CSR）」 为了实现将证书安全传递给远在千里之外的 Bob，Alice 找到 Eve，将这个 CSR 文件提交给 Eve Eve 验证了 Alice 的身份后，再使用这个 CSR 签发出完整的证书文件（Issuer 就填 Eve，然后 Eve 使用自己的私钥计算出证书的数字签名，这样就得到了最终的证书）交付给 Alice Eve 可是曾经跨越千里之遥，将自己的公钥证书分发给了 Bob，所以在给 Alice 签发证书时，他显然可能会要求 Alice 给付「签名费」。目前许多证书机构就是靠这个赚钱的，当然也有非盈利的证书颁发机构如 Let’s Encrypt. 现在 Alice 再将经 Eve 签名的证书发送给 Bob Bob 收到证书后，看到 Issuer 是 Eve，于是找出以前 Eve 给他的「自签名证书」，然后使用其中的公钥验证收到的证书 如果验证成功，就说明证书的内容是经过 Eve 认证的。如果 Eve 没老糊涂了，那这个证书应该确实就是 Alice 的。 如果验证失败，那说明这是某个攻击者伪造的证书。 在现实世界中，Eve 这个角色被称作「证书认证机构（Certification Authority, CA）」，全世界只有几十家这样的权威机构，它们都通过了各大软件厂商的严格审核，从而将根证书（CA 证书）直接内置于主流操作系统与浏览器中，也就是说早就提前分发给了因特网世界的几乎所有用户。 由于许多操作系统或软件的更新迭代缓慢（2022 年了还有人用 XP 你敢信？），根证书的有效期通常都在十年以上。 但是，如果 CA 机构直接使用自己的私钥处理各种证书签名请求，这将是非常危险的。因为全世界有海量的 HTTPS 网站，也就是说有海量的证书需求，可一共才几十家 CA 机构。频繁的动用私钥会产生私钥泄漏的风险，如果这个私钥泄漏了，那将直接影响海量网站的安全性。 PKI 架构使用「数字证书链（也叫做信任链）」的机制来解决这个问题: CA 机构首先生成自己的根证书与私钥，并使用私钥给根证书签名 因为私钥跟证书本身就是一对，因此根证书也被称作「自签名证书」 CA 根证书被直接交付给各大软硬件厂商，内置在主流的操作系统与浏览器中 然后 CA 机构再使用私钥签发一些所谓的「中间证书」，之后就把私钥雪藏了，非必要不会再拿出来使用。 根证书的私钥通常离线存储在安全地点 中间证书的有效期通常会比根证书短一些 部分中间证书会被作为备份使用，平常不会启用 CA 机构使用这些中间证书的私钥，为用户提交的所有 CSR 请求签名 画个图来表示大概是这么个样子： CA 机构也可能会在经过严格审核后，为其他机构签发中间证书，这样就能赋予其他机构签发证书的权利，而且根证书的安全性不受影响。 如果你访问某个 HTTPS 站点发现浏览器显示小绿锁，那就说明这个证书是由某个权威认证机构签发的，其信息是经过这些机构认证的。 上述这个全球互联网上，由证书认证机构、操作系统与浏览器内置的根证书、TLS 加密认证协议、OCSP 证书吊销协议等等组成的架构，我们可以称它为 Web PKI. Web PKI 通常是可信的，但是并不意味着它们可靠。历史上出现过许多由于安全漏洞 （2011 DigiNotar 攻击）或者政府要求，证书认证机构将假证书颁发给黑客或者政府机构的情况。获得假证书的人将可以随意伪造站点，而所有操作系统或浏览器都认为这些假站点是安全的（显示小绿锁）。 因为证书认证机构的可靠性问题以及一些其他的原因，部分个人、企业或其他机构（比如金融机构）会生成自己的根证书与中间证书，然后自行签发证书，构建出自己的 PKI 认证架构，我们可以将它称作内部 PKI或者私有 PKI。但是这种自己生成的根证书是未内置在操作系统与浏览器中的，为了确保安全性，用户就需要先手动在设备上安装好这个数字证书。自行签发证书的案例有： 微信、支付宝及各种银行客户端中的数字证书与安全性更高的 USB 硬件证书（U 盾），这种涉及海量资金安全甚至国家安全的场景，显然不能直接使用前述普通权威 CA 机构签发的证书。 局域网通信，通常是网络管理员生成一个本地 CA 证书安装到所有局域网设备上，再用它的私钥签发其他证书用于局域网安全通信 典型的例子是各企业的内部通讯网络，比如 Kubernetes 容器集群 现在再拿出前面 https://www.google.com 的证书截图看看，最上方有三个标签页，从左至右依次是「服务器证书」、「中间证书」、「根证书」，可以点进去分别查看这三个证书的各项参数，各位看官可以自行尝试： Google 证书内容 交叉签名按前面的描述，每个权威认证机构都拥有一个正在使用的根证书，使用它签发出几个中间证书后，就会把它离线存储在安全地点，平常仅使用中间证书签发终端实体证书。这样实际上每个权威认证机构的证书都形成一颗证书树，树的顶端就是根证书。 实际上在 PKI 体系中，一些证书链上的中间证书会被使用多个根证书进行签名——我们称这为交叉签名。交叉签名的主要目的是提升证书的兼容性——客户端只要安装有其中任何一个根证书，就能正常验证这个中间证书。从而使中间证书在较老的设备也能顺利通过证书验证。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#a-chain-of-trust"},{"categories":["tech"],"content":" 2. 证书信任链前面介绍证书内容时，提到了每个证书都包含「签发者（Issuer）」信息，并且还包含「签发者」使用「证书内容」与「签发者私钥」生成的数字签名。 那么在证书交换时，如何验证证书的真实性、完整性及来源身份呢？根据「数字签名」算法的原理，显然需要使用「签发者公钥」来验证「被签发证书」中的签名。 仍然辛苦 Alice 与 Bob 来演示下这个流程: 假设现在 Alice 生成了自己的公私钥对，她想将公钥发送给远在千里之外的 Bob，以便与 Bob 进行加密通讯 但是如果 Alice 直接发送公钥给 Bob，Bob 并无法验证其来源是 Alice，也无法验证该公钥是否被中间人篡改 PKI 引入了一个可信赖的第三者（Trusted third party，TTP）来解决这个问题。在 Alice 与 Bob 的案例中，就是说还有个第三者 Eve，他使用自己的私钥为自己的公钥证书签了名，生成了一个「自签名证书」，并且已经提前将这个「自签名证书」分发（比如当面交付、预置在操作系统中等手段）给了 Alice 跟 Bob. 现在 Alice 首先使用自己的公钥以及个人信息制作了自己的公钥证书，但是这个证书还缺乏一个 Issuer 属性以及数字签名，我们把它叫做「证书签名请求（Certificate Signing Request, CSR）」 为了实现将证书安全传递给远在千里之外的 Bob，Alice 找到 Eve，将这个 CSR 文件提交给 Eve Eve 验证了 Alice 的身份后，再使用这个 CSR 签发出完整的证书文件（Issuer 就填 Eve，然后 Eve 使用自己的私钥计算出证书的数字签名，这样就得到了最终的证书）交付给 Alice Eve 可是曾经跨越千里之遥，将自己的公钥证书分发给了 Bob，所以在给 Alice 签发证书时，他显然可能会要求 Alice 给付「签名费」。目前许多证书机构就是靠这个赚钱的，当然也有非盈利的证书颁发机构如 Let’s Encrypt. 现在 Alice 再将经 Eve 签名的证书发送给 Bob Bob 收到证书后，看到 Issuer 是 Eve，于是找出以前 Eve 给他的「自签名证书」，然后使用其中的公钥验证收到的证书 如果验证成功，就说明证书的内容是经过 Eve 认证的。如果 Eve 没老糊涂了，那这个证书应该确实就是 Alice 的。 如果验证失败，那说明这是某个攻击者伪造的证书。 在现实世界中，Eve 这个角色被称作「证书认证机构（Certification Authority, CA）」，全世界只有几十家这样的权威机构，它们都通过了各大软件厂商的严格审核，从而将根证书（CA 证书）直接内置于主流操作系统与浏览器中，也就是说早就提前分发给了因特网世界的几乎所有用户。 由于许多操作系统或软件的更新迭代缓慢（2022 年了还有人用 XP 你敢信？），根证书的有效期通常都在十年以上。 但是，如果 CA 机构直接使用自己的私钥处理各种证书签名请求，这将是非常危险的。因为全世界有海量的 HTTPS 网站，也就是说有海量的证书需求，可一共才几十家 CA 机构。频繁的动用私钥会产生私钥泄漏的风险，如果这个私钥泄漏了，那将直接影响海量网站的安全性。 PKI 架构使用「数字证书链（也叫做信任链）」的机制来解决这个问题: CA 机构首先生成自己的根证书与私钥，并使用私钥给根证书签名 因为私钥跟证书本身就是一对，因此根证书也被称作「自签名证书」 CA 根证书被直接交付给各大软硬件厂商，内置在主流的操作系统与浏览器中 然后 CA 机构再使用私钥签发一些所谓的「中间证书」，之后就把私钥雪藏了，非必要不会再拿出来使用。 根证书的私钥通常离线存储在安全地点 中间证书的有效期通常会比根证书短一些 部分中间证书会被作为备份使用，平常不会启用 CA 机构使用这些中间证书的私钥，为用户提交的所有 CSR 请求签名 画个图来表示大概是这么个样子： CA 机构也可能会在经过严格审核后，为其他机构签发中间证书，这样就能赋予其他机构签发证书的权利，而且根证书的安全性不受影响。 如果你访问某个 HTTPS 站点发现浏览器显示小绿锁，那就说明这个证书是由某个权威认证机构签发的，其信息是经过这些机构认证的。 上述这个全球互联网上，由证书认证机构、操作系统与浏览器内置的根证书、TLS 加密认证协议、OCSP 证书吊销协议等等组成的架构，我们可以称它为 Web PKI. Web PKI 通常是可信的，但是并不意味着它们可靠。历史上出现过许多由于安全漏洞 （2011 DigiNotar 攻击）或者政府要求，证书认证机构将假证书颁发给黑客或者政府机构的情况。获得假证书的人将可以随意伪造站点，而所有操作系统或浏览器都认为这些假站点是安全的（显示小绿锁）。 因为证书认证机构的可靠性问题以及一些其他的原因，部分个人、企业或其他机构（比如金融机构）会生成自己的根证书与中间证书，然后自行签发证书，构建出自己的 PKI 认证架构，我们可以将它称作内部 PKI或者私有 PKI。但是这种自己生成的根证书是未内置在操作系统与浏览器中的，为了确保安全性，用户就需要先手动在设备上安装好这个数字证书。自行签发证书的案例有： 微信、支付宝及各种银行客户端中的数字证书与安全性更高的 USB 硬件证书（U 盾），这种涉及海量资金安全甚至国家安全的场景，显然不能直接使用前述普通权威 CA 机构签发的证书。 局域网通信，通常是网络管理员生成一个本地 CA 证书安装到所有局域网设备上，再用它的私钥签发其他证书用于局域网安全通信 典型的例子是各企业的内部通讯网络，比如 Kubernetes 容器集群 现在再拿出前面 https://www.google.com 的证书截图看看，最上方有三个标签页，从左至右依次是「服务器证书」、「中间证书」、「根证书」，可以点进去分别查看这三个证书的各项参数，各位看官可以自行尝试： Google 证书内容 交叉签名按前面的描述，每个权威认证机构都拥有一个正在使用的根证书，使用它签发出几个中间证书后，就会把它离线存储在安全地点，平常仅使用中间证书签发终端实体证书。这样实际上每个权威认证机构的证书都形成一颗证书树，树的顶端就是根证书。 实际上在 PKI 体系中，一些证书链上的中间证书会被使用多个根证书进行签名——我们称这为交叉签名。交叉签名的主要目的是提升证书的兼容性——客户端只要安装有其中任何一个根证书，就能正常验证这个中间证书。从而使中间证书在较老的设备也能顺利通过证书验证。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#交叉签名"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书链、公私钥对）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼…为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者.p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： text -----BEGIN PRIVATE KEY----- MIGHTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： text -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： text # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: shell # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: shell openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#3-证书的存储格式与编码标准"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书链、公私钥对）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼…为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者.p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： text -----BEGIN PRIVATE KEY----- MIGHTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： text -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： text # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: shell # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: shell openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#编码存储格式-der-与-pem"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书链、公私钥对）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼…为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者.p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： text -----BEGIN PRIVATE KEY----- MIGHTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： text -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： text # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: shell # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: shell openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#pkcs1"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书链、公私钥对）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼…为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者.p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： text -----BEGIN PRIVATE KEY----- MIGHTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： text -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： text # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: shell # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: shell openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#pkcs7--cms"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书链、公私钥对）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼…为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者.p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： text -----BEGIN PRIVATE KEY----- MIGHTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： text -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： text # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: shell # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: shell openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#pkcs8"},{"categories":["tech"],"content":" 3. 证书的存储格式与编码标准 证书的格式这一块，是真的五花八门…沉重的历史包袱… X509 只规定了证书应该包含哪些信息，但是未定义证书该如何存储。为了解决证书的描述与编码存储问题，又出现了如下标准： ASN.1 结构：是一种描述证书格式的方法。 它类似 protobuf 数据描述语言、SQL DDL ASN.1 只规定了该如何描述证书，未定义该如何编码。 将 ASN.1 结构编码存储的格式有 DER：一种二进制编码格式 PEM：DER 是二进制格式，不便于复制粘贴，因此出现了 PEM，它是一个文本编码格式（其实就是把 DER 编码后的数据再 Base64 编码下…） 某些场景下，X.509 信息不够丰富，因此又设计了一些信息更丰富（例如可以包含证书链、公私钥对）的证书封装格式，包括 PKCS #7 和 PKCS #12 仍然用 ASN.1 格式描述 基本都是用 DER 编码 下面详细介绍下这些相关的标准与格式。 编码存储格式 DER 与 PEMDER 是由国际电信联盟（ITU）在 ITU-T X.690标准中定义的一种数据编码规则，用于将 ASN.1 结构的信息编码为二进制数据。直接以 DER 格式存储的证书，大都使用 .cer .crt .der 拓展名，在 Windows 系统比较常见。 而 PEM 格式，即 Privacy-Enhanced Mail，是 openssl 默认使用的证书格式。可用于编码公钥、私钥、公钥证书等多种密码学信息。PEM 其实就是在 DER 的基础上多做一步——使用 Base64 将 DER 编码出的二进制数据再处理一次，编码成字符串再存储。好处是存储、传输要方便很多，可以直接复制粘贴。 一个 2048 位 RSA 公钥的 PEM 文件内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PEM 格式的数据通常以 .pem .key .crt .cer 等拓展名存储，直接 cat 一下是不是字符串，就能确认该文件是否是 PEM 格式了。 因为纯文本格式处理起来很方便，大部分场景下证书、公钥、私钥等信息都会被编码成 PEM 格式再进行存储、传输。 openssl 默认使用的输入输出均 PEM 格式。 PKCS#1PKCS#1 是专用于编码 RSA 公私钥的标准，通常被编码为 PEM 格式存储。openssl 生成的 RSA 密钥对默认使用此格式。 这是一个比较陈旧的格式，openssl 之所以默认使用它，主要是为了兼容性。通常建议使用更安全的 PKCS#8 而不是这个。 一个使用 PKCS#1 标准的 2048 位 RSA 公钥文件，内容如下： pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyl6q6BkEcEUi9V1/Q7il bngnh1YzG1tM4Hd6XCZQ35OzDN4my9eXWtjoL8YvLYqlYTJqhTHpuptgjF/lmlhg WIMKNNcuDAbvmWExRyZateVrjO9OtgkyJCuGhaum0TIUC+dbZ9L9xsdK/fU1L5BB nPRSYMloH8uE1CbK/DhFUiKp36aHZFfqLPicY3c6/N+k2kIJCEWBY0SROqpqy2Iz yCIP54JSoOoGz6pdtWhd5cEeicr9e7f/WixEES6fgavqIHzhSJBVctpMgFPjFZ/x JJhQVf23WKb3YQQ/0Uc8O7OTDXoUfuJP9UgqvKNh4hPfJA+a4nxkDYhTPfrLHfKY YwIDAQAB -----END PUBLIC KEY----- PKCS#7 / CMS 头疼…为什么这么多五花八门的格式… PKCS#7/CMS，是一个多用途的证书描述格式。它包含一个数据填充规则，这个填充规则常被用在需要数据填充的分组加密、数字签名等算法中。 另外据说 PKCS#7 也可以被用来描述证书，并以 DER/PEM 格式保存，后缀通常使用 .p7b 或者.p7c, 这个暂时存疑吧，有需要再研究了。 PKCS#8PKCS#8 是一个专门用于编码私钥的标准，可用于编码 DSA/RSA/ECC 私钥。它通常被编码成 PEM 格式存储。 前面介绍了专门用于编码 RSA 的 PKCS#1 标准比较陈旧，而且曾经出过漏洞。因此通常建议使用更安全的 PKCS#8 来取代 PKCS#1. C# Java 等编程语言通常要求使用此格式的私钥，而 Python 的pyca/cryptography 则支持多种编码格式。 一个非加密 ECC 私钥的 PKCS#8 格式如下： text -----BEGIN PRIVATE KEY----- MIGHTAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQglQanBRiYVPX7F2Rd 4CqyjEN0K4qfHw4tM/yMIh21wamhRANCAARsxaI4jT1b8zbDlFziuLngPcExbYzz ePAHUmgWL/ZCeqlODF/l/XvimkjaWC2huu1OSWB9EKuG+mKFY2Y5k+vF -----END PRIVATE KEY----- 一个加密 PKCS#8 私钥的 PEM 格式私钥如下： text -----BEGIN ENCRYPTED PRIVATE KEY----- Base64 编码内容 -----END ENCRYPTED PRIVATE KEY----- 可使用如下 openssl 命令将 RSA/ECC 私钥转换为 PKCS#8 格式： text # RSA openssl pkcs8 -topk8 -inform PEM -in rsa-private-key.pem -outform PEM -nocrypt -out rsa-private-key-pkcs8.pem # ECC 的转换命令与 RSA 完全一致 openssl pkcs8 -topk8 -inform PEM -in ecc-private-key.pem -outform PEM -nocrypt -out ecc-private-key-pkcs8.pem PKCS#12PKCS#12 是一个归档文件格式，用于实现存储多个私钥及相关的 X.509 证书。 因为保存了私钥，为了安全性它通常是加密的，需要使用 passphrase 解密后才能使用。 PKCS#12 的常用拓展名为 .p12 .pfx. PKCS#12 的主要使用场景是安全地保存、传输私钥及相关的 X.509 证书，比如： 微信/支付宝等支付相关的数字证书，通常使用 PKCS#12 格式存储，使用商户号做加密密码，然后编码为 base64 再提供给用户 安卓的 APK 签名证书通常使用 PKCS#12 格式存储，拓展名为 .keystore 或者 .jks. PEM 格式转 PKCS#12（公钥和私钥都放里面）: shell # openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 # 按提示输入保护密码 从 PKCS#12 中分别提取出 PEM 格式的公钥与私钥: shell openssl pkcs12 -in xxx.p12 -out xxx.crt -clcerts -nokeys openssl pkcs12 -in xxx.p12 -out xxx.key -nocerts -nodes ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#pkcs12"},{"categories":["tech"],"content":" 4. 证书支持保护的域名类型TLS 证书支持配置多个域名，并且支持所谓的通配符（泛）域名。但是通配符域名证书的匹配规则，和 DNS 解析中的匹配规则并不一致！ 根据证书选型和购买 - 阿里云文档 的解释，通配符证书只支持同级匹配，，也就是说 * 不能匹配域名中的 . 这个字符，详细说明如下: *.aliyun.com 能用于 xx.aliyun.com 和 abc.aliyun.com，但不能用于 aliyun.com,a.b.aliyun.com 或 bb.xx.aliyun.com.。 一个证书能支持多个域名配置，比如一个包含了 aliyun.com, *.a.aliyun.com,*.b.aliyun.com 三个域名的证书，可以同时用于 aliyun.com, xx.a.aliyun.com,yy.b.aliyun.com。 要想保护多个二三级子域，只能在生成 TLS 证书时，添加多个通配符域名。因此设计域名规则时，要考虑到这点，尽量不要使用层级太深的域名！有些信息可以通过 - 来拼接以减少域名层级，比如阿里云的 oss 域名: 公网: oss-cn-shenzhen.aliyuncs.com 内网: oss-cn-shenzhen-internal.aliyuncs.com 此外也可直接为 IP 地址签发证书，IP 地址可以记录在证书的 SAN 属性中。在自己生成的证书链中可以为局域网 IP 或局域网域名生成本地签名证书。此外在因特网中也有一些权威认证机构提供为公网 IP 签发证书的服务，一个例子是 Cloudflare 的 https://1.1.1.1, 使用 Firefox 查看其证书，可以看到是一个由 DigiCert 签发的 ECC 证书，使用了 P-256 曲线： Cloudflare 的 IP 证书 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:4","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#4-证书支持保护的域名类型"},{"categories":["tech"],"content":" 5. 生成自己的证书链 OpenSSL 是目前使用最广泛的网络加密算法库，这里以它为例介绍证书的生成。另外也可以考虑使用 CloudFalre 开源的 PKI 工具cfssl. 前面介绍了，在局域网通信中通常使用本地证书链（私有 PKI）来保障通信安全，这通常有如下几个原因。 在内网环境下，管理员将本地 CA 证书安装到所有局域网设备上，因此并无必要向权威 CA 机构申请证书 内网环境使用的可能是非公网域名（xxx.local/xxx.lan/xxx.srv 等），甚至可能直接使用局域网 IP 通信，权威 CA 机构不签发这种类型的证书 本地证书链完全受自己控制，可以自己设置安全强度、证书年限等等，而且不受权威 CA 机构影响。 权威 CA 机构不签发客户端证书，因为客户端不一定有固定的 IP 地址或者域名。客户端证书需要自己签发。 下面介绍下如何使用 OpenSSL 生成一个本地 CA 证书链，并签发用于安全通信的服务端证书，可用于 HTTPS/QUIC 等协议。 1. 生成 RSA 证书链到目前为止 RSA 仍然是应用最广泛的非对称加密方案，几乎所有的根证书都是使用的 2048 位或者 4096 位的 RSA 密钥对。 对于 RSA 算法而言，越长的密钥能提供越高的安全性，当前使用最多的 RSA 密钥长度仍然是 2048 位，但是 2048 位已被一些人认为不够安全了，密码学家更建议使用 3072 位或者 4096 位的密钥。 生成一个 2048 位的 RSA 证书链的流程如下: OpenSSL 的 CSR 配置文件官方文档:https://www.openssl.org/docs/manmaster/man1/openssl-req.html 编写证书签名请求的配置文件 csr.conf: conf [ req ] prompt = no req_extensions = v3_ext distinguished_name = req_distinguished_name [ req_distinguished_name ] countryName = US stateOrProvinceName = NYK localityName = NYK organizationName = Ryan4Yin organizationalUnitName = Ryan4Yin commonName = writefor.fun # deprecated, use subjectAltName(SAN) instead emailAddress = rayn4yin@linux.com [ alt_names ] DNS.1 = writefor.fun DNS.2 = *.writefor.fun [ v3_ext ] subjectAltName=@alt_names basicConstraints = CA:false extendedKeyUsage = serverAuth 此文件的详细文档:OpenSSL file formats and conventions - OpenSSL 3.3 生成证书链与服务端证书: shell # 1. 生成本地 CA 根证书的私钥 openssl genrsa -out ca.key 2048 # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代。但太长了也不合适，安全性比较差。 ## 这里设了 10 年 openssl req -x509 -new -SHA512 -key ca.key -subj \"/CN=Ryan4Yin's Root CA 1\" -days 3650 -out ca.crt # 3. 生成服务端证书的 RSA 私钥（2048 位） openssl genrsa -out server.key 2048 # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -SHA512 -key server.key -out server.csr -config csr.conf # 5. 使用 CA 根证书直接签发服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -SHA512 -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf # 6. 查看生成的证书链 openssl x509 -in ca.crt -noout -text openssl x509 -in server.crt -noout -text 简单起见这里没有生成中间证书，直接使用根证书签发了用于安全通信的服务端证书。 2. 生成 ECC 证书链在上一篇文章中我们已经介绍过了，ECC 加密方案是新一代非对称加密算法，是 RSA 的继任者，在安全性相同的情况下，ECC 拥有比 RSA 更快的计算速度、更少的内存以及更短的密钥长度。 对于 ECC 加密方案而言，不同的椭圆曲线生成的密钥对提供了不同程度的安全性。各个组织（ANSI X9.62、NIST、SECG）命名了多种曲线，可通过如下命名查看 openssl 支持的所有椭圆曲线名称: shell openssl ecparam -list_curves 目前（2022 年）在 TLS 协议以及 JWT 签名算法中，应用最广泛的椭圆曲线仍然是 NIST 系列： P-256: 目前仍然应用最为广泛的椭圆曲线 在 openssl 中对应的名称为 prime256v1 P-384 在 openssl 中对应的名称为 secp384r1 P-521 在 openssl 中对应的名称为 secp521r1 但是我们也看到 X25519(Curve25519) 正在越来越流行，因为美国政府有前科 （NSA 在 RSA 加密算法中安置后门是怎么一回事，有何影响？——知乎），NIST 标准被怀疑可能有后门，目前很多人都在推动使用 Curve25519 等社区方案取代掉 NIST 标准曲线。限于篇幅，感兴趣请自行了解。 生成一个使用 P-384 曲线的 ECC 证书的示例如下: 编写证书签名请求的配置文件 ecc-csr.conf（跟前面 RSA 使用的配置文件没任何区别）: conf [ req ] prompt = no req_extensions = v3_ext distinguished_name = req_distinguished_name [ req_distinguished_name ] countryName = US stateOrProvinceName = NYK localityName = NYK organizationName = Ryan4Yin organizationalUnitName = Ryan4Yin commonName = writefor.fun # deprecated, use subjectAltName(SAN) instead emailAddress = rayn4yin@linux.com [ alt_names ] DNS.1 = writefor.fun DNS.2 = *.writefor.fun [ v3_ext ] subjectAltName=@alt_names basicConstraints = CA:false extendedKeyUsage = serverAuth 此文件的详细文档:OpenSSL file formats and conventions - OpenSSL 3.3 生成证书链与服务端证书: shell # 1. 生成本地 CA 根证书的私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-ca.key # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代，这里设了 100 年 openssl req -x509 -new -SHA512 -key ecc-ca.key -subj \"/CN=Ryan4Yin's Root CA 1\" -days 36500 -out ecc-ca.crt # 3. 生成服务端证书的 EC 私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-server.key # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -SHA512 -key ecc-server.key -out ecc-server.csr -config ecc-csr.conf # 5. 使用 CA 根证书直接签发 ECC 服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -SHA512 -in ecc-server.csr -CA ecc-ca.crt -CAkey ecc-ca.key \\ -CAcreateserial -out ecc-server.crt -days 3650 \\ -extension","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:5","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#5-生成自己的证书链"},{"categories":["tech"],"content":" 5. 生成自己的证书链 OpenSSL 是目前使用最广泛的网络加密算法库，这里以它为例介绍证书的生成。另外也可以考虑使用 CloudFalre 开源的 PKI 工具cfssl. 前面介绍了，在局域网通信中通常使用本地证书链（私有 PKI）来保障通信安全，这通常有如下几个原因。 在内网环境下，管理员将本地 CA 证书安装到所有局域网设备上，因此并无必要向权威 CA 机构申请证书 内网环境使用的可能是非公网域名（xxx.local/xxx.lan/xxx.srv 等），甚至可能直接使用局域网 IP 通信，权威 CA 机构不签发这种类型的证书 本地证书链完全受自己控制，可以自己设置安全强度、证书年限等等，而且不受权威 CA 机构影响。 权威 CA 机构不签发客户端证书，因为客户端不一定有固定的 IP 地址或者域名。客户端证书需要自己签发。 下面介绍下如何使用 OpenSSL 生成一个本地 CA 证书链，并签发用于安全通信的服务端证书，可用于 HTTPS/QUIC 等协议。 1. 生成 RSA 证书链到目前为止 RSA 仍然是应用最广泛的非对称加密方案，几乎所有的根证书都是使用的 2048 位或者 4096 位的 RSA 密钥对。 对于 RSA 算法而言，越长的密钥能提供越高的安全性，当前使用最多的 RSA 密钥长度仍然是 2048 位，但是 2048 位已被一些人认为不够安全了，密码学家更建议使用 3072 位或者 4096 位的密钥。 生成一个 2048 位的 RSA 证书链的流程如下: OpenSSL 的 CSR 配置文件官方文档:https://www.openssl.org/docs/manmaster/man1/openssl-req.html 编写证书签名请求的配置文件 csr.conf: conf [ req ] prompt = no req_extensions = v3_ext distinguished_name = req_distinguished_name [ req_distinguished_name ] countryName = US stateOrProvinceName = NYK localityName = NYK organizationName = Ryan4Yin organizationalUnitName = Ryan4Yin commonName = writefor.fun # deprecated, use subjectAltName(SAN) instead emailAddress = rayn4yin@linux.com [ alt_names ] DNS.1 = writefor.fun DNS.2 = *.writefor.fun [ v3_ext ] subjectAltName=@alt_names basicConstraints = CA:false extendedKeyUsage = serverAuth 此文件的详细文档:OpenSSL file formats and conventions - OpenSSL 3.3 生成证书链与服务端证书: shell # 1. 生成本地 CA 根证书的私钥 openssl genrsa -out ca.key 2048 # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代。但太长了也不合适，安全性比较差。 ## 这里设了 10 年 openssl req -x509 -new -SHA512 -key ca.key -subj \"/CN=Ryan4Yin's Root CA 1\" -days 3650 -out ca.crt # 3. 生成服务端证书的 RSA 私钥（2048 位） openssl genrsa -out server.key 2048 # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -SHA512 -key server.key -out server.csr -config csr.conf # 5. 使用 CA 根证书直接签发服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -SHA512 -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf # 6. 查看生成的证书链 openssl x509 -in ca.crt -noout -text openssl x509 -in server.crt -noout -text 简单起见这里没有生成中间证书，直接使用根证书签发了用于安全通信的服务端证书。 2. 生成 ECC 证书链在上一篇文章中我们已经介绍过了，ECC 加密方案是新一代非对称加密算法，是 RSA 的继任者，在安全性相同的情况下，ECC 拥有比 RSA 更快的计算速度、更少的内存以及更短的密钥长度。 对于 ECC 加密方案而言，不同的椭圆曲线生成的密钥对提供了不同程度的安全性。各个组织（ANSI X9.62、NIST、SECG）命名了多种曲线，可通过如下命名查看 openssl 支持的所有椭圆曲线名称: shell openssl ecparam -list_curves 目前（2022 年）在 TLS 协议以及 JWT 签名算法中，应用最广泛的椭圆曲线仍然是 NIST 系列： P-256: 目前仍然应用最为广泛的椭圆曲线 在 openssl 中对应的名称为 prime256v1 P-384 在 openssl 中对应的名称为 secp384r1 P-521 在 openssl 中对应的名称为 secp521r1 但是我们也看到 X25519(Curve25519) 正在越来越流行，因为美国政府有前科 （NSA 在 RSA 加密算法中安置后门是怎么一回事，有何影响？——知乎），NIST 标准被怀疑可能有后门，目前很多人都在推动使用 Curve25519 等社区方案取代掉 NIST 标准曲线。限于篇幅，感兴趣请自行了解。 生成一个使用 P-384 曲线的 ECC 证书的示例如下: 编写证书签名请求的配置文件 ecc-csr.conf（跟前面 RSA 使用的配置文件没任何区别）: conf [ req ] prompt = no req_extensions = v3_ext distinguished_name = req_distinguished_name [ req_distinguished_name ] countryName = US stateOrProvinceName = NYK localityName = NYK organizationName = Ryan4Yin organizationalUnitName = Ryan4Yin commonName = writefor.fun # deprecated, use subjectAltName(SAN) instead emailAddress = rayn4yin@linux.com [ alt_names ] DNS.1 = writefor.fun DNS.2 = *.writefor.fun [ v3_ext ] subjectAltName=@alt_names basicConstraints = CA:false extendedKeyUsage = serverAuth 此文件的详细文档:OpenSSL file formats and conventions - OpenSSL 3.3 生成证书链与服务端证书: shell # 1. 生成本地 CA 根证书的私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-ca.key # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代，这里设了 100 年 openssl req -x509 -new -SHA512 -key ecc-ca.key -subj \"/CN=Ryan4Yin's Root CA 1\" -days 36500 -out ecc-ca.crt # 3. 生成服务端证书的 EC 私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-server.key # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -SHA512 -key ecc-server.key -out ecc-server.csr -config ecc-csr.conf # 5. 使用 CA 根证书直接签发 ECC 服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -SHA512 -in ecc-server.csr -CA ecc-ca.crt -CAkey ecc-ca.key \\ -CAcreateserial -out ecc-server.crt -days 3650 \\ -extension","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:5","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#1-生成-rsa-证书链"},{"categories":["tech"],"content":" 5. 生成自己的证书链 OpenSSL 是目前使用最广泛的网络加密算法库，这里以它为例介绍证书的生成。另外也可以考虑使用 CloudFalre 开源的 PKI 工具cfssl. 前面介绍了，在局域网通信中通常使用本地证书链（私有 PKI）来保障通信安全，这通常有如下几个原因。 在内网环境下，管理员将本地 CA 证书安装到所有局域网设备上，因此并无必要向权威 CA 机构申请证书 内网环境使用的可能是非公网域名（xxx.local/xxx.lan/xxx.srv 等），甚至可能直接使用局域网 IP 通信，权威 CA 机构不签发这种类型的证书 本地证书链完全受自己控制，可以自己设置安全强度、证书年限等等，而且不受权威 CA 机构影响。 权威 CA 机构不签发客户端证书，因为客户端不一定有固定的 IP 地址或者域名。客户端证书需要自己签发。 下面介绍下如何使用 OpenSSL 生成一个本地 CA 证书链，并签发用于安全通信的服务端证书，可用于 HTTPS/QUIC 等协议。 1. 生成 RSA 证书链到目前为止 RSA 仍然是应用最广泛的非对称加密方案，几乎所有的根证书都是使用的 2048 位或者 4096 位的 RSA 密钥对。 对于 RSA 算法而言，越长的密钥能提供越高的安全性，当前使用最多的 RSA 密钥长度仍然是 2048 位，但是 2048 位已被一些人认为不够安全了，密码学家更建议使用 3072 位或者 4096 位的密钥。 生成一个 2048 位的 RSA 证书链的流程如下: OpenSSL 的 CSR 配置文件官方文档:https://www.openssl.org/docs/manmaster/man1/openssl-req.html 编写证书签名请求的配置文件 csr.conf: conf [ req ] prompt = no req_extensions = v3_ext distinguished_name = req_distinguished_name [ req_distinguished_name ] countryName = US stateOrProvinceName = NYK localityName = NYK organizationName = Ryan4Yin organizationalUnitName = Ryan4Yin commonName = writefor.fun # deprecated, use subjectAltName(SAN) instead emailAddress = rayn4yin@linux.com [ alt_names ] DNS.1 = writefor.fun DNS.2 = *.writefor.fun [ v3_ext ] subjectAltName=@alt_names basicConstraints = CA:false extendedKeyUsage = serverAuth 此文件的详细文档:OpenSSL file formats and conventions - OpenSSL 3.3 生成证书链与服务端证书: shell # 1. 生成本地 CA 根证书的私钥 openssl genrsa -out ca.key 2048 # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代。但太长了也不合适，安全性比较差。 ## 这里设了 10 年 openssl req -x509 -new -SHA512 -key ca.key -subj \"/CN=Ryan4Yin's Root CA 1\" -days 3650 -out ca.crt # 3. 生成服务端证书的 RSA 私钥（2048 位） openssl genrsa -out server.key 2048 # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -SHA512 -key server.key -out server.csr -config csr.conf # 5. 使用 CA 根证书直接签发服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -SHA512 -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf # 6. 查看生成的证书链 openssl x509 -in ca.crt -noout -text openssl x509 -in server.crt -noout -text 简单起见这里没有生成中间证书，直接使用根证书签发了用于安全通信的服务端证书。 2. 生成 ECC 证书链在上一篇文章中我们已经介绍过了，ECC 加密方案是新一代非对称加密算法，是 RSA 的继任者，在安全性相同的情况下，ECC 拥有比 RSA 更快的计算速度、更少的内存以及更短的密钥长度。 对于 ECC 加密方案而言，不同的椭圆曲线生成的密钥对提供了不同程度的安全性。各个组织（ANSI X9.62、NIST、SECG）命名了多种曲线，可通过如下命名查看 openssl 支持的所有椭圆曲线名称: shell openssl ecparam -list_curves 目前（2022 年）在 TLS 协议以及 JWT 签名算法中，应用最广泛的椭圆曲线仍然是 NIST 系列： P-256: 目前仍然应用最为广泛的椭圆曲线 在 openssl 中对应的名称为 prime256v1 P-384 在 openssl 中对应的名称为 secp384r1 P-521 在 openssl 中对应的名称为 secp521r1 但是我们也看到 X25519(Curve25519) 正在越来越流行，因为美国政府有前科 （NSA 在 RSA 加密算法中安置后门是怎么一回事，有何影响？——知乎），NIST 标准被怀疑可能有后门，目前很多人都在推动使用 Curve25519 等社区方案取代掉 NIST 标准曲线。限于篇幅，感兴趣请自行了解。 生成一个使用 P-384 曲线的 ECC 证书的示例如下: 编写证书签名请求的配置文件 ecc-csr.conf（跟前面 RSA 使用的配置文件没任何区别）: conf [ req ] prompt = no req_extensions = v3_ext distinguished_name = req_distinguished_name [ req_distinguished_name ] countryName = US stateOrProvinceName = NYK localityName = NYK organizationName = Ryan4Yin organizationalUnitName = Ryan4Yin commonName = writefor.fun # deprecated, use subjectAltName(SAN) instead emailAddress = rayn4yin@linux.com [ alt_names ] DNS.1 = writefor.fun DNS.2 = *.writefor.fun [ v3_ext ] subjectAltName=@alt_names basicConstraints = CA:false extendedKeyUsage = serverAuth 此文件的详细文档:OpenSSL file formats and conventions - OpenSSL 3.3 生成证书链与服务端证书: shell # 1. 生成本地 CA 根证书的私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-ca.key # 2. 使用私钥签发出 CA 根证书 ## CA 根证书的有效期尽量设长一点，因为不方便更新换代，这里设了 100 年 openssl req -x509 -new -SHA512 -key ecc-ca.key -subj \"/CN=Ryan4Yin's Root CA 1\" -days 36500 -out ecc-ca.crt # 3. 生成服务端证书的 EC 私钥，使用 P-384 曲线，密钥长度 384 位 openssl ecparam -genkey -name secp384r1 -out ecc-server.key # 4. 通过第一步编写的配置文件，生成证书签名请求（公钥+申请者信息） openssl req -new -SHA512 -key ecc-server.key -out ecc-server.csr -config ecc-csr.conf # 5. 使用 CA 根证书直接签发 ECC 服务端证书，这里指定服务端证书的有效期为 3650 天 openssl x509 -req -SHA512 -in ecc-server.csr -CA ecc-ca.crt -CAkey ecc-ca.key \\ -CAcreateserial -out ecc-server.crt -days 3650 \\ -extension","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:5","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#2-生成-ecc-证书链"},{"categories":["tech"],"content":" 6. 证书的类型按照数字证书的生成方式进行分类，证书有三种类型: 公网受信任证书 或者叫 Web PKI 证书: 即由权威证书认证机构签名的的证书。 几乎所有终端都预装了这些权威证书认证机构（CA）的根证书，因此这类证书会被浏览器、小程序等第三方应用/服务商验证并信任。 申请证书时需要验证你对域名/IP 的所有权，也就使证书无法伪造 如果你的 API 需要提供给第三方应用/服务商/用户访问，那就需要向权威 CA 机构申请此类证书 本地签名证书: 即由本地 CA 证书签名的数字证书 本地 CA 证书，就是自己使用 openssl 等工具生成的 CA 证书 这类证书的缺点是无法与第三方应用/服务商建立安全的连接 如果客户端是完全可控的（比如是自家的 APP，或者是接入了域控的企业局域网设备），完全可以在所有客户端都安装上自己生成的 CA 证书。这种场景下使用此类证书是安全可靠的，可以不向权威 CA 机构申请证书 自签名证书: 前面介绍了根证书是一个自签名证书，它使用根证书的私钥为根证书签名 这里的「自签名证书」是指直接使用根证书进行网络通讯，缺点是证书的更新迭代会很麻烦，而且安全性低。 总的来说，权威 CA 机构颁发的「公网受信任证书」，可以被第三方应用信任，但是自己生成的不行。而越贵的权威证书，安全性与可信度就越高，或者可以保护更多的域名。 在客户端可控的情况下，可以考虑自己生成证书链并签发「本地签名证书」，将本地 CA 证书预先安装在客户端中用于验证。 而「自签名证书」主要是生成方便，能不用还是尽量不要使用。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:6","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#6-证书的类型"},{"categories":["tech"],"content":" 7. 向权威 CA 机构申请「公网受信任证书」向权威机构申请的公网受信任证书，可以直接应用在边界网关上，用于给公网用户提供 TLS 加密访问服务，比如各种 HTTPS 站点、API。这是需求最广的一类数字证书服务。 而证书的申请与管理方式又分为两种： 通过ACMEv2（Automated Certificate Management Environment (ACME) 协议进行证书的自动化申请与管理。有许多权威机构支持使用此开放协议申请证书，按是否收费可分为如下两类： 免费服务 Let’s Encrypt: 众所周知，它提供三个月有效期的免费证书。 ZeroSSL: 也是一个比较有名的 SSL 证书服务，它既有免费服务，也有付费服务。 通过 ACME 协议支持不限数量的 90 天证书，也支持多域名证书与泛域名证书。 它相比 Let’s Encrypt 的优势是，它提供一个证书控制台，可以查看与管理用户当前的所有证书，了解其状态。 付费服务 DigiCert: 这个非常有名（但也是相当贵），官方文档Digicert - Third-party ACME client automation Google Trust Services: Google 推出的公网证书服务，也是三个月有效期，其根证书交叉验证了 GlobalSign。官方文档Automate Public Certificates Lifecycle Management via RFC 8555 (ACME) Entrust: 官方文档Entrust’s ACME implementation GlobalSign: 官方文档GlobalSign ACME Service ACME 相关的自动化工具 很多代理工具都有提供基于 ACMEv2 协议的证书申请与自动更新，比如: Traefik Caddy docker-letsencrypt-nginx-proxy-companion 网上也有一些 certbot 插件，可以通过 DNS 提供商的 API 进行 ACMEv2 证书的申请与自动更新，比如: certbot-dns-aliyun terraform 也有相关 provider:terraform-provider-acme cert-manager: kubernetes 中的证书管理工具，支持 ACMEv2，也支持创建与管理私有证书。我写过一篇文章介绍此工具的使用Kubernetes 中的证书管理工具 - cert-manager 通过一些权威 CA 机构或代理商提供的 Web 网站，手动填写信息来申请与更新证书。 这个流程相对会比较繁琐。 这些权威机构提供的证书服务，提供的证书又有不同的分级，这里详细介绍下三种不同的证书级别，以及该如何选用： Domain Validated（DV）证书 应用最广泛的证书类型，我接触最多的就是这种。各云厂商提供的免费 SSL 证书也都是这种类型。 仅验证域名所有权，验证步骤最少，价格最低，仅需要数分钟即可签发。 优点就是易于签发，很适合做自动化。 各云厂商（AWS/GCP/Cloudflare，以及 Vercel/Github 的站点服务）给自家服务提供的免费证书都是 DV 证书，Let’s Encrypt 的证书也是这个类型。 很明显这些证书的签发都非常方便，而且仅验证域名所有权。 但是 AWS/GCP/Cloudflare/Vercel/Github 提供的 DV 证书都仅能在它们的云服务上使用，不提供私钥导出功能！ Organization Validated (OV) 证书 （貌似我也接触地比较少，不做评价） 据说是企业 SSL 证书的首选，通过企业认证确保企业 SSL 证书的真实性。 除域名所有权外，CA 机构还会审核组织及企业的真实性，包括注册状况、联系方式、恶意软件等内容。 如果要做合规化，可能至少也得用 OV 这个级别的证书。 Extended Validation（EV）证书 （我基本没接触过） 最严格的认证方式，CA 机构会深度审核组织及企业各方面的信息。 被认为适合用于大型企业、金融机构等组织或企业。 而且仅支持签发单域名、多域名证书，不支持签发泛域名证书，安全性杠杠的。 完整的证书申请流程如下: 证书申请流程 为了方便用户，图中的申请人（Applicant）自行处理的部分，目前很多证书申请网站也可以自动处理，用户只需要提供相关信息即可。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:7","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#7-向权威-ca-机构申请公网受信任证书"},{"categories":["tech"],"content":" 8. 证书的寿命对于公开服务，服务端证书的有效期不要超过 825 天（27 个月）！另外从 2020 年 11 月起，新申请的服务端证书有效期已经缩短到了 398 天（13 个月）。目前 Apple/Mozilla/Chrome 都发表了相应声明，证书有效期超过上述限制的，将被浏览器/Apple设备禁止使用。 而对于其他用途的证书，如果更换起来很麻烦，可以考虑放宽条件。比如 kubernetes 集群的加密证书，可以考虑有效期设长一些，比如 10 年。 据云原生安全破局｜如何管理周期越来越短的数字证书？所述，大量知名企业如特斯拉/微软/领英/爱立信都曾因未及时更换 TLS 证书导致服务暂时不可用。 因此 TLS 证书最好是设置自动轮转，并确保证书有效期过短时能触发告警！纯人工维护或纯自动化都不可靠！ 目前很多 Web 服务器/代理，都支持自动轮转 Let’s Encrypt 证书。另外 Vault 等安全工具，也支持自动轮转私有证书。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:8","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#8-证书的寿命"},{"categories":["tech"],"content":" 9. 使用 OpenSSL 验证证书、查看证书信息 shell # 查看证书(crt)信息 openssl x509 -noout -text -in server.crt # 查看证书请求(csr)信息 openssl req -noout -text -in server.csr # 查看 RSA 私钥(key)信息 openssl rsa -noout -text -in server.key # 验证证书是否可信 ## 1. 使用系统的证书链进行验证 openssl verify server.crt ## 2. 使用指定的 CA 证书进行验证 openssl verify -CAfile ca.crt server.crt ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:3:9","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#9-使用-openssl-验证证书查看证书信息"},{"categories":["tech"],"content":" 二、TLS 协议TLS 协议，中文名为「传输层安全协议」，是一个安全通信协议，被用于在网络上进行安全通信。 TLS 协议通常与 HTTP / FTP / SMTP 等协议一起使用以实现加密通讯，这种组合协议通常被缩写为 HTTPS / SFTP / SMTPS. 在讲TLS 协议前， 还是先复习下「对称密码算法」与「非对称密码算法」两个密码体系的特点。 对称密码算法（如 AES/ChaCha20）: 计算速度快、安全强度高，但是缺乏安全交换密钥的手段、密钥的保存和管理也很困难 非对称密码算法（如 RSA/ECC）: 解决了上述对称密码算法的两个缺陷——通过数字证书 + PKI 公钥基础架构实现了身份认证，再通过 DHE/ECDHE 实现了安全的对称密钥交换。 但是非对称密码算法要比对称密码算法更复杂，计算速度也慢得多。因此实际使用上通常结合使用这两种密码算法，各取其长，以实现高速且安全的网络通讯。我们通常称结合使用对称密码算法以及非对称密码算法的加密方案为「混合加密方案」。 TLS 协议就是一个「混合加密方案」，它借助数字证书与 PKI 公钥基础架构、DHE/ECDHE 密钥交换协议以及对称加密方案这三者，实现了安全的加密通讯。 基于经典 DHKE 协议的 TLS 握手流程如下： 基于经典 DHKE 协议的 TLS 握手 而在支持「完美前向保密（Perfect Forward Secrecy）」的 TLS1.2 或 TLS1.3 协议中，经典 DH 协议被 ECDHE 协议取代。变化之一是进行最初的握手协议从经典 DHKE 换成了基于 ECC 的 ECDH 协议， 变化之二是在每次通讯过程中也在不断地进行密钥交换，生成新的对称密钥供下次通讯使用，其细节参见写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS。 TLS 协议通过应用 ECDHE 密钥交换协议，提供了「完美前向保密（Perfect Forward Secrecy）」特性，也就是说它能够保护过去进行的通讯不受密钥在未来暴露的威胁。即使攻击者破解出了一个「对称密钥」，也只能获取到一次事务中的数据，其他事务的数据安全性完全不受影响。 另外注意一点是，CA 证书和服务端证书都只在 TLS 协议握手的前三个步骤中有用到，之后的通信就与它们无关了。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#二tls-协议"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 各操作系统（Android/IOS/MacOS/Windows）与浏览器基本都在很早的版本中就已经支持 TLS1.2+ 了， 站在 2022 年这个时间节点看，我们已经可以完全废止 TLS1.1 协议。实际上各大云厂商也是这么干的，比如 AWS 自身的 API 对 TLS1.1 的支持就已确定将在 2023 年 6 月废止，2022 年就开始频繁扫描并提醒各位仍然在使用低版本 TLS 协议的客户升级。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性到底如何。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如ssl-config - mozilla，这个网站提供三个安全等级的配置: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Modern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态， 这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。因为 OCSP 信息会带有 CA 证书的签名及有效期，客户端可以直接通过签名验证 OCSP 信息的真实性与有效性，这样就避免了客户端访问 OCSP 服务器带来的开销。 而另一个方法，就是选用 ocsp 服务器在目标用户区域速度快的 CA 机构签发证书。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: conf $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling，如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#1-密码套件与-tls-历史版本"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 各操作系统（Android/IOS/MacOS/Windows）与浏览器基本都在很早的版本中就已经支持 TLS1.2+ 了， 站在 2022 年这个时间节点看，我们已经可以完全废止 TLS1.1 协议。实际上各大云厂商也是这么干的，比如 AWS 自身的 API 对 TLS1.1 的支持就已确定将在 2023 年 6 月废止，2022 年就开始频繁扫描并提醒各位仍然在使用低版本 TLS 协议的客户升级。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性到底如何。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如ssl-config - mozilla，这个网站提供三个安全等级的配置: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Modern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态， 这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。因为 OCSP 信息会带有 CA 证书的签名及有效期，客户端可以直接通过签名验证 OCSP 信息的真实性与有效性，这样就避免了客户端访问 OCSP 服务器带来的开销。 而另一个方法，就是选用 ocsp 服务器在目标用户区域速度快的 CA 机构签发证书。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: conf $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling，如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#tls-11"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 各操作系统（Android/IOS/MacOS/Windows）与浏览器基本都在很早的版本中就已经支持 TLS1.2+ 了， 站在 2022 年这个时间节点看，我们已经可以完全废止 TLS1.1 协议。实际上各大云厂商也是这么干的，比如 AWS 自身的 API 对 TLS1.1 的支持就已确定将在 2023 年 6 月废止，2022 年就开始频繁扫描并提醒各位仍然在使用低版本 TLS 协议的客户升级。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性到底如何。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如ssl-config - mozilla，这个网站提供三个安全等级的配置: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Modern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态， 这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。因为 OCSP 信息会带有 CA 证书的签名及有效期，客户端可以直接通过签名验证 OCSP 信息的真实性与有效性，这样就避免了客户端访问 OCSP 服务器带来的开销。 而另一个方法，就是选用 ocsp 服务器在目标用户区域速度快的 CA 机构签发证书。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: conf $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling，如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#tls-12"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 各操作系统（Android/IOS/MacOS/Windows）与浏览器基本都在很早的版本中就已经支持 TLS1.2+ 了， 站在 2022 年这个时间节点看，我们已经可以完全废止 TLS1.1 协议。实际上各大云厂商也是这么干的，比如 AWS 自身的 API 对 TLS1.1 的支持就已确定将在 2023 年 6 月废止，2022 年就开始频繁扫描并提醒各位仍然在使用低版本 TLS 协议的客户升级。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性到底如何。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如ssl-config - mozilla，这个网站提供三个安全等级的配置: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Modern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态， 这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。因为 OCSP 信息会带有 CA 证书的签名及有效期，客户端可以直接通过签名验证 OCSP 信息的真实性与有效性，这样就避免了客户端访问 OCSP 服务器带来的开销。 而另一个方法，就是选用 ocsp 服务器在目标用户区域速度快的 CA 机构签发证书。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: conf $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling，如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#tls-13"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 各操作系统（Android/IOS/MacOS/Windows）与浏览器基本都在很早的版本中就已经支持 TLS1.2+ 了， 站在 2022 年这个时间节点看，我们已经可以完全废止 TLS1.1 协议。实际上各大云厂商也是这么干的，比如 AWS 自身的 API 对 TLS1.1 的支持就已确定将在 2023 年 6 月废止，2022 年就开始频繁扫描并提醒各位仍然在使用低版本 TLS 协议的客户升级。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性到底如何。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如ssl-config - mozilla，这个网站提供三个安全等级的配置: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Modern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态， 这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。因为 OCSP 信息会带有 CA 证书的签名及有效期，客户端可以直接通过签名验证 OCSP 信息的真实性与有效性，这样就避免了客户端访问 OCSP 服务器带来的开销。 而另一个方法，就是选用 ocsp 服务器在目标用户区域速度快的 CA 机构签发证书。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: conf $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling，如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#如何设置-tls-协议的版本密码套件参数"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 各操作系统（Android/IOS/MacOS/Windows）与浏览器基本都在很早的版本中就已经支持 TLS1.2+ 了， 站在 2022 年这个时间节点看，我们已经可以完全废止 TLS1.1 协议。实际上各大云厂商也是这么干的，比如 AWS 自身的 API 对 TLS1.1 的支持就已确定将在 2023 年 6 月废止，2022 年就开始频繁扫描并提醒各位仍然在使用低版本 TLS 协议的客户升级。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性到底如何。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如ssl-config - mozilla，这个网站提供三个安全等级的配置: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Modern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态， 这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。因为 OCSP 信息会带有 CA 证书的签名及有效期，客户端可以直接通过签名验证 OCSP 信息的真实性与有效性，这样就避免了客户端访问 OCSP 服务器带来的开销。 而另一个方法，就是选用 ocsp 服务器在目标用户区域速度快的 CA 机构签发证书。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: conf $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling，如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#nginx-的-tls-协议配置"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 各操作系统（Android/IOS/MacOS/Windows）与浏览器基本都在很早的版本中就已经支持 TLS1.2+ 了， 站在 2022 年这个时间节点看，我们已经可以完全废止 TLS1.1 协议。实际上各大云厂商也是这么干的，比如 AWS 自身的 API 对 TLS1.1 的支持就已确定将在 2023 年 6 月废止，2022 年就开始频繁扫描并提醒各位仍然在使用低版本 TLS 协议的客户升级。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性到底如何。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如ssl-config - mozilla，这个网站提供三个安全等级的配置: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Modern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态， 这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。因为 OCSP 信息会带有 CA 证书的签名及有效期，客户端可以直接通过签名验证 OCSP 信息的真实性与有效性，这样就避免了客户端访问 OCSP 服务器带来的开销。 而另一个方法，就是选用 ocsp 服务器在目标用户区域速度快的 CA 机构签发证书。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: conf $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling，如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#ocsp-证书验证协议"},{"categories":["tech"],"content":" 1. 密码套件与 TLS 历史版本密码套件（Cipher_suite）是 TLS 协议中一组用于实现安全通讯的密码学算法，类似于我们前面学习过的加密方案。不同密码学算法的组合形成不同的密码套件，算法组合的差异使这些密码套件具有不同的性能与安全性，另外 TLS 协议的更新迭代也导致各密码套件拥有不同的兼容性。通常越新推出的密码套件的安全性越高，但是兼容性就越差（旧设备不支持）。 密码套件的名称由它使用的各种密码学算法名称组成，而且有固定的格式，以TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 为例介绍下： TLS: 定义了此套件适用的协议，通常固定为 TLS ECDHE: 密钥交换算法 RSA: 数字证书认证算法 AES_128_GCM: 使用的对称加密方案，这是一个基于 AES 与 GCM 模式的对称认证加密方案，使用 128 位密钥 SHA256: 哈希函数，用于 HMAC 算法实现消息认证 TLS 固定使用 HMAC 算法进行消息认证 TLS 协议的前身是 SSL 协议，TLS/SSL 的发展历程展示如下： SSL/TLS 的历史版本 SSL 协议早在 2015 年就被各大主流浏览器废除了，TLS1.0 感觉也基本没站点在用了，这俩就直接跳过了。 下面分别介绍下 TLS1.1 TLS1.2 与 TLS1.3. TLS 1.1TLS 1.1 在 RFC4346 中定义，于 2006 年 4 月发布。 TLS 1.1 是 TLS 1.0 的一个补丁，主要更新包括： 添加对CBC攻击的保护 隐式初始向量 IV 被替换成一个显式的 IV 修复分组密码模式中填充算法的 bug 支持 IANA 登记的参数 TLS 1.1及其之前的算法曾经被广泛应用，它目前已知的缺陷如下： 不支持 PFS 完全前向保密 不支持 AEAD 认证加密算法 为了兼容性，保留了很多不安全的算法 TLS 1.1 已经不够安全了，不过一些陈年老站点或许还在使用它。 各操作系统（Android/IOS/MacOS/Windows）与浏览器基本都在很早的版本中就已经支持 TLS1.2+ 了， 站在 2022 年这个时间节点看，我们已经可以完全废止 TLS1.1 协议。实际上各大云厂商也是这么干的，比如 AWS 自身的 API 对 TLS1.1 的支持就已确定将在 2023 年 6 月废止，2022 年就开始频繁扫描并提醒各位仍然在使用低版本 TLS 协议的客户升级。 TLS 1.2TLS 1.2 在 RFC5246 中定义，于 2008 年 8 月发发布。 可选支持 PFS 完全前向保密 移除对 MD5 与 SHA-1 签名算法的支持 添加对 HMAC-SHA-256 及 HMAC-SHA-384 消息认证算法的支持 添加对 AEAD 加密认证方案的支持 去除 forback 回到 SSL 协议的能力，提升安全性 为了兼容性，保留了很多不安全的算法 如果你使用 TLS 1.2，需要小心地选择密码套件，避开不安全的套件，就能实现足够高的安全性。 TLS 1.3TLS 1.3 做了一次大刀阔斧的更新，是一个里程碑式的版本，其更新总结如下： 移除对如下算法的支持 哈希函数 SHA1/MD5 所有非 AEAD 加密认证的密码方案（CBC 模式） 移除对 RC4 与 3DES 加密算法的支持 移除了静态 RSA 与 DH 密钥交换算法 支持高性能的 Ed25519/Ed448 签名认证算法、X25519 密钥协商算法 支持高性能的 ChaCha20-Poly1305 对称认证加密方案 将密钥交换算法与公钥认证算法从密码套件中分离出来 比如原来的 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密码套件将被拆分为 ECDHE 算法、RSA 身份认证算法、以及 TLS_AES_128_GCM_SHA256 密码套件 这样密码套件就只包含一个 AEAD 认证加密方案，以及一个哈希函数了 仅支持前向安全的密钥交换算法 DHE 或 ECDHE 支持最短 0-RTT 的 TLS 握手（会话恢复） TLS 1.3 从协议中删除了所有不安全的算法或协议，可以说只要你的通讯用了 TLS 1.3，那你的数据就安全了（当然前提是你的私钥没泄漏）。 如何设置 TLS 协议的版本、密码套件参数我们前面已经学习了对称加密、非对称加密、密钥交换三部分知识，对照 TLS 套件的名称，应该能很容易判断出哪些是安全的、哪些不够安全，哪些支持前向保密、哪些不支持。 一个非常好用的「站点 HTTPS 安全检测」网站是 https://myssl.com/，使用它测试知乎网的检测结果如下： SSL/TLS 的历史版本 能看到知乎为了兼容性，目前仍然支持 TLS1.0 与 TLS1.1，另外目前还不支持 TLS1.3. 此外，知乎仍然支持很多已经不安全的加密套件，myssl.com 专门使用黄色标识出了这些不安全的加密套件，我们总结下主要特征： 部分密码套件使用了不安全的对称加密算法 3DES 其他被标识为黄色的套件虽然使用了安全的对称加密算法，但是不支持 PFS 前向保密 此外 myssl.com 还列出了许多站点更详细的信息，包括 TLS1.3 的会话恢复，以及后面将会介绍的公钥固定、HTTP严格传输安全等信息： SSL/TLS 的历史版本 Nginx 的 TLS 协议配置以前为 Nginx 等程序配置 HTTPS 协议时，我最头疼的就是其中密码套件参数 ssl_ciphers，为了安全性，需要配置超长的一大堆选用的密码套件名称，我可以说一个都看不懂，但是为了把网站搞好还是得硬着头皮搜索复制粘贴，实际上也不清楚安全性到底如何。 为了解决这个问题，Mozilla/DigitalOcean 都搞过流行 Web 服务器的 TLS 配置生成工具，比如ssl-config - mozilla，这个网站提供三个安全等级的配置: 「Intermediate」: 查看生成出的 ssl-cipher 属性，发现它只支持 ECDHE/DHE 开头的算法。因此它保证前向保密。 对于需要通过浏览器访问的 API，推荐选择这个等级。 「Modern」: 只支持 TLSv1.3，该协议废弃掉了过往所有不安全的算法，保证前向保密，安全性极高，性能也更好。 对于不需要通过浏览器等旧终端访问的 API，请直接选择这个等级。 「Old」: 除非你的用户使用非常老的终端进行访问，否则请不要考虑这个选项！ 可以点进去查看详细的 TLS 套件配置。 OCSP 证书验证协议 https://www.ssl.com/blogs/how-do-browsers-handle-revoked-ssl-tls-certificates/ https://imququ.com/post/why-can-not-turn-on-ocsp-stapling.html https://www.digicert.com/help/ 前面提到除了数字证书自带的有效期外，为了在私钥泄漏的情况下，能够吊销对应的证书，PKI 公钥基础设施还提供了 OCSP（Online Certificate Status Protocol）证书状态查询协议。 这导致了一些问题： Chrome/Firefox 等浏览器都会定期通过 OCSP 协议去请求 CA 机构的 OCSP 服务器验证证书状态， 这可能会拖慢 HTTPS 协议的响应速度。 所谓的定期是指超过上一个 OCSP 响应的 nextUpdate 时间（一般为 7 天），或者如果该值为空的话，Firefox 默认 24h 后会重新查询 OCSP 状态。 因为客户端直接去请求 CA 机构的 OCSP 地址获取证书状态，这就导致 CA 机构可以获取到一些对应站点的用户信息（IP 地址、网络状态等）。 为了解决这两个问题，rfc6066 定义了 OCSP stapling 功能，它使服务器可以提前访问 OCSP 获取证书状态信息并缓存到本地，基本 Nginx/Caddy 等各大 Web 服务器或网关，都支持 OCSP stapling 协议。 在客户端使用 TLS 协议访问 HTTPS 服务时，服务端会直接在握手阶段将缓存的 OCSP 信息发送给客户端。因为 OCSP 信息会带有 CA 证书的签名及有效期，客户端可以直接通过签名验证 OCSP 信息的真实性与有效性，这样就避免了客户端访问 OCSP 服务器带来的开销。 而另一个方法，就是选用 ocsp 服务器在目标用户区域速度快的 CA 机构签发证书。 可以使用如下命令测试，确认站点是否启用了 ocsp stapling: conf $ openssl s_client -connect www.digicert.com:443 -servername www.digicert.com -status -tlsextdebug \u003c /dev/null 2\u003e\u00261 | grep -i \"OCSP response\" 如果输出包含 OCSP Response Status: successful 就说明站点支持 ocsp stapling，如果输出内容为 OCSP response: no response sent 则说明站点不支持ocsp stapling。 实际上 Google/AWS 等大多数站点都不会启用也不需要启用 ocsp stapling，一是因为它们自己就是证书颁发机构，OCSP 服务器也归它们自己管，不存在隐私的问题。二是它们的 OCSP 服务器遍布","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#alpn-应用层协议协商"},{"categories":["tech"],"content":" 2. mTLS 双向认证TLS 协议（tls1.0+，RFC:TLS1.2 - RFC5246）也定义了可选的服务端请求验证客户端证书的方法。这个方法是可选的。如果使用上这个方法，那客户端和服务端就会在 TLS 协议的握手阶段进行互相认证。这种验证方式被称为双向 TLS 认证(mTLS, mutual TLS)。 传统的「TLS 单向认证」技术，只在客户端去验证服务端是否可信。而「TLS 双向认证（mTLS）」，则添加了服务端验证客户端是否可信的步骤（第三步）: 客户端发起请求 「验证服务端是否可信」: 服务端将自己的 TLS 证书发送给客户端，客户端通过自己的 CA 证书链验证这个服务端证书。 「验证客户端是否可信」: 客户端将自己的 TLS 证书发送给服务端，服务端使用它的 CA 证书链验证该客户端证书。 协商对称加密算法及密钥 使用对称加密进行后续通信。 因为相比传统的 TLS，mTLS 只是添加了「验证客户端」这样一个步骤，所以这项技术也被称为「Client Authentication」. mTLS 需要用到两套 TLS 证书: 服务端证书: 这个证书的内容以及申请流程，前面介绍过了。 客户端证书: 客户端证书貌似对证书信息（如 CN/SAN 域名）没有任何要求，只要证书能通过服务端的 CA 签名验证就行。 使用 openssl 生成 TLS 客户端证书（ca 和 csr.conf 可以直接使用前面生成服务端证书用到的，也可以另外生成）: shell # 1. 生成 2048 位 的 RSA 密钥 openssl genrsa -out client.key 2048 # 2. 通过第一步编写的配置文件，生成证书签名请求 openssl req -new -key client.key -out client.csr -config csr.conf # 3. 生成最终的证书，这里指定证书有效期 3650 天 ### 使用前面生成的 ca 证书对客户端证书进行签名（客户端和服务端共用 ca 证书） openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out client.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf mTLS 的应用场景主要在「零信任网络架构」，或者叫「无边界网络」中。比如微服务之间的互相访问，就可以使用 mTLS。这样就能保证每个 RPC 调用的客户端，都是其他微服务（或者别的可信方）， 防止黑客入侵后为所欲为。 目前查到如下几个Web服务器/代理支持 mTLS: Traefik:Docs - Client Authentication (mTLS) Nginx:Using NGINX Reverse Proxy for client certificate authentication 主要参数是两个: ssl_client_certificate /etc/nginx/client-ca.pem 和ssl_verify_client on mTLS 的安全性如果将 mTLS 用在 App 安全上，存在的风险是: 客户端中隐藏的证书是否可以被提取出来，或者黑客能否 Hook 进 App 中，直接使用证书发送信息。 如果客户端私钥设置了「密码（passphrase）」，那这个密码是否能很容易被逆向出来？ mTLS 和「公钥锁定/证书锁定」对比: 公钥锁定/证书锁定: 只在客户端进行验证。 但是在服务端没有进行验证。这样就无法鉴别并拒绝第三方应用（爬虫）的请求。 加强安全的方法，是通过某种算法生成动态的签名。爬虫生成不出来这个签名，请求就被拒绝。 mTLS: 服务端和客户端都要验证对方。 保证双边可信，在客户端证书不被破解的情况下，就能 Ban 掉所有的爬虫或代理技术。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#2-mtls-双向认证"},{"categories":["tech"],"content":" 2. mTLS 双向认证TLS 协议（tls1.0+，RFC:TLS1.2 - RFC5246）也定义了可选的服务端请求验证客户端证书的方法。这个方法是可选的。如果使用上这个方法，那客户端和服务端就会在 TLS 协议的握手阶段进行互相认证。这种验证方式被称为双向 TLS 认证(mTLS, mutual TLS)。 传统的「TLS 单向认证」技术，只在客户端去验证服务端是否可信。而「TLS 双向认证（mTLS）」，则添加了服务端验证客户端是否可信的步骤（第三步）: 客户端发起请求 「验证服务端是否可信」: 服务端将自己的 TLS 证书发送给客户端，客户端通过自己的 CA 证书链验证这个服务端证书。 「验证客户端是否可信」: 客户端将自己的 TLS 证书发送给服务端，服务端使用它的 CA 证书链验证该客户端证书。 协商对称加密算法及密钥 使用对称加密进行后续通信。 因为相比传统的 TLS，mTLS 只是添加了「验证客户端」这样一个步骤，所以这项技术也被称为「Client Authentication」. mTLS 需要用到两套 TLS 证书: 服务端证书: 这个证书的内容以及申请流程，前面介绍过了。 客户端证书: 客户端证书貌似对证书信息（如 CN/SAN 域名）没有任何要求，只要证书能通过服务端的 CA 签名验证就行。 使用 openssl 生成 TLS 客户端证书（ca 和 csr.conf 可以直接使用前面生成服务端证书用到的，也可以另外生成）: shell # 1. 生成 2048 位 的 RSA 密钥 openssl genrsa -out client.key 2048 # 2. 通过第一步编写的配置文件，生成证书签名请求 openssl req -new -key client.key -out client.csr -config csr.conf # 3. 生成最终的证书，这里指定证书有效期 3650 天 ### 使用前面生成的 ca 证书对客户端证书进行签名（客户端和服务端共用 ca 证书） openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out client.crt -days 3650 \\ -extensions v3_ext -extfile csr.conf mTLS 的应用场景主要在「零信任网络架构」，或者叫「无边界网络」中。比如微服务之间的互相访问，就可以使用 mTLS。这样就能保证每个 RPC 调用的客户端，都是其他微服务（或者别的可信方）， 防止黑客入侵后为所欲为。 目前查到如下几个Web服务器/代理支持 mTLS: Traefik:Docs - Client Authentication (mTLS) Nginx:Using NGINX Reverse Proxy for client certificate authentication 主要参数是两个: ssl_client_certificate /etc/nginx/client-ca.pem 和ssl_verify_client on mTLS 的安全性如果将 mTLS 用在 App 安全上，存在的风险是: 客户端中隐藏的证书是否可以被提取出来，或者黑客能否 Hook 进 App 中，直接使用证书发送信息。 如果客户端私钥设置了「密码（passphrase）」，那这个密码是否能很容易被逆向出来？ mTLS 和「公钥锁定/证书锁定」对比: 公钥锁定/证书锁定: 只在客户端进行验证。 但是在服务端没有进行验证。这样就无法鉴别并拒绝第三方应用（爬虫）的请求。 加强安全的方法，是通过某种算法生成动态的签名。爬虫生成不出来这个签名，请求就被拒绝。 mTLS: 服务端和客户端都要验证对方。 保证双边可信，在客户端证书不被破解的情况下，就能 Ban 掉所有的爬虫或代理技术。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#mtls-的安全性"},{"categories":["tech"],"content":" 3. 其他加密通讯协议 SSH 协议首先最容易想到的应该就是是 SSH 协议（Secure SHell protocol）。SSH 与 TLS 一样都能提供加密通讯，是 PKI 公钥基础设施的早期先驱者之一。 SSH 协议应用最广泛的实现是 OpenSSH，它使用 SSH Key 而非数字证书进行身份认证，这主要是因为 OpenSSH 仅用于用户与主机之间的安全通信，不需要记录 X.509 这么繁多的信息。 我们来手动生成个 OpenSSH ed25519 密钥对试试（RSA 的生成命令完全类似）： shell ❯ ssh-keygen -t ed25519 Generating public/private ed25519 key pair. Enter file in which to save the key (/Users/admin/.ssh/id_ed25519): ed25519-key Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ed25519-key. Your public key has been saved in ed25519-key.pub. The key fingerprint is: SHA256:jgeuWVflhNXXrDDzUtW6ZV1lpBWNAj0Rstizh9Lbyg0 admin@ryan-MacBook-Pro.local The key's randomart image is: +--[ED25519 256]--+ | oo++ *%| | o =B ++B| | . = oO.+o| | . B. + +| | . S = o. + | | . + o + . | | + + E . | | + o . + | | o o . | +----[SHA256]-----+ ❯ cat ed25519-key -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW QyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQAAAKDnHOSY5xzk mAAAAAtzc2gtZWQyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQ AAAEADkVL1gZHAvBx4M5+UjVVL7ltVOC4r9tdR23CoI9iV1O7HgqespdWziJH2Y9mdKm6v nbTtx7IyJk/4IOdd2igxAAAAHGFkbWluQHJ5YW4tTWFjQm9vay1Qcm8ubG9jYWwB -----END OPENSSH PRIVATE KEY----- ❯ cat ed25519-key.pub ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIO7HgqespdWziJH2Y9mdKm6vnbTtx7IyJk/4IOdd2igx admin@ryan-MacBook-Pro.local 可以看到 SSH Key 的结构非常简单，仅包含如下三个部分： 密钥对类型: 最常见的是 ssh-rsa，另外由于安全性目前更推荐使用 ssh-ed25519 公钥的 Base64 字符串 一个 Comment，通常包含这个 Key 的用途，或者 Key 所有者的邮箱地址 通过我们前面学的非对称密码学知识可以知道，公钥能直接从私钥生成，假设你的 ssh 公钥丢失，可以通过如下命令重新生成出公钥： shell ssh-keygen -y -f xxx_rsa \u003e xxx_rsa.pub HTTP/3 与 QUIC 协议QUIC 协议，是 Google 研发并推动标准化的 TCP 协议的替代品， QUIC 是基于 UDP 协议实现的。基于 QUIC 提出的 HTTP over QUIC 协议已被标准化为RFC 9114 - HTTP/3，它做了很多大刀阔斧的改革： 传输层协议从 TCP 改成了 UDP，QUIC 自己实现的数据的可靠传输、按序到达、拥塞控制 也就是说 QUIC 绕过了陈旧的内核 TCP 协议实现，直接在用户空间实现了这些功能 通过另起炉灶，它解决了一些 TCP 协议的痛点：队头阻塞、握手延迟高、特性迭代慢、拥塞控制算法不佳等问题 在 TLS1.3 出现之前，QUIC 实现了自己的加密方案QUIC Crypto 以取代陈旧的 TLS 协议，同时兼容现有的数字证书体系 QUIC Crypto 的特点是它直接在应用层进行加密通讯的握手，并且恢复通信时可以通过缓存实现 0RTT 握手 也就说 QUIC 通过另起炉灶，解决了 TLS 的安全问题，以及握手延迟高的问题 总结一下就是，旧的实验性 HTTP-over-QUIC 协议，重新实现了 HTTP+TLS+TCP 三种协议并将它们整合到一起，这带来了极佳的性能，但也使它变得非常复杂。 QUIC 的 0RTT 握手是一个非常妙的想法，可以显著降低握手时延，TLS1.3 的设计者们将它纳入了 TLS1.3 标准中。 由于 TLS1.3 的良好特性，在 TLS1.3 协议发布后，新的 QUIC 标准RFC 9001 已经使用 TLS1.3 取代了实验阶段使用的 QUIC Crypto 加密方案，目前只有 Chromium/Chrome 仍然支持 QUIC Crypto，其他 QUIC 实现基本都只支持 TLS1.3, 详见QUIC Implementations. ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#3-其他加密通讯协议"},{"categories":["tech"],"content":" 3. 其他加密通讯协议 SSH 协议首先最容易想到的应该就是是 SSH 协议（Secure SHell protocol）。SSH 与 TLS 一样都能提供加密通讯，是 PKI 公钥基础设施的早期先驱者之一。 SSH 协议应用最广泛的实现是 OpenSSH，它使用 SSH Key 而非数字证书进行身份认证，这主要是因为 OpenSSH 仅用于用户与主机之间的安全通信，不需要记录 X.509 这么繁多的信息。 我们来手动生成个 OpenSSH ed25519 密钥对试试（RSA 的生成命令完全类似）： shell ❯ ssh-keygen -t ed25519 Generating public/private ed25519 key pair. Enter file in which to save the key (/Users/admin/.ssh/id_ed25519): ed25519-key Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ed25519-key. Your public key has been saved in ed25519-key.pub. The key fingerprint is: SHA256:jgeuWVflhNXXrDDzUtW6ZV1lpBWNAj0Rstizh9Lbyg0 admin@ryan-MacBook-Pro.local The key's randomart image is: +--[ED25519 256]--+ | oo++ *%| | o =B ++B| | . = oO.+o| | . B. + +| | . S = o. + | | . + o + . | | + + E . | | + o . + | | o o . | +----[SHA256]-----+ ❯ cat ed25519-key -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW QyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQAAAKDnHOSY5xzk mAAAAAtzc2gtZWQyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQ AAAEADkVL1gZHAvBx4M5+UjVVL7ltVOC4r9tdR23CoI9iV1O7HgqespdWziJH2Y9mdKm6v nbTtx7IyJk/4IOdd2igxAAAAHGFkbWluQHJ5YW4tTWFjQm9vay1Qcm8ubG9jYWwB -----END OPENSSH PRIVATE KEY----- ❯ cat ed25519-key.pub ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIO7HgqespdWziJH2Y9mdKm6vnbTtx7IyJk/4IOdd2igx admin@ryan-MacBook-Pro.local 可以看到 SSH Key 的结构非常简单，仅包含如下三个部分： 密钥对类型: 最常见的是 ssh-rsa，另外由于安全性目前更推荐使用 ssh-ed25519 公钥的 Base64 字符串 一个 Comment，通常包含这个 Key 的用途，或者 Key 所有者的邮箱地址 通过我们前面学的非对称密码学知识可以知道，公钥能直接从私钥生成，假设你的 ssh 公钥丢失，可以通过如下命令重新生成出公钥： shell ssh-keygen -y -f xxx_rsa \u003e xxx_rsa.pub HTTP/3 与 QUIC 协议QUIC 协议，是 Google 研发并推动标准化的 TCP 协议的替代品， QUIC 是基于 UDP 协议实现的。基于 QUIC 提出的 HTTP over QUIC 协议已被标准化为RFC 9114 - HTTP/3，它做了很多大刀阔斧的改革： 传输层协议从 TCP 改成了 UDP，QUIC 自己实现的数据的可靠传输、按序到达、拥塞控制 也就是说 QUIC 绕过了陈旧的内核 TCP 协议实现，直接在用户空间实现了这些功能 通过另起炉灶，它解决了一些 TCP 协议的痛点：队头阻塞、握手延迟高、特性迭代慢、拥塞控制算法不佳等问题 在 TLS1.3 出现之前，QUIC 实现了自己的加密方案QUIC Crypto 以取代陈旧的 TLS 协议，同时兼容现有的数字证书体系 QUIC Crypto 的特点是它直接在应用层进行加密通讯的握手，并且恢复通信时可以通过缓存实现 0RTT 握手 也就说 QUIC 通过另起炉灶，解决了 TLS 的安全问题，以及握手延迟高的问题 总结一下就是，旧的实验性 HTTP-over-QUIC 协议，重新实现了 HTTP+TLS+TCP 三种协议并将它们整合到一起，这带来了极佳的性能，但也使它变得非常复杂。 QUIC 的 0RTT 握手是一个非常妙的想法，可以显著降低握手时延，TLS1.3 的设计者们将它纳入了 TLS1.3 标准中。 由于 TLS1.3 的良好特性，在 TLS1.3 协议发布后，新的 QUIC 标准RFC 9001 已经使用 TLS1.3 取代了实验阶段使用的 QUIC Crypto 加密方案，目前只有 Chromium/Chrome 仍然支持 QUIC Crypto，其他 QUIC 实现基本都只支持 TLS1.3, 详见QUIC Implementations. ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#ssh-协议"},{"categories":["tech"],"content":" 3. 其他加密通讯协议 SSH 协议首先最容易想到的应该就是是 SSH 协议（Secure SHell protocol）。SSH 与 TLS 一样都能提供加密通讯，是 PKI 公钥基础设施的早期先驱者之一。 SSH 协议应用最广泛的实现是 OpenSSH，它使用 SSH Key 而非数字证书进行身份认证，这主要是因为 OpenSSH 仅用于用户与主机之间的安全通信，不需要记录 X.509 这么繁多的信息。 我们来手动生成个 OpenSSH ed25519 密钥对试试（RSA 的生成命令完全类似）： shell ❯ ssh-keygen -t ed25519 Generating public/private ed25519 key pair. Enter file in which to save the key (/Users/admin/.ssh/id_ed25519): ed25519-key Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in ed25519-key. Your public key has been saved in ed25519-key.pub. The key fingerprint is: SHA256:jgeuWVflhNXXrDDzUtW6ZV1lpBWNAj0Rstizh9Lbyg0 admin@ryan-MacBook-Pro.local The key's randomart image is: +--[ED25519 256]--+ | oo++ *%| | o =B ++B| | . = oO.+o| | . B. + +| | . S = o. + | | . + o + . | | + + E . | | + o . + | | o o . | +----[SHA256]-----+ ❯ cat ed25519-key -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW QyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQAAAKDnHOSY5xzk mAAAAAtzc2gtZWQyNTUxOQAAACDux4KnrKXVs4iR9mPZnSpur5207ceyMiZP+CDnXdooMQ AAAEADkVL1gZHAvBx4M5+UjVVL7ltVOC4r9tdR23CoI9iV1O7HgqespdWziJH2Y9mdKm6v nbTtx7IyJk/4IOdd2igxAAAAHGFkbWluQHJ5YW4tTWFjQm9vay1Qcm8ubG9jYWwB -----END OPENSSH PRIVATE KEY----- ❯ cat ed25519-key.pub ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIO7HgqespdWziJH2Y9mdKm6vnbTtx7IyJk/4IOdd2igx admin@ryan-MacBook-Pro.local 可以看到 SSH Key 的结构非常简单，仅包含如下三个部分： 密钥对类型: 最常见的是 ssh-rsa，另外由于安全性目前更推荐使用 ssh-ed25519 公钥的 Base64 字符串 一个 Comment，通常包含这个 Key 的用途，或者 Key 所有者的邮箱地址 通过我们前面学的非对称密码学知识可以知道，公钥能直接从私钥生成，假设你的 ssh 公钥丢失，可以通过如下命令重新生成出公钥： shell ssh-keygen -y -f xxx_rsa \u003e xxx_rsa.pub HTTP/3 与 QUIC 协议QUIC 协议，是 Google 研发并推动标准化的 TCP 协议的替代品， QUIC 是基于 UDP 协议实现的。基于 QUIC 提出的 HTTP over QUIC 协议已被标准化为RFC 9114 - HTTP/3，它做了很多大刀阔斧的改革： 传输层协议从 TCP 改成了 UDP，QUIC 自己实现的数据的可靠传输、按序到达、拥塞控制 也就是说 QUIC 绕过了陈旧的内核 TCP 协议实现，直接在用户空间实现了这些功能 通过另起炉灶，它解决了一些 TCP 协议的痛点：队头阻塞、握手延迟高、特性迭代慢、拥塞控制算法不佳等问题 在 TLS1.3 出现之前，QUIC 实现了自己的加密方案QUIC Crypto 以取代陈旧的 TLS 协议，同时兼容现有的数字证书体系 QUIC Crypto 的特点是它直接在应用层进行加密通讯的握手，并且恢复通信时可以通过缓存实现 0RTT 握手 也就说 QUIC 通过另起炉灶，解决了 TLS 的安全问题，以及握手延迟高的问题 总结一下就是，旧的实验性 HTTP-over-QUIC 协议，重新实现了 HTTP+TLS+TCP 三种协议并将它们整合到一起，这带来了极佳的性能，但也使它变得非常复杂。 QUIC 的 0RTT 握手是一个非常妙的想法，可以显著降低握手时延，TLS1.3 的设计者们将它纳入了 TLS1.3 标准中。 由于 TLS1.3 的良好特性，在 TLS1.3 协议发布后，新的 QUIC 标准RFC 9001 已经使用 TLS1.3 取代了实验阶段使用的 QUIC Crypto 加密方案，目前只有 Chromium/Chrome 仍然支持 QUIC Crypto，其他 QUIC 实现基本都只支持 TLS1.3, 详见QUIC Implementations. ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#http3-与-quic-协议"},{"categories":["tech"],"content":" 4. TLS 协议攻防战虽然 TLS 协议是一个安全的加密协议，但是我们知道向whistle / charles / fiddler 等代理调试工具、以及深信服的企业员工流量监测工具，都能实现对 HTTPS 流量的监听。 其原理是这些代理工具会在启动时首先要求将它自己的私有 CA 证书添加到系统证书中，这样客户端 （手机 APP / 浏览器等应用）在建立 TLS 连接时会被代理工具拦截，代理工具返回一个它伪造的证书，该证书会匹配上上一步提前添加到系统中的 CA 证书从而成功建立连接。后续再由代理工具将请求转发给真正的服务端，这样代理工具就能实现对 HTTPS 流量的监听。 这实质是一次中继攻击，其中最关键的一步就是，它需要在客户端系统上添加自己的 CA 证书，这样客户端就会信任它伪造的证书。 读者如果读到了这里，建议检查一下自己系统中的证书列表，看看是否有这类被插入的恶意证书，自己流量被监听了而不自知。 既然有攻击手段，那就有防御手段，许多大厂的 APP 都会使用一些技术来防止自己的流量被监听，下面介绍几种常见的防御手段。 1. 证书锁定（Certificate Pining）技术即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的 CA 证书库进行证书验证，而 Fiddler 等代理工具可以将自己的 CA 证书添加到该证书库中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定（Certificate Pining, 或者 SSL Pinning）」技术。方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听: 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2. 公钥锁定（Public Key Pining）技术前面介绍过证书的结构，它其实包含了公钥、有效期与一系列的其他信息。使用了证书锁定技术，会导致证书的有效期也被锁定，APK 内的证书指纹就必须随着证书一起更新。 更好的做法是仅锁定证书中的公钥，即「公钥锁定」技术。「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 「公钥锁定」是更推荐的锁定技术。 3. HTTPS 严格传输安全 - HSTSHSTS，即 HTTP Strict Transport Security，是一项安全技术，它允许服务端在返回 HTTPS 响应时， 通过 Headers 明确要求客户端，在之后的一段时间内必须使用安全的 HTTPS 协议访问服务端。 举个例子，假设站点 https://example.com/ 的响应头中有Strict-Transport-Security: max-age=31536000; includeSubDomains，这表示服务端要求客户端 （比如浏览器）： 在接下来的 31536000 秒（即一年）中，客户端向 example.com 或其子域名发送 HTTP 请求时，必须采用HTTPS来发起连接。 比如用户在浏览器地址栏输入 http://example.com/ 时，浏览器应自动将 http 改写为 https 再发起请求 在接下来的 31536000 秒（即一年）中，如果 example.com 服务器提供的证书无效，用户不能忽略浏览器的证书警告继续访问网站。 也就是说一旦证书失效，站点将完全无法访问，直至服务端修复证书问题。 一旦证书失效，HTTPS 其实就不是严格安全的了，可能会遭遇中间人攻击。也就是说 HSTS 通过牺牲站点的可访问性来避免中间人攻击。 4. TLS 协议的逆向手段要获取一个应用的 HTTPS 数据，有两个方向: 服务端入侵: 现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 不过如果你获得了服务器 root 权限，可以在 openssl 上做文章，比如篡改 openssl？ 客户端逆向+爬虫: 客户端是离用户最近的地方，也是最容易被突破的地方。 方法一在前面介绍过了，就是在客户端系统中提前安装私有 CA 证书，再通过代理工具进行监听，这种方法能监听到所有普通 HTTP 流量。 安卓上可以通过 XPosed+JustTrustMe 等技术，直接 Hook 底层 HTTP 库绕过证书检查，实现监听 HTTPS 流量。这种方法的缺点是只能监听适配的库，所以以前有人说 flutter 应用安全性好，因为它比较新鲜，逆向资料少，逆向难度就高。 对 mTLS 这类更安全的协议，会更复杂些。 我见过的破解手段之一：找到老版本的安装包，发现很容易就能提取出客户端证书… wiki 列出了一些 TLS 协议的安全问题：https://en.wikipedia.org/wiki/Transport_Layer_Security#Security TO BE DONE… ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:4","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#4-tls-协议攻防战"},{"categories":["tech"],"content":" 4. TLS 协议攻防战虽然 TLS 协议是一个安全的加密协议，但是我们知道向whistle / charles / fiddler 等代理调试工具、以及深信服的企业员工流量监测工具，都能实现对 HTTPS 流量的监听。 其原理是这些代理工具会在启动时首先要求将它自己的私有 CA 证书添加到系统证书中，这样客户端 （手机 APP / 浏览器等应用）在建立 TLS 连接时会被代理工具拦截，代理工具返回一个它伪造的证书，该证书会匹配上上一步提前添加到系统中的 CA 证书从而成功建立连接。后续再由代理工具将请求转发给真正的服务端，这样代理工具就能实现对 HTTPS 流量的监听。 这实质是一次中继攻击，其中最关键的一步就是，它需要在客户端系统上添加自己的 CA 证书，这样客户端就会信任它伪造的证书。 读者如果读到了这里，建议检查一下自己系统中的证书列表，看看是否有这类被插入的恶意证书，自己流量被监听了而不自知。 既然有攻击手段，那就有防御手段，许多大厂的 APP 都会使用一些技术来防止自己的流量被监听，下面介绍几种常见的防御手段。 1. 证书锁定（Certificate Pining）技术即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的 CA 证书库进行证书验证，而 Fiddler 等代理工具可以将自己的 CA 证书添加到该证书库中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定（Certificate Pining, 或者 SSL Pinning）」技术。方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听: 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2. 公钥锁定（Public Key Pining）技术前面介绍过证书的结构，它其实包含了公钥、有效期与一系列的其他信息。使用了证书锁定技术，会导致证书的有效期也被锁定，APK 内的证书指纹就必须随着证书一起更新。 更好的做法是仅锁定证书中的公钥，即「公钥锁定」技术。「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 「公钥锁定」是更推荐的锁定技术。 3. HTTPS 严格传输安全 - HSTSHSTS，即 HTTP Strict Transport Security，是一项安全技术，它允许服务端在返回 HTTPS 响应时， 通过 Headers 明确要求客户端，在之后的一段时间内必须使用安全的 HTTPS 协议访问服务端。 举个例子，假设站点 https://example.com/ 的响应头中有Strict-Transport-Security: max-age=31536000; includeSubDomains，这表示服务端要求客户端 （比如浏览器）： 在接下来的 31536000 秒（即一年）中，客户端向 example.com 或其子域名发送 HTTP 请求时，必须采用HTTPS来发起连接。 比如用户在浏览器地址栏输入 http://example.com/ 时，浏览器应自动将 http 改写为 https 再发起请求 在接下来的 31536000 秒（即一年）中，如果 example.com 服务器提供的证书无效，用户不能忽略浏览器的证书警告继续访问网站。 也就是说一旦证书失效，站点将完全无法访问，直至服务端修复证书问题。 一旦证书失效，HTTPS 其实就不是严格安全的了，可能会遭遇中间人攻击。也就是说 HSTS 通过牺牲站点的可访问性来避免中间人攻击。 4. TLS 协议的逆向手段要获取一个应用的 HTTPS 数据，有两个方向: 服务端入侵: 现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 不过如果你获得了服务器 root 权限，可以在 openssl 上做文章，比如篡改 openssl？ 客户端逆向+爬虫: 客户端是离用户最近的地方，也是最容易被突破的地方。 方法一在前面介绍过了，就是在客户端系统中提前安装私有 CA 证书，再通过代理工具进行监听，这种方法能监听到所有普通 HTTP 流量。 安卓上可以通过 XPosed+JustTrustMe 等技术，直接 Hook 底层 HTTP 库绕过证书检查，实现监听 HTTPS 流量。这种方法的缺点是只能监听适配的库，所以以前有人说 flutter 应用安全性好，因为它比较新鲜，逆向资料少，逆向难度就高。 对 mTLS 这类更安全的协议，会更复杂些。 我见过的破解手段之一：找到老版本的安装包，发现很容易就能提取出客户端证书… wiki 列出了一些 TLS 协议的安全问题：https://en.wikipedia.org/wiki/Transport_Layer_Security#Security TO BE DONE… ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:4","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#1-证书锁定certificate-pining技术"},{"categories":["tech"],"content":" 4. TLS 协议攻防战虽然 TLS 协议是一个安全的加密协议，但是我们知道向whistle / charles / fiddler 等代理调试工具、以及深信服的企业员工流量监测工具，都能实现对 HTTPS 流量的监听。 其原理是这些代理工具会在启动时首先要求将它自己的私有 CA 证书添加到系统证书中，这样客户端 （手机 APP / 浏览器等应用）在建立 TLS 连接时会被代理工具拦截，代理工具返回一个它伪造的证书，该证书会匹配上上一步提前添加到系统中的 CA 证书从而成功建立连接。后续再由代理工具将请求转发给真正的服务端，这样代理工具就能实现对 HTTPS 流量的监听。 这实质是一次中继攻击，其中最关键的一步就是，它需要在客户端系统上添加自己的 CA 证书，这样客户端就会信任它伪造的证书。 读者如果读到了这里，建议检查一下自己系统中的证书列表，看看是否有这类被插入的恶意证书，自己流量被监听了而不自知。 既然有攻击手段，那就有防御手段，许多大厂的 APP 都会使用一些技术来防止自己的流量被监听，下面介绍几种常见的防御手段。 1. 证书锁定（Certificate Pining）技术即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的 CA 证书库进行证书验证，而 Fiddler 等代理工具可以将自己的 CA 证书添加到该证书库中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定（Certificate Pining, 或者 SSL Pinning）」技术。方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听: 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2. 公钥锁定（Public Key Pining）技术前面介绍过证书的结构，它其实包含了公钥、有效期与一系列的其他信息。使用了证书锁定技术，会导致证书的有效期也被锁定，APK 内的证书指纹就必须随着证书一起更新。 更好的做法是仅锁定证书中的公钥，即「公钥锁定」技术。「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 「公钥锁定」是更推荐的锁定技术。 3. HTTPS 严格传输安全 - HSTSHSTS，即 HTTP Strict Transport Security，是一项安全技术，它允许服务端在返回 HTTPS 响应时， 通过 Headers 明确要求客户端，在之后的一段时间内必须使用安全的 HTTPS 协议访问服务端。 举个例子，假设站点 https://example.com/ 的响应头中有Strict-Transport-Security: max-age=31536000; includeSubDomains，这表示服务端要求客户端 （比如浏览器）： 在接下来的 31536000 秒（即一年）中，客户端向 example.com 或其子域名发送 HTTP 请求时，必须采用HTTPS来发起连接。 比如用户在浏览器地址栏输入 http://example.com/ 时，浏览器应自动将 http 改写为 https 再发起请求 在接下来的 31536000 秒（即一年）中，如果 example.com 服务器提供的证书无效，用户不能忽略浏览器的证书警告继续访问网站。 也就是说一旦证书失效，站点将完全无法访问，直至服务端修复证书问题。 一旦证书失效，HTTPS 其实就不是严格安全的了，可能会遭遇中间人攻击。也就是说 HSTS 通过牺牲站点的可访问性来避免中间人攻击。 4. TLS 协议的逆向手段要获取一个应用的 HTTPS 数据，有两个方向: 服务端入侵: 现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 不过如果你获得了服务器 root 权限，可以在 openssl 上做文章，比如篡改 openssl？ 客户端逆向+爬虫: 客户端是离用户最近的地方，也是最容易被突破的地方。 方法一在前面介绍过了，就是在客户端系统中提前安装私有 CA 证书，再通过代理工具进行监听，这种方法能监听到所有普通 HTTP 流量。 安卓上可以通过 XPosed+JustTrustMe 等技术，直接 Hook 底层 HTTP 库绕过证书检查，实现监听 HTTPS 流量。这种方法的缺点是只能监听适配的库，所以以前有人说 flutter 应用安全性好，因为它比较新鲜，逆向资料少，逆向难度就高。 对 mTLS 这类更安全的协议，会更复杂些。 我见过的破解手段之一：找到老版本的安装包，发现很容易就能提取出客户端证书… wiki 列出了一些 TLS 协议的安全问题：https://en.wikipedia.org/wiki/Transport_Layer_Security#Security TO BE DONE… ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:4","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#2-公钥锁定public-key-pining技术"},{"categories":["tech"],"content":" 4. TLS 协议攻防战虽然 TLS 协议是一个安全的加密协议，但是我们知道向whistle / charles / fiddler 等代理调试工具、以及深信服的企业员工流量监测工具，都能实现对 HTTPS 流量的监听。 其原理是这些代理工具会在启动时首先要求将它自己的私有 CA 证书添加到系统证书中，这样客户端 （手机 APP / 浏览器等应用）在建立 TLS 连接时会被代理工具拦截，代理工具返回一个它伪造的证书，该证书会匹配上上一步提前添加到系统中的 CA 证书从而成功建立连接。后续再由代理工具将请求转发给真正的服务端，这样代理工具就能实现对 HTTPS 流量的监听。 这实质是一次中继攻击，其中最关键的一步就是，它需要在客户端系统上添加自己的 CA 证书，这样客户端就会信任它伪造的证书。 读者如果读到了这里，建议检查一下自己系统中的证书列表，看看是否有这类被插入的恶意证书，自己流量被监听了而不自知。 既然有攻击手段，那就有防御手段，许多大厂的 APP 都会使用一些技术来防止自己的流量被监听，下面介绍几种常见的防御手段。 1. 证书锁定（Certificate Pining）技术即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的 CA 证书库进行证书验证，而 Fiddler 等代理工具可以将自己的 CA 证书添加到该证书库中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定（Certificate Pining, 或者 SSL Pinning）」技术。方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听: 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2. 公钥锁定（Public Key Pining）技术前面介绍过证书的结构，它其实包含了公钥、有效期与一系列的其他信息。使用了证书锁定技术，会导致证书的有效期也被锁定，APK 内的证书指纹就必须随着证书一起更新。 更好的做法是仅锁定证书中的公钥，即「公钥锁定」技术。「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 「公钥锁定」是更推荐的锁定技术。 3. HTTPS 严格传输安全 - HSTSHSTS，即 HTTP Strict Transport Security，是一项安全技术，它允许服务端在返回 HTTPS 响应时， 通过 Headers 明确要求客户端，在之后的一段时间内必须使用安全的 HTTPS 协议访问服务端。 举个例子，假设站点 https://example.com/ 的响应头中有Strict-Transport-Security: max-age=31536000; includeSubDomains，这表示服务端要求客户端 （比如浏览器）： 在接下来的 31536000 秒（即一年）中，客户端向 example.com 或其子域名发送 HTTP 请求时，必须采用HTTPS来发起连接。 比如用户在浏览器地址栏输入 http://example.com/ 时，浏览器应自动将 http 改写为 https 再发起请求 在接下来的 31536000 秒（即一年）中，如果 example.com 服务器提供的证书无效，用户不能忽略浏览器的证书警告继续访问网站。 也就是说一旦证书失效，站点将完全无法访问，直至服务端修复证书问题。 一旦证书失效，HTTPS 其实就不是严格安全的了，可能会遭遇中间人攻击。也就是说 HSTS 通过牺牲站点的可访问性来避免中间人攻击。 4. TLS 协议的逆向手段要获取一个应用的 HTTPS 数据，有两个方向: 服务端入侵: 现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 不过如果你获得了服务器 root 权限，可以在 openssl 上做文章，比如篡改 openssl？ 客户端逆向+爬虫: 客户端是离用户最近的地方，也是最容易被突破的地方。 方法一在前面介绍过了，就是在客户端系统中提前安装私有 CA 证书，再通过代理工具进行监听，这种方法能监听到所有普通 HTTP 流量。 安卓上可以通过 XPosed+JustTrustMe 等技术，直接 Hook 底层 HTTP 库绕过证书检查，实现监听 HTTPS 流量。这种方法的缺点是只能监听适配的库，所以以前有人说 flutter 应用安全性好，因为它比较新鲜，逆向资料少，逆向难度就高。 对 mTLS 这类更安全的协议，会更复杂些。 我见过的破解手段之一：找到老版本的安装包，发现很容易就能提取出客户端证书… wiki 列出了一些 TLS 协议的安全问题：https://en.wikipedia.org/wiki/Transport_Layer_Security#Security TO BE DONE… ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:4","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#3-https-严格传输安全---hsts"},{"categories":["tech"],"content":" 4. TLS 协议攻防战虽然 TLS 协议是一个安全的加密协议，但是我们知道向whistle / charles / fiddler 等代理调试工具、以及深信服的企业员工流量监测工具，都能实现对 HTTPS 流量的监听。 其原理是这些代理工具会在启动时首先要求将它自己的私有 CA 证书添加到系统证书中，这样客户端 （手机 APP / 浏览器等应用）在建立 TLS 连接时会被代理工具拦截，代理工具返回一个它伪造的证书，该证书会匹配上上一步提前添加到系统中的 CA 证书从而成功建立连接。后续再由代理工具将请求转发给真正的服务端，这样代理工具就能实现对 HTTPS 流量的监听。 这实质是一次中继攻击，其中最关键的一步就是，它需要在客户端系统上添加自己的 CA 证书，这样客户端就会信任它伪造的证书。 读者如果读到了这里，建议检查一下自己系统中的证书列表，看看是否有这类被插入的恶意证书，自己流量被监听了而不自知。 既然有攻击手段，那就有防御手段，许多大厂的 APP 都会使用一些技术来防止自己的流量被监听，下面介绍几种常见的防御手段。 1. 证书锁定（Certificate Pining）技术即使使用了 TLS 协议对流量进行加密，并且保证了前向保密，也无法保证流量不被代理！ 这是因为客户端大多是直接依靠了操作系统内置的 CA 证书库进行证书验证，而 Fiddler 等代理工具可以将自己的 CA 证书添加到该证书库中。 为了防止流量被 Fiddler 等工具使用上述方式监听流量，出现了「证书锁定（Certificate Pining, 或者 SSL Pinning）」技术。方法是在客户端中硬编码证书的指纹（Hash值，或者直接保存整个证书的内容也行），在建立 TLS 连接前，先计算使用的证书的指纹是否匹配，否则就中断连接。 这种锁定方式需要以下几个前提才能确保流量不被监听: 客户端中硬编码的证书指纹不会被篡改。 指纹验证不能被绕过。 目前有公开技术（XPosed+JustTrustMe）能破解 Android 上常见的 HTTPS 请求库，直接绕过证书检查。 针对上述问题，可以考虑加大绕过的难度。或者 App 检测自己是否运行在 Xposed 等虚拟环境下。 用于 TLS 协议的证书不会频繁更换。（如果更换了，指纹就对不上了。） 而对于第三方的 API，因为我们不知道它们会不会更换 TLS 证书，就不能直接将证书指纹硬编码在客户端中。这时可以考虑从服务端获取这些 API 的证书指纹（附带私钥签名用于防伪造）。 为了实现证书的轮转(rotation)，可以在新版本的客户端中包含多个证书指纹，这样能保证同时有多个可信证书，达成证书的轮转。（类比 JWT 的公钥轮转机制） 证书锁定技术几乎等同于 SSH 协议的 StrictHostKeyChecking 选项，客户端会验证服务端的公钥指纹（key fingerprint），验证不通过则断开连接。 2. 公钥锁定（Public Key Pining）技术前面介绍过证书的结构，它其实包含了公钥、有效期与一系列的其他信息。使用了证书锁定技术，会导致证书的有效期也被锁定，APK 内的证书指纹就必须随着证书一起更新。 更好的做法是仅锁定证书中的公钥，即「公钥锁定」技术。「公钥锁定」比「证书锁定」更灵活，这样证书本身其实就可以直接轮转了（证书有过期时间），而不需要一个旧证书和新证书共存的中间时期。 「公钥锁定」是更推荐的锁定技术。 3. HTTPS 严格传输安全 - HSTSHSTS，即 HTTP Strict Transport Security，是一项安全技术，它允许服务端在返回 HTTPS 响应时， 通过 Headers 明确要求客户端，在之后的一段时间内必须使用安全的 HTTPS 协议访问服务端。 举个例子，假设站点 https://example.com/ 的响应头中有Strict-Transport-Security: max-age=31536000; includeSubDomains，这表示服务端要求客户端 （比如浏览器）： 在接下来的 31536000 秒（即一年）中，客户端向 example.com 或其子域名发送 HTTP 请求时，必须采用HTTPS来发起连接。 比如用户在浏览器地址栏输入 http://example.com/ 时，浏览器应自动将 http 改写为 https 再发起请求 在接下来的 31536000 秒（即一年）中，如果 example.com 服务器提供的证书无效，用户不能忽略浏览器的证书警告继续访问网站。 也就是说一旦证书失效，站点将完全无法访问，直至服务端修复证书问题。 一旦证书失效，HTTPS 其实就不是严格安全的了，可能会遭遇中间人攻击。也就是说 HSTS 通过牺牲站点的可访问性来避免中间人攻击。 4. TLS 协议的逆向手段要获取一个应用的 HTTPS 数据，有两个方向: 服务端入侵: 现代应用的服务端突破难度通常都比较客户端高，注入等漏洞底层框架就有处理。 不过如果你获得了服务器 root 权限，可以在 openssl 上做文章，比如篡改 openssl？ 客户端逆向+爬虫: 客户端是离用户最近的地方，也是最容易被突破的地方。 方法一在前面介绍过了，就是在客户端系统中提前安装私有 CA 证书，再通过代理工具进行监听，这种方法能监听到所有普通 HTTP 流量。 安卓上可以通过 XPosed+JustTrustMe 等技术，直接 Hook 底层 HTTP 库绕过证书检查，实现监听 HTTPS 流量。这种方法的缺点是只能监听适配的库，所以以前有人说 flutter 应用安全性好，因为它比较新鲜，逆向资料少，逆向难度就高。 对 mTLS 这类更安全的协议，会更复杂些。 我见过的破解手段之一：找到老版本的安装包，发现很容易就能提取出客户端证书… wiki 列出了一些 TLS 协议的安全问题：https://en.wikipedia.org/wiki/Transport_Layer_Security#Security TO BE DONE… ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:4","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#4-tls-协议的逆向手段"},{"categories":["tech"],"content":" 5. 在边缘侧卸载 TLSTLS 加密是一个安全协议，工程上虽然有 Google 等公司力推所谓「零信任加密」方案，在所有通信场景下都应用 mTLS 等加密技术。但是为了成本与性能考量，绝大部分公司都选择仅在公网使用 HTTPS， 在可信内网场景下使用纯 HTTP。其做法就是在边缘网关层卸载 TLS 协议，再将内部的 HTTP 请求负载均衡到后端服务上。 所谓卸载 TLS 协议，就是指它对外提供 TLS 协议端口，但是使用 HTTP 等裸协议与上游服务通信。 在边缘侧卸载 HTTPS 的好处主要有： 内网环境都使用了纯 HTTP，性能更好，延迟更低，成本更低。 参见一篇讲 TLS 的好文分享，如果通过 CDN 在边缘节点卸载 TLS，然后使用纯 HTTP 回源，能显著降低请求延迟。 我本人就于 2022 年，在 AWS 上通过这个手段优化了一波广告业务 API 的延迟，广告收益有明显上涨。 ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:4:5","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#5-在边缘侧卸载-tls"},{"categories":["tech"],"content":" 三、其他推荐读物 内容会有些重叠，但各有新知 图解密码技术 - [日]结城浩 给工程师：关于证书（certificate）和公钥基础设施（PKI）的一切 有关 TLS/SSL 证书（我所知道的）的一切 - 卡瓦邦噶 HTTPS 隐私安全的一些实践 - Laisky ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:5:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#三其他推荐读物"},{"categories":["tech"],"content":" 四、参考 HTTPS 温故知新（三） —— 直观感受 TLS 握手流程(上) HTTPS 温故知新（五） —— TLS 中的密钥计算 A complete overview of SSL/TLS and its cryptographic system Certificates - Kubernetes Docs 证书选型和购买 - 阿里云文档 云原生安全破局｜如何管理周期越来越短的数字证书？ 另外两个关于 CN(Common Name) 和 SAN(Subject Altnative Name) 的问答: Can not get rid of net::ERR_CERT_COMMON_NAME_INVALID error in chrome with self-signed certificates SSL - How do Common Names (CN) and Subject Alternative Names (SAN) work together? 关于证书锁定/公钥锁定技术: Certificate and Public Key Pinning - OWASP Difference between certificate pinning and public key pinning ","date":"2022-03-14","objectID":"/posts/about-tls-cert/:6:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","HTTPS","TLS","SSL","OpenSSL","PKI","数字证书","证书","SSH","QUIC","HTTP/3","安全"],"title":"写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议","uri":"/posts/about-tls-cert/#四参考"},{"categories":["tech"],"content":" 本文部分内容翻译自 Practical-Cryptography-for-Developers-Book，笔者补充了密码学历史以及 openssl 命令示例，并重写了 RSA/ECC 算法原理、代码示例等内容。 这篇文章中会涉及到一些数论知识，本文不会详细介绍这些数学知识，可以在有疑惑的时候自行查找相关知识，或者选择跳过相关内容。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:0:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#"},{"categories":["tech"],"content":" 一、公钥密码学 / 非对称密码学在介绍非对称密钥加密方案和算法之前，我们首先要了解公钥密码学的概念。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:1:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#一公钥密码学--非对称密码学"},{"categories":["tech"],"content":" 密码学的历史从第一次世界大战、第二次世界大战到 1976 年这段时期密码的发展阶段，被称为「近代密码阶段」。在近代密码阶段，所有的密码系统都使用对称密码算法——使用相同的密钥进行加解密。当时使用的密码算法在拥有海量计算资源的现代人看来都是非常简单的，我们经常看到各种讲述一二战的谍战片，基本都包含破译电报的片段。 第一二次世界大战期间，无线电被广泛应用于军事通讯，围绕无线电通讯的加密破解攻防战极大地影响了战局。 公元20世纪初，第一次世界大战进行到关键时刻，英国破译密码的专门机构「40号房间」利用缴获的德国密码本破译了著名的「齐默尔曼电报」，其内容显示德国打算联合墨西哥对抗可能会参战的美国，这促使美国放弃中立对德宣战，从而彻底改变了一战的走势。 1943 年，美国从破译的日本电报中得知山本五十六将于 4 月 18 日乘中型轰炸机，由 6 架战斗机护航，到中途岛视察。美国总统罗斯福亲自做出决定截击山本，山本乘坐的飞机在去往中途岛的路上被美军击毁，战争天才山本五十六机毁人亡，日本海军从此一蹶不振。 此外，在二次世界大战中，美军将印第安纳瓦霍土著语言作为密码使用，并特别征募使用印第安纳瓦霍通信兵。在二次世界大战日美的太平洋战场上，美国海军军部让北墨西哥和亚历桑那印第安纳瓦霍族人使用纳瓦霍语进行情报传递。纳瓦霍语的语法、音调及词汇都极为独特，不为世人所知道，当时纳瓦霍族以外的美国人中，能听懂这种语言的也就一二十人。这是密码学和语言学的成功结合，纳瓦霍语密码成为历史上从未被破译的密码。 在 1976 年 Malcolm J. Williamson 公开发表了现在被称为「Diffie–Hellman 密钥交换，DHKE」的算法，并提出了「公钥密码学」的概念，这是密码学领域一项划时代的发明，它宣告了「近代密码阶段」的终结，是「现代密码学」的起点。 言归正传，对称密码算法的问题有两点： 「需要安全的通道进行密钥交换」，早期最常见的是面对面交换密钥 每个点对点通信都需要使用不同的密钥，密钥的管理会变得很困难 如果你需要跟 100 个朋友安全通信，你就要维护 100 个不同的对称密钥，而且还得确保它们不泄漏。 这会导致巨大的「密钥交换」跟「密钥保存与管理」的成本。「公钥密码学」最大的优势就是，它解决了这两个问题： 「公钥密码学」可以在不安全的信道上安全地进行密钥交换，第三方即使监听到通信过程，但是 （几乎）无法破解出密钥。 每个人只需要公开自己的公钥，就可以跟其他任何人安全地通信。 如果你需要跟 100 个朋友安全通信，你们只需要公开自己的公钥。发送消息时使用对方的公钥加密，接收消息时使用自己的私钥解密即可。 只有你自己的私钥需要保密，所有的公钥都可以公开，这就显著降低了密钥的维护成本。 因此公钥密码学成为了现代密码学的基石，而「公钥密码学」的诞生时间 1976 年被认为是现代密码学的开端。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:1:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#密码学的历史"},{"categories":["tech"],"content":" 公钥密码学的概念公钥密码系统的密钥始终以公钥 + 私钥对的形式出现，公钥密码系统提供数学框架和算法来生成公钥+ 私钥对。公钥通常与所有人共享，而私钥则保密。公钥密码系统在设计时就确保了在预期的算力下，几乎不可能从其公开的公钥逆向演算出对应的私钥。 公钥密码系统主要有三大用途：加密与解密、签名与验证、密钥交换。每种算法都需要使用到公钥和私钥，比如由公钥加密的消息只能由私钥解密，由私钥签名的消息需要用公钥验证。 由于加密解密、签名验证均需要两个不同的密钥，故「公钥密码学」也被称为「非对称密码学」。 比较著名的公钥密码系统有：RSA、ECC（椭圆曲线密码学）、ElGamal、Diffie-Hellman、ECDH、ECDSA 和 EdDSA。许多密码算法都是以这些密码系统为基础实现的，例如 RSA 签名、RSA 加密/解密、ECDH 密钥交换以及 ECDSA 和 EdDSA 签名。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:1:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#公钥密码学的概念"},{"categories":["tech"],"content":" 量子安全性 参考文档：https://en.wikipedia.org/wiki/Post-quantum_cryptography 目前流行的公钥密码系统基本都依赖于 IFP（整数分解问题）、DLP（离散对数问题）或者 ECDLP（椭圆曲线离散对数问题），这导致这些算法都是量子不安全（quantum-unsafe）的。 如果人类进入量子时代，IFP / DLP / ECDLP 的难度将大大降低，目前流行的 RSA、ECC、ElGamal、Diffie-Hellman、ECDH、ECDSA 和 EdDSA 等公钥密码算法都将被淘汰。 目前已经有一些量子安全的公钥密码系统问世，但是因为它们需要更长的密钥、更长的签名等原因，目前还未被广泛使用。 一些量子安全的公钥密码算法举例：NewHope、NTRU、GLYPH、BLISS、XMSS、Picnic 等， 有兴趣的可以自行搜索相关文档。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:1:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#量子安全性"},{"categories":["tech"],"content":" 二、非对称加密方案简介非对称加密要比对称加密复杂，有如下几个原因： 使用密钥对进行加解密，导致其算法更为复杂 只能加密/解密很短的消息 在 RSA 系统中，输入消息需要被转换为大整数（例如使用 OAEP 填充），然后才能被加密为密文。（密文实质上就是另一个大整数） 一些非对称密码系统（如 ECC）不直接提供加密能力，需要结合使用更复杂的方案才能实现加解密 此外，非对称密码比对称密码慢非常多。比如 RSA 加密比 AES 慢 1000 倍，跟 ChaCha20 就更没法比了。 为了解决上面提到的这些困难并支持加密任意长度的消息，现代密码学使用「非对称加密方案」来实现消息加解密。又因为「对称加密方案」具有速度快、支持加密任意长度消息等特性，「非对称加密方案」通常组合使用对称加密算法与非对称加密算法。比如「密钥封装机制 KEM（key encapsulation mechanisms)）」与「集成加密方案 IES（Integrated Encryption Scheme）」 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:2:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#二非对称加密方案简介"},{"categories":["tech"],"content":" 1. 密钥封装机制 KEM顾名思义，KEM 就是仅使用非对称加密算法加密另一个密钥，实际数据的加解密由该密钥完成。 密钥封装机制 KEM 的加密流程（使用公钥加密传输对称密钥）： 密钥封装机制 KEM 的解密流程（使用私钥解密出对称密钥，然后再使用这个对称密钥解密数据）： RSA-OAEP, RSA-KEM, ECIES-KEM 和 PSEC-KEM. 都是 KEM 加密方案。 密钥封装（Key encapsulation）与密钥包裹（Key wrapping）主要区别在于使用的是对称加密算法、还是非对称加密算法： 密钥封装（Key encapsulation）指使用非对称密码算法的公钥加密另一个密钥。 密钥包裹（Key wrapping）指使用对称密码算法加密另一个密钥。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:2:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#1-密钥封装机制-kem"},{"categories":["tech"],"content":" 1. 密钥封装机制 KEM顾名思义，KEM 就是仅使用非对称加密算法加密另一个密钥，实际数据的加解密由该密钥完成。 密钥封装机制 KEM 的加密流程（使用公钥加密传输对称密钥）： 密钥封装机制 KEM 的解密流程（使用私钥解密出对称密钥，然后再使用这个对称密钥解密数据）： RSA-OAEP, RSA-KEM, ECIES-KEM 和 PSEC-KEM. 都是 KEM 加密方案。 密钥封装（Key encapsulation）与密钥包裹（Key wrapping）主要区别在于使用的是对称加密算法、还是非对称加密算法： 密钥封装（Key encapsulation）指使用非对称密码算法的公钥加密另一个密钥。 密钥包裹（Key wrapping）指使用对称密码算法加密另一个密钥。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:2:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#密钥封装key-encapsulation与密钥包裹key-wrapping"},{"categories":["tech"],"content":" 2. 集成加密方案 IES集成加密方案 (IES) 在密钥封装机制（KEM）的基础上，添加了密钥派生算法 KDF、消息认证算法 MAC 等其他密码学算法以达成更高的安全性。 在 IES 方案中，非对称算法（如 RSA 或 ECC）跟 KEM 一样，都是用于加密或封装对称密钥，然后通过对称密钥（如 AES 或 Chacha20）来加密输入消息。 DLIES（离散对数集成加密方案）和 ECIES（椭圆曲线集成加密方案）都是 IES 方案。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:2:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#2-集成加密方案-ies"},{"categories":["tech"],"content":" 三、RSA 密码系统RSA 密码系统是最早的公钥密码系统之一，它基于RSA 问题和整数分解问题 （IFP）的计算难度。RSA 算法以其作者（Rivest–Shamir–Adleman）的首字母命名。 RSA 算法在计算机密码学的早期被广泛使用，至今仍然是数字世界应用最广泛的密码算法。但是随着 ECC 密码学的发展，ECC 正在非对称密码系统中慢慢占据主导地位，因为它比 RSA 具有更高的安全性和更短的密钥长度。 RSA 算法提供如下几种功能： 密钥对生成：生成随机私钥（通常大小为 1024-4096 位）和相应的公钥。 加密解密：使用公钥加密消息（消息要先转换为 [0…key_length] 范围内的整数），然后使用密钥解密。 数字签名：签署消息（使用私钥）和验证消息签名（使用公钥）。 数字签名实际上是通过 Hash 算法 + 加密解密功能实现的。后面会介绍到，它与一般加解密流程的区别，在于数字签名使用私钥加密，再使用公钥解密。 密钥交换：安全地传输密钥，用于以后的加密通信。 RSA 可以使用不同长度的密钥：1024、2048、3072、4096、8129、16384 甚至更多位。目前 3072 位及以上的密钥长度被认为是安全的，曾经大量使用的 2048 位 RSA 现在被破解的风险在不断提升，已经不推荐使用了。 更长的密钥提供更高的安全性，但会消耗更多的计算时间，同时签名也会变得更长，因此需要在安全性和速度之间进行权衡。非常长的 RSA 密钥（例如 50000 位或 65536 位）对于实际使用可能太慢，例如密钥生成可能需要几分钟到几个小时。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:3:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#三rsa-密码系统"},{"categories":["tech"],"content":" RSA 密钥对生成RSA 密钥对的生成跟我们在本系列文章的第 5 篇介绍的「DHKE 密钥交换算法」会有些类似，但是要更复杂一点。 首先看下我们怎么使用 openssl 生成一个 1024 位的 RSA 密钥对（仅用做演示，实际应用中建议 3072 位）： OpenSSL 是目前使用最广泛的网络加密算法库，支持非常多流行的现代密码学算法，几乎所有操作系统都会内置 openssl. text # 生成 1024 位的 RSA 私钥 ❯ openssl genrsa -out rsa-private-key.pem 1024 Generating RSA private key, 1024 bit long modulus .................+++ .....+++ e is 65537 (0x10001) # 使用私钥生成对应的公钥文件 ❯ openssl rsa -in rsa-private-key.pem -pubout -out rsa-public-key.pem writing RSA key # 查看私钥内容 ❯ cat rsa-private-key.pem -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQDNE8QZLJZXREOeWZ2ilAzGC4Kjq/PfsFzrXGj8g3IaS4/J3JrB o3qEq/k9XoRzOmNPyvWCj2FAY7A099d7qX4ztthBpUM2ePDIYDvhL0EpfQqbhe+Q aagcFpuKTshGR2wBjH0Cl1/WxJkfIUMmWYU+m4iKLw9KfLX6BjmSgWB6HQIDAQAB AoGADb5NXgKG8MI6ZdpLniGd2Yfb8WwMo+kF0SAYSRPmCa0WrciC9ocmJs3/ngU/ ixlWnnpTibRiKBaGMIaLglYRhvbvibUo8PH4woIidTho2e6swF2aqILk6YFJDpxX FCFdbXM4Cm2MqbD4VtmhCYqbvuiyEUci83YrRP0jJGNt0GECQQDyZgdi8JlFQFH8 1QRHjLN57v5bHQamv7Qb77hlbdbg1wTYO+H8tsOB181TEHA7uN8hxkzyYZy+goRx n0hvJcQXAkEA2JWhCb7oG1eal1aUdgofxhlWnkoFeWHay2zgDWSqmGKyDt0Cb1jq XTdN9dchnqfptWN2/QPLDgM+/9g39/zv6wJATC1sXNeoE29nVMHNGn9JWCSXoyK4 GGdevvjTRm0Cfp6UUzBekQEO6Btd16Du5JXw6bhcLkAm9mgmH18jcGq5+QJBALnr aDv3d0PRZdE372WMt03UfniOzjgueiVaJtMYcSEyx+reabKvvy+ZxACfVirdtU+S PJhhYzN6MeBp+VGV/VUCQBXz0LyM08roWi6DiaRwJIbYx+WCKEOGXQ9QsZND+sGr pOpugr3mcUge5dcZGKtsOUx2xRVmg88nSWMQVkTlsjQ= -----END RSA PRIVATE KEY----- # 查看私钥的详细参数 ❯ openssl rsa -noout -text -in rsa-private-key.pem Private-Key: (1024 bit) modulus: 00:cd:13:c4:19:2c:96:57:44:43:9e:59:9d:a2:94: 0c:c6:0b:82:a3:ab:f3:df:b0:5c:eb:5c:68:fc:83: 72:1a:4b:8f:c9:dc:9a:c1:a3:7a:84:ab:f9:3d:5e: 84:73:3a:63:4f:ca:f5:82:8f:61:40:63:b0:34:f7: d7:7b:a9:7e:33:b6:d8:41:a5:43:36:78:f0:c8:60: 3b:e1:2f:41:29:7d:0a:9b:85:ef:90:69:a8:1c:16: 9b:8a:4e:c8:46:47:6c:01:8c:7d:02:97:5f:d6:c4: 99:1f:21:43:26:59:85:3e:9b:88:8a:2f:0f:4a:7c: b5:fa:06:39:92:81:60:7a:1d publicExponent: 65537 (0x10001) privateExponent: 0d:be:4d:5e:02:86:f0:c2:3a:65:da:4b:9e:21:9d: d9:87:db:f1:6c:0c:a3:e9:05:d1:20:18:49:13:e6: 09:ad:16:ad:c8:82:f6:87:26:26:cd:ff:9e:05:3f: 8b:19:56:9e:7a:53:89:b4:62:28:16:86:30:86:8b: 82:56:11:86:f6:ef:89:b5:28:f0:f1:f8:c2:82:22: 75:38:68:d9:ee:ac:c0:5d:9a:a8:82:e4:e9:81:49: 0e:9c:57:14:21:5d:6d:73:38:0a:6d:8c:a9:b0:f8: 56:d9:a1:09:8a:9b:be:e8:b2:11:47:22:f3:76:2b: 44:fd:23:24:63:6d:d0:61 prime1: 00:f2:66:07:62:f0:99:45:40:51:fc:d5:04:47:8c: b3:79:ee:fe:5b:1d:06:a6:bf:b4:1b:ef:b8:65:6d: d6:e0:d7:04:d8:3b:e1:fc:b6:c3:81:d7:cd:53:10: 70:3b:b8:df:21:c6:4c:f2:61:9c:be:82:84:71:9f: 48:6f:25:c4:17 prime2: 00:d8:95:a1:09:be:e8:1b:57:9a:97:56:94:76:0a: 1f:c6:19:56:9e:4a:05:79:61:da:cb:6c:e0:0d:64: aa:98:62:b2:0e:dd:02:6f:58:ea:5d:37:4d:f5:d7: 21:9e:a7:e9:b5:63:76:fd:03:cb:0e:03:3e:ff:d8: 37:f7:fc:ef:eb exponent1: 4c:2d:6c:5c:d7:a8:13:6f:67:54:c1:cd:1a:7f:49: 58:24:97:a3:22:b8:18:67:5e:be:f8:d3:46:6d:02: 7e:9e:94:53:30:5e:91:01:0e:e8:1b:5d:d7:a0:ee: e4:95:f0:e9:b8:5c:2e:40:26:f6:68:26:1f:5f:23: 70:6a:b9:f9 exponent2: 00:b9:eb:68:3b:f7:77:43:d1:65:d1:37:ef:65:8c: b7:4d:d4:7e:78:8e:ce:38:2e:7a:25:5a:26:d3:18: 71:21:32:c7:ea:de:69:b2:af:bf:2f:99:c4:00:9f: 56:2a:dd:b5:4f:92:3c:98:61:63:33:7a:31:e0:69: f9:51:95:fd:55 coefficient: 15:f3:d0:bc:8c:d3:ca:e8:5a:2e:83:89:a4:70:24: 86:d8:c7:e5:82:28:43:86:5d:0f:50:b1:93:43:fa: c1:ab:a4:ea:6e:82:bd:e6:71:48:1e:e5:d7:19:18: ab:6c:39:4c:76:c5:15:66:83:cf:27:49:63:10:56: 44:e5:b2:34 # 查看公钥内容 ❯ cat rsa-public-key.pem -----BEGIN PUBLIC KEY----- MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDNE8QZLJZXREOeWZ2ilAzGC4Kj q/PfsFzrXGj8g3IaS4/J3JrBo3qEq/k9XoRzOmNPyvWCj2FAY7A099d7qX4ztthB pUM2ePDIYDvhL0EpfQqbhe+QaagcFpuKTshGR2wBjH0Cl1/WxJkfIUMmWYU+m4iK Lw9KfLX6BjmSgWB6HQIDAQAB -----END PUBLIC KEY----- # 查看公钥的参数 ❯ openssl rsa -noout -text -pubin -in rsa-public-key.pem Public-Key: (1024 bit) Modulus: 00:cd:13:c4:19:2c:96:57:44:43:9e:59:9d:a2:94: 0c:c6:0b:82:a3:ab:f3:df:b0:5c:eb:5c:68:fc:83: 72:1a:4b:8f:c9:dc:9a:c1:a3:7a:84:ab:f9:3d:5e: 84:73:3a:63:4f:ca:f5:82:8f:61:40:63:b0:34:f7: d7:7b:a9:7e:33:b6:d8:41:a5:43:36:78:f0:c8:60: 3b:","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:3:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#rsa-密钥对生成"},{"categories":["tech"],"content":" RSA 加密与解密RSA 加密算法，一次只能加密一个小于 $n$ 的非负整数，假设明文为整数 $msg$，加密算法如下： $$ \\text{encryptedMsg} = msg^e \\mod n $$ 通常的手段是，先使用EAOP 将被加密消息编码成一个个符合条件的整数，再使用上述公式一个个加密。 解密的方法，就是对被每一段加密的数据 $encryptedMsg$，进行如下运算： $$ \\text{decryptedMsg} = \\text{encryptedMsg}^d \\mod n $$ RSA 解密运算的证明 这里的证明需要用到一些数论知识，觉得不容易理解的话，建议自行查找相关资料。 证明流程如下： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026\\text{encryptedMsg}^d \u0026\\mod n \\\\ \u0026= \u0026{(msg^e \\mod n)}^d \u0026\\mod n \\\\ \u0026= \u0026{msg^{ed}} \u0026\\mod n \\\\ \u0026= \u0026{msg^{ed}} \u0026\\mod {pq} \\end{alignedat} $$ 接下来将下面两个等式代入上述计算中： 我们在前面的「密钥对生成」一节中有给出等式：$ed = 1 + (p-1)(q-1) \\cdot k$ 因为 $0 \\le msg \\lt n$ 以及 $n = pq$，有 $msg \\mod pq = msg$ 这样就得到： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026{msg^{ed}} \u0026\\mod {pq} \\\\ \u0026= \u0026{(msg \\mod pq) \\cdot (msg^{ed-1} \\mod pq)} \u0026\\mod {pq} \\\\ \u0026= \u0026{msg \\cdot (msg^{(p-1)(q-1) \\cdot k} \\mod pq)} \u0026\\mod {pq} \\end{alignedat} $$ 又有费马小定理指出，在 $a$ 为整数，$p$ 为质数的情况下，有同余等式 $$a^{p-1} \\equiv 1 {\\pmod p}$$ 因为我们的模数 $n=pq$ 并不是质数，不能直接利用费马小定理给出的同余公式。但是 $p$, $q$ 两数都为质数，我们可以分别计算方程 对 $p$ 以及 $q$ 取模的结果，然后再根据中国剩余定理得出通解，也就得到我们需要的结果。 对于模 $p$ 的情况，计算方法如下： 当 $msg = 0 \\mod p$ 时，${msg^{ed}} \\mod p = 0 \\equiv msg \\pmod p$ 当 $msg \\ne 0 \\mod p$ 时，利用费马小定理，有 $$ \\begin{alignedat}{2} msg^{ed} \u0026= \u0026{msg \\cdot (msg^{(p-1)(q-1) \\cdot k} \\mod p)} \u0026\\pmod {p} \\\\ \u0026= \u0026msg \\cdot (msg^{(p-1)} \\mod p)^{(q-1) \\cdot k} \u0026\\pmod p \\\\ \u0026= \u0026msg \\cdot 1^{(q-1) \\cdot k} \u0026\\pmod p \\\\ \u0026\\equiv \u0026msg \\pmod p \\end{alignedat} $$ 同理，对模 $q$ 的情况，也能得到等式 $$msg^{ed} \\equiv msg \\pmod q$$ 有了上面两个结果，根据中国剩余定理，就能得到 $$msg^{ed} \\equiv msg \\pmod {pq}$$ 现在再接续前面的计算： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026{msg^{ed}} \u0026\\pmod {pq} \\\\ \u0026= \u0026msg \u0026\\pmod {pq} \\\\ \u0026= \u0026msg \\end{alignedat} $$ 这样就证明了，解密操作得到的就是原始信息。 因为非对称加解密非常慢，对于较大的文件，通常会分成两步加密来提升性能：首先用使用对称加密算法来加密数据，再使用 RSA 等非对称加密算法加密上一步用到的「对称密钥」。 下面我们用 Python 来验证下 RSA 算法的加解密流程： python # pip install cryptography==36.0.1 from pathlib import Path from cryptography.hazmat.primitives import serialization # 私钥 key_path = Path(\"./rsa-private-key.pem\") private_key = serialization.load_pem_private_key( key_path.read_bytes(), password=None, ) private = private_key.private_numbers() public = private_key.public_key().public_numbers() d = private.d # 公钥 n = public.n e = public.e def int_to_bytes(x: int) -\u003e bytes: return x.to_bytes((x.bit_length() + 7) // 8, 'big') def int_from_bytes(xbytes: bytes) -\u003e int: return int.from_bytes(xbytes, 'big') def fast_power_modular(b: int, p: int, m: int): \"\"\" 快速模幂运算：b^p % m 复杂度： O(log p) 因为 RSA 的底数跟指数都非常大，如果先进行幂运算，最后再取模，计算结果会越来越大，导致速度非常非常慢 根据模幂运算的性质 b^(ab) % m = (b^a % m)^b % m, 可以通过边进行幂运算边取模，极大地提升计算速度 \"\"\" res = 1 while p: if p \u0026 0x1: res *= b b = b ** 2 % m p \u003e\u003e= 1 return res % m # 明文 original_msg = b\"an example\" print(f\"{original_msg=}\") # 加密 msg_int = int_from_bytes(original_msg) encrypt_int = msg_int ** e % n encrypt_msg = int_to_bytes(encrypt_int) print(f\"{encrypt_msg=}\") # 解密 # decrypt_int = encrypt_int ** d % n # 因为 d 非常大，直接使用公式计算会非常非常慢，所以不能这么算 decrypt_int = fast_power_modular(encrypt_int, d, n) decrypt_msg = int_to_bytes(decrypt_int) print(f\"{decrypt_msg=}\") # 应该与原信息完全一致 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:3:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#rsa-加密与解密"},{"categories":["tech"],"content":" RSA 加密与解密RSA 加密算法，一次只能加密一个小于 $n$ 的非负整数，假设明文为整数 $msg$，加密算法如下： $$ \\text{encryptedMsg} = msg^e \\mod n $$ 通常的手段是，先使用EAOP 将被加密消息编码成一个个符合条件的整数，再使用上述公式一个个加密。 解密的方法，就是对被每一段加密的数据 $encryptedMsg$，进行如下运算： $$ \\text{decryptedMsg} = \\text{encryptedMsg}^d \\mod n $$ RSA 解密运算的证明 这里的证明需要用到一些数论知识，觉得不容易理解的话，建议自行查找相关资料。 证明流程如下： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026\\text{encryptedMsg}^d \u0026\\mod n \\\\ \u0026= \u0026{(msg^e \\mod n)}^d \u0026\\mod n \\\\ \u0026= \u0026{msg^{ed}} \u0026\\mod n \\\\ \u0026= \u0026{msg^{ed}} \u0026\\mod {pq} \\end{alignedat} $$ 接下来将下面两个等式代入上述计算中： 我们在前面的「密钥对生成」一节中有给出等式：$ed = 1 + (p-1)(q-1) \\cdot k$ 因为 $0 \\le msg \\lt n$ 以及 $n = pq$，有 $msg \\mod pq = msg$ 这样就得到： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026{msg^{ed}} \u0026\\mod {pq} \\\\ \u0026= \u0026{(msg \\mod pq) \\cdot (msg^{ed-1} \\mod pq)} \u0026\\mod {pq} \\\\ \u0026= \u0026{msg \\cdot (msg^{(p-1)(q-1) \\cdot k} \\mod pq)} \u0026\\mod {pq} \\end{alignedat} $$ 又有费马小定理指出，在 $a$ 为整数，$p$ 为质数的情况下，有同余等式 $$a^{p-1} \\equiv 1 {\\pmod p}$$ 因为我们的模数 $n=pq$ 并不是质数，不能直接利用费马小定理给出的同余公式。但是 $p$, $q$ 两数都为质数，我们可以分别计算方程 对 $p$ 以及 $q$ 取模的结果，然后再根据中国剩余定理得出通解，也就得到我们需要的结果。 对于模 $p$ 的情况，计算方法如下： 当 $msg = 0 \\mod p$ 时，${msg^{ed}} \\mod p = 0 \\equiv msg \\pmod p$ 当 $msg \\ne 0 \\mod p$ 时，利用费马小定理，有 $$ \\begin{alignedat}{2} msg^{ed} \u0026= \u0026{msg \\cdot (msg^{(p-1)(q-1) \\cdot k} \\mod p)} \u0026\\pmod {p} \\\\ \u0026= \u0026msg \\cdot (msg^{(p-1)} \\mod p)^{(q-1) \\cdot k} \u0026\\pmod p \\\\ \u0026= \u0026msg \\cdot 1^{(q-1) \\cdot k} \u0026\\pmod p \\\\ \u0026\\equiv \u0026msg \\pmod p \\end{alignedat} $$ 同理，对模 $q$ 的情况，也能得到等式 $$msg^{ed} \\equiv msg \\pmod q$$ 有了上面两个结果，根据中国剩余定理，就能得到 $$msg^{ed} \\equiv msg \\pmod {pq}$$ 现在再接续前面的计算： $$ \\begin{alignedat}{2} \\text{decryptedMsg} \u0026= \u0026{msg^{ed}} \u0026\\pmod {pq} \\\\ \u0026= \u0026msg \u0026\\pmod {pq} \\\\ \u0026= \u0026msg \\end{alignedat} $$ 这样就证明了，解密操作得到的就是原始信息。 因为非对称加解密非常慢，对于较大的文件，通常会分成两步加密来提升性能：首先用使用对称加密算法来加密数据，再使用 RSA 等非对称加密算法加密上一步用到的「对称密钥」。 下面我们用 Python 来验证下 RSA 算法的加解密流程： python # pip install cryptography==36.0.1 from pathlib import Path from cryptography.hazmat.primitives import serialization # 私钥 key_path = Path(\"./rsa-private-key.pem\") private_key = serialization.load_pem_private_key( key_path.read_bytes(), password=None, ) private = private_key.private_numbers() public = private_key.public_key().public_numbers() d = private.d # 公钥 n = public.n e = public.e def int_to_bytes(x: int) -\u003e bytes: return x.to_bytes((x.bit_length() + 7) // 8, 'big') def int_from_bytes(xbytes: bytes) -\u003e int: return int.from_bytes(xbytes, 'big') def fast_power_modular(b: int, p: int, m: int): \"\"\" 快速模幂运算：b^p % m 复杂度： O(log p) 因为 RSA 的底数跟指数都非常大，如果先进行幂运算，最后再取模，计算结果会越来越大，导致速度非常非常慢 根据模幂运算的性质 b^(ab) % m = (b^a % m)^b % m, 可以通过边进行幂运算边取模，极大地提升计算速度 \"\"\" res = 1 while p: if p \u0026 0x1: res *= b b = b ** 2 % m p \u003e\u003e= 1 return res % m # 明文 original_msg = b\"an example\" print(f\"{original_msg=}\") # 加密 msg_int = int_from_bytes(original_msg) encrypt_int = msg_int ** e % n encrypt_msg = int_to_bytes(encrypt_int) print(f\"{encrypt_msg=}\") # 解密 # decrypt_int = encrypt_int ** d % n # 因为 d 非常大，直接使用公式计算会非常非常慢，所以不能这么算 decrypt_int = fast_power_modular(encrypt_int, d, n) decrypt_msg = int_to_bytes(decrypt_int) print(f\"{decrypt_msg=}\") # 应该与原信息完全一致 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:3:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#rsa-解密运算的证明"},{"categories":["tech"],"content":" RSA 数字签名前面证明了可以使用公钥加密，再使用私钥解密。 实际上从上面的证明也可以看出来，顺序是完全可逆的，先使用私钥加密，再使用公钥解密也完全是可行的。这种运算被我们用在数字签名算法中。 数字签名的方法为： 首先计算原始数据的 Hash 值，比如 SHA256 使用私钥对计算出的 Hash 值进行加密，得到数字签名 其他人使用公开的公钥进行解密出 Hash 值，再对原始数据计算 Hash 值对比，如果一致，就说明数据未被篡改 Python 演示： python # pip install cryptography==36.0.1 from hashlib import sha512 from pathlib import Path from cryptography.hazmat.primitives import serialization key_path = Path(\"./rsa-private-key.pem\") private_key = serialization.load_pem_private_key( key_path.read_bytes(), password=None, ) private = private_key.private_numbers() public = private_key.public_key().public_numbers() d = private.d n = public.n e = public.e # RSA sign the message msg = b'A message for signing' hash = int.from_bytes(sha512(msg).digest(), byteorder='big') signature = pow(hash, d, n) print(\"Signature:\", hex(signature)) # RSA verify signature msg = b'A message for signing' hash = int.from_bytes(sha512(msg).digest(), byteorder='big') hashFromSignature = pow(signature, e, n) print(\"Signature valid:\", hash == hashFromSignature) ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:3:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#rsa-数字签名"},{"categories":["tech"],"content":" 四、ECC 密码系统 ECC 椭圆曲线密码学，于 1985 年被首次提出，并于 2004 年开始被广泛应用。ECC 被认为是 RSA 的继任者，新一代的非对称加密算法。 其最大的特点在于相同密码强度下，ECC 的密钥和签名的大小都要显著低于 RSA. 256bits 的 ECC 密钥，安全性与 3072bits 的 RSA 密钥安全性相当。 其次 ECC 的密钥对生成、密钥交换与签名算法的速度都要比 RSA 快。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#四ecc-密码系统"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点查看图形的变化情况：https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 椭圆曲线上的运算跟我们所熟知的实数域运算不太一样，它在现实生活中并无实际意义，但是它的一些性质使其很适合被应用在密码学中。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线与椭圆曲线没有第三个交点，前面定义的加法规则在这种情况下失效了。为了解决这个问题，我们假设这条直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。其中最简单的算法是「double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 Add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 Add，间接可以推算出 $k$. 因此这个算法会有计时攻击的风险。基于「double-and-add」修改的蒙哥马利阶梯 （Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y*{2}-y*{1}} {x*{2}-x*{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#椭圆曲线的数学原理简介"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点查看图形的变化情况：https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 椭圆曲线上的运算跟我们所熟知的实数域运算不太一样，它在现实生活中并无实际意义，但是它的一些性质使其很适合被应用在密码学中。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线与椭圆曲线没有第三个交点，前面定义的加法规则在这种情况下失效了。为了解决这个问题，我们假设这条直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。其中最简单的算法是「double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 Add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 Add，间接可以推算出 $k$. 因此这个算法会有计时攻击的风险。基于「double-and-add」修改的蒙哥马利阶梯 （Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y*{2}-y*{1}} {x*{2}-x*{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#椭圆曲线上的运算"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点查看图形的变化情况：https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 椭圆曲线上的运算跟我们所熟知的实数域运算不太一样，它在现实生活中并无实际意义，但是它的一些性质使其很适合被应用在密码学中。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线与椭圆曲线没有第三个交点，前面定义的加法规则在这种情况下失效了。为了解决这个问题，我们假设这条直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。其中最简单的算法是「double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 Add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 Add，间接可以推算出 $k$. 因此这个算法会有计时攻击的风险。基于「double-and-add」修改的蒙哥马利阶梯 （Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y*{2}-y*{1}} {x*{2}-x*{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#1-加法与负元"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点查看图形的变化情况：https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 椭圆曲线上的运算跟我们所熟知的实数域运算不太一样，它在现实生活中并无实际意义，但是它的一些性质使其很适合被应用在密码学中。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线与椭圆曲线没有第三个交点，前面定义的加法规则在这种情况下失效了。为了解决这个问题，我们假设这条直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。其中最简单的算法是「double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 Add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 Add，间接可以推算出 $k$. 因此这个算法会有计时攻击的风险。基于「double-and-add」修改的蒙哥马利阶梯 （Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y*{2}-y*{1}} {x*{2}-x*{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#2-二倍运算"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点查看图形的变化情况：https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 椭圆曲线上的运算跟我们所熟知的实数域运算不太一样，它在现实生活中并无实际意义，但是它的一些性质使其很适合被应用在密码学中。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线与椭圆曲线没有第三个交点，前面定义的加法规则在这种情况下失效了。为了解决这个问题，我们假设这条直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。其中最简单的算法是「double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 Add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 Add，间接可以推算出 $k$. 因此这个算法会有计时攻击的风险。基于「double-and-add」修改的蒙哥马利阶梯 （Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y*{2}-y*{1}} {x*{2}-x*{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#3-无穷远点"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点查看图形的变化情况：https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 椭圆曲线上的运算跟我们所熟知的实数域运算不太一样，它在现实生活中并无实际意义，但是它的一些性质使其很适合被应用在密码学中。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线与椭圆曲线没有第三个交点，前面定义的加法规则在这种情况下失效了。为了解决这个问题，我们假设这条直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。其中最简单的算法是「double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 Add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 Add，间接可以推算出 $k$. 因此这个算法会有计时攻击的风险。基于「double-and-add」修改的蒙哥马利阶梯 （Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y*{2}-y*{1}} {x*{2}-x*{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#4-k-倍运算"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点查看图形的变化情况：https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 椭圆曲线上的运算跟我们所熟知的实数域运算不太一样，它在现实生活中并无实际意义，但是它的一些性质使其很适合被应用在密码学中。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线与椭圆曲线没有第三个交点，前面定义的加法规则在这种情况下失效了。为了解决这个问题，我们假设这条直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。其中最简单的算法是「double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 Add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 Add，间接可以推算出 $k$. 因此这个算法会有计时攻击的风险。基于「double-and-add」修改的蒙哥马利阶梯 （Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y*{2}-y*{1}} {x*{2}-x*{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#5-有限域上的椭圆曲线"},{"categories":["tech"],"content":" 椭圆曲线的数学原理简介在数学中，椭圆曲线（Elliptic Curves）是一种平面曲线，由如下方程定义的点的集合组成（$A-J$ 均为常数）： $$ Ax^3 + Bx^2y + Cxy^2 + Dy^3 + Ex^2 + Fxy + Gy^2 + Hx + Iy + J = 0 $$ 而 ECC 只使用了其中很简单的一个子集（$a, b$ 均为常数）： $$ y^2 = x^3 + ax + b $$ 比如著名的 NIST 曲线 secp256k1 就是基于如下椭圆曲线方程： $$ y^2 = x^3 + 7 $$ 椭圆曲线大概长这么个形状： 椭圆曲线跟椭圆的关系，就犹如雷锋跟雷峰塔、Java 跟 JavaScript… 你可以通过如下网站手动调整 $a$ 与 $b$ 的值，拖动曲线的交点查看图形的变化情况：https://www.desmos.com/calculator/ialhd71we3?lang=zh-CN 椭圆曲线上的运算数学家在椭圆曲线上定义了一些运算规则，ECC 就依赖于这些规则，下面简单介绍下我们用得到的部分。 椭圆曲线上的运算跟我们所熟知的实数域运算不太一样，它在现实生活中并无实际意义，但是它的一些性质使其很适合被应用在密码学中。 1. 加法与负元对于曲线上的任意两点 $A$ 与 $B$，我们定义过 $A, B$ 的直线与曲线的交点为 $-(A+B)$，而 $-(A+B)$ 相对于 x 轴的对称点即为 $A+B$: 上述描述一是定义了椭圆曲线的加法规则，二是定义了椭圆曲线上的负元运算。 2. 二倍运算在加法规则中，如果 $A=B$，我们定义曲线在 $A$ 点的切线与曲线的交点为 $-2A$，于是得到二倍运算的规则： 3. 无穷远点对于 $(-A) + A$ 这种一个值与其负元本身相加的情况，我们会发现过这两点的直线与椭圆曲线没有第三个交点，前面定义的加法规则在这种情况下失效了。为了解决这个问题，我们假设这条直线与椭圆曲线相交于无穷远点 $O_{\\infty}$. 4. k 倍运算我们在前面已经定义了椭圆曲线上的加法运算、二倍运算以及无穷远点，有了这三个概念，我们就能定义k 倍运算 了。 K 倍运算最简单的计算方法，就是不断地进行加法运算，但是也有许多更高效的算法。其中最简单的算法是「double-and-add」，它要求首先 $k$ 拆分成如下形式 $$ k = k_{0}+2k_{1}+2^{2}k_{2}+\\cdots +2^{m}k_{m} \\\\ \\text{其中} k_{0}~..~k_{m}\\in {0,1},m=\\lfloor \\log _{2}{k}\\rfloor $$ 然后再迭代计算其中各项的值，它的运算复杂度为 $log_{2}(k)$. 因 Double 和 Add 的执行时间不同，根据执行时间就可以知道是执行 Double 还是 Add，间接可以推算出 $k$. 因此这个算法会有计时攻击的风险。基于「double-and-add」修改的蒙哥马利阶梯 （Montgomery Ladder）是可以避免计时分析的作法，这里就不详细介绍了。 5. 有限域上的椭圆曲线椭圆曲线是连续且无限的，而计算机却更擅长处理离散的、存在上限的整数，因此 ECC 使用「有限域上的椭圆曲线」进行计算。 「有限域（也被称作 Galois Filed, 缩写为 GF）」顾名思义，就是指只有有限个数值的域。 有限域上的椭圆曲线方程，通过取模的方式将曲线上的所有值都映射到同一个有限域内。有限域 $\\mathbb {F} _{p}$ 上的 EC 椭圆曲线方程为： $$ y^2 = x^3 + ax + b (\\mod p), 0 \\le x \\le p $$ 目前主要有两种有限域在 ECC 中被广泛应用： 以素数为模的整数域: $\\mathbb {F} _{p}$ 在通用处理器上计算很快 以 2 的幂为模的整数域: $\\mathbb {F} _{2^{m}}$ 当使用专用硬件时，计算速度很快 通过限制 x 为整数，并使用取模进行了映射后，椭圆曲线的形状已经面目全非了，它的加减法也不再具有几何意义。但是它的一些特性仍然跟椭圆曲线很类似，各种公式基本加个 $\\mod p$ 就变成了它的有限域版本： 无穷远点 $O_{\\infty}$ 是零元，$O_{\\infty} + O_{\\infty} = O_{\\infty}$，$O_{\\infty} + P = P$ $P_{x, y}$ 的负元为 $P_{x, -y}$,，并且有 $P + (-P) = O_{\\infty}$ $P * 0 = O_{\\infty}$ 如果 $P_{x1, y1} + Q_{x2, y2} = R_{x3, y3}$，则其坐标有如下关系 $x3 = (k^2 - x1 - x2) \\mod p$ $y3 = (k(x1 - x3) - y1) \\mod p$ 斜率 $k$ 的计算 如果 $P=Q$，则 $k=\\dfrac {3x^{2}+a} {2y_{1}}$ 否则 $k=\\dfrac {y*{2}-y*{1}} {x*{2}-x*{1}} $ ECDLP 椭圆曲线离散对数问题前面已经介绍了椭圆曲线上的 k 倍运算 及相关的高效算法，但是我们还没有涉及到除法。 椭圆曲线上的除法是一个尚未被解决的难题——「ECDLP 椭圆曲线离散对数问题」： 已知 $kG$ 与基点 $G$，求整数 $k$ 的值。 目前并没有有效的手段可以快速计算出 $k$ 的值。比较直观的方法应该是从基点 $G$ 开始不断进行加法运算，直到结果与 $kG$ 相等。 目前已知的 ECDLP 最快的解法所需步骤为 $\\sqrt{k}$，而 k 倍运算高效算法前面已经介绍过了，所需步骤为 $log_2(k)$。在 $k$ 非常大的情况下，它们的计算用时将会有指数级的差距。 椭圆曲线上的 k 倍运算与素数上的幂运算很类似，因此 ECC 底层的数学难题 ECDLP 与 RSA 的离散对数问题 DLP 也有很大相似性。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecdlp-椭圆曲线离散对数问题"},{"categories":["tech"],"content":" ECC 密钥对生成首先，跟 RSA 一样，让我们先看下怎么使用 openssl 生成一个使用 prime256v1 曲线的 ECC 密钥对： shell # 列出 openssl 支持的所有曲线名称 openssl ecparam -list_curves # 生成 ec 算法的私钥，使用 prime256v1 算法，密钥长度 256 位。（强度大于 2048 位的 RSA 密钥） openssl ecparam -genkey -name prime256v1 -out ecc-private-key.pem # 通过密钥生成公钥 openssl ec -in ecc-private-key.pem -pubout -out ecc-public-key.pem # 查看私钥内容 ❯ cat ecc-private-key.pem -----BEGIN EC PARAMETERS----- BggqhkjOPQMBBw== -----END EC PARAMETERS----- -----BEGIN EC PRIVATE KEY----- MHcCAQEEIGm3wT/m4gDaoJGKfAHDXV2BVtdyb/aPTITJR5B6KVEtoAoGCCqGSM49 AwEHoUQDQgAE5IEIorw0WU5+om/UgfyYSKosiGO6Hpe8hxkqL5GUVPyu4LJkfw/e 99zhNJatliZ1Az/yCKww5KrXC8bQ9wGQvw== -----END EC PRIVATE KEY----- # 查看私钥的详细参数 ❯ openssl ec -noout -text -in ecc-private-key.pem read EC key Private-Key: (256 bit) priv: 69:b7:c1:3f:e6:e2:00:da:a0:91:8a:7c:01:c3:5d: 5d:81:56:d7:72:6f:f6:8f:4c:84:c9:47:90:7a:29: 51:2d pub: 04:e4:81:08:a2:bc:34:59:4e:7e:a2:6f:d4:81:fc: 98:48:aa:2c:88:63:ba:1e:97:bc:87:19:2a:2f:91: 94:54:fc:ae:e0:b2:64:7f:0f:de:f7:dc:e1:34:96: ad:96:26:75:03:3f:f2:08:ac:30:e4:aa:d7:0b:c6: d0:f7:01:90:bf ASN1 OID: prime256v1 NIST CURVE: P-256 # 查看公钥内容 ❯ cat ecc-public-key.pem -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE5IEIorw0WU5+om/UgfyYSKosiGO6 Hpe8hxkqL5GUVPyu4LJkfw/e99zhNJatliZ1Az/yCKww5KrXC8bQ9wGQvw== -----END PUBLIC KEY----- # 查看公钥的参数 ❯ openssl ec -noout -text -pubin -in ecc-public-key.pem read EC key Private-Key: (256 bit) pub: 04:e4:81:08:a2:bc:34:59:4e:7e:a2:6f:d4:81:fc: 98:48:aa:2c:88:63:ba:1e:97:bc:87:19:2a:2f:91: 94:54:fc:ae:e0:b2:64:7f:0f:de:f7:dc:e1:34:96: ad:96:26:75:03:3f:f2:08:ac:30:e4:aa:d7:0b:c6: d0:f7:01:90:bf ASN1 OID: prime256v1 NIST CURVE: P-256 可以看到 ECC 算法的公钥私钥都比 RSA 小了非常多，数据量小，却能带来同等的安全强度，这是 ECC 相比 RSA 最大的优势。 私钥的参数： priv: 私钥，一个 256bits 的大整数，对应我们前面介绍的 $k 倍运算$中的 $k$ pub: 公钥，是一个椭圆曲线（EC）上的坐标 ${x, y}$，也就是我们 well-known 的基点 $G$ ASN1 OID: prime256v1, 椭圆曲线的名称 NIST CURVE: P-256 使用安全随机数生成器即可直接生成出 ECC 的私钥 priv，因此 ECC 的密钥对生成速度非常快。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecc-密钥对生成"},{"categories":["tech"],"content":" ECDH 密钥交换这个在前面写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS已经介绍过了，不过这里再复述一遍： Alice 跟 Bob 协商好椭圆曲线的各项参数，以及基点 G，这些参数都是公开的。 Alice 生成一个随机的 ECC 密钥对（公钥：$alicePrivate * G$, 私钥: $alicePrivate$） Bob 生成一个随机的 ECC 密钥对（公钥：$bobPrivate * G$, 私钥: $bobPrivate$） 两人通过不安全的信道交换公钥 Alice 将 Bob 的公钥乘上自己的私钥，得到共享密钥 $sharedKey = (bobPrivate * G) * alicePrivate$ Bob 将 Alice 的公钥乘上自己的私钥，得到共享密钥 $sharedKey = (alicePrivate * G) * bobPrivate$ 因为 $(a * G) * b = (b * G) * a$，Alice 与 Bob 计算出的共享密钥应该是相等的 这样两方就通过 ECDH 完成了密钥交换。而 ECDH 的安全性，则由 ECDLP 问题提供保证。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecdh-密钥交换"},{"categories":["tech"],"content":" ECC 加密与解密ECC 本身并没有提供加密与解密的功能，但是我们可以借助 ECDH 迂回实现加解密。流程如下： Bob 想要将消息 M 安全地发送给 Alice，他手上已经拥有了 Alice 的 ECC 公钥 alicePubKey Bob 首先使用如下算法生成出「共享密钥」+「密文公钥」 随机生成一个临时用的密文 ECC 密钥对 密文私钥 ciphertextPrivKey：生成一个安全随机数作为私钥即可 密文公钥 ciphertextPubKey：使用此公式从私钥生成ciphertextPubKey =ciphertextPrivKey * G 使用 ECDH 算法计算出「共享密钥」：sharedECCKey = alicePubKey * ciphertextPrivKey 为了确保安全性，每份密文都应该使用不同的「临时 ECC 密钥对」作为「密文密钥对」， 不应该直接使用「Bob 的密钥对」！「Bob 的密钥对」只在 Alice 回复密文消息给 Bob 时才应该被用到。 Bob 使用「共享密钥」与对称加密算法加密消息，得到密文 C 比如使用 AES-256-GCM 或者 ChaCha20-Poly1305 进行对称加密 Bob 将 C 与「密文公钥 ciphertextPubKey」打包传输给 Alice Alice 使用「密文公钥」与自己的私钥计算出「共享密钥」sharedECCKey = ciphertextPubKey * alicePrivKey 根据 ECDH 算法可知，这里计算出的共享密钥 sharedECCKey，跟 Bob 加密数据使用的共享密钥是完全一致的 Alice 使用计算出的共享密钥解密 C 得到消息 M 实际上就是消息的发送方先生成一个临时的 ECC 密钥对，然后借助 ECDH 协议计算出共享密钥用于加密。消息的接收方同样通过 ECDH 协议计算出共享密钥再解密数据。 使用 Python 演示如下： python # pip install tinyec # \u003c= ECC 曲线库 from tinyec import registry import secrets # 使用这条曲线进行演示 curve = registry.get_curve('brainpoolP256r1') def compress_point(point): return hex(point.x) + hex(point.y % 2)[2:] def ecc_calc_encryption_keys(pubKey): \"\"\" 安全地生成一个随机 ECC 密钥对，然后按 ECDH 流程计算出共享密钥 sharedECCKey 最后返回（共享密钥, 临时 ECC 公钥 ciphertextPubKey） \"\"\" ciphertextPrivKey = secrets.randbelow(curve.field.n) ciphertextPubKey = ciphertextPrivKey * curve.g sharedECCKey = pubKey * ciphertextPrivKey return (sharedECCKey, ciphertextPubKey) def ecc_calc_decryption_key(privKey, ciphertextPubKey): sharedECCKey = ciphertextPubKey * privKey return sharedECCKey # 1. 首先生成出 Alice 的 ECC 密钥对 privKey = secrets.randbelow(curve.field.n) pubKey = privKey * curve.g print(\"private key:\", hex(privKey)) print(\"public key:\", compress_point(pubKey)) # 2. Alice 将公钥发送给 Bob # 3. Bob 使用 Alice 的公钥生成出（共享密钥, 临时 ECC 公钥 ciphertextPubKey） (encryptKey, ciphertextPubKey) = ecc_calc_encryption_keys(pubKey) print(\"ciphertext pubKey:\", compress_point(ciphertextPubKey)) print(\"encryption key:\", compress_point(encryptKey)) # 4. Bob 使用共享密钥 encryptKey 加密数据，然后将密文与 ciphertextPubKey 一起发送给 Alice # 5. Alice 使用自己的私钥 + ciphertextPubKey 计算出共享密钥 decryptKey decryptKey = ecc_calc_decryption_key(privKey, ciphertextPubKey) print(\"decryption key:\", compress_point(decryptKey)) # 6. Alice 使用 decryptKey 解密密文得到原始消息 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:4","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecc-加密与解密"},{"categories":["tech"],"content":" ECC 数字签名前面已经介绍了 RSA 签名，这里介绍下基于 ECC 的签名算法。 基于 ECC 的签名算法主要有两种：ECDSA 与 EdDSA，以及 EdDSA 的变体。其中 ECDSA 算法稍微有点复杂，而安全强度跟它基本一致的 EdDSA 的算法更简洁更易于理解，在使用特定曲线的情况下 EdDSA 还要比 ECDSA 更快一点，因此现在通常更推荐使用 EdDSA 算法。 EdDSA 与 Ed25519 签名算法EdDSA（Edwards-curve Digital Signature Algorithm）是一种现代的安全数字签名算法，它使用专为性能优化的椭圆曲线，如 255bits 曲线 edwards25519 和 448bits 曲线 edwards448. EdDSA 签名算法及其变体 Ed25519 和 Ed448 在技术上在RFC8032 中进行了描述。 首先，用户需要基于 edwards25519 或者 edwards448 曲线，生成一个 ECC 密钥对。生成私钥的时候，算法首先生成一个随机数，然后会对随机数做一些变换以确保安全性，防范计时攻击等攻击手段。对于 edwards25519 公私钥都是 32 字节，而对于 edwards448 公私钥都是 57 字节。 对于 edwards25519 输出的签名长度为 64 字节，而对于 Ed448 输出为 114 字节。 具体的算法虽然比 ECDSA 简单，但还是有点难度的，这里就直接略过了。 下面给出个 ed25519 的计算示例： python # pip install cryptography==36.0.1 from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey # 也可用 openssl 生成，都没啥毛病 private_key = Ed25519PrivateKey.generate() # 签名 signature = private_key.sign(b\"my authenticated message\") # 显然 ECC 的公钥 kG 也能直接从私钥 k 生成 public_key = private_key.public_key() # 验证 # Raises InvalidSignature if verification fails public_key.verify(signature, b\"my authenticated message\") ed448 的代码也完全类似： python # pip install cryptography==36.0.1 from cryptography.hazmat.primitives.asymmetric.ed448 import Ed448PrivateKey private_key = Ed448PrivateKey.generate() signature = private_key.sign(b\"my authenticated message\") public_key = private_key.public_key() # Raises InvalidSignature if verification fails public_key.verify(signature, b\"my authenticated message\") ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:5","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecc-数字签名"},{"categories":["tech"],"content":" ECC 数字签名前面已经介绍了 RSA 签名，这里介绍下基于 ECC 的签名算法。 基于 ECC 的签名算法主要有两种：ECDSA 与 EdDSA，以及 EdDSA 的变体。其中 ECDSA 算法稍微有点复杂，而安全强度跟它基本一致的 EdDSA 的算法更简洁更易于理解，在使用特定曲线的情况下 EdDSA 还要比 ECDSA 更快一点，因此现在通常更推荐使用 EdDSA 算法。 EdDSA 与 Ed25519 签名算法EdDSA（Edwards-curve Digital Signature Algorithm）是一种现代的安全数字签名算法，它使用专为性能优化的椭圆曲线，如 255bits 曲线 edwards25519 和 448bits 曲线 edwards448. EdDSA 签名算法及其变体 Ed25519 和 Ed448 在技术上在RFC8032 中进行了描述。 首先，用户需要基于 edwards25519 或者 edwards448 曲线，生成一个 ECC 密钥对。生成私钥的时候，算法首先生成一个随机数，然后会对随机数做一些变换以确保安全性，防范计时攻击等攻击手段。对于 edwards25519 公私钥都是 32 字节，而对于 edwards448 公私钥都是 57 字节。 对于 edwards25519 输出的签名长度为 64 字节，而对于 Ed448 输出为 114 字节。 具体的算法虽然比 ECDSA 简单，但还是有点难度的，这里就直接略过了。 下面给出个 ed25519 的计算示例： python # pip install cryptography==36.0.1 from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey # 也可用 openssl 生成，都没啥毛病 private_key = Ed25519PrivateKey.generate() # 签名 signature = private_key.sign(b\"my authenticated message\") # 显然 ECC 的公钥 kG 也能直接从私钥 k 生成 public_key = private_key.public_key() # 验证 # Raises InvalidSignature if verification fails public_key.verify(signature, b\"my authenticated message\") ed448 的代码也完全类似： python # pip install cryptography==36.0.1 from cryptography.hazmat.primitives.asymmetric.ed448 import Ed448PrivateKey private_key = Ed448PrivateKey.generate() signature = private_key.sign(b\"my authenticated message\") public_key = private_key.public_key() # Raises InvalidSignature if verification fails public_key.verify(signature, b\"my authenticated message\") ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:5","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#eddsa-与-ed25519-签名算法"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$， 随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好， 可能会导致生成出的子群的阶较小。前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) =0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)=0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798,0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的 （别问，我也不懂什么叫「双有理等价」…）。不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://www.ietf.org/rfc/rfc7748.html#section-4.1 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{255} - 19$, 其他域参数如下： 阶 $n = 2^{252} + 0x14def9dea2f79cd65812631a5cf5d3ed$ 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519） 双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法Ed25519. Curve448, X448 和 Ed448 https://www.ietf.org/rfc/rfc7748.html#section-4.2 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{448} - 2^{224} - 1$，其他域参数： 阶 $n = 2^{446} - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d$ 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。公钥也被编码为 448 位整数。 基于 Curve44","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#密码学常用椭圆曲线介绍"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$， 随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好， 可能会导致生成出的子群的阶较小。前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) =0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)=0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798,0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的 （别问，我也不懂什么叫「双有理等价」…）。不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://www.ietf.org/rfc/rfc7748.html#section-4.1 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{255} - 19$, 其他域参数如下： 阶 $n = 2^{252} + 0x14def9dea2f79cd65812631a5cf5d3ed$ 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519） 双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法Ed25519. Curve448, X448 和 Ed448 https://www.ietf.org/rfc/rfc7748.html#section-4.2 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{448} - 2^{224} - 1$，其他域参数： 阶 $n = 2^{446} - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d$ 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。公钥也被编码为 448 位整数。 基于 Curve44","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#生成点-g"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$， 随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好， 可能会导致生成出的子群的阶较小。前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) =0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)=0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798,0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的 （别问，我也不懂什么叫「双有理等价」…）。不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://www.ietf.org/rfc/rfc7748.html#section-4.1 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{255} - 19$, 其他域参数如下： 阶 $n = 2^{252} + 0x14def9dea2f79cd65812631a5cf5d3ed$ 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519） 双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法Ed25519. Curve448, X448 和 Ed448 https://www.ietf.org/rfc/rfc7748.html#section-4.2 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{448} - 2^{224} - 1$，其他域参数： 阶 $n = 2^{446} - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d$ 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。公钥也被编码为 448 位整数。 基于 Curve44","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#椭圆曲线的域参数"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$， 随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好， 可能会导致生成出的子群的阶较小。前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) =0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)=0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798,0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的 （别问，我也不懂什么叫「双有理等价」…）。不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://www.ietf.org/rfc/rfc7748.html#section-4.1 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{255} - 19$, 其他域参数如下： 阶 $n = 2^{252} + 0x14def9dea2f79cd65812631a5cf5d3ed$ 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519） 双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法Ed25519. Curve448, X448 和 Ed448 https://www.ietf.org/rfc/rfc7748.html#section-4.2 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{448} - 2^{224} - 1$，其他域参数： 阶 $n = 2^{446} - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d$ 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。公钥也被编码为 448 位整数。 基于 Curve44","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#secp256k1"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$， 随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好， 可能会导致生成出的子群的阶较小。前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) =0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)=0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798,0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的 （别问，我也不懂什么叫「双有理等价」…）。不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://www.ietf.org/rfc/rfc7748.html#section-4.1 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{255} - 19$, 其他域参数如下： 阶 $n = 2^{252} + 0x14def9dea2f79cd65812631a5cf5d3ed$ 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519） 双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法Ed25519. Curve448, X448 和 Ed448 https://www.ietf.org/rfc/rfc7748.html#section-4.2 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{448} - 2^{224} - 1$，其他域参数： 阶 $n = 2^{446} - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d$ 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。公钥也被编码为 448 位整数。 基于 Curve44","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#edwards-曲线"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$， 随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好， 可能会导致生成出的子群的阶较小。前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) =0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)=0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798,0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的 （别问，我也不懂什么叫「双有理等价」…）。不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://www.ietf.org/rfc/rfc7748.html#section-4.1 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{255} - 19$, 其他域参数如下： 阶 $n = 2^{252} + 0x14def9dea2f79cd65812631a5cf5d3ed$ 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519） 双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法Ed25519. Curve448, X448 和 Ed448 https://www.ietf.org/rfc/rfc7748.html#section-4.2 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{448} - 2^{224} - 1$，其他域参数： 阶 $n = 2^{446} - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d$ 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。公钥也被编码为 448 位整数。 基于 Curve44","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#curve25519-x25519-和-ed25519"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$， 随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好， 可能会导致生成出的子群的阶较小。前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) =0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)=0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798,0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的 （别问，我也不懂什么叫「双有理等价」…）。不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://www.ietf.org/rfc/rfc7748.html#section-4.1 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{255} - 19$, 其他域参数如下： 阶 $n = 2^{252} + 0x14def9dea2f79cd65812631a5cf5d3ed$ 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519） 双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法Ed25519. Curve448, X448 和 Ed448 https://www.ietf.org/rfc/rfc7748.html#section-4.2 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{448} - 2^{224} - 1$，其他域参数： 阶 $n = 2^{446} - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d$ 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。公钥也被编码为 448 位整数。 基于 Curve44","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#curve448-x448-和-ed448"},{"categories":["tech"],"content":" 密码学常用椭圆曲线介绍在介绍密码学中的常用椭圆曲线前，需要先介绍一下椭圆曲线的阶（order）以及辅助因子（cofactor）这两个概念。 首先还得介绍下数学中「循环群」的概念，它是指能由单个元素所生成的群，在 ECC 中这就是预先定义好的基点 $G$. 一个有限域上的椭圆曲线可以形成一个有限「循环代数群」，它由曲线上的所有点组成。椭圆曲线的阶被定义为该曲线上所有点的个数（包括无穷远点）。 有些曲线加上 G 点可以形成一个单一循环群，这一个群包含了曲线上的所有点。而其他的曲线加上 G 点则形成多个不相交的循环子群，每个子群包含了曲线的一个子集。对于上述第二种情况，假设曲线上的点被拆分到了 h 个循环子群中，每个子群的阶都是 r，那这时整个群的阶就是 $n = h * r$，其中子群的个数 h 被称为辅助因子。 有限域上的椭圆曲线的阶都是有限的，也就是说对于曲线上任意一点 $G$，我们计算它的数乘 $kG$， 随着整数 $k$ 的增大，一定会存在某个 $k$ 使 $kG = O_{\\infty}$ 成立，然后 $k$ 继续增大时，因为 $O_{\\infty} * P = O_{\\infty}$，$kG$ 的值就固定为 $O_{\\infty}$ 了，更大的 $k$ 值已经失去了意义。 因此 ECC 中要求 $kG$ 中的私钥 $k$ 符合条件 $0 \\le k \\le r$，也就是说总的私钥数量是受 $r$ 限制的。 辅助因子通过用如下公式表示： $$ h = n / r $$ 其中 $n$ 是曲线的阶，$r$ 是每个子群的阶，$h$ 是辅助因子。如果曲线形成了一个单一循环群，那显然 $h = 1$，否则 $h \u003e 1$ 举例如下： secp256k1 的辅助因子为 1 Curve25519 的辅助因子为 8 Curve448 的辅助因子为 4 生成点 G生成点 G 的选择是很有讲究的，虽然每个循环子群都包含有很多个生成点，但是 ECC 只会谨慎的选择其中一个。首先 G 点必须要能生成出整个循环子群，其次还需要有尽可能高的计算性能。 数学上已知某些椭圆曲线上，不同的生成点生成出的循环子群，阶也是不同的。如果 G 点选得不好， 可能会导致生成出的子群的阶较小。前面我们已经提过子群的阶 $r$ 会限制总的私钥数量，导致算法强度变弱！因此不恰当的 $G$ 点可能会导致我们遭受「小子群攻击」。为了避免这种风险，建议尽量使用被广泛使用的加密库，而不是自己撸一个。 椭圆曲线的域参数ECC椭圆曲线由一组椭圆曲线域参数描述，如曲线方程参数、场参数和生成点坐标。这些参数在各种密码学标准中指定，你可以网上搜到相应的 RFC 或 NIST 文档。 这些标准定义了一组命名曲线的参数，例如 secp256k1、P-521、brainpoolP512t1 和 SM2. 这些加密标准中描述的有限域上的椭圆曲线得到了密码学家的充分研究和分析，并被认为具有一定的安全强度。 也有一些密码学家（如 Daniel Bernstein）认为，官方密码标准中描述的大多数曲线都是「不安全的」，并定义了他们自己的密码标准，这些标准在更广泛的层面上考虑了 ECC 安全性。 开发人员应该仅使用各项标准文档给出的、经过密码学家充分研究的命名曲线。 secp256k1此曲线被应用在比特币中，它的域参数如下： p (modulus) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F n (order; size; the count of all possible EC points) =0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141 a (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数) =0x0000000000000000000000000000000000000000000000000000000000000000 b (方程 $y^2 ≡ x^3 + a*x + b (\\mod p)$ 中的常数)=0x0000000000000000000000000000000000000000000000000000000000000007 g (the curve generator point G {x, y}) = (0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798,0x483ada7726a3c4655da4fbfc0e1108a8fd17b448a68554199c47d08ffb10d4b8) h (cofactor, typically 1) = 01 Edwards 曲线椭圆曲线方程除了我们前面使用的 Weierstrass 形式 $$y^2 = (x^3 + ax + b) \\mod p$$ 外，还可以被写成其他多种形式，这些不同的形式是双有理等价的 （别问，我也不懂什么叫「双有理等价」…）。不同的方程形式在计算机的数值计算上可能会存在区别。 为了性能考虑，ECC 在部分场景下会考虑使用 Edwards 曲线形式进行计算，该方程形式如下： $$ x^{2}+y^{2}=1+dx^{2}y^{2} $$ 画个图长这样： 知名的 Edwards 曲线有： Curve1174 (251-bit) Curve25519 (255-bit) Curve383187 (383-bit) Curve41417 (414-bit) Curve448 (448-bit) E-521 (521-bit) … Curve25519, X25519 和 Ed25519 https://www.ietf.org/rfc/rfc7748.html#section-4.1 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed25519/ 只要域参数选得好，Edwards 就可以以非常高的性能实现 ECC 密钥交换、数字签名、混合加密方案。 一个例子就是 Curve25519，它是 Edwards 曲线，其 Montgomery 形式的定义如下： $$ y^{2}=x^{3}+486662x^{2}+x $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{255} - 19$, 其他域参数如下： 阶 $n = 2^{252} + 0x14def9dea2f79cd65812631a5cf5d3ed$ 辅因子 h = 8 虽然此曲线并未以 Edwards 形式定义，但是它已被证明与如下扭曲 Edwards 曲线（edwards25519） 双有理等价： $$ -x^2 + y^2 = 1 + 37095705934669439343138083508754565189542113879843219016388785533085940283555 x^2 y^2 $$ 上面给出的这种 Edwards 形式与前文给出的 Weierstrass 形式完全等价，是专为计算速度优化而设计成这样的。 Curve25519 由 Daniel Bernstein 领导的密码学家团队精心设计，在多个设计和实现层面上达成了非常高的性能，同时不影响安全性。 Curve25519 的构造使其避免了许多潜在的实现缺陷。根据设计，它不受定时攻击的影响，并且它接受任何 32 字节的字符串作为有效的公钥，并且不需要验证。它能提供 125.8bits 的安全强度（有时称为 ~ 128bits 安全性） Curve25519 的私钥为 251 位，通常编码为 256 位整数（32 个字节，64 个十六进制数字）。公钥通常也编码为 256 位整数（255 位 y 坐标 + 1 位 x 坐标），这对开发人员来说非常方便。 基于 Curve25519 派生出了名为 X25519 的 ECDH 算法，以及基于 EdDSA 的高速数字签名算法Ed25519. Curve448, X448 和 Ed448 https://www.ietf.org/rfc/rfc7748.html#section-4.2 https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ed448/ Curve448（Curve448-Goldilocks）是一种非扭曲 Edwards 曲线，它的方程定义如下： $$ x^2 + y^2 = 1 - 39081 x^2 y^2 $$ 其被定义在有限域 $\\mathbb {F} _{p}$ 上，$p = 2^{448} - 2^{224} - 1$，其他域参数： 阶 $n = 2^{446} - 0x8335dc163bb124b65129c96fde933d8d723a70aadc873d6d54a7bb0d$ 辅助因子 h = 4 与 Curve25519 一样，Curve448 也等价于前面给出的 Weierstrass 形式，选择 Edwards 形式主要是因为它能显著提升性能。 Curve448 提供 222.8 位的安全强度。Curve448 的私钥为 446 位，通常编码为 448 位整数（56 个字节，112 个十六进制数字）。公钥也被编码为 448 位整数。 基于 Curve44","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:6","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#该选择哪种椭圆曲线"},{"categories":["tech"],"content":" ECIES - 集成加密方案在文章开头我们已经介绍了集成加密方案 (IES)，它在密钥封装机制（KEM）的基础上，添加了密钥派生算法 KDF、消息认证算法 MAC 等其他密码学算法以达成我们对消息的安全性、真实性、完全性的需求。 而 ECIES 也完全类似，是在 ECC + 对称加密算法的基础上，添加了许多其他的密码学算法实现的。 ECIES 是一个加密框架，而不是某种固定的算法。它可以通过插拔不同的算法，形成不同的实现。比如「secp256k1 + Scrypt + AES-GCM + HMAC-SHA512」。 大概就介绍到这里吧，后续就请在需要用到时自行探索相关的细节咯。 ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:4:7","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#ecies---集成加密方案"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book A complete overview of SSL/TLS and its cryptographic system 密码发展史之近现代密码 - 中国国家密码管理局 RFC6090 - Fundamental Elliptic Curve Cryptography Algorithms Which elliptic curve should I use? ","date":"2022-03-09","objectID":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/:5:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","非对称加密","安全","RSA","ECC"],"title":"写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC","uri":"/posts/practical-cryptography-basics-7-asymmetric-key-ciphers/#参考"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book，笔者补充了部分代码示例。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:0:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#"},{"categories":["tech"],"content":" 零、术语介绍两个常用动词： 加密：cipher 或者 encrypt 解密：decipher 或者 decrypt 另外有几个名词有必要解释： cipher: 指用于加解密的「密码算法」，有时也被直接翻译成「密码」 cryptographic algorithm: 密码学算法，泛指密码学相关的各类算法 ciphertext: 密文，即加密后的信息。对应的词是明文 plaintext password: 这个应该不需要解释，就是我们日常用的各种字符或者数字密码，也可称作口令，通常都比较短（绝大部分用户密码应该都只有 8 - 16 位）。 因为站点太多，密码太难记，现代社会正在逐步推荐使用生物特征（指纹、面部识别等，如 pass key、手机指纹识别）或者硬件密钥（U2F）来替代传统的密码。 passphrase: 翻译成「密码词组」，也就是用一个个单词组合而成的 Password，特点是长度比较长，而且比随机密码更容易记忆。 如果你用 ssh/gnupg/openssl 等工具生成或使用过密钥，应该对它不陌生，它们的 passphrase 长度不受限，因此可以使用类似 you-are-not-my-enemy-but-I'm-your-father 这样的词组方式作为其密码，强度高，而且好记。 在密码学里面，最容易搞混的词估计就是「密码」了，cipher/password/passphrase 都可以被翻译成「密码」，需要注意下其中区别。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:1:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#零术语介绍"},{"categories":["tech"],"content":" 一、什么是对称加密在密码学中，有两种加密方案被广泛使用：「对称加密」与「非对称加密」。 对称加密是指，使用相同的密钥进行消息的加密与解密。因为这个特性，我们也称这个密钥为「共享密钥（Shared Secret Key）」，示意图如下： 现代密码学中广泛使用的对称加密算法（ciphers） 有：AES（AES-128、AES-192、AES-256）、ChaCha20、Twofish、IDEA、Serpent、Camelia、RC6、CAST 等。其中绝大多数都是「块密码算法（Block Cipher）」或者叫「分组密码算法」，这种算法一次只能加密固定大小的块（例如 128 位）；少部分是「流密码算法（Stream Cipher）」，流密码算法将数据逐字节地加密为密文流。 通过使用称为「分组密码工作模式」的技术，可以将「分组密码算法」转换为「流密码算法」。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:2:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#一什么是对称加密"},{"categories":["tech"],"content":" 量子安全性即使计算机进入量子时代，仍然可以沿用当前的对称密码算法。因为大多数现代对称密钥密码算法都是抗量子的（quantum-resistant），这意味当使用长度足够的密钥时，强大的量子计算机无法破坏其安全性。目前来看 256 位的 AES/Twofish 在很长一段时间内都将是 量子安全 的。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:2:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#量子安全性"},{"categories":["tech"],"content":" 二、对称加密方案的结构我们在第一章「概览」里介绍过，单纯使用数据加密算法只能保证数据的安全性，并不能满足我们对消息真实性、完整性与不可否认性的需求，因此通常我们会将对称加密算法跟其他算法组合成一个「对称加密方案」来使用，这种多个密码学算法组成的「加密方案」能同时保证数据的安全性、真实性、完整性与不可否认性。 一个分组加密方案通常会包含如下几种算法： 将密码转换为密钥的密钥派生算法 KDF（如 Scrypt 或 Argon2）：通过使用 KDF，加密方案可以允许用户使用字符密码作为「Shared Secret Key」，并使密码的破解变得困难和缓慢 分组密码工作模式（用于将分组密码转换为流密码，如 CBC 或 CTR）+ 消息填充算法（如 PKCS7）：分组密码算法（如 AES）需要借助这两种算法，才能加密任意大小的数据 分组密码算法（如 AES）：使用密钥安全地加密固定长度的数据块 大多数流行的对称加密算法，都是分组密码算法 消息认证算法（如HMAC）：用于验证消息的真实性、完整性、不可否认性 而一个流密码加密方案本身就能加密任意长度的数据，因此不需要「分组密码模式」与「消息填充算法」。 如 AES-256-CTR-HMAC-SHA256 就表示一个使用 AES-256 与 Counter 分组模式进行加密，使用 HMAC-SHA256 进行消息认证的加密方案。其他流行的对称加密方案还有 ChaCha20-Poly1305 和 AES-128-GCM 等，其中 ChaCha20-Poly130 是一个流密码加密方案。我们会在后面单独介绍这两种加密方案。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:3:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#二对称加密方案的结构"},{"categories":["tech"],"content":" 三、分组密码工作模式前面简单介绍了「分组密码工作模式」可以将「分组密码算法」转换为「流密码算法」，从而实现加密任意长度的数据，这里主要就具体介绍下这个分组密码工作模式（下文简称为「分组模式」或者「XXX 模式」）。 加密方案的名称中就带有具体的「分组模式」名称，如： AES-256-GCM - 具有 256 位加密密钥和 GCM 分组模式的 AES 密码 AES-128-CTR - 具有 128 位加密密钥和 CTR 分组模式的 AES 密码 Serpent-128-CBC - 具有 128 位加密密钥和 CBC 分组模式的 Serpent 密码 「分组密码工作模式」背后的主要思想是把明文分成多个长度固定的组，再在这些分组上重复应用分组密码算法进行加密/解密，以实现安全地加密/解密任意长度的数据。 某些分组模式（如 CBC）要求将输入拆分为分组，并使用填充算法（例如添加特殊填充字符）将最末尾的分组填充到块大小。也有些分组模式（如 CTR、CFB、OFB、CCM、EAX 和 GCM）根本不需要填充，因为它们在每个步骤中，都直接在明文部分和内部密码状态之间执行异或（XOR）运算. 使用「分组模式」加密大量数据的流程基本如下： 初始化加密算法状态（使用加密密钥 + 初始向量 IV） 加密数据的第一个分组 使用加密密钥和其他参数转换加密算法的当前状态 加密下一个分组 再次转换加密状态 再加密下一分组 依此类推，直到处理完所有输入数据 解密的流程跟加密完全类似：先初始化算法，然后依次解密所有分组，中间可能会涉及到加密状态的转换。 下面我们来具体介绍下 CTR 与 GCM 两个常见的分组模式。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:4:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#三分组密码工作模式"},{"categories":["tech"],"content":" 0. 初始向量 IV介绍具体的分组模式前，需要先了解下初始向量 IV（Initialization Vector）这个概念，它有时也被称作 Salt 或者 Nonce。初始向量 IV 通常是一个随机数，主要作用是往密文中添加随机性，使同样的明文被多次加密也会产生不同的密文，从而确保密文的不可预测性。 IV 的大小应与密码块大小相同，例如 AES、Serpent 和 Camellia 都只支持 128 位密码块，那么它们需要的 IV 也必须也 128 位。 IV 通常无需保密，但是应当足够随机（无法预测），而且不允许重用，应该对每条加密消息使用随机且不可预测的 IV。 一个常见错误是使用相同的对称密钥和相同的 IV 加密多条消息，这使得针对大多数分组模式的各种加密攻击成为可能。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#0-初始向量-iv"},{"categories":["tech"],"content":" 1. CTR (Counter) 分组模式 参考文档: https://csrc.nist.gov/publications/detail/sp/800-38a/final 下图说明了「CTR 分组工作模式」的加密解密流程，基本上就是将明文/密文拆分成一个个长度固定的分组，然后使用一定的算法进行加密与解密： 可以看到两图中左边的第一个步骤，涉及到三个参数： Nonce，初始向量 IV 的别名，前面已经介绍过了。 Counter: 一个计数器，最常用的 Counter 实现是「从 0 开始，每次计算都自增 1」 Key: 对称加密的密钥 Plaintext: 明文的一个分组。除了最后一个分组外，其他分组的长度应该跟 Key 相同 CTR 模式加解密的算法使用公式来表示如下： $$ \\begin{alignedat}{2} C_i \u0026= P_i \\oplus O_i, \u0026\\text{for } i \u0026= 1, 2 … n-1 \\\\ P_i \u0026= C_i \\oplus O_i, \u0026\\text{for } i \u0026= 1, 2 … n-1 \\\\ O_i \u0026= \\text{CIPH}_{key}(\\text{Nonce} + I_i), \u0026\\text{for } i \u0026= 1, 2 … n-1 \\end{alignedat} $$ 公式的符号说明如下 $C_i$ 表示密文的第 $i$ 个分组 $P_i$ 表示明文的第 $i$ 个 分组 $O_i$ 是一个中间量，第三个公式是它的计算方法 $I_i$ 表示计数器返回的第 $i$ 个值，其长度应与分组的长度相同 $\\text{CIPH}_{key}$ 表示使用密钥 $key$ 的对称加密算法 上面的公式只描述了 $ 0 \\ge i \\le n-1$ 的场景，最后一个分组 $i = n$ 要特殊一些——它的长度可能比 Key 要短。CTR 模式加解密这最后这个分组时，会直接忽略掉 $O_n$ 末尾多余的 bytes. 这种处理方式使得 CTR 模式不需要使用填充算法对最后一个分组进行填充，而且还使密文跟明文的长度完全一致。我们假设最后一个分组的长度为 $u$，它的加解密算法描述如下（$MSB_u(O_n)$ 表示取 $O_n$ 的 u 个最高有效位）： $$ \\begin{alignedat}{2} C_{n} \u0026= P_{n} \\oplus {MSB_u}(O_n) \\\\ P_{n} \u0026= C_{n} \\oplus {MSB_u}(O_n)\\\\ O_n \u0026= \\text{CIPH}_{key}(\\text{Nonce} + I_n) \\end{alignedat} $$ 可以看到，因为异或 XOR 的对称性，加密跟解密的算法是完全相同的，直接 XOR $O_i$ 即可。 Python 中最流行的密码学库是cryptography，requests 的底层曾经就使用了它（新版本已经换成使用标准库 ssl 了），下面我们使用这个库来演示下 AES-256-CTR 算法： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a test message, hahahahahaha~\" # 使用 32bytes 的 key，即使用算法 AES-256-CTR key = os.urandom(32) # key =\u003e b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' # AES 算法的 block 大小是固定的 128bits，即 16 bytes, IV 长度需要与 block 一致 iv = os.urandom(16) # iv =\u003e b'\\x88[\\xc9\\n`\\xe4\\xc2^\\xaf\\xdc\\x1e\\xfd.c\u003e=' # 1. 发送方加密数据 ## 构建 AES-256-CTR 的 cipher，然后加密数据，得到密文 cipher = Cipher(algorithms.AES(key), modes.CTR(iv)) encryptor = cipher.encryptor() ciphertext = encryptor.update(plaintext) + encryptor.finalize() # ciphertext =\u003e b'\\x9b6(\\x1d\\xfd\\xde\\x96S\\x8b\\x8f\\x90\\xc5}ou\\x9e\\xb1\\xbd\\x9af\\xb8\\xdc\\xec\\xbf\\xa3\"\\x18^\\xac\\x14\\xc8s2*\\x1a\\xcf\\x1d' # 2. 发送方将 iv + ciphertext 发送给接收方 # 3. 接收方解密数据 # 接收方使用自己的 key + 接收到的 iv，构建 cipher，然后解密出原始数据 cipher = Cipher(algorithms.AES(key), modes.CTR(iv)) decryptor = cipher.decryptor() decryptor.update(ciphertext) + decryptor.finalize() 从上面的算法描述能感觉到，CTR 算法还蛮简单的。下面我使用 Python 写一个能够 work 的 CTR 实现： python def xor_bytes(a, b): \"\"\"Returns a new byte array with the elements xor'ed. if len(a) != len(b), extra parts are discard. \"\"\" return bytes(i^j for i, j in zip(a, b)) def inc_bytes(a): \"\"\" Returns a new byte array with the value increment by 1 \"\"\" out = list(a) for i in reversed(range(len(out))): if out[i] == 0xFF: out[i] = 0 else: out[i] += 1 break return bytes(out) def split_blocks(message, block_size, require_padding=True): \"\"\" Split `message` with fixed length `block_size` \"\"\" assert len(message) % block_size == 0 or not require_padding return [message[i:i+16] for i in range(0, len(message), block_size)] def encrypt_ctr(block_cipher, plaintext, iv): \"\"\" Encrypts `plaintext` using CTR mode with the given nounce/IV. \"\"\" assert len(iv) == 16 blocks = [] nonce = iv for plaintext_block in split_blocks(plaintext, block_size=16, require_padding=False): # CTR mode encrypt: plaintext_block XOR encrypt(nonce) o = bytes(block_cipher.encrypt(nonce)) block = xor_bytes(plaintext_block, o) # extra parts of `o` are discard in this step blocks.append(block) nonce = inc_bytes(nonce) return b''.join(blocks) # 加密与解密的算法完全一致 decrypt_ctr = encrypt_ctr 接下来验证下算法的正确性： python # Python 官方库未提供 AES 实现，因此需要先装下这个库： # pip install pyaes==1.6.1 from pyaes import AES # AES-256-CTR - plaintext key 都与前面的测试代码完全一致 plaintext = b\"this is a test message, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' # 1. 发送方加密数据 # 首先生成一个随机 IV，为了对比，这里使用前面生成好的数据 iv = b'\\x88[\\xc9\\n`\\xe4\\xc2^\\xaf\\xdc\\x1e\\xfd.c\u003e=' aes_cipher = AES(key) ciphertext = encrypt_ctr(aes_cipher, plaintext, ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:4:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#counter_mode"},{"categories":["tech"],"content":" 2. GCM (Galois/Counter) 分组模式GCM (Galois/Counter) 模式在 CTR 模式的基础上，添加了消息认证的功能，而且同时还具有与 CTR 模式相同的并行计算能力。因此相比 CTR 模式，GCM 不仅速度一样快，还能额外提供对消息完整性、真实性的验证能力。 下图直观地解释了 GCM 块模式（Galois/Counter 模式）的工作原理： GCM 模式新增的 Auth Tag，计算起来会有些复杂，我们就直接略过了，对原理感兴趣的可以看下Galois/Counter_Mode_wiki. ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:4:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#2-gcm-galoiscounter-分组模式"},{"categories":["tech"],"content":" 3. 如何选用块模式一些 Tips: 常用的安全块模式是 CBC（密码块链接）、CTR（计数器）和 GCM（伽罗瓦/计数器模式），它们需要一个随机（不可预测的）初始化向量 (IV)，也称为 nonce 或 salt 「CTR（Counter）」块模式在大多数情况下是一个不错的选择，因为它具有很强的安全性和并行处理能力，允许任意输入数据长度（无填充）。但它不提供身份验证和完整性，只提供加密 GCM（Galois/Counter Mode）块模式继承了 CTR 模式的所有优点，并增加了加密消息认证能力。GCM 是在对称密码中实现认证加密的快速有效的方法，强烈推荐 CBC 模式在固定大小的分组上工作。因此，在将输入数据拆分为分组后，应使用填充算法使最后一个分组的长度一致。大多数应用程序使用 PKCS7 填充方案或 ANSI X.923. 在某些情况下，CBC 阻塞模式可能容易受到「padding oracle」攻击，因此最好避免使用 CBC 模式 众所周知的不安全块模式是 ECB（电子密码本），它将相等的输入块加密为相等的输出块（无加密扩散能力）。不要使用 ECB 块模式！它可能会危及整个加密方案。 CBC、CTR 和 GCM 模式等大多数块都支持「随机访问」解密。比如在视频播放器中的任意时间偏移处寻找，播放加密的视频流 总之，建议使用 CTR (Counter) 或 GCM (Galois/Counter) 分组模式。其他的分组在某些情况下可能会有所帮助，但很可能有安全隐患，因此除非你很清楚自己在做什么，否则不要使用其他分组模式！ CTR 和 GCM 加密模式有很多优点：它们是安全的（目前没有已知的重大缺陷），可以加密任意长度的数据而无需填充，可以并行加密和解密分组（在多核 CPU 中）并可以直接解密任意一个密文分组。因此它们适用于加密加密钱包、文档和流视频（用户可以按时间查找）。GCM 还提供消息认证，是一般情况下密码块模式的推荐选择。 请注意，GCM、CTR 和其他分组模式会泄漏原始消息的长度，因为它们生成的密文长度与明文消息的长度相同。如果您想避免泄露原始明文长度，可以在加密前向明文添加一些随机字节（额外的填充数据），并在解密后将其删除。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:4:4","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#3-如何选用块模式"},{"categories":["tech"],"content":" 四、对称加密算法与对称加密方案前面啰嗦了这么多，下面进入正题：对称加密算法 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#四对称加密算法与对称加密方案"},{"categories":["tech"],"content":" 1. 安全的对称加密算法目前应用最广泛的对称加密算法，是 AES 跟 Salsa20 / ChaCha20 这两个系列。 1. AES (Rijndael) wiki: https://en.wikipedia.org/wiki/Advanced_Encryption_Standard AES（高级加密标准，也称为 Rijndael）是现代 IT 行业中最流行和广泛使用的对称加密算法。AES 被证明是高度安全、快速且标准化的，到目前为止没有发现任何明显的弱点或攻击手段，而且几乎在所有平台上都得到了很好的支持。 AES 是 128 位分组密码，使用 128、192 或 256 位密钥。它通常与分组模式组合成分组加密方案（如 AES-CTR 或 AES-GCM）以处理流数据。在大多数分组模式中，AES 还需要一个随机的 128 位初始向量 IV。 Rijndael (AES) 算法可免费用于任何用途，而且非常流行。很多站点都选择 AES 作为 TLS 协议的一部分，以实现安全通信。现代 CPU 硬件基本都在微处理器级别实现了 AES 指令以加速 AES 加密解密操作。 这里有一个纯 Python 的 AES 实现可供参考: AES encryption in pure Python - boppreh 我们在前面的 CTR 分组模式中已经使用 Python 实践了 AES-256-CTR 加密方案。而实际上更常用的是支持集成身份验证加密（AEAD）的 AES-256-GCM 加密方案，它的优势我们前面已经介绍过了，这里我们使用 Python 演示下如何使用： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import ( Cipher, algorithms, modes ) def encrypt(key, plaintext, associated_data): # Generate a random 96-bit IV. iv = os.urandom(12) # Construct an AES-GCM Cipher object with the given key and a # randomly generated IV. encryptor = Cipher( algorithms.AES(key), modes.GCM(iv), ).encryptor() # associated_data will be authenticated but not encrypted, # it must also be passed in on decryption. encryptor.authenticate_additional_data(associated_data) # Encrypt the plaintext and get the associated ciphertext. # GCM does not require padding. ciphertext = encryptor.update(plaintext) + encryptor.finalize() return (iv, ciphertext, encryptor.tag) def decrypt(key, associated_data, iv, ciphertext, tag): # Construct a Cipher object, with the key, iv, and additionally the # GCM tag used for authenticating the message. decryptor = Cipher( algorithms.AES(key), modes.GCM(iv, tag), ).decryptor() # We put associated_data back in or the tag will fail to verify # when we finalize the decryptor. decryptor.authenticate_additional_data(associated_data) # Decryption gets us the authenticated plaintext. # If the tag does not match an InvalidTag exception will be raised. return decryptor.update(ciphertext) + decryptor.finalize() # 接下来进行算法验证 plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' associated_data = b\"authenticated but not encrypted payload\" # 被用于消息认证的关联数据 # 1. 发送方加密消息 iv, ciphertext, tag = encrypt( key, plaintext, associated_data ) # 2. 发送方将 associated_data iv ciphertext tag 打包发送给接收方 # 3. 接收方使用自己的 key 验证并解密数据 descrypt_text = decrypt( key, associated_data, iv, ciphertext, tag ) 2. Salsa20 / ChaCha20 wiki: https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant Salsa20 及其改进的变体 ChaCha（ChaCha8、ChaCha12、ChaCha20）和 XSalsa20 是由密码学家 Daniel Bernstein 设计的现代、快速的对称流密码家族。 Salsa20 密码是对称流密码设计竞赛 eSTREAM（2004-2008）的决赛选手之一，它随后与相关的 BLAKE 哈希函数一起被广泛采用。 Salsa20 及其变体是免版税的，没有专利。 Salsa20 密码将 128 位或 256 位对称密钥 + 随机生成的 64 位随机数（初始向量）和无限长度的数据流作为输入，并生成长度相同的加密数据流作为输出输入流。 ChaCha20-Poly1305Salsa20 应用最为广泛的是认证加密方案：ChaCha20-Poly1305，即组合使用 ChaCha20 与消息认证算法 Poly1305，它们都由密码学家 Bernstein 设计。 ChaCha20-Poly1305 已被证明足够安全，不过跟 GCM 一样它的安全性也依赖于足够随机的初始向量 IV，另外 ChaCha20-Poly1305 也不容易遭受计时攻击。 ChaCha20 算法比 AES 要快得多，但是目前（2025 年）主流的 PC 端 x86_64 CPU 与移动端 ARM CPU 基本都在硬件层面实现了 AES 算法，这大大提升了该算法的性能，因此 ChaCha20 算法目前反而并无性能优势。 以下是一个 ChaCha20 的 Python 示例： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' nonce = os.urandom(16) algorithm = algorithms.ChaCha20(key, nonce) # ChaCha20 是一个流密码，mode 必须为 None cipher = Cipher(algorithm, mode=None) # 1. 加密 encryptor = cipher.encryptor() ct = encryptor.update(plaintext) # 2. 解密 decryptor = cipher.decryptor() decryptor.update(ct) 3. 其他流行的对称加密算法还有一些其他的现代安全对称密码，它们的应用不如 AES 和 ChaCha20 这么广泛，但在程序员和信息安全社区中仍然很流行： Serpent - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Twofish - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Camellia - 安全对称密钥分组密码（分组大小：128 位；密钥","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#1-安全的对称加密算法"},{"categories":["tech"],"content":" 1. 安全的对称加密算法目前应用最广泛的对称加密算法，是 AES 跟 Salsa20 / ChaCha20 这两个系列。 1. AES (Rijndael) wiki: https://en.wikipedia.org/wiki/Advanced_Encryption_Standard AES（高级加密标准，也称为 Rijndael）是现代 IT 行业中最流行和广泛使用的对称加密算法。AES 被证明是高度安全、快速且标准化的，到目前为止没有发现任何明显的弱点或攻击手段，而且几乎在所有平台上都得到了很好的支持。 AES 是 128 位分组密码，使用 128、192 或 256 位密钥。它通常与分组模式组合成分组加密方案（如 AES-CTR 或 AES-GCM）以处理流数据。在大多数分组模式中，AES 还需要一个随机的 128 位初始向量 IV。 Rijndael (AES) 算法可免费用于任何用途，而且非常流行。很多站点都选择 AES 作为 TLS 协议的一部分，以实现安全通信。现代 CPU 硬件基本都在微处理器级别实现了 AES 指令以加速 AES 加密解密操作。 这里有一个纯 Python 的 AES 实现可供参考: AES encryption in pure Python - boppreh 我们在前面的 CTR 分组模式中已经使用 Python 实践了 AES-256-CTR 加密方案。而实际上更常用的是支持集成身份验证加密（AEAD）的 AES-256-GCM 加密方案，它的优势我们前面已经介绍过了，这里我们使用 Python 演示下如何使用： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import ( Cipher, algorithms, modes ) def encrypt(key, plaintext, associated_data): # Generate a random 96-bit IV. iv = os.urandom(12) # Construct an AES-GCM Cipher object with the given key and a # randomly generated IV. encryptor = Cipher( algorithms.AES(key), modes.GCM(iv), ).encryptor() # associated_data will be authenticated but not encrypted, # it must also be passed in on decryption. encryptor.authenticate_additional_data(associated_data) # Encrypt the plaintext and get the associated ciphertext. # GCM does not require padding. ciphertext = encryptor.update(plaintext) + encryptor.finalize() return (iv, ciphertext, encryptor.tag) def decrypt(key, associated_data, iv, ciphertext, tag): # Construct a Cipher object, with the key, iv, and additionally the # GCM tag used for authenticating the message. decryptor = Cipher( algorithms.AES(key), modes.GCM(iv, tag), ).decryptor() # We put associated_data back in or the tag will fail to verify # when we finalize the decryptor. decryptor.authenticate_additional_data(associated_data) # Decryption gets us the authenticated plaintext. # If the tag does not match an InvalidTag exception will be raised. return decryptor.update(ciphertext) + decryptor.finalize() # 接下来进行算法验证 plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' associated_data = b\"authenticated but not encrypted payload\" # 被用于消息认证的关联数据 # 1. 发送方加密消息 iv, ciphertext, tag = encrypt( key, plaintext, associated_data ) # 2. 发送方将 associated_data iv ciphertext tag 打包发送给接收方 # 3. 接收方使用自己的 key 验证并解密数据 descrypt_text = decrypt( key, associated_data, iv, ciphertext, tag ) 2. Salsa20 / ChaCha20 wiki: https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant Salsa20 及其改进的变体 ChaCha（ChaCha8、ChaCha12、ChaCha20）和 XSalsa20 是由密码学家 Daniel Bernstein 设计的现代、快速的对称流密码家族。 Salsa20 密码是对称流密码设计竞赛 eSTREAM（2004-2008）的决赛选手之一，它随后与相关的 BLAKE 哈希函数一起被广泛采用。 Salsa20 及其变体是免版税的，没有专利。 Salsa20 密码将 128 位或 256 位对称密钥 + 随机生成的 64 位随机数（初始向量）和无限长度的数据流作为输入，并生成长度相同的加密数据流作为输出输入流。 ChaCha20-Poly1305Salsa20 应用最为广泛的是认证加密方案：ChaCha20-Poly1305，即组合使用 ChaCha20 与消息认证算法 Poly1305，它们都由密码学家 Bernstein 设计。 ChaCha20-Poly1305 已被证明足够安全，不过跟 GCM 一样它的安全性也依赖于足够随机的初始向量 IV，另外 ChaCha20-Poly1305 也不容易遭受计时攻击。 ChaCha20 算法比 AES 要快得多，但是目前（2025 年）主流的 PC 端 x86_64 CPU 与移动端 ARM CPU 基本都在硬件层面实现了 AES 算法，这大大提升了该算法的性能，因此 ChaCha20 算法目前反而并无性能优势。 以下是一个 ChaCha20 的 Python 示例： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' nonce = os.urandom(16) algorithm = algorithms.ChaCha20(key, nonce) # ChaCha20 是一个流密码，mode 必须为 None cipher = Cipher(algorithm, mode=None) # 1. 加密 encryptor = cipher.encryptor() ct = encryptor.update(plaintext) # 2. 解密 decryptor = cipher.decryptor() decryptor.update(ct) 3. 其他流行的对称加密算法还有一些其他的现代安全对称密码，它们的应用不如 AES 和 ChaCha20 这么广泛，但在程序员和信息安全社区中仍然很流行： Serpent - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Twofish - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Camellia - 安全对称密钥分组密码（分组大小：128 位；密钥","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#1-aes-rijndael"},{"categories":["tech"],"content":" 1. 安全的对称加密算法目前应用最广泛的对称加密算法，是 AES 跟 Salsa20 / ChaCha20 这两个系列。 1. AES (Rijndael) wiki: https://en.wikipedia.org/wiki/Advanced_Encryption_Standard AES（高级加密标准，也称为 Rijndael）是现代 IT 行业中最流行和广泛使用的对称加密算法。AES 被证明是高度安全、快速且标准化的，到目前为止没有发现任何明显的弱点或攻击手段，而且几乎在所有平台上都得到了很好的支持。 AES 是 128 位分组密码，使用 128、192 或 256 位密钥。它通常与分组模式组合成分组加密方案（如 AES-CTR 或 AES-GCM）以处理流数据。在大多数分组模式中，AES 还需要一个随机的 128 位初始向量 IV。 Rijndael (AES) 算法可免费用于任何用途，而且非常流行。很多站点都选择 AES 作为 TLS 协议的一部分，以实现安全通信。现代 CPU 硬件基本都在微处理器级别实现了 AES 指令以加速 AES 加密解密操作。 这里有一个纯 Python 的 AES 实现可供参考: AES encryption in pure Python - boppreh 我们在前面的 CTR 分组模式中已经使用 Python 实践了 AES-256-CTR 加密方案。而实际上更常用的是支持集成身份验证加密（AEAD）的 AES-256-GCM 加密方案，它的优势我们前面已经介绍过了，这里我们使用 Python 演示下如何使用： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import ( Cipher, algorithms, modes ) def encrypt(key, plaintext, associated_data): # Generate a random 96-bit IV. iv = os.urandom(12) # Construct an AES-GCM Cipher object with the given key and a # randomly generated IV. encryptor = Cipher( algorithms.AES(key), modes.GCM(iv), ).encryptor() # associated_data will be authenticated but not encrypted, # it must also be passed in on decryption. encryptor.authenticate_additional_data(associated_data) # Encrypt the plaintext and get the associated ciphertext. # GCM does not require padding. ciphertext = encryptor.update(plaintext) + encryptor.finalize() return (iv, ciphertext, encryptor.tag) def decrypt(key, associated_data, iv, ciphertext, tag): # Construct a Cipher object, with the key, iv, and additionally the # GCM tag used for authenticating the message. decryptor = Cipher( algorithms.AES(key), modes.GCM(iv, tag), ).decryptor() # We put associated_data back in or the tag will fail to verify # when we finalize the decryptor. decryptor.authenticate_additional_data(associated_data) # Decryption gets us the authenticated plaintext. # If the tag does not match an InvalidTag exception will be raised. return decryptor.update(ciphertext) + decryptor.finalize() # 接下来进行算法验证 plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' associated_data = b\"authenticated but not encrypted payload\" # 被用于消息认证的关联数据 # 1. 发送方加密消息 iv, ciphertext, tag = encrypt( key, plaintext, associated_data ) # 2. 发送方将 associated_data iv ciphertext tag 打包发送给接收方 # 3. 接收方使用自己的 key 验证并解密数据 descrypt_text = decrypt( key, associated_data, iv, ciphertext, tag ) 2. Salsa20 / ChaCha20 wiki: https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant Salsa20 及其改进的变体 ChaCha（ChaCha8、ChaCha12、ChaCha20）和 XSalsa20 是由密码学家 Daniel Bernstein 设计的现代、快速的对称流密码家族。 Salsa20 密码是对称流密码设计竞赛 eSTREAM（2004-2008）的决赛选手之一，它随后与相关的 BLAKE 哈希函数一起被广泛采用。 Salsa20 及其变体是免版税的，没有专利。 Salsa20 密码将 128 位或 256 位对称密钥 + 随机生成的 64 位随机数（初始向量）和无限长度的数据流作为输入，并生成长度相同的加密数据流作为输出输入流。 ChaCha20-Poly1305Salsa20 应用最为广泛的是认证加密方案：ChaCha20-Poly1305，即组合使用 ChaCha20 与消息认证算法 Poly1305，它们都由密码学家 Bernstein 设计。 ChaCha20-Poly1305 已被证明足够安全，不过跟 GCM 一样它的安全性也依赖于足够随机的初始向量 IV，另外 ChaCha20-Poly1305 也不容易遭受计时攻击。 ChaCha20 算法比 AES 要快得多，但是目前（2025 年）主流的 PC 端 x86_64 CPU 与移动端 ARM CPU 基本都在硬件层面实现了 AES 算法，这大大提升了该算法的性能，因此 ChaCha20 算法目前反而并无性能优势。 以下是一个 ChaCha20 的 Python 示例： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' nonce = os.urandom(16) algorithm = algorithms.ChaCha20(key, nonce) # ChaCha20 是一个流密码，mode 必须为 None cipher = Cipher(algorithm, mode=None) # 1. 加密 encryptor = cipher.encryptor() ct = encryptor.update(plaintext) # 2. 解密 decryptor = cipher.decryptor() decryptor.update(ct) 3. 其他流行的对称加密算法还有一些其他的现代安全对称密码，它们的应用不如 AES 和 ChaCha20 这么广泛，但在程序员和信息安全社区中仍然很流行： Serpent - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Twofish - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Camellia - 安全对称密钥分组密码（分组大小：128 位；密钥","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#2-salsa20--chacha20"},{"categories":["tech"],"content":" 1. 安全的对称加密算法目前应用最广泛的对称加密算法，是 AES 跟 Salsa20 / ChaCha20 这两个系列。 1. AES (Rijndael) wiki: https://en.wikipedia.org/wiki/Advanced_Encryption_Standard AES（高级加密标准，也称为 Rijndael）是现代 IT 行业中最流行和广泛使用的对称加密算法。AES 被证明是高度安全、快速且标准化的，到目前为止没有发现任何明显的弱点或攻击手段，而且几乎在所有平台上都得到了很好的支持。 AES 是 128 位分组密码，使用 128、192 或 256 位密钥。它通常与分组模式组合成分组加密方案（如 AES-CTR 或 AES-GCM）以处理流数据。在大多数分组模式中，AES 还需要一个随机的 128 位初始向量 IV。 Rijndael (AES) 算法可免费用于任何用途，而且非常流行。很多站点都选择 AES 作为 TLS 协议的一部分，以实现安全通信。现代 CPU 硬件基本都在微处理器级别实现了 AES 指令以加速 AES 加密解密操作。 这里有一个纯 Python 的 AES 实现可供参考: AES encryption in pure Python - boppreh 我们在前面的 CTR 分组模式中已经使用 Python 实践了 AES-256-CTR 加密方案。而实际上更常用的是支持集成身份验证加密（AEAD）的 AES-256-GCM 加密方案，它的优势我们前面已经介绍过了，这里我们使用 Python 演示下如何使用： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import ( Cipher, algorithms, modes ) def encrypt(key, plaintext, associated_data): # Generate a random 96-bit IV. iv = os.urandom(12) # Construct an AES-GCM Cipher object with the given key and a # randomly generated IV. encryptor = Cipher( algorithms.AES(key), modes.GCM(iv), ).encryptor() # associated_data will be authenticated but not encrypted, # it must also be passed in on decryption. encryptor.authenticate_additional_data(associated_data) # Encrypt the plaintext and get the associated ciphertext. # GCM does not require padding. ciphertext = encryptor.update(plaintext) + encryptor.finalize() return (iv, ciphertext, encryptor.tag) def decrypt(key, associated_data, iv, ciphertext, tag): # Construct a Cipher object, with the key, iv, and additionally the # GCM tag used for authenticating the message. decryptor = Cipher( algorithms.AES(key), modes.GCM(iv, tag), ).decryptor() # We put associated_data back in or the tag will fail to verify # when we finalize the decryptor. decryptor.authenticate_additional_data(associated_data) # Decryption gets us the authenticated plaintext. # If the tag does not match an InvalidTag exception will be raised. return decryptor.update(ciphertext) + decryptor.finalize() # 接下来进行算法验证 plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' associated_data = b\"authenticated but not encrypted payload\" # 被用于消息认证的关联数据 # 1. 发送方加密消息 iv, ciphertext, tag = encrypt( key, plaintext, associated_data ) # 2. 发送方将 associated_data iv ciphertext tag 打包发送给接收方 # 3. 接收方使用自己的 key 验证并解密数据 descrypt_text = decrypt( key, associated_data, iv, ciphertext, tag ) 2. Salsa20 / ChaCha20 wiki: https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant Salsa20 及其改进的变体 ChaCha（ChaCha8、ChaCha12、ChaCha20）和 XSalsa20 是由密码学家 Daniel Bernstein 设计的现代、快速的对称流密码家族。 Salsa20 密码是对称流密码设计竞赛 eSTREAM（2004-2008）的决赛选手之一，它随后与相关的 BLAKE 哈希函数一起被广泛采用。 Salsa20 及其变体是免版税的，没有专利。 Salsa20 密码将 128 位或 256 位对称密钥 + 随机生成的 64 位随机数（初始向量）和无限长度的数据流作为输入，并生成长度相同的加密数据流作为输出输入流。 ChaCha20-Poly1305Salsa20 应用最为广泛的是认证加密方案：ChaCha20-Poly1305，即组合使用 ChaCha20 与消息认证算法 Poly1305，它们都由密码学家 Bernstein 设计。 ChaCha20-Poly1305 已被证明足够安全，不过跟 GCM 一样它的安全性也依赖于足够随机的初始向量 IV，另外 ChaCha20-Poly1305 也不容易遭受计时攻击。 ChaCha20 算法比 AES 要快得多，但是目前（2025 年）主流的 PC 端 x86_64 CPU 与移动端 ARM CPU 基本都在硬件层面实现了 AES 算法，这大大提升了该算法的性能，因此 ChaCha20 算法目前反而并无性能优势。 以下是一个 ChaCha20 的 Python 示例： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' nonce = os.urandom(16) algorithm = algorithms.ChaCha20(key, nonce) # ChaCha20 是一个流密码，mode 必须为 None cipher = Cipher(algorithm, mode=None) # 1. 加密 encryptor = cipher.encryptor() ct = encryptor.update(plaintext) # 2. 解密 decryptor = cipher.decryptor() decryptor.update(ct) 3. 其他流行的对称加密算法还有一些其他的现代安全对称密码，它们的应用不如 AES 和 ChaCha20 这么广泛，但在程序员和信息安全社区中仍然很流行： Serpent - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Twofish - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Camellia - 安全对称密钥分组密码（分组大小：128 位；密钥","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#chacha20-poly1305"},{"categories":["tech"],"content":" 1. 安全的对称加密算法目前应用最广泛的对称加密算法，是 AES 跟 Salsa20 / ChaCha20 这两个系列。 1. AES (Rijndael) wiki: https://en.wikipedia.org/wiki/Advanced_Encryption_Standard AES（高级加密标准，也称为 Rijndael）是现代 IT 行业中最流行和广泛使用的对称加密算法。AES 被证明是高度安全、快速且标准化的，到目前为止没有发现任何明显的弱点或攻击手段，而且几乎在所有平台上都得到了很好的支持。 AES 是 128 位分组密码，使用 128、192 或 256 位密钥。它通常与分组模式组合成分组加密方案（如 AES-CTR 或 AES-GCM）以处理流数据。在大多数分组模式中，AES 还需要一个随机的 128 位初始向量 IV。 Rijndael (AES) 算法可免费用于任何用途，而且非常流行。很多站点都选择 AES 作为 TLS 协议的一部分，以实现安全通信。现代 CPU 硬件基本都在微处理器级别实现了 AES 指令以加速 AES 加密解密操作。 这里有一个纯 Python 的 AES 实现可供参考: AES encryption in pure Python - boppreh 我们在前面的 CTR 分组模式中已经使用 Python 实践了 AES-256-CTR 加密方案。而实际上更常用的是支持集成身份验证加密（AEAD）的 AES-256-GCM 加密方案，它的优势我们前面已经介绍过了，这里我们使用 Python 演示下如何使用： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import ( Cipher, algorithms, modes ) def encrypt(key, plaintext, associated_data): # Generate a random 96-bit IV. iv = os.urandom(12) # Construct an AES-GCM Cipher object with the given key and a # randomly generated IV. encryptor = Cipher( algorithms.AES(key), modes.GCM(iv), ).encryptor() # associated_data will be authenticated but not encrypted, # it must also be passed in on decryption. encryptor.authenticate_additional_data(associated_data) # Encrypt the plaintext and get the associated ciphertext. # GCM does not require padding. ciphertext = encryptor.update(plaintext) + encryptor.finalize() return (iv, ciphertext, encryptor.tag) def decrypt(key, associated_data, iv, ciphertext, tag): # Construct a Cipher object, with the key, iv, and additionally the # GCM tag used for authenticating the message. decryptor = Cipher( algorithms.AES(key), modes.GCM(iv, tag), ).decryptor() # We put associated_data back in or the tag will fail to verify # when we finalize the decryptor. decryptor.authenticate_additional_data(associated_data) # Decryption gets us the authenticated plaintext. # If the tag does not match an InvalidTag exception will be raised. return decryptor.update(ciphertext) + decryptor.finalize() # 接下来进行算法验证 plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' associated_data = b\"authenticated but not encrypted payload\" # 被用于消息认证的关联数据 # 1. 发送方加密消息 iv, ciphertext, tag = encrypt( key, plaintext, associated_data ) # 2. 发送方将 associated_data iv ciphertext tag 打包发送给接收方 # 3. 接收方使用自己的 key 验证并解密数据 descrypt_text = decrypt( key, associated_data, iv, ciphertext, tag ) 2. Salsa20 / ChaCha20 wiki: https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant Salsa20 及其改进的变体 ChaCha（ChaCha8、ChaCha12、ChaCha20）和 XSalsa20 是由密码学家 Daniel Bernstein 设计的现代、快速的对称流密码家族。 Salsa20 密码是对称流密码设计竞赛 eSTREAM（2004-2008）的决赛选手之一，它随后与相关的 BLAKE 哈希函数一起被广泛采用。 Salsa20 及其变体是免版税的，没有专利。 Salsa20 密码将 128 位或 256 位对称密钥 + 随机生成的 64 位随机数（初始向量）和无限长度的数据流作为输入，并生成长度相同的加密数据流作为输出输入流。 ChaCha20-Poly1305Salsa20 应用最为广泛的是认证加密方案：ChaCha20-Poly1305，即组合使用 ChaCha20 与消息认证算法 Poly1305，它们都由密码学家 Bernstein 设计。 ChaCha20-Poly1305 已被证明足够安全，不过跟 GCM 一样它的安全性也依赖于足够随机的初始向量 IV，另外 ChaCha20-Poly1305 也不容易遭受计时攻击。 ChaCha20 算法比 AES 要快得多，但是目前（2025 年）主流的 PC 端 x86_64 CPU 与移动端 ARM CPU 基本都在硬件层面实现了 AES 算法，这大大提升了该算法的性能，因此 ChaCha20 算法目前反而并无性能优势。 以下是一个 ChaCha20 的 Python 示例： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes plaintext = b\"this is a paintext, hahahahahaha~\" key = b'\\x96\\xec.\\xc7\\xd5\\x1b/5\\xa1\\x10s\\x9d\\xd5\\x10z\\xdc\\x90\\xb5\\x1cm\"\u003ex\\xfd \\xd5\\xc5\\xaf\\x19\\xd1Z\\xbb' nonce = os.urandom(16) algorithm = algorithms.ChaCha20(key, nonce) # ChaCha20 是一个流密码，mode 必须为 None cipher = Cipher(algorithm, mode=None) # 1. 加密 encryptor = cipher.encryptor() ct = encryptor.update(plaintext) # 2. 解密 decryptor = cipher.decryptor() decryptor.update(ct) 3. 其他流行的对称加密算法还有一些其他的现代安全对称密码，它们的应用不如 AES 和 ChaCha20 这么广泛，但在程序员和信息安全社区中仍然很流行： Serpent - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Twofish - 安全对称密钥分组密码（密钥大小：128、192 或 256 位），公众所有（Public Domain），完全免费 Camellia - 安全对称密钥分组密码（分组大小：128 位；密钥","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#3-其他流行的对称加密算法"},{"categories":["tech"],"content":" 2. 不安全的对称加密算法如下这些对称加密算法曾经很流行，但现在被认为是不安全的或有争议的安全性，不建议再使用： DES - 56 位密钥大小，可以被暴力破解 3DES（三重 DES, TDES）- 64 位密码，被认为不安全，已在 2017 年被 NIST 弃用. RC2 - 64 位密码，被认为不安全 RC4 - 流密码，已被破解，网上存在大量它的破解资料 Blowfish - 旧的 64 位密码，已被破坏 Sweet32: Birthday attacks on 64-bit block ciphers in TLS and OpenVPN GHOST - 俄罗斯 64 位分组密码，有争议的安全性，被认为有风险 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#2-不安全的对称加密算法"},{"categories":["tech"],"content":" 对称认证加密算法 AE / AEAD我们在前面第三篇文章「MAC 与密钥派生函数 KDF」中介绍过 AE 认证加密及其变体 AEAD. 一些对称加密方案提供集成身份验证加密（AEAD），比如使用了 GCM 分组模式的加密方案 AES-GCM， 而其他加密方案（如 AES-CBC 和 AES-CTR）自身不提供身份验证能力，需要额外添加。 最流行的认证加密（AEAD）方案有如下几个，我们在之前已经简单介绍过它们： ChaCha20-Poly1305 具有集成 Poly1305 身份验证器的 ChaCha20 流密码（集成身份验证 AEAD 加密） 使用 256 位密钥和 96 位随机数（初始向量） 极高的性能 在硬件不支持 AES 加速指令时（如路由器、旧手机等硬件上），推荐使用此算法 AES-256-GCM 我们在前面的 GCM 模式一节，使用 Python 实现并验证了这个 AES-256-GCM 加密方案 使用 256 位密钥和 128 位随机数（初始向量） 较高的性能 在硬件支持 AES 加速时（如桌面、服务器等场景），更推荐使用此算法 AES-128-GCM 跟 AES-256-GCM 一样，区别在于它使用 128 位密钥，安全性弱于 ChaCha20-Poly1305 与 AES-256-GCM. 目前被广泛应用在 HTTPS 等多种加密场景下，但是正在慢慢被前面两种方案取代 今天的大多数应用程序应该优先选用上面这些加密方案进行对称加密，而不是自己造轮子。上述方案是高度安全的、经过验证的、经过良好测试的，并且大多数加密库都已经提供了高效的实现，可以说是开箱即用。 目前应用最广泛的对称加密方案应该是 AES-128-GCM，而 ChaCha20-Poly1305 因为其极高的性能，也越来越多地被应用在 TLS1.2、TLS1.3、QUIC/HTTP3、Wireguard、SSH 等协议中。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:5:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#对称认证加密算法-ae--aead"},{"categories":["tech"],"content":" 五、AES 算法案例：以太坊钱包加密在这一小节我们研究一个现实中的 AES 应用场景：以太坊区块链的标准加密钱包文件格式。我们将看到 AES-128-CTR 密码方案如何与 Scrypt 和 MAC 相结合，通过字符密码安全地实现经过身份验证的对称密钥加密。 以太坊 UTC / JSON 钱包在比特币和以太坊等区块链网络中，区块链资产持有者的私钥存储在称为加密钱包的特殊密钥库中。通常，这些加密钱包是本地硬盘上的文件，并使用字符密码加密。 在以太坊区块链中，加密钱包以一种特殊的加密格式在内部存储，称为「UTC / JSON 钱包（密钥库文件）」或「Web3 秘密存储定义」。这是一种加密钱包的文件格式，被广泛应用在 geth 和 Parity（以太坊的主要协议实现）、MyEtherWallet（流行的在线客户端以太坊钱包）、MetaMask（广泛使用的浏览器内以太坊钱包）、ethers.js 和 Nethereum 库以及许多其他与以太坊相关的技术和工具中。 以太坊 UTC/JSON 密钥库将加密的私钥、加密数据、加密算法及其参数保存为 JSON 文本文档。 UTC / JSON 钱包的一个示例如下： yaml { \"version\": 3, \"id\": \"07a9f767-93c5-4842-9afd-b3b083659f04\", \"address\": \"aef8cad64d29fcc4ed07629b9e896ebc3160a8d0\", \"Crypto\": { \"ciphertext\": \"99d0e66c67941a08690e48222a58843ef2481e110969325db7ff5284cd3d3093\", \"cipherparams\": { \"iv\": \"7d7fabf8dee2e77f0d7e3ff3b965fc23\" }, \"cipher\": \"aes-128-ctr\", \"kdf\": \"scrypt\", \"kdfparams\": { \"dklen\": 32, \"salt\": \"85ad073989d461c72358ccaea3551f7ecb8e672503cb05c2ee80cfb6b922f4d4\", \"n\": 8192, \"r\": 8, \"p\": 1, }, \"mac\": \"06dcf1cc4bffe1616fafe94a2a7087fd79df444756bb17c93af588c3ab02a913\", }, } 上述 json 内容也是认证对称加密的一个典型示例，可以很容易分析出它的一些组成成分： kdf: 用于从字符密码派生出密钥的 KDF 算法名称，这里用的是 scrypt kdfparams: KDF 算法的参数，如迭代参数、盐等… ciphertext: 钱包内容的密文，通常这就是一个被加密的 256 位私钥 cipher + cipherparams: 对称加密算法的名称及参数，这里使用了 AES-128-CTR，并给出了初始向量 IV mac: 由 MAC 算法生成的消息认证码，被用于验证解密密码的正确性 以太坊使用截取派生密钥的一部分，拼接上完整密文，然后进行 keccak-256 哈希运算得到 MAC 值 其他钱包相关的信息 默认情况下，密钥派生函数是 scrypt 并使用的是弱 scrypt 参数（n=8192 成本因子，r=8 块大小，p=1 并行化），因此建议使用长而复杂的密码以避免钱包被暴力解密。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:6:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#五aes-算法案例以太坊钱包加密"},{"categories":["tech"],"content":" 五、AES 算法案例：以太坊钱包加密在这一小节我们研究一个现实中的 AES 应用场景：以太坊区块链的标准加密钱包文件格式。我们将看到 AES-128-CTR 密码方案如何与 Scrypt 和 MAC 相结合，通过字符密码安全地实现经过身份验证的对称密钥加密。 以太坊 UTC / JSON 钱包在比特币和以太坊等区块链网络中，区块链资产持有者的私钥存储在称为加密钱包的特殊密钥库中。通常，这些加密钱包是本地硬盘上的文件，并使用字符密码加密。 在以太坊区块链中，加密钱包以一种特殊的加密格式在内部存储，称为「UTC / JSON 钱包（密钥库文件）」或「Web3 秘密存储定义」。这是一种加密钱包的文件格式，被广泛应用在 geth 和 Parity（以太坊的主要协议实现）、MyEtherWallet（流行的在线客户端以太坊钱包）、MetaMask（广泛使用的浏览器内以太坊钱包）、ethers.js 和 Nethereum 库以及许多其他与以太坊相关的技术和工具中。 以太坊 UTC/JSON 密钥库将加密的私钥、加密数据、加密算法及其参数保存为 JSON 文本文档。 UTC / JSON 钱包的一个示例如下： yaml { \"version\": 3, \"id\": \"07a9f767-93c5-4842-9afd-b3b083659f04\", \"address\": \"aef8cad64d29fcc4ed07629b9e896ebc3160a8d0\", \"Crypto\": { \"ciphertext\": \"99d0e66c67941a08690e48222a58843ef2481e110969325db7ff5284cd3d3093\", \"cipherparams\": { \"iv\": \"7d7fabf8dee2e77f0d7e3ff3b965fc23\" }, \"cipher\": \"aes-128-ctr\", \"kdf\": \"scrypt\", \"kdfparams\": { \"dklen\": 32, \"salt\": \"85ad073989d461c72358ccaea3551f7ecb8e672503cb05c2ee80cfb6b922f4d4\", \"n\": 8192, \"r\": 8, \"p\": 1, }, \"mac\": \"06dcf1cc4bffe1616fafe94a2a7087fd79df444756bb17c93af588c3ab02a913\", }, } 上述 json 内容也是认证对称加密的一个典型示例，可以很容易分析出它的一些组成成分： kdf: 用于从字符密码派生出密钥的 KDF 算法名称，这里用的是 scrypt kdfparams: KDF 算法的参数，如迭代参数、盐等… ciphertext: 钱包内容的密文，通常这就是一个被加密的 256 位私钥 cipher + cipherparams: 对称加密算法的名称及参数，这里使用了 AES-128-CTR，并给出了初始向量 IV mac: 由 MAC 算法生成的消息认证码，被用于验证解密密码的正确性 以太坊使用截取派生密钥的一部分，拼接上完整密文，然后进行 keccak-256 哈希运算得到 MAC 值 其他钱包相关的信息 默认情况下，密钥派生函数是 scrypt 并使用的是弱 scrypt 参数（n=8192 成本因子，r=8 块大小，p=1 并行化），因此建议使用长而复杂的密码以避免钱包被暴力解密。 ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:6:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#以太坊-utc--json-钱包"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book A complete overview of SSL/TLS and its cryptographic system AES encryption in pure Python - boppreh Block_cipher_mode_of_operation_wiki Galois/Counter_Mode_wiki ","date":"2022-03-06","objectID":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/:7:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","对称加密","安全","AES","ChaCha20"],"title":"写给开发人员的实用密码学（六）—— 对称密钥加密算法","uri":"/posts/practical-cryptography-basics-6-symmetric-key-ciphers/#参考"},{"categories":["life"],"content":" 原文：哪一刻你发现年轻人正在悄悄改变社会？ - 赦己 我的读后感：他的眼里有光！ 之前见过一个特别厉害的面试者，让我觉得，老一辈真的是老一辈了，他放弃了一个月薪一万三，十三薪的工作，他的演讲让我记忆非常深刻，也使得面试官面面相觑。 这个岗位算是万人过独木桥，不仅海内的很多大学生在竞争，海外的很多大学生也在努力，整个过程是这样的：简历筛选-线上一面-线上hr二面-线下主管面试-总裁轮面试（压力轮）。 我们都到了最后的一轮面试,本来就是压力轮面试，但是那天不知道为什么总裁的脾气很暴躁，对他冷嘲热讽，说了一些比较难听的话，大概的意思就是“你还小，以后需要认真学，你们太嫩了”，其实总裁的意思非常明确了，会招他，但是他太嫩需要学很多东西，但是就是他这样大人看小屁孩的感觉惹怒了他，后面他的演讲就是十分高能了，我尽量原文复述。 「你坐在我前面会不会有点点害怕呢？你看看你身边有什么人可以给你参考吗？你没有，你只能战战兢兢如履薄冰，走错一步都是深渊。你知道你在我眼里是什么吗？你只是一个猎物，一个我追逐的、猎杀的的目标，其实你哪里来的自信呢？就凭你是这个公司的总裁吗？来自职级和制度的压力我一概不屑， 反而觉得是黔驴技穷，小人做法，我不会服气，只是照做而已。 其实我也很享受被统治的感觉，上一个能统治我的人已经很久了，你知道那种纯粹的实力压服吗？我可以毫无保留地顺从他的任何意见，我从来不怀疑他的任何决定，哪怕行动后面失败了我也觉得他是对的。但是你呢？只是来自制度的威力，你的每一个决定都会遭到我的质疑。 我最讨厌的就是别人和我说，我想让你去做点什么但是你能力还不够，简直瞎扯淡，其实是你能力不够，作为一个管理者，你甚至不知道怎么用我，我如何为你卖命啊？ 我渴望的是在一个稳定的环境默默耕耘，把坏的变成好的，但是前提是我们够团队，你呢？凭你作为一个过来人的经验吗？这些东西经过时间大家都会有的，你还有其他的吗？你真的有能力把我变成你的三头六臂吗？你真的控制得住我吗？」 复述其实没那么精彩了，他支着手目光瞪着总裁的眼睛的时候超级精彩，后面他去了一个对手小公司， 相当于这边的市值来了，相差了十倍之多，但是七个月之后再见面已是兵刃交接，他成了六个人团队的小主管，耀武扬威地围着我们总部办公地盘下了一圈广告。 ","date":"2022-03-04","objectID":"/posts/the-thoughtful-youth/:0:0","series":null,"tags":[],"title":"「转」且看有思想的年轻人","uri":"/posts/the-thoughtful-youth/#"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book，笔者额外补充了 DHKE/ECDH 的代码示例，以及「PFS 完美前向保密协议 DHE/ECDHE」一节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:0:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#"},{"categories":["tech"],"content":" 一、前言在密码学中密钥交换是一种协议，功能是在两方之间安全地交换加密密钥，其他任何人都无法获得密钥的副本。通常各种加密通讯协议的第一步都是密钥交换。密钥交换技术具体来说有两种方案： 密钥协商：协议中的双方都参与了共享密钥的生成，两个代表算法是 Diffie-Hellman (DHKE) 和 Elliptic-Curve Diffie-Hellman (ECDH) 密钥传输：双方中其中一方生成出共享密钥，并通过此方案将共享密钥传输给另一方。密钥传输方案通常都通过公钥密码系统实现。比如在 RSA 密钥交换中，客户端使用它的私钥加密一个随机生成的会话密钥，然后将密文发送给服务端，服务端再使用它的公钥解密出会话密钥。 密钥交换协议无时无刻不在数字世界中运行，在你连接 WiFi 时，或者使用 HTTPS 协议访问一个网站，都会执行密钥交换协议。密钥交换有很多手段，常见手段有匿名的 DHKE 密钥协商协议、密码或预共享密钥、数字证书等等。有些通讯协议只在开始时交换一次密钥，而有些协议则会随着时间的推移不断地交换密钥。 认证密钥交换（ACHE）是一种会同时认证相关方身份的密钥交换协议，比如个人 WiFi 通常就会使用 password-authenticated key agreement (PAKE)，而如果你连接的是公开 WiFi，则会使用匿名密钥交换协议。 目前有许多用于密钥交换的密码算法。其中一些使用公钥密码系统，而另一些则使用更简单的密钥交换方案（如 Diffie-Hellman 密钥交换）；其中有些算法涉及服务器身份验证，也有些涉及客户端身份验证；其中部分算法使用密码，另一部分使用数字证书或其他身份验证机制。下面列举一些知名的密钥交换算法： Diffie-Hellman Key Exchange (DHКЕ)：传统的、应用最为广泛的密钥交换协议 Elliptic-curve Diffie–Hellman (ECDH)：基于椭圆曲线密码学的密钥交换算法，DHKE 的继任者 RSA-OAEP 和 RSA-KEM（RSA 密钥传输） PSK（预共享密钥） SRP（安全远程密码协议） FHMQV（Fully Hashed Menezes-Qu-Vanstone） ECMQV（Ellictic-Curve Menezes-Qu-Vanstone） CECPQ1（量子安全密钥协议） ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:1:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#一前言"},{"categories":["tech"],"content":" 二、Diffie–Hellman 密钥交换迪菲-赫尔曼密钥交换（Diffie–Hellman Key Exchange）是一种安全协议，它可以让双方在完全没有对方任何预先信息的条件下通过不安全信道安全地协商出一个安全密钥，而且任何窃听者都无法得知密钥信息。这个密钥可以在后续的通讯中作为对称密钥来加密通讯内容。 DHKE 可以防范嗅探攻击（窃听），但是无法抵挡中间人攻击（中继）。 DHKE 有两种实现方案： 传统的 DHKE 算法：使用离散对数实现 基于椭圆曲线密码学的 ECDH 为了理解 DHKE 如何实现在「大庭广众之下」安全地协商出密钥，我们首先使用色彩混合来形象地解释下它大致的思路。 跟编程语言的 Hello World 一样，密钥交换的解释通常会使用 Alice 跟 Bob 来作为通信双方。现在他俩想要在公开的信道上，协商出一个秘密色彩出来，但是不希望其他任何人知道这个秘密色彩。他们可以这样做： 分步解释如下： 首先 Alice 跟 Bob 沟通，确定一个初始的色彩，比如黄色。这个沟通不需要保密。 然后，Alice 跟 Bob 分别偷偷地选择出一个自己的秘密色彩，这个就得保密啦。 现在 Alice 跟 Bob，分别将初始色彩跟自己选择的秘密色彩混合，分别得到两个混合色彩。 之后，Alice 跟 Bob 再回到公开信道上，交换双方的混合色彩。 我们假设在仅知道初始色彩跟混合色彩的情况下，很难推导出被混合的秘密色彩。这样第三方就猜不出 Bob 跟 Alice 分别选择了什么秘密色彩了。 最后 Alice 跟 Bob 再分别将自己的秘密色彩，跟对方的混合色彩混合，就得到了最终的秘密色彩。这个最终色彩只有 Alice 跟 Bob 知道，信道上的任何人都无法猜出来。 DHKE 协议也是基于类似的原理，但是使用的是离散对数（discrete logarithms）跟模幂（modular exponentiations）而不是色彩混合。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:2:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#二diffiehellman-密钥交换"},{"categories":["tech"],"content":" 三、经典 DHKE 协议","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:3:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#三经典-dhke-协议"},{"categories":["tech"],"content":" 基础数学知识首先介绍下「模幂（modular exponentiations）」，它是指求 $g$ 的 $a$ 次幂模 $p$ 的值 $c$ 的过程，其中 $g$ $a$ $p$ $c$ 均为整数，公式如下： $$ g^a \\mod p = c $$ 而「离散对数（discrete logarithms）」，其实就是指模幂的逆运算，它使用如下公式表示： $$ Ind_{g}c \\equiv a {\\pmod {p}} $$ 上述公式，即指在已知整数 $g$，质数 $p$，以及余数（p 的一个原根） $c$ 的情况下，求使前面的模幂等式成立的幂指数 $a$。 已知使用计算机计算上述「模幂」是非常快速的，但是在质数 $p$ 非常大的情况下，求「离散对数」却是非常难的，这就是「离散对数难题」。 然后为了理解 DHKE 的原理，我们还需要了解下模幂运算的一个性质： $$ g^{ab} \\mod p = {(g^a \\mod p)}^b \\mod p $$ 懂了上面这些基础数学知识，下面就开始介绍 DHKE 算法。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:3:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#基础数学知识"},{"categories":["tech"],"content":" DHKE 密钥交换流程下面该轮到 Alice 跟 Bob 出场来介绍 DHKE 的过程了，先看图（下面绿色表示非秘密信息，红色表示秘密信息）： Alice 跟 Bob 协定使用两个比较独特的正整数 $p$ 跟$g$ 假设 $p=23$, $g=5$ Alice 选择一个秘密整数 $a$，计算$A$$= g^a \\mod p$ 并发送给 Bob 假设 $a=4$，则$A$$= 5^4 \\mod 23 = 4$ Bob 也选择一个秘密整数 $b$，计算$B$$= g^b \\mod p$ 并发送给 Alice 假设 $b=3$，则$B$$= 5^3 \\mod 23 = 10$ Alice 计算 $S_1 = B^a \\mod p$ $S_1 = 10^4 \\mod 23 = 18$ Bob 计算 $S_2 = A^b \\mod p$ $S_2 = 4^3 \\mod 23 = 18$ 已知 $B^a \\mod p = g^{ab} \\mod p = A^b \\mod p$，因此$S_1 = S_2 = S$ 这样 Alice 跟 Bob 就协商出了密钥 $S$ 因为离散对数的计算非常难，任何窃听者都几乎不可能通过公开的 $p$ $g$ $A$ $B$ 逆推出 $S$ 的值 在最常见的 DHKE 实现中（RFC3526），基数是 $g = 2$， 模数 $p$ 是一个 1536 到 8192 比特的大素数。而整数 $A$ $B$ 通常会使用非常大的数字（1024、2048 或 4096 比特甚至更大）以防范暴力破解。 DHKE 协议基于 Diffie-Hellman 问题的实际难度，这是计算机科学中众所周知的离散对数问题（DLP） 的变体，目前还不存在有效的算法。 使用 Python 演示下大概是这样： python # pip install cryptography==36.0.1 from cryptography.hazmat.primitives import hashes from cryptography.hazmat.primitives.asymmetric import dh # 1. 双方协商使用两个独特的正整数 g 与 p ## generator =\u003e 即基数 g，通常使用 2, 有时也使用 5 ## key_size =\u003e 模数 p 的长度，通常使用 2048-3072 位（2048 位的安全性正在减弱） params = dh.generate_parameters(generator=2, key_size=2048) param_numbers = params.parameter_numbers() g = param_numbers.g # =\u003e 肯定是 2 p = param_numbers.p # =\u003e 一个 2048 位的整数 print(f\"{g=}, {p=}\") # 2. Alice 生成自己的秘密整数 a 与公开整数 A alice_priv_key = params.generate_private_key() a = alice_priv_key.private_numbers().x A = alice_priv_key.private_numbers().public_numbers.y print(f\"{a=}\") print(f\"{A=}\") # 3. Bob 生成自己的秘密整数 b 与公开整数 B bob_priv_key = params.generate_private_key() b = bob_priv_key.private_numbers().x B = bob_priv_key.private_numbers().public_numbers.y print(f\"{b=}\") print(f\"{B=}\") # 4. Alice 与 Bob 公开交换整数 A 跟 B（即各自的公钥） # 5. Alice 使用 a B 与 p 计算出共享密钥 ## 首先使用 B p g 构造出 bob 的公钥对象（实际上 g 不参与计算） bob_pub_numbers = dh.DHPublicNumbers(B, param_numbers) bob_pub_key = bob_pub_numbers.public_key() ## 计算共享密钥 alice_shared_key = alice_priv_key.exchange(bob_pub_key) # 6. Bob 使用 b A 与 p 计算出共享密钥 ## 首先使用 A p g 构造出 alice 的公钥对象（实际上 g 不参与计算） alice_pub_numbers = dh.DHPublicNumbers(A, param_numbers) alice_pub_key = alice_pub_numbers.public_key() ## 计算共享密钥 bob_shared_key = bob_priv_key.exchange(alice_pub_key) # 两者应该完全相等， Alice 与 Bob 完成第一次密钥交换 alice_shared_key == bob_shared_key # 7. Alice 与 Bob 使用 shared_key 进行对称加密通讯 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:3:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#dhke-密钥交换流程"},{"categories":["tech"],"content":" 四、新一代 ECDH 协议Elliptic-Curve Diffie-Hellman (ECDH) 是一种匿名密钥协商协议，它允许两方，每方都有一个椭圆曲线公钥-私钥对，它的功能也是让双方在完全没有对方任何预先信息的条件下通过不安全信道安全地协商出一个安全密钥。 ECDH 是经典 DHKE 协议的变体，其中模幂计算被椭圆曲线的乘法计算取代，以提高安全性。 ECDH 跟前面介绍的 DHKE 非常相似，只要你理解了椭圆曲线的数学原理，结合前面已经介绍了的 DHKE，基本上可以秒懂。我会在后面「非对称算法」一文中简单介绍椭圆曲线的数学原理，不过这里也可以先提一下 ECDH 依赖的公式（其中 $a, b$ 为常数，$G$ 为椭圆曲线上的某一点的坐标 $(x, y)$）： $$ (a * G) * b = (b * G) * a $$ 这个公式还是挺直观的吧，感觉小学生也能理解个大概。下面简单介绍下 ECDH 的流程： Alice 跟 Bob 协商好椭圆曲线的各项参数，以及基点 G，这些参数都是公开的。 Alice 生成一个随机的 ECC 密钥对（公钥：$alicePrivate * G$, 私钥: $alicePrivate$） Bob 生成一个随机的 ECC 密钥对（公钥：$bobPrivate * G$, 私钥: $bobPrivate$） 两人通过不安全的信道交换公钥 Alice 将 Bob 的公钥乘上自己的私钥，得到共享密钥 $sharedKey = (bobPrivate * G) * alicePrivate$ Bob 将 Alice 的公钥乘上自己的私钥，得到共享密钥 $sharedKey = (alicePrivate * G) * bobPrivate$ 因为前面提到的公式，Alice 与 Bob 计算出的共享密钥应该是相等的 这样两方就通过 ECDH 完成了密钥交换。 而 ECDH 的安全性，则由 ECDLP 问题提供保证。这个问题是说，「通过公开的 $kG$ 以及 $G$ 这两个参数，目前没有有效的手段能快速求解出 $k$ 的值。」 从上面的流程中能看到，公钥就是 ECDLP 中的 $kG$，另外 $G$ 也是公开的，而私钥就是 ECDLP 中的 $k$。因为 ECDLP 问题的存在，攻击者破解不出 Alice 跟 Bob 的私钥。 代码示例： python # pip install tinyec # ECC 曲线库 from tinyec import registry import secrets def compress(pubKey): return hex(pubKey.x) + hex(pubKey.y % 2)[2:] curve = registry.get_curve('brainpoolP256r1') alicePrivKey = secrets.randbelow(curve.field.n) alicePubKey = alicePrivKey * curve.g print(\"Alice public key:\", compress(alicePubKey)) bobPrivKey = secrets.randbelow(curve.field.n) bobPubKey = bobPrivKey * curve.g print(\"Bob public key:\", compress(bobPubKey)) print(\"Now exchange the public keys (e.g. through Internet)\") aliceSharedKey = alicePrivKey * bobPubKey print(\"Alice shared key:\", compress(aliceSharedKey)) bobSharedKey = bobPrivKey * alicePubKey print(\"Bob shared key:\", compress(bobSharedKey)) print(\"Equal shared keys:\", aliceSharedKey == bobSharedKey) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:4:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#四新一代-ecdh-协议"},{"categories":["tech"],"content":" 五、PFS 完美前向保密协议 DHE/ECDHE前面介绍的经典 DHKE 与 ECDH 协议流程，都是在最开始时交换一次密钥，之后就一直使用该密钥通讯。因此如果密钥被破解，整个会话的所有信息对攻击者而言就完全透明了。 为了进一步提高安全性，密码学家提出了「完全前向保密（Perfect Forward Secrecy，PFS）」的概念，并在 DHKE 与 ECDH 的基础上提出了支持 PFS 的 DHE/ECDHE 协议（末尾的 E 是ephemeral 的缩写，即指所有的共享密钥都是临时的）。 「完全前向保密 PFS」是指长期使用的主密钥泄漏不会导致过去的会话密钥泄漏，从而保护过去进行的通讯不受密码或密钥在未来暴露的威胁。 下面使用 Python 演示下 DHE 协议的流程（ECDHE 的流程也完全类似）： python # pip install cryptography==36.0.1 from cryptography.hazmat.primitives import hashes from cryptography.hazmat.primitives.asymmetric import dh # 1. 双方协商使用两个独特的正整数 g 与 p ## generator =\u003e 即基数 g，通常使用 2, 有时也使用 5 ## key_size =\u003e 模数 p 的长度，通常使用 2048-3072 位（2048 位的安全性正在减弱） params = dh.generate_parameters(generator=2, key_size=2048) param_numbers = params.parameter_numbers() g = param_numbers.g # =\u003e 肯定是 2 p = param_numbers.p # =\u003e 一个 2048 位的整数 print(f\"{g=}, {p=}\") # 2. Alice 生成自己的秘密整数 a 与公开整数 A alice_priv_key = params.generate_private_key() a = alice_priv_key.private_numbers().x A = alice_priv_key.private_numbers().public_numbers.y print(f\"{a=}\") print(f\"{A=}\") # 3. Bob 生成自己的秘密整数 b 与公开整数 B bob_priv_key = params.generate_private_key() b = bob_priv_key.private_numbers().x B = bob_priv_key.private_numbers().public_numbers.y print(f\"{b=}\") print(f\"{B=}\") # 4. Alice 与 Bob 公开交换整数 A 跟 B（即各自的公钥） # 5. Alice 使用 a B 与 p 计算出共享密钥 ## 首先使用 B p g 构造出 bob 的公钥对象（实际上 g 不参与计算） bob_pub_numbers = dh.DHPublicNumbers(B, param_numbers) bob_pub_key = bob_pub_numbers.public_key() ## 计算共享密钥 alice_shared_key = alice_priv_key.exchange(bob_pub_key) # 6. Bob 使用 b A 与 p 计算出共享密钥 ## 首先使用 A p g 构造出 alice 的公钥对象（实际上 g 不参与计算） alice_pub_numbers = dh.DHPublicNumbers(A, param_numbers) alice_pub_key = alice_pub_numbers.public_key() ## 计算共享密钥 bob_shared_key = bob_priv_key.exchange(alice_pub_key) # 上面的流程跟经典 DHKE 完全一致，代码也是从前面 Copy 下来的 # 但是从这里开始，进入 DHE 协议补充的部分 shared_key_1 = bob_shared_key # 第一个共享密钥 # 7. 假设 Bob 现在要发送消息 M_b_1 给 Alice ## 首先 Bob 使用对称加密算法加密消息 M_b M_b_1 = \"Hello Alice, I'm bob~\" C_b_1 = Encrypt(M_b_1, shared_key_1) # Encrypt 是某种对称加密方案的加密算法，如 AES-256-CTR-HMAC-SHA-256 ## 然后 Bob 需要生成一个新的公私钥 b_2 与 B_2（注意 g 与 p 两个参数是不变的） bob_priv_key_2 = parameters.generate_private_key() b_2 = bob_priv_key.private_numbers().x B_2 = bob_priv_key.private_numbers().public_numbers.y print(f\"{b_2=}\") print(f\"{B_2=}\") # 8. Bob 将 C_b_1 与 B_2 一起发送给 Alice # 9. Alice 首先解密数据 C_b_1 得到原始消息 M_b_1 assert M_b_1 == Decrypt(C_b_1, shared_key_1) # Dncrypt 是某种对称加密方案的解密算法，如 AES-256-CTR-HMAC-SHA-256 ## 然后 Alice 也生成新的公私钥 a_2 与 A_2 alice_priv_key_2 = parameters.generate_private_key() ## Alice 使用 a_2 B_2 与 p 计算出新的共享密钥 shared_key_2 bob_pub_numbers_2 = dh.DHPublicNumbers(B_2, param_numbers) bob_pub_key_2 = bob_pub_numbers_2.public_key() shared_key_2 = alice_priv_key_2.exchange(bob_pub_key_2) # 10. Alice 回复 Bob 消息时，使用新共享密钥 shared_key_2 加密消息得到 C_a_1 # 然后将密文 C_a_1 与 A_2 一起发送给 Bob # 11. Bob 使用 b_2 A_2 与 p 计算出共享密钥 shared_key_2 # 然后再使用 shared_key_2 解密数据 # Bob 在下次发送消息时，会生成新的 b_3 与 B_3，将 B_3 随密文一起发送 ## 依次类推 通过上面的代码描述我们应该能理解到，Alice 与 Bob 每次交换数据，实际上都会生成新的临时共享密钥，公钥密钥在每次数据交换时都会更新。即使攻击者花了很大的代价破解了其中某一个临时共享密钥 shared_key_k（或者该密钥因为某种原因泄漏了），TA 也只能解密出其中某一次数据交换的信息 M_b_k，其他所有的消息仍然是保密的，不受此次攻击（或泄漏）的影响。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:5:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#五pfs-完美前向保密协议-dheecdhe"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book A complete overview of SSL/TLS and its cryptographic system ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-5-key-exchange/:6:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","密钥交换","安全","DH","DHE","ECDH","ECDHE"],"title":"写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS","uri":"/posts/practical-cryptography-basics-5-key-exchange/#参考"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:0:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#"},{"categories":["tech"],"content":" 一、前言在密码学中，随机性（熵）扮演了一个非常重要的角色，许多密码学算法都要求使用一个不可预测的随机数，只有在生成的随机数不可预测时，这些算法才能保证其安全性。 比如 MAC 算法中的 key 就必须是一个不可预测的值，在这个条件下 MAC 值才是不可伪造的。 另外许多的高性能算法如快速排序、布隆过滤器、蒙特卡洛方法等，都依赖于随机性，如果随机性可以被预测，或者能够找到特定的输入值使这些算法变得特别慢，那黑客就能借此对服务进行 DDoS 攻击， 以很小的成本达到让服务不可用的目的。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:1:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#一前言"},{"categories":["tech"],"content":" 二、PRNG 伪随机数生成器Pseudo-Random Number Generators(PRNG) 是一种数字序列的生成算法，它生成出的数字序列的统计学属性跟真正的随机数序列非常相似，但它生成的伪随机数序列并不是真正的随机数序列！因为该序列完全依赖于提供给 PRNG 的初始值，这个值被称为 PRNG 的种子。 算法流程如下，算法的每次迭代都生成出一个新的伪随机数： 如果输入的初始种子是相同的，PRNG 总是会生成出相同的伪随机数序列，因此 PRNG 也被称为 Deterministic Random Bit Generator (DRBG)，即确定性随机比特生成器。 实际上目前也有所谓的「硬件随机数生成器 TRNG」能生成出真正的随机数，但是因为 PRNG 的高速、低成本、可复现等原因，它仍然被大量使用在现代软件开发中。 PRNG 可用于从一个很小的初始随机性（熵）生成出大量的伪随机性，这被称做「拉伸 （Stretching）」。 PRNG 被广泛应用在前面提到的各种依赖随机性的高性能算法以及密码学算法中。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:2:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#二prng-伪随机数生成器"},{"categories":["tech"],"content":" PRNG 的实现我们在上一篇文章的「MAC 的应用」一节中提到，一个最简单的 PRNG 可以直接使用 MAC 算法实现， 用 Python 实现如下： python import hmac, hashlib def random_number_generator(seed: bytes, max_num: int): state = seed counter = 0 while True: state = hmac.new(state, bytes(counter), hashlib.sha1).digest() counter += 1 # 这里取余实际上是压缩了信息，某种程度上说，这可以保证内部的真实状态 state 不被逆向出来 yield int.from_bytes(state, byteorder=\"big\") % max_num # 测试下，计算 20 个 100 以内的随机数 gen = random_number_generator(b\"abc\", 100) print([next(gen) for _ in range(20)]) # =\u003e [71, 41, 52, 18, 51, 14, 58, 30, 70, 20, 59, 93, 3, 10, 81, 63, 48, 67, 18, 36] ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:2:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#prng-的实现"},{"categories":["tech"],"content":" 三、随机性 - 熵如果初始的 PRNG 种子是完全不可预测的，PRNG 就能保证整个随机序列都不可预测。 因此在 PRNG 中，生成出一个足够随机的种子，就变得非常重要了。 一个最简单的方法，就是收集随机性。对于桌面电脑，随机性可以从鼠标的移动点击、按键事件、网络状况等随机输入来收集。这个事情是由操作系统在内核中处理的，内核会直接为应用程序提供随机数获取的 API，比如 Linux/MacOSX 的 /dev/random 虚拟设备。 如果这个熵的生成有漏洞，就很可能造成严重的问题，一个现实事件就是安卓的 java.security.SecureRandom 漏洞导致安卓用户的比特币钱包失窃。 Python 的 random 库默认会使用当前时间作为初始 seed，这显然是不够安全的——黑客如果知道你运行程序的大概时间，就能通过遍历的方式暴力破解出你的随机数来！ ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:3:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#三随机性---熵"},{"categories":["tech"],"content":" 四、CSPRNG 密码学安全随机数生成器Cryptography Secure Random Number Generators(CSPRNG) 是一种适用于密码学领域的 PRNG，一个 PRNG 如果能够具备如下两个条件，它就是一个 CSPRNG: 能通过「下一比特测试 next-bit test」：即使有人获知了该 PRNG 的 k 位，他也无法使用合理的资源预测第 k+1 位的值 如果攻击者猜出了 PRNG 的内部状态或该状态因某种原因而泄漏，攻击者也无法重建出内部状态泄漏之前生成的所有随机数 有许多的设计都被证明可以用于构造一个 CSPRNG: 基于计数器(CTR)模式下的安全分组密码、流密码或安全哈希函数的 CSPRNG 基于数论设计的 CSPRNG，它依靠整数分解问题（IFP）、离散对数问题（DLP）或椭圆曲线离散对数问题（ECDLP）的高难度来确保安全性 CSPRNG 基于加密安全随机性的特殊设计，例如 Yarrow algorithm 和 Fortuna，这俩分别被用于 MacOS 和 FreeBSD. 大多数的 CSPRNG 结合使用来自 OS 的熵与高质量的 PRNG，并且一旦系统生成了新的熵（这可能来自用户输入、磁盘 IO、系统中断、或者硬件 RNG），CSPRNG 会立即使用新的熵来作为 PRNG 新的种子。这种不断重置 PRNG 种子的行为，使随机数变得非常难以预测。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:4:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#四csprng-密码学安全随机数生成器"},{"categories":["tech"],"content":" CSPRNG 的用途 加密程序：因为 OS 中熵的收集很缓慢，等待收集到足够多的熵再进行运算是不切实际的，因此很多的加密程序都使用 CSPRNG 来从系统的初始熵生成出足够多的伪随机熵。 其他需要安全随机数的场景 emmmm ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#csprng-的用途"},{"categories":["tech"],"content":" 如何在代码中使用 CSPRNG多数系统都内置了 CSPRNG 算法并提供了内核 API，Unix-like 系统都通过如下两个虚拟设备提供 CSPRNG: /dev/random（受限阻塞随机生成器）: 从这个设备中读取到的是内核熵池中已经收集好的熵，如果熵池空了，此设备会一直阻塞，直到收集到新的环境噪声。 /dev/urandom（不受限非阻塞随机生成器）: 它可能会返回内核熵池中的熵，也可能返回使用「之前收集的熵 + CSPRNG」计算出的安全伪随机数。它不会阻塞。 编程语言的 CSPRNG 接口或库如下： Java: java.security.SecureRandom Python: secrets 库或者 os.urandom() C#: System.Security.Cryptography.RandomNumberGenerator.Create() JavaScript: 客户端可使用 window.crypto.getRandomValues(Uint8Array)，服务端可使用crypto.randomBytes() 比如使用 Python 实现一个简单但足够安全的随机密码生成器： python import secrets import string chars = string.digits + \"your_custom_-content\" + string.ascii_letters def random_string(length: int): \"\"\"生成随机字符串\"\"\" # 注意，这里不应该使用 random 库！而应该使用 secrets code = \"\".join(secrets.choice(chars) for _ in range(length)) return code random_string(24) # =\u003e _rebBfgYs4OtkrPbYtnGmc4n ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:5:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#如何在代码中使用-csprng"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-4-secure-random-generators/:6:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","密码学","伪随机数","安全","PRNG","CSPRNG"],"title":"写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG","uri":"/posts/practical-cryptography-basics-4-secure-random-generators/#参考"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book，笔者补充了 HMAC 的 Python 实现以及 scrypt 使用示例。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:0:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#"},{"categories":["tech"],"content":" 一、MAC 消息认证码MAC 消息认证码，即 Message Authentication Code，是用于验证消息的一小段附加数据。换句话说， 能用它确认消息的真实性——消息来自指定的发件人并且没有被篡改。 MAC 值通过允许**拥有密钥的验证者检测消息内容的任何更改来保护消息的数据完整性及其真实性。 一个安全的 MAC 函数，跟加密哈希函数非常类似，也拥有如下特性： 快速：计算速度要足够快 确定性：对同样的消息跟密钥，应该总是产生同样的输出 难以分析：对消息或密钥的任何微小改动，都应该使输出完全发生变化 不可逆：从 MAC 值逆向演算出消息跟密钥应该是不可行的。 无碰撞：找到具有相同哈希的两条不同消息应该非常困难（或几乎不可能） 但是 MAC 算法比加密哈希函数多一个输入值：密钥，因此也被称为 keyed hash functions，即「加密钥的哈希函数」。 如下 Python 代码使用 key 跟 消息计算出对应的 HMAC-SHA256 值： python import hashlib, hmac, binascii key = b\"key\" msg = b\"some msg\" mac = hmac.new(key, msg, hashlib.sha256).digest() print(f\"HMAC-SHA256({key}, {msg})\", binascii.hexlify(mac).decode('utf8')) # =\u003e HMAC-SHA256(b'key', b'some msg') = 32885b49c8a1009e6d66662f8462e7dd5df769a7b725d1d546574e6d5d6e76ad HMAC 的算法实际上非常简单，我参考 wiki/HMAC 给出的伪码，编写了下面这个 Python 实现，没几行代码，但是完全 work： python import hashlib, binascii def xor_bytes(b1, b2): return bytes(a ^ c for a, c in zip(b1, b2)) def my_hmac(key, msg, hash_name): # hash =\u003e (block_size, output_size) # 单位是 bytes，数据来源于 https://en.wikipedia.org/wiki/HMAC hash_size_dict = { \"md5\": (64, 16), \"sha1\": (64, 20), \"sha224\": (64, 28), \"sha256\": (64, 32), # \"sha512/224\": (128, 28), # 这俩算法暂时不清楚在 hashlib 里叫啥名 # \"sha512/256\": (128, 32), \"sha_384\": (128, 48), \"sha_512\": (128, 64), \"sha3_224\": (144, 28), \"sha3_256\": (136, 32), \"sha3_384\": (104, 48), \"sha3_512\": (72, 64), } if hash_name not in hash_size_dict: raise ValueError(\"unknown hash_name\") block_size, output_size = hash_size_dict[hash_name] hash_ = getattr(hashlib, hash_name) # 确保 key 的长度为 block_size block_sized_key = key if len(key) \u003e block_size: block_sized_key = hash_(key).digest() # 用 hash 函数进行压缩 if len(key) \u003c block_size: block_sized_key += b'\\x00' * (block_size - len(key)) # 末尾补 0 o_key_pad = xor_bytes(block_sized_key, (b\"\\x5c\" * block_size)) # Outer padded key i_key_pad = xor_bytes(block_sized_key, (b\"\\x36\" * block_size)) # Inner padded key return hash_(o_key_pad + hash_(i_key_pad + msg).digest()).digest() # 下面验证下 key = b\"key\" msg = b\"some msg\" mac_ = my_hmac(key, msg, \"sha256\") print(f\"HMAC-SHA256({key}, {msg})\", binascii.hexlify(mac_).decode('utf8')) # 输出跟标准库完全一致： # =\u003e HMAC-SHA256(b'key', b'some msg') = 32885b49c8a1009e6d66662f8462e7dd5df769a7b725d1d546574e6d5d6e76ad ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#一mac-消息认证码"},{"categories":["tech"],"content":" MAC 与哈希函数、数字签名的区别上一篇文章提到过，哈希函数只负责生成哈希值，不负责哈希值的可靠传递。 而数字签名呢，跟 MAC 非常相似，但是数字签名使用的是非对称加密系统，更复杂，计算速度也更慢。 MAC 的功能跟数字签名一致，都是验证消息的真实性（authenticity）、完整性（integrity）、不可否认性（non-repudiation），但是 MAC 使用哈希函数或者对称密码系统来做这件事情，速度要更快， 算法也更简单。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#mac-与哈希函数数字签名的区别"},{"categories":["tech"],"content":" MAC 的应用 1. 验证消息的真实性、完整性这是最简单的一个应用场景，在通信双向都持有一个预共享密钥的前提下，通信时都附带上消息的 MAC 码。接收方也使用「收到的消息+预共享密钥」计算出 MAC 码，如果跟收到的一致，就说明消息真实无误。 注意这种应用场景中，消息是不保密的！ 2. AE 认证加密 - Authenticated encryption常用的加密方法只能保证数据的保密性，并不能保证数据的完整性。 而这里介绍的 MAC 算法，或者还未介绍的基于非对称加密的数字签名，都只能保证数据的真实性、完整性，不能保证数据被安全传输。 而认证加密，就是将加密算法与 MAC 算法结合使用的一种加密方案。 在确保 MAC 码「强不可伪造」的前提下，首先对数据进行加密，然后计算密文的 MAC 码，再同时传输密文与 MAC 码，就能同时保证数据的保密性、完整性、真实性，这种方法叫 Encrypt-then-MAC, 缩写做 EtM. 接收方在解密前先计算密文的 MAC 码与收到的对比，就能验证密文的完整性与真实性。 AE 有一种更安全的变体——带有关联数据的认证加密 (authenticated encryption with associated data，AEAD)。AEAD 将「关联数据(Associated Data, AD)」——也称为「附加验证数据 （Additional Authenticated Data, AAD）」——绑定到密文和它应该出现的上下文，以便可以检测和拒绝将有效密文“剪切并粘贴”到不同上下文的尝试。 AEAD 用于加密和未加密数据一起使用的场景（例如，在加密的网络协议中），并确保整个数据流经过身份验证和完整性保护。换句话说，AEAD 增加了检查某些内容的完整性和真实性的能力。 我们会在第六章「对称加密算法」中看到如何通过 Python 使用 AEAD 加密方案 AES-256-GCM. 3. 基于 MAC 的伪随机数生成器MAC 码的另一个用途就是伪随机数生成函数，相比直接使用熵+哈希函数的进行伪随机数计算，MAC 码因为多引入了一个变量 key，理论上它会更安全。 这种场景下，我们称 MAC 使用的密钥为 salt，即盐。 text next_seed = MAC(salt, seed) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#mac-的应用"},{"categories":["tech"],"content":" MAC 的应用 1. 验证消息的真实性、完整性这是最简单的一个应用场景，在通信双向都持有一个预共享密钥的前提下，通信时都附带上消息的 MAC 码。接收方也使用「收到的消息+预共享密钥」计算出 MAC 码，如果跟收到的一致，就说明消息真实无误。 注意这种应用场景中，消息是不保密的！ 2. AE 认证加密 - Authenticated encryption常用的加密方法只能保证数据的保密性，并不能保证数据的完整性。 而这里介绍的 MAC 算法，或者还未介绍的基于非对称加密的数字签名，都只能保证数据的真实性、完整性，不能保证数据被安全传输。 而认证加密，就是将加密算法与 MAC 算法结合使用的一种加密方案。 在确保 MAC 码「强不可伪造」的前提下，首先对数据进行加密，然后计算密文的 MAC 码，再同时传输密文与 MAC 码，就能同时保证数据的保密性、完整性、真实性，这种方法叫 Encrypt-then-MAC, 缩写做 EtM. 接收方在解密前先计算密文的 MAC 码与收到的对比，就能验证密文的完整性与真实性。 AE 有一种更安全的变体——带有关联数据的认证加密 (authenticated encryption with associated data，AEAD)。AEAD 将「关联数据(Associated Data, AD)」——也称为「附加验证数据 （Additional Authenticated Data, AAD）」——绑定到密文和它应该出现的上下文，以便可以检测和拒绝将有效密文“剪切并粘贴”到不同上下文的尝试。 AEAD 用于加密和未加密数据一起使用的场景（例如，在加密的网络协议中），并确保整个数据流经过身份验证和完整性保护。换句话说，AEAD 增加了检查某些内容的完整性和真实性的能力。 我们会在第六章「对称加密算法」中看到如何通过 Python 使用 AEAD 加密方案 AES-256-GCM. 3. 基于 MAC 的伪随机数生成器MAC 码的另一个用途就是伪随机数生成函数，相比直接使用熵+哈希函数的进行伪随机数计算，MAC 码因为多引入了一个变量 key，理论上它会更安全。 这种场景下，我们称 MAC 使用的密钥为 salt，即盐。 text next_seed = MAC(salt, seed) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#1-验证消息的真实性完整性"},{"categories":["tech"],"content":" MAC 的应用 1. 验证消息的真实性、完整性这是最简单的一个应用场景，在通信双向都持有一个预共享密钥的前提下，通信时都附带上消息的 MAC 码。接收方也使用「收到的消息+预共享密钥」计算出 MAC 码，如果跟收到的一致，就说明消息真实无误。 注意这种应用场景中，消息是不保密的！ 2. AE 认证加密 - Authenticated encryption常用的加密方法只能保证数据的保密性，并不能保证数据的完整性。 而这里介绍的 MAC 算法，或者还未介绍的基于非对称加密的数字签名，都只能保证数据的真实性、完整性，不能保证数据被安全传输。 而认证加密，就是将加密算法与 MAC 算法结合使用的一种加密方案。 在确保 MAC 码「强不可伪造」的前提下，首先对数据进行加密，然后计算密文的 MAC 码，再同时传输密文与 MAC 码，就能同时保证数据的保密性、完整性、真实性，这种方法叫 Encrypt-then-MAC, 缩写做 EtM. 接收方在解密前先计算密文的 MAC 码与收到的对比，就能验证密文的完整性与真实性。 AE 有一种更安全的变体——带有关联数据的认证加密 (authenticated encryption with associated data，AEAD)。AEAD 将「关联数据(Associated Data, AD)」——也称为「附加验证数据 （Additional Authenticated Data, AAD）」——绑定到密文和它应该出现的上下文，以便可以检测和拒绝将有效密文“剪切并粘贴”到不同上下文的尝试。 AEAD 用于加密和未加密数据一起使用的场景（例如，在加密的网络协议中），并确保整个数据流经过身份验证和完整性保护。换句话说，AEAD 增加了检查某些内容的完整性和真实性的能力。 我们会在第六章「对称加密算法」中看到如何通过 Python 使用 AEAD 加密方案 AES-256-GCM. 3. 基于 MAC 的伪随机数生成器MAC 码的另一个用途就是伪随机数生成函数，相比直接使用熵+哈希函数的进行伪随机数计算，MAC 码因为多引入了一个变量 key，理论上它会更安全。 这种场景下，我们称 MAC 使用的密钥为 salt，即盐。 text next_seed = MAC(salt, seed) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#2-ae-认证加密---authenticated-encryption"},{"categories":["tech"],"content":" MAC 的应用 1. 验证消息的真实性、完整性这是最简单的一个应用场景，在通信双向都持有一个预共享密钥的前提下，通信时都附带上消息的 MAC 码。接收方也使用「收到的消息+预共享密钥」计算出 MAC 码，如果跟收到的一致，就说明消息真实无误。 注意这种应用场景中，消息是不保密的！ 2. AE 认证加密 - Authenticated encryption常用的加密方法只能保证数据的保密性，并不能保证数据的完整性。 而这里介绍的 MAC 算法，或者还未介绍的基于非对称加密的数字签名，都只能保证数据的真实性、完整性，不能保证数据被安全传输。 而认证加密，就是将加密算法与 MAC 算法结合使用的一种加密方案。 在确保 MAC 码「强不可伪造」的前提下，首先对数据进行加密，然后计算密文的 MAC 码，再同时传输密文与 MAC 码，就能同时保证数据的保密性、完整性、真实性，这种方法叫 Encrypt-then-MAC, 缩写做 EtM. 接收方在解密前先计算密文的 MAC 码与收到的对比，就能验证密文的完整性与真实性。 AE 有一种更安全的变体——带有关联数据的认证加密 (authenticated encryption with associated data，AEAD)。AEAD 将「关联数据(Associated Data, AD)」——也称为「附加验证数据 （Additional Authenticated Data, AAD）」——绑定到密文和它应该出现的上下文，以便可以检测和拒绝将有效密文“剪切并粘贴”到不同上下文的尝试。 AEAD 用于加密和未加密数据一起使用的场景（例如，在加密的网络协议中），并确保整个数据流经过身份验证和完整性保护。换句话说，AEAD 增加了检查某些内容的完整性和真实性的能力。 我们会在第六章「对称加密算法」中看到如何通过 Python 使用 AEAD 加密方案 AES-256-GCM. 3. 基于 MAC 的伪随机数生成器MAC 码的另一个用途就是伪随机数生成函数，相比直接使用熵+哈希函数的进行伪随机数计算，MAC 码因为多引入了一个变量 key，理论上它会更安全。 这种场景下，我们称 MAC 使用的密钥为 salt，即盐。 text next_seed = MAC(salt, seed) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:1:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#3-基于-mac-的伪随机数生成器"},{"categories":["tech"],"content":" 二、KDF 密钥派生函数我们都更喜欢使用密码来保护自己的数据而不是二进制的密钥，因为相比之下二进制密钥太难记忆了， 字符形式的密码才是符合人类思维习惯的东西。 可对计算机而言就刚好相反了，现代密码学的很多算法都要求输入是一个大的数字，二进制的密钥就是这样一个大的数字。因此显然我们需要一个将字符密码（Password）转换成密钥（Key）的函数，这就是密钥派生函数 Key Derivation Function. KDF 的名字中包含了「Derivation（派生）」，顾名思义，它也可以从一个字符密码中生成多个密钥，一个常见的生成多个密钥的用法就是 BTC 的「分层确定性钱包（Hierarchical Deterministic Wallet）」。 直接使用 SHA256 之类的加密哈希函数来生成密钥是不安全的，因为为了方便记忆，通常密码并不会很长，绝大多数人的密码长度估计都不超过 15 位。甚至很多人都在使用非常常见的弱密码，如 123456 admin 生日等等。这就导致如果直接使用 SHA256 之类的算法，许多密码将很容易被暴力破解、字典攻击、彩虹表攻击等手段猜测出来！ 也有些如 HKDF（HMAC-based KDF） 之类的 KDF 算法用于生成密钥，但它们不适合用于用户密码。HKDF 的特点是计算速度快，因此它只适合用于输入熵比较高的场景，比如将 DHKE 的共享密钥转换成对称加密算法的密钥。如果用 HKDF 来从用户密码生成密钥，那么密码的熵就太低了，将很容易被暴力破解！ KDF 目前主要从如下三个维度提升 hash 碰撞难度： 时间复杂度：对应 CPU/GPU 计算资源 空间复杂度：对应 Memory 内存资源 并行维度：使用无法分解的算法，锁定只允许单线程运算 主要手段是加盐，以及多次迭代。这种设计方法被称为「密钥拉伸 Key stretching」。 KDF 的工作示意图如下： 因为相比其他加密哈希算法，KDF 具有一个独特属性——计算速度很慢，而且从设计上就使其计算速度难以提升，所以 KDF 也被称作「慢哈希算法」。 KDF 计算速度的「慢」是相对而言的，对于 SSH 等本地应用而言，KDF 通常只需要在登录时被执行一次，因此慢这么一点点完全可以接受，而且用户也完全有足够的资源执行这个 KDF 函数。对于 APP 登录设计而言，用户登录完成后后续就会使用 cookie 进行验证，单次登录时多花费点资源并不会造成大量的资源消耗。但是如果一个黑客获得了你的加密 SSH Key 或者拖库拿到了 APP 的密码数据库，想要通过 Hash 碰撞来猜测出用户的密码，那它就必须执行海量的 KDF 计算，这个时候 KDF 的威力就显现出来了——黑客将需要提供海量的 CPU/GPU 计算资源、海量的内存资源才能完成目标，而这显然得不偿失，这样 KDF 就确保了用户密码的安全性。 目前比较著名的 KDF 算法主要有如下几个： PBKDF2：这是一个非常简单的加密 KDF 算法，目前已经不推荐使用。 Bcrypt：安全性在下降，用得越来越少了。不建议使用。 根据 Wikipedia 说明，在内存小于 4MB 时 bcrypt 要强于 scrypt，在运行时间小于 1 秒时其强度要高于 argon2. 因此在一些极端资源场景下，仍旧可以考虑 bcrypt 算法。 另外需要注意的一点是，Wikipedia 说 bcrypt 并非典型的 KDF 算法，它的输出不适合直接用做加解密或签名用的 Key, 因此 OpenSSH 的实现是将 Bcrypt 的输出再用 BPKDF2 加盐处理一遍输出最终用于验证的二进制 Key，也即它的 bcrypt_pbkdf 算法。 Scrypt：可以灵活地设定使用的内存大小，在 argon2 不可用时，可使用它。 Scrypt 是目前社区使用的主流算法，知名案例如： age rclone restic Argon2：目前最强的密码 Hash 算法，在 2015 年赢得了密码 Hash 竞赛。 Linux 的硬盘加密模块 LUKS2 就支持使用 argon2id 作为它的 KDF 算法。 如果你正在开发一个新的程序，需要使用到 KDF，建议选用 argon2/scrypt. Python 中最流行的密码学库是cryptography，requests 的底层曾经就使用了它（新版本已经换成使用标准库 ssl 了），下面我们使用这个库来演示下 Scrypt 算法的使用： python # pip install cryptography==36.0.1 import os from cryptography.hazmat.primitives.kdf.scrypt import Scrypt salt = os.urandom(16) # derive kdf = Scrypt( salt=salt, length=32, n=2**14, r=8, p=1, ) key = kdf.derive(b\"my great password\") # verify kdf = Scrypt( salt=salt, length=32, n=2**14, r=8, p=1, ) kdf.verify(b\"my great password\", key) ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:2:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#二kdf-密钥派生函数"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book A complete overview of SSL/TLS and its cryptographic system ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-3-key-derivation-function/:3:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","哈希","散列","密码学","安全","MAC","HMAC","KDF","Scrypt"],"title":"写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF","uri":"/posts/practical-cryptography-basics-3-key-derivation-function/#参考"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book，笔者额外补充了「非加密哈希函数」的简单介绍。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:0:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#"},{"categories":["tech"],"content":" 一、什么是哈希函数哈希函数（Hash Function），或者叫散列函数，是一种为任何输入数据中创建一个数字指纹（也叫数字摘要）的方法，哈希函数把数据压缩（或者放大）成一个长度固定的字符串。 哈希函数的输入空间（文本或者二进制数据）是无限大，但是输出空间（一个固定长度的摘要）却是有限的。将「无限」映射到「有限」，不可避免的会有概率不同的输入得到相同的输出，这种情况我们称为碰撞（collision）。 一个简单的哈希函数是直接对输入数据/文本的字节求和。它会导致大量的碰撞，例如 hello 和 ehllo 将具有相同的哈希值。 更好的哈希函数可以使用这样的方案：它将第一个字节作为状态，然后转换状态（例如，将它乘以像 31 这样的素数），然后将下一个字节添加到状态，然后再次转换状态并添加下一个字节等。这样的操作可以显着降低碰撞概率并产生更均匀的分布。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:1:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#一什么是哈希函数"},{"categories":["tech"],"content":" 二、加密哈希函数加密哈希函数（也叫密码学哈希函数）是指一类有特殊属性的哈希函数。 一个好的「加密哈希函数」必须满足抗碰撞（collision-resistant）和不可逆（irreversible）这两个条件。抗碰撞是指攻击者很难或者几乎不可能找到两个具有相同哈希值的输入，而不可逆则是说攻击者很难或几乎不可能从算法层面通过哈希值逆向演算出原始数据。 具体而言，一个理想的加密哈希函数，应当具有如下属性： 快速：计算速度要足够快 确定性：对同样的输入，应该总是产生同样的输出 难以分析：对输入的任何微小改动，都应该使输出完全发生变化 不可逆：从其哈希值逆向演算出输入值应该是不可行的。这意味着没有比暴力破解更好的破解方法 抗碰撞：找到具有相同哈希值的两条不同消息应该非常困难（或几乎不可能） 现代加密哈希函数（如 SHA2 和 SHA3）都具有上述几个属性，并被广泛应用在多个领域，各种现代编程语言和平台的标准库中基本都包含这些常用的哈希函数。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#二加密哈希函数"},{"categories":["tech"],"content":" 量子安全性现代密码学哈希函数（如 SHA2, SHA3, BLAKE2）都被认为是量子安全的，无惧量子计算机的发展。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#量子安全性"},{"categories":["tech"],"content":" 加密哈希函数的应用 1. 数据完整性校验加密哈希函数被广泛用于文件完整性校验。如果你从网上下载的文件计算出的 SHA256 校验和 （checksum）跟官方公布的一致，那就说明文件没有损坏。 但是哈希函数自身不能保证文件的真实性，目前来讲，真实性通常是 TLS 协议要保证的，它确保你在 openssl 网站上看到的「SHA256 校验和」真实无误（未被篡改）。 现代网络基本都很难遇到文件损坏的情况了，但是在古早的低速网络中，即使 TCP 跟底层协议已经有多种数据纠错手段，下载完成的文件仍然是有可能损坏的。这也是以前 rar 压缩格式很流行的原因之一—— rar 压缩文件拥有一定程度上的自我修复能力，传输过程中损坏少量数据，仍然能正常解压。 2. 保存密码加密哈希函数还被用于密码的安全存储，现代系统使用专门设计的安全哈希算法（目前主要是scrypt） 计算用户密码的哈希摘要，保存到数据库中，这样能确保密码的安全性。除了用户自己，没有人清楚该密码的原始数据，即使数据库管理员也只能看到一个哈希摘要。 简要说明下一个现代化站点的注册登录流程： 用户与站点交互的全流程都需使用 HTTPS 安全传输密码及其他用户信息。 在用户注册时，密码被提交到服务端，服务端生成随机 Salt + 固定的全局 Pepper，然后使用这两者及用户密码计算出 password hash 值，并将 hash 值与 salt 存入数据库。接着服务端立即从内存中销毁用户密码。 在用户登录时，用户密码也首先被提交到服务端，服务端从数据库中查到对应的 Salt，从环境变量中拿到全局 Pepple，并使用这两者及用户密码再次计算 password hash 值，如果结果与数据库中的 password hash 相同，则登录成功，否则失败。不论成功与否，计算结束后用户密码将被立即销毁。 用户登录成功后，服务端返回一个 JWT Token 实现，其中明文保存了用户信息, 有效期及一个签名。 后续用户每次请求都会在 HTTP Header 中带上这个 JWT Token，服务端验证签名有效，而且尚在有效期内，则通过验证，再进行后续处理。 3. 生成唯一 ID加密哈希函数也被用于为文档或消息生成（绝大多数情况下）唯一的 ID，因此哈希值也被称为数字指纹。 注意这里说的是数字指纹，而非数字签名。数字签名是与下一篇文章介绍的「MAC」码比较类似的， 用于验证消息的真实、完整、认证作者身份的一段数据。 加密哈希函数计算出的哈希值理论上确实有碰撞的概率，但是这个概率实在太小了，因此绝大多数系统 （如 Git）都假设哈希函数是无碰撞的（collision free）。 文档的哈希值可以被用于证明该文档的存在性，或者被当成一个索引，用于从存储系统中提取文档。 使用哈希值作为唯一 ID 的典型例子，Git 版本控制系统（如3c3be25bc1757ca99aba55d4157596a8ea217698）肯定算一个，比特币地址（如1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2）也算。 4. 伪随机数生成哈希值可以被当作一个随机数看待，生成一个伪随机数的简单流程如下： 通过随机事件得到一个熵（例如键盘点击或鼠标移动），将它作为最初的随机数种子（random seed）。 添加一个 1 到熵中，进行哈希计算得到第一个随机数 再添加一个 2，进行哈希计算得到第二个随机数 以此类推 当然为了确保安全性，实际的加密随机数生成器会比这再复杂一些，我们会在后面的「随机数生成器」一节学习其中细节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#加密哈希函数的应用"},{"categories":["tech"],"content":" 加密哈希函数的应用 1. 数据完整性校验加密哈希函数被广泛用于文件完整性校验。如果你从网上下载的文件计算出的 SHA256 校验和 （checksum）跟官方公布的一致，那就说明文件没有损坏。 但是哈希函数自身不能保证文件的真实性，目前来讲，真实性通常是 TLS 协议要保证的，它确保你在 openssl 网站上看到的「SHA256 校验和」真实无误（未被篡改）。 现代网络基本都很难遇到文件损坏的情况了，但是在古早的低速网络中，即使 TCP 跟底层协议已经有多种数据纠错手段，下载完成的文件仍然是有可能损坏的。这也是以前 rar 压缩格式很流行的原因之一—— rar 压缩文件拥有一定程度上的自我修复能力，传输过程中损坏少量数据，仍然能正常解压。 2. 保存密码加密哈希函数还被用于密码的安全存储，现代系统使用专门设计的安全哈希算法（目前主要是scrypt） 计算用户密码的哈希摘要，保存到数据库中，这样能确保密码的安全性。除了用户自己，没有人清楚该密码的原始数据，即使数据库管理员也只能看到一个哈希摘要。 简要说明下一个现代化站点的注册登录流程： 用户与站点交互的全流程都需使用 HTTPS 安全传输密码及其他用户信息。 在用户注册时，密码被提交到服务端，服务端生成随机 Salt + 固定的全局 Pepper，然后使用这两者及用户密码计算出 password hash 值，并将 hash 值与 salt 存入数据库。接着服务端立即从内存中销毁用户密码。 在用户登录时，用户密码也首先被提交到服务端，服务端从数据库中查到对应的 Salt，从环境变量中拿到全局 Pepple，并使用这两者及用户密码再次计算 password hash 值，如果结果与数据库中的 password hash 相同，则登录成功，否则失败。不论成功与否，计算结束后用户密码将被立即销毁。 用户登录成功后，服务端返回一个 JWT Token 实现，其中明文保存了用户信息, 有效期及一个签名。 后续用户每次请求都会在 HTTP Header 中带上这个 JWT Token，服务端验证签名有效，而且尚在有效期内，则通过验证，再进行后续处理。 3. 生成唯一 ID加密哈希函数也被用于为文档或消息生成（绝大多数情况下）唯一的 ID，因此哈希值也被称为数字指纹。 注意这里说的是数字指纹，而非数字签名。数字签名是与下一篇文章介绍的「MAC」码比较类似的， 用于验证消息的真实、完整、认证作者身份的一段数据。 加密哈希函数计算出的哈希值理论上确实有碰撞的概率，但是这个概率实在太小了，因此绝大多数系统 （如 Git）都假设哈希函数是无碰撞的（collision free）。 文档的哈希值可以被用于证明该文档的存在性，或者被当成一个索引，用于从存储系统中提取文档。 使用哈希值作为唯一 ID 的典型例子，Git 版本控制系统（如3c3be25bc1757ca99aba55d4157596a8ea217698）肯定算一个，比特币地址（如1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2）也算。 4. 伪随机数生成哈希值可以被当作一个随机数看待，生成一个伪随机数的简单流程如下： 通过随机事件得到一个熵（例如键盘点击或鼠标移动），将它作为最初的随机数种子（random seed）。 添加一个 1 到熵中，进行哈希计算得到第一个随机数 再添加一个 2，进行哈希计算得到第二个随机数 以此类推 当然为了确保安全性，实际的加密随机数生成器会比这再复杂一些，我们会在后面的「随机数生成器」一节学习其中细节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#1-数据完整性校验"},{"categories":["tech"],"content":" 加密哈希函数的应用 1. 数据完整性校验加密哈希函数被广泛用于文件完整性校验。如果你从网上下载的文件计算出的 SHA256 校验和 （checksum）跟官方公布的一致，那就说明文件没有损坏。 但是哈希函数自身不能保证文件的真实性，目前来讲，真实性通常是 TLS 协议要保证的，它确保你在 openssl 网站上看到的「SHA256 校验和」真实无误（未被篡改）。 现代网络基本都很难遇到文件损坏的情况了，但是在古早的低速网络中，即使 TCP 跟底层协议已经有多种数据纠错手段，下载完成的文件仍然是有可能损坏的。这也是以前 rar 压缩格式很流行的原因之一—— rar 压缩文件拥有一定程度上的自我修复能力，传输过程中损坏少量数据，仍然能正常解压。 2. 保存密码加密哈希函数还被用于密码的安全存储，现代系统使用专门设计的安全哈希算法（目前主要是scrypt） 计算用户密码的哈希摘要，保存到数据库中，这样能确保密码的安全性。除了用户自己，没有人清楚该密码的原始数据，即使数据库管理员也只能看到一个哈希摘要。 简要说明下一个现代化站点的注册登录流程： 用户与站点交互的全流程都需使用 HTTPS 安全传输密码及其他用户信息。 在用户注册时，密码被提交到服务端，服务端生成随机 Salt + 固定的全局 Pepper，然后使用这两者及用户密码计算出 password hash 值，并将 hash 值与 salt 存入数据库。接着服务端立即从内存中销毁用户密码。 在用户登录时，用户密码也首先被提交到服务端，服务端从数据库中查到对应的 Salt，从环境变量中拿到全局 Pepple，并使用这两者及用户密码再次计算 password hash 值，如果结果与数据库中的 password hash 相同，则登录成功，否则失败。不论成功与否，计算结束后用户密码将被立即销毁。 用户登录成功后，服务端返回一个 JWT Token 实现，其中明文保存了用户信息, 有效期及一个签名。 后续用户每次请求都会在 HTTP Header 中带上这个 JWT Token，服务端验证签名有效，而且尚在有效期内，则通过验证，再进行后续处理。 3. 生成唯一 ID加密哈希函数也被用于为文档或消息生成（绝大多数情况下）唯一的 ID，因此哈希值也被称为数字指纹。 注意这里说的是数字指纹，而非数字签名。数字签名是与下一篇文章介绍的「MAC」码比较类似的， 用于验证消息的真实、完整、认证作者身份的一段数据。 加密哈希函数计算出的哈希值理论上确实有碰撞的概率，但是这个概率实在太小了，因此绝大多数系统 （如 Git）都假设哈希函数是无碰撞的（collision free）。 文档的哈希值可以被用于证明该文档的存在性，或者被当成一个索引，用于从存储系统中提取文档。 使用哈希值作为唯一 ID 的典型例子，Git 版本控制系统（如3c3be25bc1757ca99aba55d4157596a8ea217698）肯定算一个，比特币地址（如1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2）也算。 4. 伪随机数生成哈希值可以被当作一个随机数看待，生成一个伪随机数的简单流程如下： 通过随机事件得到一个熵（例如键盘点击或鼠标移动），将它作为最初的随机数种子（random seed）。 添加一个 1 到熵中，进行哈希计算得到第一个随机数 再添加一个 2，进行哈希计算得到第二个随机数 以此类推 当然为了确保安全性，实际的加密随机数生成器会比这再复杂一些，我们会在后面的「随机数生成器」一节学习其中细节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#2-保存密码"},{"categories":["tech"],"content":" 加密哈希函数的应用 1. 数据完整性校验加密哈希函数被广泛用于文件完整性校验。如果你从网上下载的文件计算出的 SHA256 校验和 （checksum）跟官方公布的一致，那就说明文件没有损坏。 但是哈希函数自身不能保证文件的真实性，目前来讲，真实性通常是 TLS 协议要保证的，它确保你在 openssl 网站上看到的「SHA256 校验和」真实无误（未被篡改）。 现代网络基本都很难遇到文件损坏的情况了，但是在古早的低速网络中，即使 TCP 跟底层协议已经有多种数据纠错手段，下载完成的文件仍然是有可能损坏的。这也是以前 rar 压缩格式很流行的原因之一—— rar 压缩文件拥有一定程度上的自我修复能力，传输过程中损坏少量数据，仍然能正常解压。 2. 保存密码加密哈希函数还被用于密码的安全存储，现代系统使用专门设计的安全哈希算法（目前主要是scrypt） 计算用户密码的哈希摘要，保存到数据库中，这样能确保密码的安全性。除了用户自己，没有人清楚该密码的原始数据，即使数据库管理员也只能看到一个哈希摘要。 简要说明下一个现代化站点的注册登录流程： 用户与站点交互的全流程都需使用 HTTPS 安全传输密码及其他用户信息。 在用户注册时，密码被提交到服务端，服务端生成随机 Salt + 固定的全局 Pepper，然后使用这两者及用户密码计算出 password hash 值，并将 hash 值与 salt 存入数据库。接着服务端立即从内存中销毁用户密码。 在用户登录时，用户密码也首先被提交到服务端，服务端从数据库中查到对应的 Salt，从环境变量中拿到全局 Pepple，并使用这两者及用户密码再次计算 password hash 值，如果结果与数据库中的 password hash 相同，则登录成功，否则失败。不论成功与否，计算结束后用户密码将被立即销毁。 用户登录成功后，服务端返回一个 JWT Token 实现，其中明文保存了用户信息, 有效期及一个签名。 后续用户每次请求都会在 HTTP Header 中带上这个 JWT Token，服务端验证签名有效，而且尚在有效期内，则通过验证，再进行后续处理。 3. 生成唯一 ID加密哈希函数也被用于为文档或消息生成（绝大多数情况下）唯一的 ID，因此哈希值也被称为数字指纹。 注意这里说的是数字指纹，而非数字签名。数字签名是与下一篇文章介绍的「MAC」码比较类似的， 用于验证消息的真实、完整、认证作者身份的一段数据。 加密哈希函数计算出的哈希值理论上确实有碰撞的概率，但是这个概率实在太小了，因此绝大多数系统 （如 Git）都假设哈希函数是无碰撞的（collision free）。 文档的哈希值可以被用于证明该文档的存在性，或者被当成一个索引，用于从存储系统中提取文档。 使用哈希值作为唯一 ID 的典型例子，Git 版本控制系统（如3c3be25bc1757ca99aba55d4157596a8ea217698）肯定算一个，比特币地址（如1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2）也算。 4. 伪随机数生成哈希值可以被当作一个随机数看待，生成一个伪随机数的简单流程如下： 通过随机事件得到一个熵（例如键盘点击或鼠标移动），将它作为最初的随机数种子（random seed）。 添加一个 1 到熵中，进行哈希计算得到第一个随机数 再添加一个 2，进行哈希计算得到第二个随机数 以此类推 当然为了确保安全性，实际的加密随机数生成器会比这再复杂一些，我们会在后面的「随机数生成器」一节学习其中细节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#3-生成唯一-id"},{"categories":["tech"],"content":" 加密哈希函数的应用 1. 数据完整性校验加密哈希函数被广泛用于文件完整性校验。如果你从网上下载的文件计算出的 SHA256 校验和 （checksum）跟官方公布的一致，那就说明文件没有损坏。 但是哈希函数自身不能保证文件的真实性，目前来讲，真实性通常是 TLS 协议要保证的，它确保你在 openssl 网站上看到的「SHA256 校验和」真实无误（未被篡改）。 现代网络基本都很难遇到文件损坏的情况了，但是在古早的低速网络中，即使 TCP 跟底层协议已经有多种数据纠错手段，下载完成的文件仍然是有可能损坏的。这也是以前 rar 压缩格式很流行的原因之一—— rar 压缩文件拥有一定程度上的自我修复能力，传输过程中损坏少量数据，仍然能正常解压。 2. 保存密码加密哈希函数还被用于密码的安全存储，现代系统使用专门设计的安全哈希算法（目前主要是scrypt） 计算用户密码的哈希摘要，保存到数据库中，这样能确保密码的安全性。除了用户自己，没有人清楚该密码的原始数据，即使数据库管理员也只能看到一个哈希摘要。 简要说明下一个现代化站点的注册登录流程： 用户与站点交互的全流程都需使用 HTTPS 安全传输密码及其他用户信息。 在用户注册时，密码被提交到服务端，服务端生成随机 Salt + 固定的全局 Pepper，然后使用这两者及用户密码计算出 password hash 值，并将 hash 值与 salt 存入数据库。接着服务端立即从内存中销毁用户密码。 在用户登录时，用户密码也首先被提交到服务端，服务端从数据库中查到对应的 Salt，从环境变量中拿到全局 Pepple，并使用这两者及用户密码再次计算 password hash 值，如果结果与数据库中的 password hash 相同，则登录成功，否则失败。不论成功与否，计算结束后用户密码将被立即销毁。 用户登录成功后，服务端返回一个 JWT Token 实现，其中明文保存了用户信息, 有效期及一个签名。 后续用户每次请求都会在 HTTP Header 中带上这个 JWT Token，服务端验证签名有效，而且尚在有效期内，则通过验证，再进行后续处理。 3. 生成唯一 ID加密哈希函数也被用于为文档或消息生成（绝大多数情况下）唯一的 ID，因此哈希值也被称为数字指纹。 注意这里说的是数字指纹，而非数字签名。数字签名是与下一篇文章介绍的「MAC」码比较类似的， 用于验证消息的真实、完整、认证作者身份的一段数据。 加密哈希函数计算出的哈希值理论上确实有碰撞的概率，但是这个概率实在太小了，因此绝大多数系统 （如 Git）都假设哈希函数是无碰撞的（collision free）。 文档的哈希值可以被用于证明该文档的存在性，或者被当成一个索引，用于从存储系统中提取文档。 使用哈希值作为唯一 ID 的典型例子，Git 版本控制系统（如3c3be25bc1757ca99aba55d4157596a8ea217698）肯定算一个，比特币地址（如1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2）也算。 4. 伪随机数生成哈希值可以被当作一个随机数看待，生成一个伪随机数的简单流程如下： 通过随机事件得到一个熵（例如键盘点击或鼠标移动），将它作为最初的随机数种子（random seed）。 添加一个 1 到熵中，进行哈希计算得到第一个随机数 再添加一个 2，进行哈希计算得到第二个随机数 以此类推 当然为了确保安全性，实际的加密随机数生成器会比这再复杂一些，我们会在后面的「随机数生成器」一节学习其中细节。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#4-伪随机数生成"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其版本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： text SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： text SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： text BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#安全的加密哈希算法"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其版本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： text SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： text SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： text BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#1-sha-2-sha-256-sha-512"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其版本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： text SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： text SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： text BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#2-更长的哈希值--更高的抗碰撞能力"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其版本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： text SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： text SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： text BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#3-sha-3-sha3-256-sha3-512-keccak-256"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其版本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： text SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： text SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： text BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#4-blake2--blake2s--blake2b"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其版本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： text SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： text SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： text BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#5-ripemd-160"},{"categories":["tech"],"content":" 安全的加密哈希算法 1. SHA-2, SHA-256, SHA-512SHA-2，即 Secure Hash Algorithm 2，是一组强密码哈希函数，其版本包括：SHA-256（256位哈希）、SHA-384（384位哈希）、SHA-512（512位哈希）等。基于密码概念「Merkle–Damgård 构造」，目前被认为高度安全。 SHA-2 是 SHA-1 的继任者，于 2001 年在美国作为官方加密标准发布。 SHA-2 在软件开发和密码学中被广泛使用，可用于现代商业应用。其中 SHA-256 被广泛用于 HTTPS 协议、文件完整性校验、比特币区块链等各种场景。 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha256hash = hashlib.sha256(data).digest() print(f\"SHA-256({text}) = \", binascii.hexlify(sha256hash).decode(\"utf8\")) sha384hash = hashlib.sha384(data).digest() print(f\"SHA-384({text}) = \", binascii.hexlify(sha384hash).decode(\"utf8\")) sha512hash = hashlib.sha512(data).digest() print(f\"SHA-512({text}) = \", binascii.hexlify(sha512hash).decode(\"utf8\")) 输出如下： text SHA-256('hello') = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 SHA-384('hello') = 59e1748777448c69de6b800d7a33bbfb9ff1b463e44354c3553bcdb9c666fa90125a3c79f90397bdf5f6a13de828684f SHA-512('hello') = 9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7c7acc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043 2. 更长的哈希值 == 更高的抗碰撞能力按照设计，哈希函数的输出越长，就有望实现更高的安全性和抗碰撞能力（但也有一些例外）。一般来说，128 位哈希算法比 256 位哈希算法弱，256 位哈希算法比 512 位哈希算法弱。 因此显然 SHA-512 比 SHA-256 更强。我们可以预期，SHA-512 的碰撞概率要比 SHA-256 更低。 3. SHA-3, SHA3-256, SHA3-512, Keccak-256在输出的哈希长度相同时，SHA-3（及其变体 SHA3-224、SHA3-256、SHA3-384、SHA3-512）被认为拥有比 SHA-2（SHA-224、SHA-256、SHA-384、SHA-512）更高的加密强度。例如，对于相同的哈希长度（256 位），SHA3-256 提供比 SHA-256 更高的加密强度。 SHA-3 系列函数是 Keccak 哈希家族的代表，它基于密码学概念海绵函数。而Keccak 是SHA3 NIST 比赛的冠军。 与 SHA-2 不同，SHA-3 系列加密哈希函数不易受到长度拓展攻击 Length extension attack. SHA-3 被认为是高度安全的，并于 2015 年作为美国官方推荐的加密标准发布。 以太坊（Ethereum）区块链中使用的哈希函数 Keccak-256 是 SHA3-256 的变体，在代码中更改了一些常量。 哈希函数 SHAKE128(msg, length) 和 SHAKE256(msg, length) 是 SHA3-256 和 SHA3-512 算法的变体，它们输出消息的长度可以变化。 SHA3 的 Python 代码示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") sha3_256hash = hashlib.sha3_256(data).digest() print(f\"SHA3-256({text}) = \", binascii.hexlify(sha3_256hash).decode(\"utf8\")) sha3_512hash = hashlib.sha3_512(data).digest() print(f\"SHA3-512({text}) = \", binascii.hexlify(sha3_512hash).decode(\"utf8\")) 输出： text SHA3-256('hello') = 3338be694f50c5f338814986cdf0686453a888b84f424d792af4b9202398f392 Keccak-256('hello') = 1c8aff950685c2ed4bc3174f3472287b56d9517b9c948127319a09a7a36deac8 SHA3-512('hello') = 75d527c368f2efe848ecf6b073a36767800805e9eef2b1857d5f984f036eb6df891d75f72d9b154518c1cd58835286d1da9a38deba3de98b5a53e5ed78a84976 SHAKE-128('hello', 256) = 4a361de3a0e980a55388df742e9b314bd69d918260d9247768d0221df5262380 SHAKE-256('hello', 160) = 1234075ae4a1e77316cf2d8000974581a343b9eb 4. BLAKE2 / BLAKE2s / BLAKE2bBLAKE / BLAKE2 / BLAKE2s / BLAKE2b 是一系列快速、高度安全的密码学哈希函数，提供 160 位、224 位、256 位、384 位和 512 位摘要大小的计算，在现代密码学中被广泛应用。BLAKE 进入了SHA3 NIST 比赛的决赛。 BLAKE2 函数是 BLAKE 的改进版本。 BLAKE2s（通常为 256 位）是 BLAKE2 实现，针对 32 位微处理器进行了性能优化。 BLAKE2b（通常为 512 位）是 BLAKE2 实现，针对 64 位微处理器进行了性能优化。 BLAKE2 哈希函数具有与 SHA-3 类似的安全强度，但开发人员目前仍然更倾向于使用 SHA2 和 SHA3。 BLAKE 哈希值的 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") blake2s = hashlib.new('blake2s', data).digest() print(\"BLAKE2s({text}) = \", binascii.hexlify(blake2s).decode(\"utf-8\")) blake2b = hashlib.new('blake2b', data).digest() print(\"BLAKE2b({text}) = \", binascii.hexlify(blake2b).decode(\"utf-8\")) 输出如下： text BLAKE2s('hello') = 19213bacc58dee6dbde3ceb9a47cbb330b3d86f8cca8997eb00be456f140ca25 BLAKE2b('hello') = e4cfa39a3d37be31c59609e807970799caa68a19bfaa15135f165085e01d41a65ba1e1b146aeb6bd0092b49eac214c103ccfa3a365954bbbe52f74a2b3620c94 5. RIPEMD-160RIPEMD-160, RIPE Message Digest 是一种安全哈希函数，发布于 1996 年，目前主要被应用在 PGP 和比特币中。 RIPEMD 的 160 位变体在实践中被广泛使用，而 RIPEMD-128、RIPEMD-256 和 RIPEMD-320 等其他变体并不流行，并且它们的安全优势具有争议。 建议优先使用 SHA-2 和 SHA-3 而不是 RIPEMD，因为它们输出的哈希值更长，抗碰撞能力更强。 Python 示例： python import hashlib, binascii text = 'hello' data = text.encode(\"utf8\") ripemd160 = hashlib.new('ripemd160', data).digest() print(\"RIPEMD-160({text}) = \", binascii.hexlify(ripemd160).decode(\"utf-8\")) # =\u003e RIPEMD-1","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#6-其他安全哈希算法"},{"categories":["tech"],"content":" 不安全的加密哈希算法一些老一代的加密哈希算法，如 MD5, SHA-0 和 SHA-1 被认为是不安全的，并且都存在已被发现的加密漏洞（碰撞）。不要使用 MD5、SHA-0 和 SHA-1！这些哈希函数都已被证明不够安全。 使用这些不安全的哈希算法，可能会导致数字签名被伪造、密码泄漏等严重问题！ 另外也请避免使用以下被认为不安全或安全性有争议的哈希算法： MD2, MD4, MD5, SHA-0, SHA-1, Panama, HAVAL（有争议的安全性，在 HAVAL-128 上发现了碰撞），Tiger（有争议，已发现其弱点），SipHash（它属于非加密哈希函数）。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:4","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#不安全的加密哈希算法"},{"categories":["tech"],"content":" PoW 工作量证明哈希函数区块链中的 Proof-of-Work 工作量证明挖矿算法使用了一类特殊的哈希函数，这些函数是计算密集型和内存密集型的。这些哈希函数被设计成需要消耗大量计算资源和大量内存，并且很难在硬件设备（例如集成电路或矿机）中实现，也就难以设计专用硬件来加速计算。这种哈希函数被称为抗 ASIC（ASIC-resistant）。 大部分工作量证明（Proof-of-Work）算法，都是要求计算出一个比特定值（称为挖掘难度）更大的哈希值。因为哈希值是不可预测的，为了找出符合条件的哈希值，矿工需要计算数十亿个不同的哈希值， 再从中找出最大的那个。比如，一个工作量证明问题可能会被定义成这样：已有常数 x，要求找到一个数 p，使 hash(x + p) 的前十个比特都为 0. 有许多哈希函数是专为工作量证明挖掘算法设计的，例如 ETHash、Equihash、CryptoNight 和 Cuckoo Cycle. 这些哈希函数的计算速度很慢，通常使用 GPU 硬件（如 NVIDIA GTX 1080 等显卡）或强大的 CPU 硬件（如 Intel Core i7-8700K）和大量快速 RAM 内存（如 DDR4 芯片）来执行这类算法。这些挖矿算法的目标是通过刺激小型矿工（家庭用户和小型矿场）来最大限度地减少挖矿的集中化，并限制挖矿行业中高级玩家们（他们有能力建造巨型挖矿设施和数据中心）的力量。与少数的高玩相比，大量小玩家意味着更好的去中心化。 目前大型虚拟货币挖矿公司手中的主要武器是 ASIC 矿机，因此，现代加密货币通常会要求使用「抗 ASIC 哈希算法」或「权益证明（proof-of-stake）共识协议」进行「工作量证明挖矿」，以限制这部分高级玩家，达成更好的去中心化。 因为工作量证明算法需要消耗大量能源，不够环保，以太坊等区块链已经声明未来将会升级到权益证明（Proof-of-S）这类更环保的算法。不过这里我们只关注 PoW 如何基于哈希函数实现的，不讨论这个。 1. ETHash这里简要说明下以太坊区块链中使用的 ETHash 工作量证明挖掘哈希函数背后的思想。 ETHash 是以太坊区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能快速计算），因此它被认为是抗 ASIC 的。 ETHash 的工作流程： 基于直到当前区块的整个链，为每个区块计算一个「种子」 从种子中计算出一个 16 MB 的伪随机缓存 从缓存中提取 1 GB 数据集以用于挖掘 挖掘涉及将数据集的随机切片一起进行哈希 更多信息参见 eth.wiki - ethash 2. Equihash简要解释一下 Zcash、Bitcoin Gold 和其他一些区块链中使用的 Equihash 工作量证明挖掘哈希函数背后的思想。 Equihash 是 Zcash 和 Bitcoin Gold 区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能进行快速计算），因此它被认为是抗 ASIC 的。 Equihash 的工作流程： 基于直到当前区块的整个链，使用 BLAKE2b 计算出 50 MB 哈希数据集 在生成的哈希数据集上解决「广义生日问题」（从 2097152 中挑选 512 个不同的字符串，使得它们的二进制 XOR 为零）。已知最佳的解决方案（瓦格纳算法）在指数时间内运行，因此它需要大量的内存密集型和计算密集型计算 对前面得到的结果，进行双 SHA256 计算得到最终结果，即 SHA256(SHA256(solution)) 更多信息参见 https://github.com/tromp/equihash ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:5","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#pow-工作量证明哈希函数"},{"categories":["tech"],"content":" PoW 工作量证明哈希函数区块链中的 Proof-of-Work 工作量证明挖矿算法使用了一类特殊的哈希函数，这些函数是计算密集型和内存密集型的。这些哈希函数被设计成需要消耗大量计算资源和大量内存，并且很难在硬件设备（例如集成电路或矿机）中实现，也就难以设计专用硬件来加速计算。这种哈希函数被称为抗 ASIC（ASIC-resistant）。 大部分工作量证明（Proof-of-Work）算法，都是要求计算出一个比特定值（称为挖掘难度）更大的哈希值。因为哈希值是不可预测的，为了找出符合条件的哈希值，矿工需要计算数十亿个不同的哈希值， 再从中找出最大的那个。比如，一个工作量证明问题可能会被定义成这样：已有常数 x，要求找到一个数 p，使 hash(x + p) 的前十个比特都为 0. 有许多哈希函数是专为工作量证明挖掘算法设计的，例如 ETHash、Equihash、CryptoNight 和 Cuckoo Cycle. 这些哈希函数的计算速度很慢，通常使用 GPU 硬件（如 NVIDIA GTX 1080 等显卡）或强大的 CPU 硬件（如 Intel Core i7-8700K）和大量快速 RAM 内存（如 DDR4 芯片）来执行这类算法。这些挖矿算法的目标是通过刺激小型矿工（家庭用户和小型矿场）来最大限度地减少挖矿的集中化，并限制挖矿行业中高级玩家们（他们有能力建造巨型挖矿设施和数据中心）的力量。与少数的高玩相比，大量小玩家意味着更好的去中心化。 目前大型虚拟货币挖矿公司手中的主要武器是 ASIC 矿机，因此，现代加密货币通常会要求使用「抗 ASIC 哈希算法」或「权益证明（proof-of-stake）共识协议」进行「工作量证明挖矿」，以限制这部分高级玩家，达成更好的去中心化。 因为工作量证明算法需要消耗大量能源，不够环保，以太坊等区块链已经声明未来将会升级到权益证明（Proof-of-S）这类更环保的算法。不过这里我们只关注 PoW 如何基于哈希函数实现的，不讨论这个。 1. ETHash这里简要说明下以太坊区块链中使用的 ETHash 工作量证明挖掘哈希函数背后的思想。 ETHash 是以太坊区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能快速计算），因此它被认为是抗 ASIC 的。 ETHash 的工作流程： 基于直到当前区块的整个链，为每个区块计算一个「种子」 从种子中计算出一个 16 MB 的伪随机缓存 从缓存中提取 1 GB 数据集以用于挖掘 挖掘涉及将数据集的随机切片一起进行哈希 更多信息参见 eth.wiki - ethash 2. Equihash简要解释一下 Zcash、Bitcoin Gold 和其他一些区块链中使用的 Equihash 工作量证明挖掘哈希函数背后的思想。 Equihash 是 Zcash 和 Bitcoin Gold 区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能进行快速计算），因此它被认为是抗 ASIC 的。 Equihash 的工作流程： 基于直到当前区块的整个链，使用 BLAKE2b 计算出 50 MB 哈希数据集 在生成的哈希数据集上解决「广义生日问题」（从 2097152 中挑选 512 个不同的字符串，使得它们的二进制 XOR 为零）。已知最佳的解决方案（瓦格纳算法）在指数时间内运行，因此它需要大量的内存密集型和计算密集型计算 对前面得到的结果，进行双 SHA256 计算得到最终结果，即 SHA256(SHA256(solution)) 更多信息参见 https://github.com/tromp/equihash ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:5","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#1-ethash"},{"categories":["tech"],"content":" PoW 工作量证明哈希函数区块链中的 Proof-of-Work 工作量证明挖矿算法使用了一类特殊的哈希函数，这些函数是计算密集型和内存密集型的。这些哈希函数被设计成需要消耗大量计算资源和大量内存，并且很难在硬件设备（例如集成电路或矿机）中实现，也就难以设计专用硬件来加速计算。这种哈希函数被称为抗 ASIC（ASIC-resistant）。 大部分工作量证明（Proof-of-Work）算法，都是要求计算出一个比特定值（称为挖掘难度）更大的哈希值。因为哈希值是不可预测的，为了找出符合条件的哈希值，矿工需要计算数十亿个不同的哈希值， 再从中找出最大的那个。比如，一个工作量证明问题可能会被定义成这样：已有常数 x，要求找到一个数 p，使 hash(x + p) 的前十个比特都为 0. 有许多哈希函数是专为工作量证明挖掘算法设计的，例如 ETHash、Equihash、CryptoNight 和 Cuckoo Cycle. 这些哈希函数的计算速度很慢，通常使用 GPU 硬件（如 NVIDIA GTX 1080 等显卡）或强大的 CPU 硬件（如 Intel Core i7-8700K）和大量快速 RAM 内存（如 DDR4 芯片）来执行这类算法。这些挖矿算法的目标是通过刺激小型矿工（家庭用户和小型矿场）来最大限度地减少挖矿的集中化，并限制挖矿行业中高级玩家们（他们有能力建造巨型挖矿设施和数据中心）的力量。与少数的高玩相比，大量小玩家意味着更好的去中心化。 目前大型虚拟货币挖矿公司手中的主要武器是 ASIC 矿机，因此，现代加密货币通常会要求使用「抗 ASIC 哈希算法」或「权益证明（proof-of-stake）共识协议」进行「工作量证明挖矿」，以限制这部分高级玩家，达成更好的去中心化。 因为工作量证明算法需要消耗大量能源，不够环保，以太坊等区块链已经声明未来将会升级到权益证明（Proof-of-S）这类更环保的算法。不过这里我们只关注 PoW 如何基于哈希函数实现的，不讨论这个。 1. ETHash这里简要说明下以太坊区块链中使用的 ETHash 工作量证明挖掘哈希函数背后的思想。 ETHash 是以太坊区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能快速计算），因此它被认为是抗 ASIC 的。 ETHash 的工作流程： 基于直到当前区块的整个链，为每个区块计算一个「种子」 从种子中计算出一个 16 MB 的伪随机缓存 从缓存中提取 1 GB 数据集以用于挖掘 挖掘涉及将数据集的随机切片一起进行哈希 更多信息参见 eth.wiki - ethash 2. Equihash简要解释一下 Zcash、Bitcoin Gold 和其他一些区块链中使用的 Equihash 工作量证明挖掘哈希函数背后的思想。 Equihash 是 Zcash 和 Bitcoin Gold 区块链中的工作量证明哈希函数。它是内存密集型哈希函数（需要大量 RAM 才能进行快速计算），因此它被认为是抗 ASIC 的。 Equihash 的工作流程： 基于直到当前区块的整个链，使用 BLAKE2b 计算出 50 MB 哈希数据集 在生成的哈希数据集上解决「广义生日问题」（从 2097152 中挑选 512 个不同的字符串，使得它们的二进制 XOR 为零）。已知最佳的解决方案（瓦格纳算法）在指数时间内运行，因此它需要大量的内存密集型和计算密集型计算 对前面得到的结果，进行双 SHA256 计算得到最终结果，即 SHA256(SHA256(solution)) 更多信息参见 https://github.com/tromp/equihash ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:2:5","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#2-equihash"},{"categories":["tech"],"content":" 三、非加密哈希函数加密哈希函数非常看重「加密」，为了实现更高的安全强度，费了非常多的心思、也付出了很多代价。 但是实际应用中很多场景是不需要这么高的安全性的，相反可能会对速度、随机均匀性等有更高的要求。这就催生出了很多「非加密哈希函数」。 非加密哈希函数的应用场景有很多： 哈希表 Hash Table: 在很多语言中也被称为 map/dict，它使用的算法很简单，通常就是把对象的各种属性不断乘个质数（比如 31）再相加，哈希空间会随着表的变化而变化。这里最希望的是数据的分布足够均匀。 一致性哈希：目的是解决分布式缓存的问题。在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。 高性能哈希算法：SipHash MurMurHash3 等，使用它们的目的可能是对数据进行快速去重，要求就是足够快。 有时我们甚至可能不太在意哈希碰撞的概率。也有的场景输入是有限的，这时我们可能会希望哈希函数具有可逆性。 总之非加密哈希函数也有非常多的应用，但不是本文的主题。这里就不详细介绍了，有兴趣的朋友们可以自行寻找其他资源。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:3:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#三非加密哈希函数"},{"categories":["tech"],"content":" 参考 Practical-Cryptography-for-Developers-Book 漫谈非加密哈希算法 开发中常见的一些Hash函数（一） ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-2-hash/:4:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","密码学","哈希","散列","安全","SHA-2","SHA-3"],"title":"写给开发人员的实用密码学（二）—— 哈希函数","uri":"/posts/practical-cryptography-basics-2-hash/#参考"},{"categories":["tech"],"content":" 本文主要翻译自 Practical-Cryptography-for-Developers-Book ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:0:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#"},{"categories":["tech"],"content":" 零、前言你是软件开发人员吗？有时你会需要在日常工作中使用哈希、加密或数字签名等密码学工具吗？你认为密码学很复杂，充满了数学知识，而且只适合书呆子吗？不，不是这样滴，每个开发人员都可以学习如何使用加密算法。 从开发人员的角度理解密码学概念不需要你是一个厉害的数学家。这个系列的文章将尽量以最浅显的方式教你应用密码学的基础知识，而且包含大量循序渐进的代码示例和实践练习——就像你学习 Web 开发、数据库或 APP 开发一样。 没错，如果你能够学会 Web 开发或 RESTful 服务，那么你也完全可以学会实用密码学。这就像学习一个新的 API 或一个新的 Web 开发框架，只要掌握了概念 + 加密库 API + 工具 + 最佳实践，你就学会了实用密码学~ 在这个系列中，你将学习如何使用密码算法和密码系统，如哈希、MAC 码和密钥派生函数 (KDF)、随机生成器、密钥交换协议、对称密码算法、加密方案、非对称密码系统、公钥密码学、椭圆曲线、数字签名和量子安全加密算法，以及现代加密工具和库。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:1:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#零前言"},{"categories":["tech"],"content":" 一、现代密码学概览密码学已经从第一代广泛应用的密码学算法（比如已经退役的 MD5 跟 DES），发展到现代密码学算法 （如 SHA-3, Argon2 以及 ChaCha20）。 让我们首先跟一些基本的密码学概念混个脸熟： 哈希函数（Hash Function，也称散列函数），如 SHA-256, SHA3, RIPEMD 等 哈希消息认证码 HMAC 密钥派生函数 KDF，如 Scrypt 密钥交换算法，如 Diffie-Hellman 密钥交换协议 对称密钥加密方案，如 AES-256-CTR-HMAC-SHA-256 使用公私钥的非对称密钥加密方案，如 RSA 和 ECC, secp256k1 曲线跟 Ed25519 密码系统 数字签名算法，如 ECDSA 熵（entropy）与安全随机数生成 量子安全密码学 上述这些概念涉及到技术被广泛应用在 IT 领域，如果你有过一些开发经验，可能会很熟悉其中部分名词。如果不熟也没任何关系，本书的目的就是帮你搞清楚这些概念。 这个系列的文章会按上面给出的顺序，依次介绍这些密码学概念以及如何在日常开发中使用它们。 不过在开始学习之前，我们先来了解一下什么是密码学，以及密码学的几大用途。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:2:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#一现代密码学概览"},{"categories":["tech"],"content":" 二、什么是密码学密码学（Cryptography）是提供信息安全和保护的科学。它在我们的数字世界中无处不在，当你打开网站时、发送电子邮件时、连接到 WiFi 网络时，使用账号密码登录 APP 时、使用二步认证验证码认证身份时，都有涉及到密码学相关技术。因此开发人员应该对密码学有基本的了解， 以避免写出不安全的代码。至少也得知道如何使用密码算法和密码库，了解哈希、对称密码算法、非对称密码算法（cipher）与加密方案这些概念，知晓数字签名及其背后的密码系统和算法。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:3:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#二什么是密码学"},{"categories":["tech"],"content":" 三、密码学的用途","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:4:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#三密码学的用途"},{"categories":["tech"],"content":" 1. 加密与密钥密码学的一大用途，就是进行数据的安全存储和安全传输。这可能涉及使用对称或非对称加密方案加密和解密数据，其中一个或多个密钥用于将数据从明文转换为加密形式或者相反。 对称加密（如 AES、Twofish 和 ChaCha20）指加密和解密消息都使用一个相同的密钥，而非对称加密使用公钥密码系统（如 RSA 或 ECC）和密钥对来进行这两项操作，两个密钥凑成一对，用其中一把密钥加密的消息必须使用另一把解密。 单纯使用加密算法是不够的，这是因为有的加密算法只能按块进行加密，而且很多加密算法并不能保证密文的真实性、完整性。因此现实中我们通常会使用加密方案进行数据的加密解密。加密方案是结合了加密算法、消息认证或数字签名算法、块密码模式等多种算法，能同时保证数据的安全性、真实性、完整性的一套加密方案，如 AES-256-CTR-HMAC-SHA-256、ChaCha20-Poly1305 或 ECIES-secp256k1-AES-128-GCM。后面我们会学到，加密方案的名称就是使用到的各种密码算法名称的组合。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:4:1","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#1-加密与密钥"},{"categories":["tech"],"content":" 2. 数字签名与消息认证密码学提供了保证消息真实性（authenticity）、完整性（integrity）和不可否认性 （non-repudiation）的方法：数字签名算法与消息认证（MAC）算法。 大多数数字签名算法（如 DSA、ECDSA 和 EdDSA）使用非对称密钥对（私钥和公钥）干这个活：消息由私钥签名，签名由相应的公钥验证。在银行系统中，数字签名用于签署和批准付款。在区块链签名交易中，用户可以将区块链资产从一个地址转移到另一个地址，确保转移操作的真实、完整、不可否认。 消息认证算法（如 HMAC）和消息认证码（MAC 码）也是密码学的一部分。MAC 跟数字签名的功能实际上是一致的，区别在于 MAC 使用哈希算法或者对称加密系统。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:4:2","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#2-数字签名与消息认证"},{"categories":["tech"],"content":" 3. 安全随机数密码学的另一个部分，是熵（entropy，指不可预测的随机性）和随机数的安全生成（例如使用 CSPRNG）。 安全随机数理论上是不可预测的，开发人员需要关心的是你使用的随机数生成器是否足够安全。很多编程语言中被广泛使用的随机数生成器都是不安全的（比如 Python 的 random 库），如果你在对安全有严格要求的场景下使用了这种不安全的随机生成器，可能会黑客被预测到它生成的随机数，导致系统或者 APP 被黑客入侵。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:4:3","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#3-安全随机数"},{"categories":["tech"],"content":" 4. 密钥交换密码学定义了密钥交换算法（如 Diffie-Hellman 密钥交换和 ECDH）和密钥构建方案，用于在需要安全传输消息的两方之间安全地构建加密密钥。这种算法通常在两方之间建立新的安全连接时执行， 例如当你打开一个现代 HTTPS 网站或连接到 WiFi 网络时。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:4:4","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#4-密钥交换"},{"categories":["tech"],"content":" 5. 加密哈希与 Password 哈希密码学提供了加密哈希函数（如 SHA-3 和 BLAKE2）将消息转换为消息摘要/数字指纹（固定长度的散列），确保无法逆向出原始消息，并且几乎不可能找到具有相同哈希值的两条不同消息。 例如，在区块链系统中，哈希用于生成区块链地址、交易 ID 以及许多其他算法和协议。在 Git 中， 加密哈希用于为文件和提交生成唯一 ID。 而密钥派生函数（如 Scrypt 和 Argon2）通过从基于文本的 Password 安全地派生出哈希值（或密钥），并且这种算法还通过注入随机参数（盐）和使用大量迭代和计算资源使密码破解速度变慢。 密码学提供密钥的生成手段。因为人类只擅长记忆字符形式的密码，但是各种加密算法需要的密钥都是一个非常大的、保密的数字，所以密码学需要提供手段将字符串转换成密钥。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:5:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#5-加密哈希与-password-哈希"},{"categories":["tech"],"content":" 四、混淆与扩散在密码学当中，香农提出的混淆（confusion）与扩散（diffusion）是设计安全密码学算法的两个原则。 混淆使密文和对称加密中密钥的映射关系变得尽可能的复杂，使之难以分析。如果使用了混淆，那么输出密文中的每个比特位都应该依赖于密钥和输入数据的多个部分，确保两者无法建立直接映射。 混淆常用的方法是「替换」与「排列」。 「扩散」将明文的统计结构扩散到大量密文中，隐藏明文与密文之间的统计学关系。使单个明文或密钥位的影响尽可能扩大到更多的密文中去，确保改变输入中的任意一位都应该导致输出中大约一半的位发生变化，反过来改变输出密文的任一位，明文中大约一半的位也必须发生变化。 扩散常用的方法是「置换」。 这两个原则被包含在大多数哈希函数、MAC 算法、随机数生成器、对称和非对称密码算法中。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:6:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#四混淆与扩散"},{"categories":["tech"],"content":" 五、密码库说了这么多，作为一个程序员，我学习密码学的目的，是为了搞懂如何在编程语言中使用现代密码库， 并从中挑选合适的算法、使用合适的 API 参数。 程序员经常会自嘲日常复制粘贴，但是在编写涉及到密码学的代码时，一定要谨慎处理！盲目地从 Internet 复制/粘贴代码或遵循博客中的示例可能会导致安全问题；曾经安全的代码、算法或者最佳实践，随着时间的推移也可能变得不再安全。 本系列文章的后续部分，会分别介绍上述密码学概念，并使用 Python 演示其用法，其他语言的写法网上也很容易找到。 ","date":"2022-03-01","objectID":"/posts/practical-cryptography-basics-1/:7:0","series":["写给开发人员的实用密码学"],"tags":["Cryptography","Hash","KDF","密码学","安全","哈希","加解密","签名"],"title":"写给开发人员的实用密码学（一）—— 概览","uri":"/posts/practical-cryptography-basics-1/#五密码库"},{"categories":["life"],"content":" 本文转载自朋友写的写给优秀程序员看的马拉松指南🏃 - Chuanyi， 读下来感觉写得超棒超正能量，征得他同意后转载过来分享下嘿嘿~ 文中术语：PB(Personal Best) PW(Personal Worst) BQ(Boston Qualify) 4 月 11 日，气温 16-19 度，东风。前两天看预报，说可能会有雷阵雨，当天看预报，雷阵雨又延后到了下午两点多，真是天公作美啊。早上八点气温非常舒适，我预感到我又要 PB 了。 今年仙马的路线非常平稳，起伏不到三十米。正是农历草长莺飞二月天，沿途春意盎然，令人心情愉悦。只是十公里处的折返点非常恼人，拐弯后跑出几十米就立刻折返再次拐弯，减速到 0 并再次加速浪费了数秒。我知道此举是为了凑距离，但是完全可以取消这个折返点，延长终点。路线上的这个凸起，不禁让我联想到 Ph.D 的使命。 一名博士的使命 仙马四年，声名鹊起，一路摘得铜牌、银牌、金牌，今年又被“世界田径“正式列为标牌赛事，这些年出圈的努力和成果都蕴含在这个尖尖上了，我如此解读，组委会应该没有意见吧。 4 月 11 日早上，东风三级，起步向东，有些逆风，不利。吹面不寒杨柳风，逆风带来了凉爽，一路汗水都被吹干，全身上下始终都保持着干燥舒适。 第一公里计划是五分配，但是太过于兴奋，有些失控，但仍然压着 440；后五公里状态来临，逐渐将配速提到并保持在 430；六公里多迎来一个南北走向的下坡，借着势能的释放，配速拉到 410 以内。由南向北，春风拂面，夹道樱花，落英缤纷，我踩着碳板，好像踏着粉色的云霞。此时手表却一直在耳机里提醒我配速过高，机器终归是无情的，不懂风月，难知我心；七到十六公里折返点终于开始顺风，此时我稳定了 4:25 上下配速，喝了两次水，感觉还不错，并没有什么痛苦。十二公里，为了防止临近终点力量不足，我掏出一个柠檬味能量胶，迅速挤在嘴里，并在后面的水站取了一杯水。到了十三公里外已经能看见折返的第一梯队了； 十六公里折返开始一个长达三公里的缓坡，中途听见一位大哥在和同伴谈论后面如何如何难跑。以我的状态看来，我不以为然，缓坡没让我失速太多。十九公里路过我的母校北门，门口有我校传统艺能舞龙舞狮，不过PB 目标不允许我驻足拍一张，有些遗憾；越跑越欢，转眼二十公里，前方是熟悉的校友团服，追上前去看，是张书记，打了招呼后，我便全力冲刺，最终成绩 133，意料之外，情理之中， 冬天堆有氧的效果体现了。 青春仙林，大爱仙马。17 年首届，门外汉，门外看；18年入门，陈子豪（2016 年南京市大学生运动会 1500 米冠军田径小霸王2016年南京市运会1500m一骑绝尘！） 带我们在仙林校区开始练，一圈刚好就是 5km 的绝佳跑场。19 年终于参加了我的第一次仙马，去年仙马因疫情停办一年，时隔一年后再次回归。故地重游，取得 PB，这段仙马记忆永远不会斑驳。 仙马成绩证书 第七场半程了, 成绩一路提升，156 =\u003e 149 =\u003e 147 =\u003e null =\u003e 157 =\u003e 138 =\u003e 133，每次的进步都会令我无比激动。 生活好似一个湖泊，平水如鉴，岁月静好，固然优雅；但不流动的水是容易腐败的，需要一些外来的扰动，狂风骤雨之下，浊浪翻滚，却也注入了全新的生命力。每次 PB 都是我对这平静生活狂风暴雨一般的拷问，我的生命力不应该只局限在钢筋水泥之间。 ","date":"2022-02-26","objectID":"/posts/likenttt-2021-04-11-xianlin-half-marathon-1_33_12/:0:0","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」仙马赛记——我又 PB 了","uri":"/posts/likenttt-2021-04-11-xianlin-half-marathon-1_33_12/#"},{"categories":["life"],"content":" 本文转载自朋友写的写给优秀程序员看的马拉松指南🏃 - Chuanyi， 读下来感觉写得超棒超正能量，征得他同意后转载过来分享下嘿嘿~ 本文所描述的广州马拉松赛事时间为 2020-12-13 文中术语：PB(Personal Best) PW(Personal Worst) BQ(Boston Qualify) ","date":"2022-02-26","objectID":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/:0:0","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」MIRT出征广马——首次摸到330的边儿","uri":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/#"},{"categories":["life"],"content":" 赛前计划目标成绩：3:37 平均配速：5:08 前21km 5:13 加减 5s 后21km 5:03 加减 5s 实际：3:30:15 ","date":"2022-02-26","objectID":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/:1:0","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」MIRT出征广马——首次摸到330的边儿","uri":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/#赛前计划"},{"categories":["life"],"content":" 补给能量胶6支 绿灰绿灰黄红 服用时机 颜色 赛前5分钟 绿 10 灰 20 绿 30 灰 35 黄 40 红 ","date":"2022-02-26","objectID":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/:2:0","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」MIRT出征广马——首次摸到330的边儿","uri":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/#补给"},{"categories":["life"],"content":" 赛中简记我和兴勇师兄一起从 C 区出发，按照 500 配速跑，观察状态。 九公里处追上旦哥，旦哥此次担任 345 Pacer ，赛前他向我们许诺，如果能追上他就能摸一摸他的光头。说实话，我对这颗光头是垂涎已久，心想这是罕有的机会，舍我其谁。当我远远地望见飘动的气球时，我就忍不住兴奋的呼喊旦哥，渐渐距离迫近到数米，旦哥心照不宣地兑现承诺，主动伸过他的头， 让我摸。紧实的光头满是汗水，很滑，竟没有一丝头发，既不扎手也没有阻滞感，像极了一颗剥了壳的鸡蛋，我一个程序员也不禁为之动容，设计师（旦哥是一名声音设计师）这么伤头发吗，幸亏我没入这行。师兄决意一路跟着旦哥，我遂和他分别，去追赶 330 兔子，配速始终稳定在 450～500。 此后至 38km 之间配速稳定在这个区间里，看看风景，胡思乱想。猎德大桥是一个不小的挑战，迂回冲坡上引桥，下桥减速绕弯弯，起伏之间容易跑崩，但我始终平稳。下了桥是一个超长的折返，双向车道被绿化带切开，木棉（也许是合欢）一字排开，树干跟保龄球一样臃肿粗壮，草地上洒满了粉的花，白的穗，这南国的冬天竟然好似江南的樱花季。去程左前方已经稀稀落落地有人折回，能拉开这么多距离，是精英选手无疑了，他们跑姿大多都很美观，服装、配件、摆臂、踏步令我欣赏了好一阵。但是也有一些跑者，跑姿不那么具有观赏性的，用力过度，姿势僵硬，力量运用地不太经济，近似一种暴力美学了，我认为他们中有一些人只是暂时领先而已。后来我也折返了，此时再往左前方看，人群开始密集了起来，这种视角仿佛和原来的自己打了个照面。目光数次和几个聚集的配速员集团相遇，我试图在人群中分辨出我的队友们，却始终搜索无获，我数度怀疑是不是我已经走神儿错过了他们。21km 附近几个隧道也是不小的挑战，U型隧道起伏大，下坡要适当利用势能但也要避免心率过高，上坡要适当减速增加抓地力，地面湿滑，摩擦系数减小，要防止滑倒。过了半程以后，广州塔近在咫尺，仰之弥高，我的精神还很轻松，决定钻之弥坚。 今天是国家公祭日，十点，脑子里想到南京城此时应该鸣笛的，不禁热泪盈眶，随后心中默哀了一分多钟。昭昭前事，惕惕后人。永矢弗谖，祈愿和平。 天气预报显示今日气温在 21～22 度，但湿度较大，体感温度高于 22 度。好在穿的是背心，体表散热面积大，心率始终控制在 175 以下。每逢水站喝一小杯水，每五公里吃两片盐丸，按着计划吃能量胶。得益于以上种种努力，前 38 公里都还轻松，甚至游刃有余。但是第 39 公里我开始感到疲惫，感到厌倦，看看手表，发现心率已经到达 189，心下想这是终点前跑崩的前兆啊。一个多月前无锡马拉松折戟的经历还令我心有余悸。第 39 和 40 公里分别跑出了 521 和 523 的配速，心里很慌。到达水站后，再度补充盐丸和能量胶，并不断给自己做心里建设工作：这是难得的机遇，跑团南下首秀，集团给了莫大支持，跑团组委会也做了大量筹备；年初 PB(Personal Best)，年尾 PW(Personal Worst)，有点虎头蛇尾，接下来，坚持鏖战，后悔几天，放弃躺倒，懊悔半年。吃完最后一支柠檬味的能量胶，喝饱了水，重新出发。最后两公里跑回 451 配速，安全完赛并 PB。冲过终点，顿时感到一种超然🤯的轻快，广州是一座充满希望的城市！不过差 15 秒就能达到广马的 BQ(Boston Qualify)，仍然有一丝遗憾。但无论如何，取得这样的成绩我已经很满意了。 我很庆幸，从学生时代就培养起来的长跑爱好，可以陪伴我走进职业生涯并顽强保持至今，一路上我收获一群互相砥砺支持的好朋友。易方达基金的 slogan：乐于在长跑中取得胜利，这也是广马重要收获之一。胜利，指挑战并超越过去的自己，beat yesterday。寄予希望，并不断挑战超越，这是一种痛并快乐着的幸福。长跑如此，生活职业亦如是。 ","date":"2022-02-26","objectID":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/:3:0","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」MIRT出征广马——首次摸到330的边儿","uri":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/#赛中简记"},{"categories":["life"],"content":" 技术总结 广马当天高达二十多摄氏度，通常来说，这样的温度并不特别友好，无疑，这丰富了我高温作战的经验。及时补给，盐丸和能量胶，如果天热尤其要补水和盐丸。后来和旦哥复盘，我抱怨道中途补充了九次水，后面几乎逢水站必停。旦哥说：如果你不补水，你怎么保证后面不会崩呢？我恍然大悟，吃到第个三馒头饱了，功劳绝不只是第三个馒头的，前两个馒头亦是关键。 预先规划好时间和配速。根据目的估算并严格执行。如果是跑成绩，不要高估，避免过度透支能力，也不要低估，一直躺在舒适的成绩上。 烟雾弹还是要放的，这是赛前乐趣所在，烟雾弹放出去了，进可凡学，退可务实。 训练需要注意提高体能和心理承受阈值。体能耗尽，其后是心有余而力不足；心态崩溃，无心再战，先前努力便俱付东流。 ","date":"2022-02-26","objectID":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/:3:1","series":null,"tags":["长跑","马拉松","运动"],"title":"「转」MIRT出征广马——首次摸到330的边儿","uri":"/posts/likenttt-2020-12-13-guangzhou-marathon-3_30_15/#技术总结"},{"categories":["tech"],"content":" 个人笔记，只会列出我自己容易忘掉的命令，方便查阅。 内容比较多，适合当参考手册用。可能不太适合从头读到尾… 本文主要介绍 Linux 命令，顺带介绍下 Windows/MacOSX. 其中 awk 只需要学会些常用的用法就够了，这类命令行工具比较适合临时处理一些文本，可读性与维护性都比较差。对于需求比较复杂，而且需要长期使用的脚本，建议使用 Python/Go 等语言编写。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:0:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#"},{"categories":["tech"],"content":" 一、Linux","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#一linux"},{"categories":["tech"],"content":" 1. 后台运行 shell # 1. 后台运行命令 nohup python xxx.py \u0026 也可以使用 tmux，tmux 提供的 session 功能比 nohup 更好用，后面会介绍 tmux ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-后台运行"},{"categories":["tech"],"content":" 2. 查找替换 sed/awk 与 json/yaml 处理 jq/yqgrep 常用命令： bash ## 只在目录中所有的 .py 和 .dart 文件中递归搜索字符\"main()\" # -r 表示递归搜索 grep \"main()\" . -r --include *.{py, dart} cat xxx | grep -10 \"main()\" # 显示匹配行的前后 10 行 cat xxx | grep -A 10 -B 5 \"main()\" # 显示匹配行的前 5 行和后 10 行 ## 在 .js 文件中搜索关键字 xxxxx 并仅展示关键字前后 40 个字符（用在 .js 等被压缩过的文本文件上很有效） cat *.js | grep -o -P '.{0,40}xxxxx.{0,40}' # 正则搜索（`egrep` 是 `grep -E` 的简写，表示使用扩展正则表达式） cat xxx | egrep 'pattern1|pattern2|pattern3' # 反向搜索（不包含某个关键字） cat xxx | grep -v 'pattern' # 高亮显示搜索结果 cat xxx | grep --color 'pattern' # 通过 find 查找所有子文件夹中的 config.xml 文件，并在其中查找包含关键字 \u003crecipients\u003e 的行 # 再通过 awk 提取出文件名和需要的关键字 # -maxdepth 2 表示最大递归深度为 2（子文件夹） # -exec echo \\'{}\\' \\; 表示将 find 命令的输出使用 echo 命令添加单引号包裹起来，防止文件名中有空格导致 grep 命令出错 # xargs grep '\u003crecipients\u003e' 表示将 find 命令的输出作为 grep 命令的输入 # awk -F '[:\u003c\u003e]' '{print $1,$4}' 表示使用 : \u003c \u003e 作为分隔符，打印出第 1 列和第 4 列 find -maxdepth 2 -name \"config.xml\" -exec echo \\'{}\\' \\; | xargs grep '\u003crecipients\u003e' | awk -F '[:\u003c\u003e]' '{print $1,$4}' # 在文件夹中递归查找所有中文字符（在做中英双语内容维护时比较有用） # -P 表示使用 Perl 正则表达式，[\\x{4e00}-\\x{9f5a}] 表示匹配所有中文字符 grep -P '[\\x{4e00}-\\x{9f5a}]' -r . 更 morden 的命令是 ripgrep，它的速度比 grep 快很多，而且默认就是正则匹配、递归搜索、高亮显示搜索结果： bash # 在当前文件夹递归搜索所有包含关键字 \"main()\" 的文件 # 注意 rg 默认使用 rust 的正则匹配库 # 也可通过 -P 参数使用 PCRE2 正则匹配库 rg \"main\\(\\)\" # 在指定文件夹递归搜索所有包含关键字 \"main()\" 的文件 rg \"main\\(\\)\" \u003cpath\u003e # 类似 sed 的功能，替换文件中的字符串 # -N 表示不输出匹配的行，只输出替换后的结果 # -r 表示 replace 替换 rg --passthru -N 'and' -r '\u0026' ip.txt \u003e ip2.txt sed 常用命令： bash ## 1） 全文搜索并替换 ### -i --in-place 原地替换（修改原文件） ### -i=SUFFIX 替换后的文件添加 SUFFIX 这个后缀 ### -r 使用拓展的正则表达式，注意此正则不支持 \\d\\w\\s 等语法，必须使用 [0-9] [a-zA-Z] 等来替换！！！ sed -ri \"s/pattern_str/replace_str/g\" `grep \"key_pattern\" 'path_pattern' -rl` ## 2）文件名搜索，替换文件内容 sed -ri \"s/pattern_str/replace_str/g\" `find . -name \"pattern\"` ## 3）批量转换大小写 # 将当前文件夹内，所有的 gitlab URL 都转换成小写 # \\L 转小写 \\U 转大写 sed -ri 's@http://GITLAB.*.git@\\L\u0026@g' `find . -name pubspec*` # 只有包含 2 的行才会替换逗号为破折号 printf '1,2,3,4\\na,b,c,d\\n' | sed '/2/ s/,/-/g' # 只打印出被替换成功的行 sed -n 's/warm/cool/gp' rhymes.txt # 删除掉第 3-8 行，保留其他行 seq 15 24 | sed '3,8 d' # 删除除 3-8 行之外的所有行 seq 15 24 | sed '3,8! d' # 删除掉第 3 行到第 11(3+8) 行，保留其他行 seq 15 24 | sed '3,+8 d' # 对于所有包含 2 的行，都替换逗号为破折号 printf '1,2,3,4\\na,b,c,d\\n' | sed '/2/ s/,/-/g' # 对于所有不包含 2 的行，都替换逗号为破折号 printf '1,2,3,4\\na,b,c,d\\n' | sed '/2/! s/,/-/g' # 在第 2 行之后插入一行，内容为 hello seq 1 3 | sed '2a hello' # 将第 2 行的内容改为 hello seq 1 3 | sed '2c hello' # 将能匹配上指定正则表达式的行，改为 hello seq 1 5 | sed -E '/(11|22)/c hello' # 在第二行的前面插入一行，内容为 hello seq 1 3 | sed '2i hello' # 将第 2-4 行的内容改为 hello（改完后只有一行，而不是 3 行） seq 5 | sed '2,4c hello' # 将第 2-4 行每一行的内容都改为 hello（改完后是 3 行） seq 5 | sed '2,4 s/.*/hello/' # 一次使用多条指令 seq 4 | sed -e '2c hi' -e '3a bye' ## 4) 拷贝文件，并且保持文件夹结构（--parents 表示保持文件夹结构） cp --parents `find \u003csrc-dir\u003e -name *.py` \u003cdst-dir\u003e awk 用于按列处理文本，它比 sed 更强大更复杂，常用命令： bash ## 1. 单独选出第 1 列的文本 cat xxx.txt | awk -F '{print $1}' | head ## 2. 可以使用 -F 指定分隔符，打印出多列 awk -F ',' '{print $1,$2}'| head ### -F 也可以使用正则表达式指定多个分隔符 awk -F '[,;]' '{print $1,$2}'| head # 在文件开头与末尾分别添加字符串 seq 2 | awk 'BEGIN{print \"---\"} 1; END{print \"%%%\"}' ## 3. 打印出行数 cat log_test | awk '{print NR,$1}' | more ## 4. if 判断语句 cat log_test | awk '{if($11\u003e300) print($1,$11)}' cat log_test | awk '{print $11}' | sort -n | uniq -c # 求和 cat data|awk '{sum+=$1} END {print \"Sum = \", sum}' # 求平均 cat data|awk '{sum+=$1} END {print \"Average = \", sum/NR}' # 求最大值 cat data|awk 'BEGIN {max = 0} {if ($1\u003emax) max=$1 fi} END {print \"Max=\", max}' # 求最小值（min的初始值设置一个超大数即可） awk 'BEGIN {min = 1999999} {if ($1\u003cmin) min=$1 fi} END {print \"Min=\", min}' jq/yq 常用命令： bash # 1. jq 是一个命令行 json 处理工具 ## 从 json 中查询某一个字段的值 jq -r '.message' xxx.json ## 也可从 stdin 读取 json cat xxx.json | jq -r '.message' ## 从 json 内容中删除多个 key ## -r 表示输出文本采用 raw 格式 jq -r \"del(.dataplane.insync, .dataplane.outdated)\" xxx.json ## 更多 jq 的用法参见官方文档：https://stedolan.github.io/jq/tutorial/ # 2. yq 是用于处理 yaml 配置的命令行工具，参数跟 jq 高度相似 ## 从 yaml 配置中删除多个 key yq \"del(.dataplane.insync, .dataplane.outdated)\" xxx.yaml ## 或者从 stdin 读取输入 cat xxx.yaml | yq \"del(.dataplane.insyn","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:2","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#2-查找替换-sedawk-与-jsonyaml-处理-jqyq"},{"categories":["tech"],"content":" 3. 压缩相关 shell # 直接 cat 压缩文件的内容 zcat xxx.gz | more # gzip xzcat xxx.xz | more # xz tar -axvf xxx.tar.* # 通过后缀识别压缩格式，智能解压 以及日常可通过压缩 + bas64 编码来通过 IM 等聊天软件快速传输数据量不多的文件夹： bash # 压缩文件夹并通过 base64 编码 tar czv \u003cfolder-name\u003e | base64 \u003e xxx # 解压消息到文件夹 cat \"xxx\" | base64 -d | tar zxv 更多命令参见常见压缩格式的区别，及 Linux 下的压缩相关指令 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:3","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#3-压缩相关"},{"categories":["tech"],"content":" 4. 文件拷贝与同步各种 Linux 发行版都自带 scp/ssh，这两个工具功能简单，一般够用。 另外就是更强大也更复杂的 rsync，部分发行版会自带 rsync。 下面分别介绍下。 1. ssh/scp shell # 如果使用 ssh 命令进行文件传输，可安装 pv 命令查看传输速度（pipeviewer） ## ubuntu sudo apt-get install pv ## centos sudo yum install epel-release sudo yum install pv ## 1)从本地上传到服务器 ### 使用 ssh 的好处是流式传输不会占用目标机器的存储空间，适合传输可能引起空间不足的大文件，并在目标机器上实时处理该文件。 cat \u003cfilename\u003e | pv | ssh \u003cuser\u003e@\u003chost\u003e -p 22 \"cat - \u003e \u003cnew-filename\u003e\" tar cz \u003cfilename or foldername or glob\u003e | pv | ssh \u003cuser\u003e@\u003chost\u003e -p 22 \"tar xz\" # 压缩传输 ## scp 命令比 ssh 命令更简洁（但是不适合用于传文件夹，它会破坏文件的权限设置，把文件夹弄得一团糟） scp -P 22 \u003cfilename\u003e \u003cuser\u003e@\u003chost\u003e:\u003cfolder-name or filename\u003e # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） ## 2) 从服务器下载到本地 ssh \u003cuser\u003e@\u003chost\u003e -p 22 \"tar cz \u003cfilename or foldername or glob\u003e\" | pv | tar xz # 压缩传输 scp -P 22 \u003cuser\u003e@\u003chost\u003e:\u003cfolder-name or filename\u003e \u003cfilename\u003e # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） 2. rsyncrsync 的功能其实和前面的 scp/(tar+ssh) 是一样的，将文件从一个地方拷贝到另一个地方。区别在于它只做增量同步，在多次拷贝文件时，只拷贝（同步）修改过的部分，很多场景下可以大大加快拷贝 /备份速度。 rsync 的常用命令： shell # 将一个文件夹归档、压缩，并通过 ssh 协议（默认）同步到另一个地方 # -a, --archive # 归档模式，保留文件的所有元信息，等同于 `-rlptgoD` # -r, --recursive # 递归复制文件夹，`-a` 隐含了这个参数，通常都用 -a。 # -v, --verbose # 输出详细信息 # --progress # 显示传输进度 # -z, --compress # 传输文件时进行压缩 rsync -avz --progress src host:dest rsync -avz --progress -e \"ssh -p225\" /path/src user@host:dest # 使用非默认的 ssh 端口进行传输 rsync -avz --progress -e \"ssh -i id_xxx\" /path/src user@host:dest # 使用指定的私钥连接 ssh 服务端，其他各种 ssh 参数都可以在这里指定 # --exclude 排除掉某些不需要的文件(夹) rsync -avz --progress --exclude \"foor/bar\" src user@host:dest # 有时我们希望在同步数据时修改文件的 user/group # --chown # 设置文件的 user:group，必须与 `-og`/`--owner --group` 同时使用！（`-a` 隐含了 `-og`） rsync -avz --progress --chown=root:root src user@host:dest # 传输时修改 user/group 为 root # 详细说明 src 和 dest 的位置 rsync -avz --progress path/src user@host:/tmp # 将 src 拷贝到远程主机的 /tmp 中（得到 /tmp/src） ## 注意 src 结尾有 / rsync -avz --progress path/src/ user@host:/tmp/src # 将 src 目录中的文件拷贝到远程主机的 /tmp/src 目录中（同样得到 /tmp/src） # 有时候我们在传输文件时不希望保留文件的元信息 # rsync 默认不会删除 dest 中多余的文件，使用 --delete 可让 rsync 删除这部分无关的文件 # 对 src 文件夹进行完全镜像，保证两个文件夹的内容一模一样，不多不少 rsync -avz --progress --delete src user@host:dest # 也可以使用 --ignore-existing 让 rsync 忽略掉 dest 已经存在的文件。就是只同步新增的文件。 rsync -avz --progress --ignore-existing src user@host:dest 另外也有使用双冒号 :: 分隔的传输命令，这种命令使用 rsync 协议进行传输，要求目标主机启用 rsync-daemon。用得会比 ssh 少一些，暂时不做介绍。 rsync 详细文档参见 https://rsync.samba.org/documentation.html，或者 man rsync. ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:4","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#4-文件拷贝与同步"},{"categories":["tech"],"content":" 4. 文件拷贝与同步各种 Linux 发行版都自带 scp/ssh，这两个工具功能简单，一般够用。 另外就是更强大也更复杂的 rsync，部分发行版会自带 rsync。 下面分别介绍下。 1. ssh/scp shell # 如果使用 ssh 命令进行文件传输，可安装 pv 命令查看传输速度（pipeviewer） ## ubuntu sudo apt-get install pv ## centos sudo yum install epel-release sudo yum install pv ## 1)从本地上传到服务器 ### 使用 ssh 的好处是流式传输不会占用目标机器的存储空间，适合传输可能引起空间不足的大文件，并在目标机器上实时处理该文件。 cat | pv | ssh @ -p 22 \"cat - \u003e \" tar cz | pv | ssh @ -p 22 \"tar xz\" # 压缩传输 ## scp 命令比 ssh 命令更简洁（但是不适合用于传文件夹，它会破坏文件的权限设置，把文件夹弄得一团糟） scp -P 22 @: # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） ## 2) 从服务器下载到本地 ssh @ -p 22 \"tar cz \" | pv | tar xz # 压缩传输 scp -P 22 @: # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） 2. rsyncrsync 的功能其实和前面的 scp/(tar+ssh) 是一样的，将文件从一个地方拷贝到另一个地方。区别在于它只做增量同步，在多次拷贝文件时，只拷贝（同步）修改过的部分，很多场景下可以大大加快拷贝 /备份速度。 rsync 的常用命令： shell # 将一个文件夹归档、压缩，并通过 ssh 协议（默认）同步到另一个地方 # -a, --archive # 归档模式，保留文件的所有元信息，等同于 `-rlptgoD` # -r, --recursive # 递归复制文件夹，`-a` 隐含了这个参数，通常都用 -a。 # -v, --verbose # 输出详细信息 # --progress # 显示传输进度 # -z, --compress # 传输文件时进行压缩 rsync -avz --progress src host:dest rsync -avz --progress -e \"ssh -p225\" /path/src user@host:dest # 使用非默认的 ssh 端口进行传输 rsync -avz --progress -e \"ssh -i id_xxx\" /path/src user@host:dest # 使用指定的私钥连接 ssh 服务端，其他各种 ssh 参数都可以在这里指定 # --exclude 排除掉某些不需要的文件(夹) rsync -avz --progress --exclude \"foor/bar\" src user@host:dest # 有时我们希望在同步数据时修改文件的 user/group # --chown # 设置文件的 user:group，必须与 `-og`/`--owner --group` 同时使用！（`-a` 隐含了 `-og`） rsync -avz --progress --chown=root:root src user@host:dest # 传输时修改 user/group 为 root # 详细说明 src 和 dest 的位置 rsync -avz --progress path/src user@host:/tmp # 将 src 拷贝到远程主机的 /tmp 中（得到 /tmp/src） ## 注意 src 结尾有 / rsync -avz --progress path/src/ user@host:/tmp/src # 将 src 目录中的文件拷贝到远程主机的 /tmp/src 目录中（同样得到 /tmp/src） # 有时候我们在传输文件时不希望保留文件的元信息 # rsync 默认不会删除 dest 中多余的文件，使用 --delete 可让 rsync 删除这部分无关的文件 # 对 src 文件夹进行完全镜像，保证两个文件夹的内容一模一样，不多不少 rsync -avz --progress --delete src user@host:dest # 也可以使用 --ignore-existing 让 rsync 忽略掉 dest 已经存在的文件。就是只同步新增的文件。 rsync -avz --progress --ignore-existing src user@host:dest 另外也有使用双冒号 :: 分隔的传输命令，这种命令使用 rsync 协议进行传输，要求目标主机启用 rsync-daemon。用得会比 ssh 少一些，暂时不做介绍。 rsync 详细文档参见 https://rsync.samba.org/documentation.html，或者 man rsync. ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:4","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-sshscp"},{"categories":["tech"],"content":" 4. 文件拷贝与同步各种 Linux 发行版都自带 scp/ssh，这两个工具功能简单，一般够用。 另外就是更强大也更复杂的 rsync，部分发行版会自带 rsync。 下面分别介绍下。 1. ssh/scp shell # 如果使用 ssh 命令进行文件传输，可安装 pv 命令查看传输速度（pipeviewer） ## ubuntu sudo apt-get install pv ## centos sudo yum install epel-release sudo yum install pv ## 1)从本地上传到服务器 ### 使用 ssh 的好处是流式传输不会占用目标机器的存储空间，适合传输可能引起空间不足的大文件，并在目标机器上实时处理该文件。 cat | pv | ssh @ -p 22 \"cat - \u003e \" tar cz | pv | ssh @ -p 22 \"tar xz\" # 压缩传输 ## scp 命令比 ssh 命令更简洁（但是不适合用于传文件夹，它会破坏文件的权限设置，把文件夹弄得一团糟） scp -P 22 @: # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） ## 2) 从服务器下载到本地 ssh @ -p 22 \"tar cz \" | pv | tar xz # 压缩传输 scp -P 22 @: # 通过 scp 传输，传文件夹时记得添加 -r 参数（recursive） 2. rsyncrsync 的功能其实和前面的 scp/(tar+ssh) 是一样的，将文件从一个地方拷贝到另一个地方。区别在于它只做增量同步，在多次拷贝文件时，只拷贝（同步）修改过的部分，很多场景下可以大大加快拷贝 /备份速度。 rsync 的常用命令： shell # 将一个文件夹归档、压缩，并通过 ssh 协议（默认）同步到另一个地方 # -a, --archive # 归档模式，保留文件的所有元信息，等同于 `-rlptgoD` # -r, --recursive # 递归复制文件夹，`-a` 隐含了这个参数，通常都用 -a。 # -v, --verbose # 输出详细信息 # --progress # 显示传输进度 # -z, --compress # 传输文件时进行压缩 rsync -avz --progress src host:dest rsync -avz --progress -e \"ssh -p225\" /path/src user@host:dest # 使用非默认的 ssh 端口进行传输 rsync -avz --progress -e \"ssh -i id_xxx\" /path/src user@host:dest # 使用指定的私钥连接 ssh 服务端，其他各种 ssh 参数都可以在这里指定 # --exclude 排除掉某些不需要的文件(夹) rsync -avz --progress --exclude \"foor/bar\" src user@host:dest # 有时我们希望在同步数据时修改文件的 user/group # --chown # 设置文件的 user:group，必须与 `-og`/`--owner --group` 同时使用！（`-a` 隐含了 `-og`） rsync -avz --progress --chown=root:root src user@host:dest # 传输时修改 user/group 为 root # 详细说明 src 和 dest 的位置 rsync -avz --progress path/src user@host:/tmp # 将 src 拷贝到远程主机的 /tmp 中（得到 /tmp/src） ## 注意 src 结尾有 / rsync -avz --progress path/src/ user@host:/tmp/src # 将 src 目录中的文件拷贝到远程主机的 /tmp/src 目录中（同样得到 /tmp/src） # 有时候我们在传输文件时不希望保留文件的元信息 # rsync 默认不会删除 dest 中多余的文件，使用 --delete 可让 rsync 删除这部分无关的文件 # 对 src 文件夹进行完全镜像，保证两个文件夹的内容一模一样，不多不少 rsync -avz --progress --delete src user@host:dest # 也可以使用 --ignore-existing 让 rsync 忽略掉 dest 已经存在的文件。就是只同步新增的文件。 rsync -avz --progress --ignore-existing src user@host:dest 另外也有使用双冒号 :: 分隔的传输命令，这种命令使用 rsync 协议进行传输，要求目标主机启用 rsync-daemon。用得会比 ssh 少一些，暂时不做介绍。 rsync 详细文档参见 https://rsync.samba.org/documentation.html，或者 man rsync. ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:4","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#2-rsync"},{"categories":["tech"],"content":" 5. Tmux 输入 tmux 启动一个 tmux 会话。（或者用 tmux new -s \u003csession-name\u003e 启动一个命名会话） 输入 python xxx.py，python 进程开始运行。 按快捷键 ctrl+b，然后再按一下 d 脱离(detach)当前会话。此时 python 进程进入后台运行，关闭当前终端对 python 进程没有影响。 输入 tmux ls 可以查看当前正在后台运行的会话。（命名会话会显示名称，否则只显示 id） 通过 tmux attach -t \u003csession-name/id\u003e 重新接入后台会话。 缩写 tmux a -t \u003csession\u003e 或者通过 tmux kill-session -t \u003csession-name/id\u003e 杀死一个后台会话。 常用快捷键： yaml # prefix 表示 `ctrl`+`b` # pane 的切分与选择 prefix \" # 在下方新建一个 pane prefix % # 在右侧新建一个 pane prefix `方向键` # 光标移动到指定方向的 pane 中 # 使用方向键滚动窗口内容 prefix [ # 进入翻页模式，可使用 page up/down，或者方向键来浏览 pane 的内容 # 使用鼠标滚轮来滚动窗口内容（也可以把此命令添加到 `~/.tmux.conf` 中使它永久生效） prefix `:` 然后输入 `set-window-option -g mode-mouse on` # （调整 pane 大小）将当前的 pane 向给定的方向扩容 5 行或者 5 列 # 按住 ALT 时快速重复敲击「方向键」，能快速调整，否则就得从 prefix 开始重新输入 prefix `Alt` + `方向键` # 将当前窗格全屏显示，第二次使用此命令，会将窗格还原 prefix z # 交换 pane 的位置 prefix { # 当前窗格与上一个窗格交换位置 prefix } # 当前窗格与下一个窗格交换位置 # session 相关操作 prefix s # 查看 session 列表，并通过方向键选择 session prefix `number` # 通过数字标签选择 session # window 相关操作（关系：每个 session 可以包含多个 window，每个 window 里面又可以有多个 pane） prefix c # 新建 window prefix w # 通过数字标签选择 window 参考文档： https://github.com/tmux/tmux/wiki/Getting-Started https://www.ruanyifeng.com/blog/2019/10/tmux.html ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:5","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#5-tmux"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： shell # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: shell for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 shell # 单行 if 语句 if [ true ]; then \u003ccommand\u003e; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell 脚本中的 set 指令，比如 set -x 和 set -e参见：Shell 脚本中的 set 指令，比如 set -x 和 set -e 4. 实用小工具 shell # URL 编解码 alias urldecode='python3 -c \"import sys, urllib.parse as ul; print(ul.unquote_plus(sys.stdin.read()))\"' alias urlencode='python3 -c \"import sys, urllib.parse as ul; print(ul.quote_plus(sys.stdin.read()))\"' # 使用方法 echo \"xxx\" | urldecode cat file | urlencode 5. 查询历史记录临时版： shell # 查询命令行历史记录，并带上时间 HISTTIMEFORMAT=\"%F %T %z \" history 一劳永逸版： shell # 将环境变量加入 .bashrc echo 'HISTTIMEFORMAT=\"%F %T \"' \u003e\u003e ~/.bashrc source ~/.bashrc # 查询历史记录 history 6. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#6-bash-shell-基础"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： shell # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: shell for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 shell # 单行 if 语句 if [ true ]; then ; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell 脚本中的 set 指令，比如 set -x 和 set -e参见：Shell 脚本中的 set 指令，比如 set -x 和 set -e 4. 实用小工具 shell # URL 编解码 alias urldecode='python3 -c \"import sys, urllib.parse as ul; print(ul.unquote_plus(sys.stdin.read()))\"' alias urlencode='python3 -c \"import sys, urllib.parse as ul; print(ul.quote_plus(sys.stdin.read()))\"' # 使用方法 echo \"xxx\" | urldecode cat file | urlencode 5. 查询历史记录临时版： shell # 查询命令行历史记录，并带上时间 HISTTIMEFORMAT=\"%F %T %z \" history 一劳永逸版： shell # 将环境变量加入 .bashrc echo 'HISTTIMEFORMAT=\"%F %T \"' \u003e\u003e ~/.bashrc source ~/.bashrc # 查询历史记录 history 6. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-for-循环"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： shell # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: shell for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 shell # 单行 if 语句 if [ true ]; then ; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell 脚本中的 set 指令，比如 set -x 和 set -e参见：Shell 脚本中的 set 指令，比如 set -x 和 set -e 4. 实用小工具 shell # URL 编解码 alias urldecode='python3 -c \"import sys, urllib.parse as ul; print(ul.unquote_plus(sys.stdin.read()))\"' alias urlencode='python3 -c \"import sys, urllib.parse as ul; print(ul.quote_plus(sys.stdin.read()))\"' # 使用方法 echo \"xxx\" | urldecode cat file | urlencode 5. 查询历史记录临时版： shell # 查询命令行历史记录，并带上时间 HISTTIMEFORMAT=\"%F %T %z \" history 一劳永逸版： shell # 将环境变量加入 .bashrc echo 'HISTTIMEFORMAT=\"%F %T \"' \u003e\u003e ~/.bashrc source ~/.bashrc # 查询历史记录 history 6. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#2-if-语句"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： shell # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: shell for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 shell # 单行 if 语句 if [ true ]; then ; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell 脚本中的 set 指令，比如 set -x 和 set -e参见：Shell 脚本中的 set 指令，比如 set -x 和 set -e 4. 实用小工具 shell # URL 编解码 alias urldecode='python3 -c \"import sys, urllib.parse as ul; print(ul.unquote_plus(sys.stdin.read()))\"' alias urlencode='python3 -c \"import sys, urllib.parse as ul; print(ul.quote_plus(sys.stdin.read()))\"' # 使用方法 echo \"xxx\" | urldecode cat file | urlencode 5. 查询历史记录临时版： shell # 查询命令行历史记录，并带上时间 HISTTIMEFORMAT=\"%F %T %z \" history 一劳永逸版： shell # 将环境变量加入 .bashrc echo 'HISTTIMEFORMAT=\"%F %T \"' \u003e\u003e ~/.bashrc source ~/.bashrc # 查询历史记录 history 6. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#3-shell-脚本中的-set-指令比如-set--x-和-set--e"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： shell # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: shell for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 shell # 单行 if 语句 if [ true ]; then ; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell 脚本中的 set 指令，比如 set -x 和 set -e参见：Shell 脚本中的 set 指令，比如 set -x 和 set -e 4. 实用小工具 shell # URL 编解码 alias urldecode='python3 -c \"import sys, urllib.parse as ul; print(ul.unquote_plus(sys.stdin.read()))\"' alias urlencode='python3 -c \"import sys, urllib.parse as ul; print(ul.quote_plus(sys.stdin.read()))\"' # 使用方法 echo \"xxx\" | urldecode cat file | urlencode 5. 查询历史记录临时版： shell # 查询命令行历史记录，并带上时间 HISTTIMEFORMAT=\"%F %T %z \" history 一劳永逸版： shell # 将环境变量加入 .bashrc echo 'HISTTIMEFORMAT=\"%F %T \"' \u003e\u003e ~/.bashrc source ~/.bashrc # 查询历史记录 history 6. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#4-实用小工具"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： shell # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: shell for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 shell # 单行 if 语句 if [ true ]; then ; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell 脚本中的 set 指令，比如 set -x 和 set -e参见：Shell 脚本中的 set 指令，比如 set -x 和 set -e 4. 实用小工具 shell # URL 编解码 alias urldecode='python3 -c \"import sys, urllib.parse as ul; print(ul.unquote_plus(sys.stdin.read()))\"' alias urlencode='python3 -c \"import sys, urllib.parse as ul; print(ul.quote_plus(sys.stdin.read()))\"' # 使用方法 echo \"xxx\" | urldecode cat file | urlencode 5. 查询历史记录临时版： shell # 查询命令行历史记录，并带上时间 HISTTIMEFORMAT=\"%F %T %z \" history 一劳永逸版： shell # 将环境变量加入 .bashrc echo 'HISTTIMEFORMAT=\"%F %T \"' \u003e\u003e ~/.bashrc source ~/.bashrc # 查询历史记录 history 6. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#5-查询历史记录"},{"categories":["tech"],"content":" 6. Bash Shell 基础目标：能使用 shell 编写 10 行以内的脚本。更长的脚本可以使用 Python 编写，就没必要折腾 Shell 了。 1. For 循环单行 for 循环，有时候很有用： shell # 数字枚举 for i in $(seq 1 5); do echo $i; done # sh/bash 都支持 for i in {1..5}; do echo $i; done # sh 不支持此语法 # 文件枚举，可使用 glob 语法进行文件匹配 for f in *; do echo $f; done for f in /etc/*.py; do echo $f; done # 使用 find 进行文件枚举 for f in $(find . -name *.py); do echo $f; done 单行 for 循环加几个换行就得到多行 for 循环，格式如下：写脚本用得到，不过更建议用 python: shell for i in $(seq 1 5) do echo $i done # sh/bash 都支持 2. if 语句 shell # 单行 if 语句 if [ true ]; then ; fi # if else if [ expression ] then Statement(s) to be executed if expression is true else Statement(s) to be executed if expression is not true fi 3. Shell 脚本中的 set 指令，比如 set -x 和 set -e参见：Shell 脚本中的 set 指令，比如 set -x 和 set -e 4. 实用小工具 shell # URL 编解码 alias urldecode='python3 -c \"import sys, urllib.parse as ul; print(ul.unquote_plus(sys.stdin.read()))\"' alias urlencode='python3 -c \"import sys, urllib.parse as ul; print(ul.quote_plus(sys.stdin.read()))\"' # 使用方法 echo \"xxx\" | urldecode cat file | urlencode 5. 查询历史记录临时版： shell # 查询命令行历史记录，并带上时间 HISTTIMEFORMAT=\"%F %T %z \" history 一劳永逸版： shell # 将环境变量加入 .bashrc echo 'HISTTIMEFORMAT=\"%F %T \"' \u003e\u003e ~/.bashrc source ~/.bashrc # 查询历史记录 history 6. 其他资料 shell_scripts: 实用 shell 小脚本 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:6","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#6-其他资料"},{"categories":["tech"],"content":" 7. socket 连接查询 - ss/netcat/lsof查看 socket 信息可以帮我们回答下列问题： 我的程序是不是真的在监听我指定的端口？ 我的程序是在监听 127.0.0.1（本机），还是在监听 0.0.0.0（整个网络） 进程们分别在使用哪些端口？ 我的连接数是否达到了上限？ 现在较新版本的 Ubuntu 和 CentOS 都已经使用 iproute2 替换掉了 net-tools，如果你还需要使用陈旧的 route netstat 等命令，需要手动安装 net-tools。 我们可以使用 ss(socket statistics) 或者 netstat 命令来查看 socket 信息: shell # 查看 socket 连接的统计信息 # 主要统计处于各种状态的 tcp sockets 数量，以及其他 sockets 的统计信息 ss --summary ss -s # 缩写 # 查看哪个进程在监听 80 端口 # --listening 列出所有正在被监听的 socket # --processes 显示出每个 socket 对应的 process 名称和 pid # --numeric 直接打印数字端口号（不解析协议名称） ss --listening --processes --numeric | grep 80 ss -nlp | grep 80 # 缩写 ss -lp | grep http # 解析协议名称，然后通过协议名搜索监听 ## 使用过时的 netstat ### -t tcp ### -u udp netstat -tunlp | grep \":80\" # 查看 sshd 当前使用的端口号 ss --listening --processes | grep sshd ## 使用过时的 netstat netstat -tunlp | grep \u003cpid\u003e # pid 通过 ps 命令获得 # 列出所有的 tcp sockets，包括所有的 socket 状态 ss --tcp --all # 只列出正在 listen 的 socket ss --listening # 列出所有 ESTABLISHED 的 socket（默认行为） ss # 统计 TCP 连接数 ss | grep ESTAB | wc -l # 列出所有 ESTABLISHED 的 socket，并且给出连接的计时器 ss --options # 查看所有来自 192.168.5 的 sockets ss dst 192.168.1.5 # 查看本机与服务器 192.168.1.100 建立的 sockets ss src 192.168.1.5 TCP 连接数受 Linux 文件描述符上限控制，可以通过如下方法查看已用文件句柄的数量。 shell # 已用文件描述符数量 lsof | wc -l # 文件描述符上限 ulimit -n ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:7","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#socket-commands"},{"categories":["tech"],"content":" 8. 其他网络相关命令主要是 iproute2 dhclient lsof 等 shell # 查看路由表 routel # 旧的 net-tools 包中的命令 ip route ls # iproute2 提供的新命令 # DHCP，先释放旧租约，再建立新租约 sudo dhclient -r eth0 \u0026\u0026 sudo dhclient eth0 # 查看 DHCP 租期 cat /var/lib/dhcp/dhcpd.leases # 清理 DNS 缓存 ## 1. 如果你使用的是 systemd-resolve，使用此命令 sudo systemd-resolve --flush-caches sudo systemd-resolve --statistics # 查看缓存状态 ## 2. 如果使用的是 dnsmasq，使用此命令 sudo systemctl restart dnsmasq sudo killall -HUP dnsmasq # 直接发送 HUP 信号也可以 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:8","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#8-其他网络相关命令"},{"categories":["tech"],"content":" 9. 容器网络诊断 - nsenterDocker 容器有自己的 namespace，直接通过宿主机的 ss 命令是查看不到容器的 socket 信息的。 比较直观的方法是直接通过 docker exec 在容器中通过 ss 命令。但是这要求容器中必须自带 ss 等程序，有的精简镜像可能不会自带它。 通过 nsenter 可以直接进入到容器的指定 namespace 中，这样就能直接查询容器网络相关的信息了。 shell docker ps | grep xxx echo CONTAINER=xxx # 容器名称或 ID # 1. 查询到容器对应的 pid PID=$(docker inspect --format {{.State.Pid}} $CONTAINER) # 2. nsenter 通过 pid 进入容器的 network namespace，执行 ss 查看 socket 信息 nsenter --target $PID --net ss -s nsenter 这个工具貌似是 docker 自带的或者是系统内置命令，只要装了 docker，ubuntu/centos 都可以直接使用这个命令。 nsenter 是一个进入名字空间的工具，功能不仅仅局限在「网络诊断」，还有更多用法。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:9","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#9-容器网络诊断---nsenter"},{"categories":["tech"],"content":" 10. 用户与群组 shell ## 查看用户属于哪些群组 groups \u003cuser-name\u003e # 方法一 id \u003cusername\u003e # 方法二，它会额外列出 gid/uid cat /etc/group | grep \u003cuser-name\u003e # 方法三，直接查看配置 ## 查看群组中有哪些用户，第一列是群组，最后一列是用户名 cat /etc/group | grep \u003cgroup-name\u003e ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:1:10","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#10-用户与群组"},{"categories":["tech"],"content":" 二、PowershellPowershell 是微软推出的一款新一代 shell，它的特点之一是，命令都有一致的命名规则：谓词-名词，谓词表示动作：Get/Set/Stop/Start 等，名词指示操作对象：Service/Member/ChildItem/Command 等。 这样的命名格式使我们可以很容易地猜测到自己需要的命令的名称。 为了使用方便，powershell 还提供了一些常用命令的缩写，并且添加了大量类似 Linux 命令的别名。 还有就是，Windows 默认不区分字母大小写，日常使用可以全部小写。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:2:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#二powershell"},{"categories":["tech"],"content":" 1. 实用命令 powershell # 删除文件/文件夹 remove-item xxx -confirm ri xxx # 别名1 rm xxx # 别名2 rmdir xxx # etc... # 复制 copy-item xxx xx -r cp -r xxx xx # 显示工作目录 get-location gl pwd # 切换工作目录 set-location xxx sl xxx cd xxx # 查看环境变量 get-childitem env: gci env: gci env:PATH # 查看 PATH 变量 $env:XXX=\"value\" # 临时设置环境变量 $env:Path += \";SomeRandomPath\" # 临时在 Path 末尾添加新路径 ## 以下三行命令只对 windows 有效，linux 下无效 [Environment]::SetEnvironmentVariable(\"XXX\", $env:XXX + \";value\", [EnvironmentVariableTarget]::User) # 修改当前用户的环境变量（永久），只对新进程有效 [Environment]::SetEnvironmentVariable(\"XXX\", \"value\", [EnvironmentVariableTarget]::Machine) # 给这台电脑设置环境变量（永久），只对新进程有效，需要管理员权限 [Environment]::SetEnvironmentVariable(\"XXX\", $env:XXX + \";value\", \"User\") # target 也可用字符串指定 # 删除文件/文件夹 rm xxx # 删除文件夹时会进入交互界面，按提示输入就行。 # 查看命名位置（类似 Linux Shell 的 which） get-command xxx gcm xxx # 通过关键字查找 powershell 命令 gcm | select-string \u003ckeyword\u003e # 通过关键字查找 powershell 命令和环境变量中的程序，比较慢 gcm * | select-string \u003ckeyword\u003e # 查看别名对应的真实命令 get-alias # 类似 linux 的 find/ls 命令 get-childitem -Recurse -Include *.py gci -r -i *.py # 清空终端的输出 clear-host clear # 查看文件内容 get-content xx.py | more get-content xx.py | out-host -paging cat xx.py gc xx.py # 字符串搜索，不能对对象使用 # 类似 linux 的 grep 命令 cat xxx.log | select-string \u003cpattern\u003e gci env: | out-string -stream | select-string \u003cpattern\u003e # 需要先使用 out-string 将对象转换成 string gci env: | where-object {$_.Name -like \u003cpattern\u003e} # 计算输出的行数/对象个数 gci env: | measure-object gci env: | measure # 这是缩写 # 关机/重启 stop-computer restart-computer # windows 计算 hash 值 # 功能等同于 linux 下的 sha256sum/sha1sum/sha512sum/md5sum Get-FileHash -Path /path/to/file -Algorithm SHA256 Get-FileHash -Path /path/to/file -Algorithm SHA256 | Format-List # 用 format 修改格式化效果 # base64 编解码 [Convert]::ToBase64String([Text.Encoding]::UTF8.GetBytes(\"xxx\")) # base64 编码 [Text.Encoding]::UTF8.GetString([Convert]::FromBase64String(\"eHh4\")) # 解码 另外 windows 同样自带 ssh/scp 命令，参数也和 linux 一致 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:2:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-实用命令"},{"categories":["tech"],"content":" 2. 进程相关命令 powershell # 查看所有进程 get-process | more ps | more # 别名 # 查找某进程（替代掉 tasklist） get-process -name exp*,power* # 使用正则查找进程 get-process | select-string \u003cpattern\u003e # 效果同上 # 通过 id 杀掉某进程（替代掉 taskkill） # 也可以通过 -Name 用正则匹配进程 stop-process \u003cpid\u003e kill \u003cpid\u003e # 别名 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:2:2","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#2-进程相关命令"},{"categories":["tech"],"content":" 3. 网络相关命令 powershell ## 1. dns 相关(dns-client) Clear-DnsClientCache # 清除 dns 缓存（替换掉 `ipconfig /flushdns`） Get-DnsClientCache # 查看 dns 缓存 Resolve-DnsName baidu.com # 解析域名 # 更新 DHCP 租约 ipconfig /renew ## 2. TCP/IP 相关命令 Get-Command Get-Net* # 查看所有 TCP/IP 相关的命令 Get-NetIPAddress # 查看 IP 地址 Get-NetIPInterface # 查看 IP 接口 Get-NetRoute # 查看路由表 Get-NetNeighbor # 获取链路层 MAC 地址缓存 Get-NetTCPConnection # 查看 TCP 连接 ### 也可以对 TCP/IP 的 IP 地址、接口、路由表进行增删改 New-NetRoute Remove-NetNeighbor # 清除 MAC 地址缓存 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:2:3","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#3-网络相关命令"},{"categories":["tech"],"content":" 4. socket 信息查询 - netstatWindows 系统和 macOS 一样，也没有 ss，但是自带 netstat，该命令和 Linux 下的 netstat 有一定差别，具体使用方法如下： powershell netstat -? # 查看使用帮助，很清晰易懂 # 查看那个进程在监听 80 端口，最后一列是进程的 Pid netstat -ano | findstr 80 # windows 命令 netstat -ano | select-string 80 # powershell 命令，就是把 findstr 替换成 select-string # 不仅列出 Pid，还给出 Pid 对应的可执行文件名称（需要管理员权限） netstat -ano -b | select-string 80 # powershell 命令 # 列出所有 ESTABLISHED 的 socket（默认行为） netstat # 列出所有正在监听的端口 netstat -ano | findstr LISTENING # 只列出 TCP 连接 netstat -ano -p TCP # 查看路由表 route -? # 查看使用帮助，很清晰易懂 route print # 查看所有路由信息 route print -4 # 仅 ipv4 比如我们遇到端口占用问题时，就可以通过上述命令查找到端口对应的 Pid，然后使用 kill \u003cPid\u003e 命令（powershell stop-process 的别名）杀死对应的进程。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:2:4","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#4-socket-信息查询---netstat"},{"categories":["tech"],"content":" 三、Mac OS XMac OS X 系统也是 unix-like 系统，也使用 zsh/bash，因此大部分命令基本都跟 Linux 没啥区别， 可以直接参考前面 Linux 一节的内容。 但是要注意一些坑： macos 自带的 tar 并不是 gnutar，命令使用方式不一样！ 解决：brew install gnu-tar，安装好后通过 gtar 调用，参数就跟 linux 一致了。 网络相关的命令区别较大，后面会详细介绍。 MacOSX 使用 launchpad 作为系统服务管理器，跟 systemd 区别很大。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:3:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#三mac-os-x"},{"categories":["tech"],"content":" 1. 查看 socket 信息Mac OS X 系统目前没有 ss，但是自带 netstat，该命令和 Linux 下的 netstat 有一定差别， 而且还很慢，还不能显示 pid. 所以 stackoverflow 上更推荐使用 lsof，几条常用命令记录如下 shell # -n 表示不显示主机名 # -P 表示不显示端口俗称 # 不加 sudo 只能查看以当前用户运行的程序 # 通用格式： sudo lsof -nP -iTCP:端口号 -sTCP:LISTEN # 查看所有 tcp 连接 lsof -nP -iTCP # 查看所有监听端口相关的信息（command/pid） lsof -nP -iTCP -sTCP:LISTEN ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:3:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-查看-socket-信息"},{"categories":["tech"],"content":" 2. 其他网络相关命令清理 DNS 缓存： shell # macos 10.10+ sudo dscacheutil -flushcache sudo killall -HUP mDNSResponder # 其他版本请自己网上搜... shell # 查看所有网络接口及相关参数（ip/mac/type...） ifconfig # 查看路由表 netstat -nr ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:3:2","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#2-其他网络相关命令"},{"categories":["tech"],"content":" 四、跨平台程序","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#四跨平台程序"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#1-vim"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#常用技巧"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#多行修改"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#多行替换基本和-sed-一致"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#将选中部分写入到文件"},{"categories":["tech"],"content":" 1. vim 常用技巧移动： 0/^ 回到行首，$ 去到行末 w 跳到下个单词的首部，e 跳到下个单词末尾 也能使用 2w 2e 这种命令按单词数量跳转 删除 d 或修改 c: dw 删除单词，d2w 删除两个单词 d$ 删除到行末，d0/d^ 删除到行首 d(/d{/d[[ 删除到文件首部，d)/d}/d]] 删除到文件末尾 r 替换一个字符，R 持续往后替换 多行修改多行插入，主要用于加注释之类的： 光标停留在你需要插入文本的地方 ctrl+v 进入 visual block 模式，选中多行 输入 I，进入编辑模式 输入 # 注释或者其他字符，但是注意不能输入换行符！也不能删除？ 按两下 Esc，依次退出 Insert 和 visual block 模式，就插入成功了 多行删除： v 进入 visual 模式，在第一行，选中你想要删除的文本块 或者也可以先进入 visual block 模式，再通过左右方向键选择文本。 ctrl+v 进入 visual block 模式，选中多行 visual block 的特点是它是垂直选择，而 visual 模式是段落选择 按 d 键就能删除被选中的所有内容。 多行替换（基本和 sed 一致）多行行首插入注释符号 # :1,6 s/^/#/g :2,$ s/^/#/g 注：此为2行至尾行 :% s/^/#/g 注：此为所有行 这里使用了正则表达式 ^ 匹配行首，改成 $ 就可在行尾进行批量修改。 此外，它的分隔符也不仅限于 \\，也可以用 @ 等符号，方便阅读。比如： :1,6 s@^@#@g :2,$ s@^@#@g 注：此为2行至尾行 :% s@^@#@g 注：此为所有行 使用 vim 的这个正则匹配功能，不仅能进行插入，也能完成删除、替换的功能。 将选中部分写入到文件 首先按 v 进入 visual 模式，选中需要的内容 按 :，应该会显示 :'\u003c,'\u003e，表示对选中部分进行操作 输入内容 w new.txt，此时显示效果应该是 :'\u003c,'\u003ew new.txt 回车就能完成文件写入 问题：在 vim 中粘贴 yaml 时缩进会变得一团糟解决方法：在命令模式下输入 :set paste 进入粘贴模式，然后再粘贴 yaml 内容。 注意行首可能会丢失几个字符，需要手动补上。 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:4:1","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#问题在-vim-中粘贴-yaml-时缩进会变得一团糟"},{"categories":["tech"],"content":" 参考 如何在 Linux 中查看进程占用的端口号 github - nsenter 使用 lsof 代替 Mac OS X 中的 netstat 查看占用端口的程序 aws 常用命令 ","date":"2022-02-13","objectID":"/posts/common-commands-for-various-operating-systems/:5:0","series":null,"tags":["Linux","MacOSX","Windows","CLI","Powershell","Shell","tmux","rsync","vim","awk"],"title":"Linux/Windows/MacOSX 系统常用命令集锦","uri":"/posts/common-commands-for-various-operating-systems/#参考"},{"categories":["tech"],"content":" 个人笔记，不保证正确。 内容比较多，建议参照目录浏览。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:0:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#"},{"categories":["tech"],"content":" 一、标准库","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#一标准库"},{"categories":["tech"],"content":" 1. 文件路径 - pathlib提供了 OS 无关的文件路径抽象，可以完全替代旧的 os.path 和 glob. 学会了 pathlib.Path，你就会了 Python 处理文件路径的所有功能。 1. 路径解析与拼接 python3 from pathlib import Path data_folder = Path(\"./source_data/text_files/\") data_file = data_folder / \"raw_data.txt\" # Path 重载了 / 操作符，路径拼接超级方便 # 路径的解析 data_file.parent # 获取父路径，这里的结果就是 data_folder data_folder.parent # 会返回 Path(\"source_data\") data_file.parents[1] # 即获取到 data_file 的上上层目录，结果和上面一样是 Path(\"source_data\") data_file.parents[2] # 上上上层目录，Path(\".\") dara_file.name # 文件名 \"raw_data.txt\" dara_file.suffix # 文件的后缀（最末尾的）\".txt\"，还可用 suffixes 获取所有后缀 data_file.stem # 去除掉最末尾的后缀后（只去除一个），剩下的文件名：raw_data # 替换文件名或者文件后缀 data_file.with_name(\"test.txt\") # 变成 .../test.txt data_file.with_suffix(\".pdf\") # 变成 .../raw_data.pdf # 当前路径与另一路径 的相对路径 data_file.relative_to(data_folder) # PosixPath('raw_data.txt') 2. pathlib 常用函数 python3 if not data_folder.exists(): data_folder.mkdir(parents=True) # 直接创建文件夹，如果父文件夹不存在，也自动创建 if not filename.exists(): # 文件是否存在 filename.touch() # 直接创建空文件，或者用 filename.open() 直接获取文件句柄 # 路径类型判断 if data_file.is_file(): # 是文件 print(data_file, \"is a file\") elif data_file.is_dir(): # 是文件夹 for child in p.iterdir(): # 通过 Path.iterdir() 迭代文件夹中的内容 print(child) # 路径解析 # 获取文件的绝对路径（符号链接也会被解析到真正的文件） filename.resolve() # 在不区分大小写的系统上（Windows），这个函数也会将大小写转换成实际的形式。 # 可以直接获取 Home 路径或者当前路径 Path.home() / \"file.txt\" # 有时需要以 home 为 base path 来构建文件路径 Path.cwd() / \"file.txt\" # 或者基于当前路径构建 还有很多其它的实用函数，可在使用中慢慢探索。 3. glob 通配符pathlib 也提供了 glob 支持，也就是广泛用在路径匹配上的一种简化正则表达式。 python data_file.match(glob_pattern) # 返回 True 或 False，表示文件路径与给出的 glob pattern 是否匹配 for py_file in data_folder.glob(\"*/*.py\"): # 匹配当前路径下的子文件夹中的 py 文件，会返回一个可迭代对象 print(py_file) # 反向匹配，相当于 glob 模式开头添加 \"**/\" for py_file in data_folder.glob(\"**/*.py\"): # 匹配当前路径下的所有 py 文件（所有子文件夹也会被搜索），返回一个可迭代对象 print(py_file) glob 中的 * 表示任意字符，而 ** 则表示任意层目录。（在大型文件树上使用 ** 速度会很慢！） ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-文件路径---pathlib"},{"categories":["tech"],"content":" 1. 文件路径 - pathlib提供了 OS 无关的文件路径抽象，可以完全替代旧的 os.path 和 glob. 学会了 pathlib.Path，你就会了 Python 处理文件路径的所有功能。 1. 路径解析与拼接 python3 from pathlib import Path data_folder = Path(\"./source_data/text_files/\") data_file = data_folder / \"raw_data.txt\" # Path 重载了 / 操作符，路径拼接超级方便 # 路径的解析 data_file.parent # 获取父路径，这里的结果就是 data_folder data_folder.parent # 会返回 Path(\"source_data\") data_file.parents[1] # 即获取到 data_file 的上上层目录，结果和上面一样是 Path(\"source_data\") data_file.parents[2] # 上上上层目录，Path(\".\") dara_file.name # 文件名 \"raw_data.txt\" dara_file.suffix # 文件的后缀（最末尾的）\".txt\"，还可用 suffixes 获取所有后缀 data_file.stem # 去除掉最末尾的后缀后（只去除一个），剩下的文件名：raw_data # 替换文件名或者文件后缀 data_file.with_name(\"test.txt\") # 变成 .../test.txt data_file.with_suffix(\".pdf\") # 变成 .../raw_data.pdf # 当前路径与另一路径 的相对路径 data_file.relative_to(data_folder) # PosixPath('raw_data.txt') 2. pathlib 常用函数 python3 if not data_folder.exists(): data_folder.mkdir(parents=True) # 直接创建文件夹，如果父文件夹不存在，也自动创建 if not filename.exists(): # 文件是否存在 filename.touch() # 直接创建空文件，或者用 filename.open() 直接获取文件句柄 # 路径类型判断 if data_file.is_file(): # 是文件 print(data_file, \"is a file\") elif data_file.is_dir(): # 是文件夹 for child in p.iterdir(): # 通过 Path.iterdir() 迭代文件夹中的内容 print(child) # 路径解析 # 获取文件的绝对路径（符号链接也会被解析到真正的文件） filename.resolve() # 在不区分大小写的系统上（Windows），这个函数也会将大小写转换成实际的形式。 # 可以直接获取 Home 路径或者当前路径 Path.home() / \"file.txt\" # 有时需要以 home 为 base path 来构建文件路径 Path.cwd() / \"file.txt\" # 或者基于当前路径构建 还有很多其它的实用函数，可在使用中慢慢探索。 3. glob 通配符pathlib 也提供了 glob 支持，也就是广泛用在路径匹配上的一种简化正则表达式。 python data_file.match(glob_pattern) # 返回 True 或 False，表示文件路径与给出的 glob pattern 是否匹配 for py_file in data_folder.glob(\"*/*.py\"): # 匹配当前路径下的子文件夹中的 py 文件，会返回一个可迭代对象 print(py_file) # 反向匹配，相当于 glob 模式开头添加 \"**/\" for py_file in data_folder.glob(\"**/*.py\"): # 匹配当前路径下的所有 py 文件（所有子文件夹也会被搜索），返回一个可迭代对象 print(py_file) glob 中的 * 表示任意字符，而 ** 则表示任意层目录。（在大型文件树上使用 ** 速度会很慢！） ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-路径解析与拼接"},{"categories":["tech"],"content":" 1. 文件路径 - pathlib提供了 OS 无关的文件路径抽象，可以完全替代旧的 os.path 和 glob. 学会了 pathlib.Path，你就会了 Python 处理文件路径的所有功能。 1. 路径解析与拼接 python3 from pathlib import Path data_folder = Path(\"./source_data/text_files/\") data_file = data_folder / \"raw_data.txt\" # Path 重载了 / 操作符，路径拼接超级方便 # 路径的解析 data_file.parent # 获取父路径，这里的结果就是 data_folder data_folder.parent # 会返回 Path(\"source_data\") data_file.parents[1] # 即获取到 data_file 的上上层目录，结果和上面一样是 Path(\"source_data\") data_file.parents[2] # 上上上层目录，Path(\".\") dara_file.name # 文件名 \"raw_data.txt\" dara_file.suffix # 文件的后缀（最末尾的）\".txt\"，还可用 suffixes 获取所有后缀 data_file.stem # 去除掉最末尾的后缀后（只去除一个），剩下的文件名：raw_data # 替换文件名或者文件后缀 data_file.with_name(\"test.txt\") # 变成 .../test.txt data_file.with_suffix(\".pdf\") # 变成 .../raw_data.pdf # 当前路径与另一路径 的相对路径 data_file.relative_to(data_folder) # PosixPath('raw_data.txt') 2. pathlib 常用函数 python3 if not data_folder.exists(): data_folder.mkdir(parents=True) # 直接创建文件夹，如果父文件夹不存在，也自动创建 if not filename.exists(): # 文件是否存在 filename.touch() # 直接创建空文件，或者用 filename.open() 直接获取文件句柄 # 路径类型判断 if data_file.is_file(): # 是文件 print(data_file, \"is a file\") elif data_file.is_dir(): # 是文件夹 for child in p.iterdir(): # 通过 Path.iterdir() 迭代文件夹中的内容 print(child) # 路径解析 # 获取文件的绝对路径（符号链接也会被解析到真正的文件） filename.resolve() # 在不区分大小写的系统上（Windows），这个函数也会将大小写转换成实际的形式。 # 可以直接获取 Home 路径或者当前路径 Path.home() / \"file.txt\" # 有时需要以 home 为 base path 来构建文件路径 Path.cwd() / \"file.txt\" # 或者基于当前路径构建 还有很多其它的实用函数，可在使用中慢慢探索。 3. glob 通配符pathlib 也提供了 glob 支持，也就是广泛用在路径匹配上的一种简化正则表达式。 python data_file.match(glob_pattern) # 返回 True 或 False，表示文件路径与给出的 glob pattern 是否匹配 for py_file in data_folder.glob(\"*/*.py\"): # 匹配当前路径下的子文件夹中的 py 文件，会返回一个可迭代对象 print(py_file) # 反向匹配，相当于 glob 模式开头添加 \"**/\" for py_file in data_folder.glob(\"**/*.py\"): # 匹配当前路径下的所有 py 文件（所有子文件夹也会被搜索），返回一个可迭代对象 print(py_file) glob 中的 * 表示任意字符，而 ** 则表示任意层目录。（在大型文件树上使用 ** 速度会很慢！） ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-pathlib-常用函数"},{"categories":["tech"],"content":" 1. 文件路径 - pathlib提供了 OS 无关的文件路径抽象，可以完全替代旧的 os.path 和 glob. 学会了 pathlib.Path，你就会了 Python 处理文件路径的所有功能。 1. 路径解析与拼接 python3 from pathlib import Path data_folder = Path(\"./source_data/text_files/\") data_file = data_folder / \"raw_data.txt\" # Path 重载了 / 操作符，路径拼接超级方便 # 路径的解析 data_file.parent # 获取父路径，这里的结果就是 data_folder data_folder.parent # 会返回 Path(\"source_data\") data_file.parents[1] # 即获取到 data_file 的上上层目录，结果和上面一样是 Path(\"source_data\") data_file.parents[2] # 上上上层目录，Path(\".\") dara_file.name # 文件名 \"raw_data.txt\" dara_file.suffix # 文件的后缀（最末尾的）\".txt\"，还可用 suffixes 获取所有后缀 data_file.stem # 去除掉最末尾的后缀后（只去除一个），剩下的文件名：raw_data # 替换文件名或者文件后缀 data_file.with_name(\"test.txt\") # 变成 .../test.txt data_file.with_suffix(\".pdf\") # 变成 .../raw_data.pdf # 当前路径与另一路径 的相对路径 data_file.relative_to(data_folder) # PosixPath('raw_data.txt') 2. pathlib 常用函数 python3 if not data_folder.exists(): data_folder.mkdir(parents=True) # 直接创建文件夹，如果父文件夹不存在，也自动创建 if not filename.exists(): # 文件是否存在 filename.touch() # 直接创建空文件，或者用 filename.open() 直接获取文件句柄 # 路径类型判断 if data_file.is_file(): # 是文件 print(data_file, \"is a file\") elif data_file.is_dir(): # 是文件夹 for child in p.iterdir(): # 通过 Path.iterdir() 迭代文件夹中的内容 print(child) # 路径解析 # 获取文件的绝对路径（符号链接也会被解析到真正的文件） filename.resolve() # 在不区分大小写的系统上（Windows），这个函数也会将大小写转换成实际的形式。 # 可以直接获取 Home 路径或者当前路径 Path.home() / \"file.txt\" # 有时需要以 home 为 base path 来构建文件路径 Path.cwd() / \"file.txt\" # 或者基于当前路径构建 还有很多其它的实用函数，可在使用中慢慢探索。 3. glob 通配符pathlib 也提供了 glob 支持，也就是广泛用在路径匹配上的一种简化正则表达式。 python data_file.match(glob_pattern) # 返回 True 或 False，表示文件路径与给出的 glob pattern 是否匹配 for py_file in data_folder.glob(\"*/*.py\"): # 匹配当前路径下的子文件夹中的 py 文件，会返回一个可迭代对象 print(py_file) # 反向匹配，相当于 glob 模式开头添加 \"**/\" for py_file in data_folder.glob(\"**/*.py\"): # 匹配当前路径下的所有 py 文件（所有子文件夹也会被搜索），返回一个可迭代对象 print(py_file) glob 中的 * 表示任意字符，而 ** 则表示任意层目录。（在大型文件树上使用 ** 速度会很慢！） ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-glob-通配符"},{"categories":["tech"],"content":" 2. 时间日期处理python3 在时间日期处理方面，有标准库 datetime 跟 calender，也有流行的第三方库 arrow 跟 maya. 标准库 datetime 有时候不太方便，比如没有提供解析 iso 格式的函数。另外就是用标准库时，经常需要自定义格式化串。相比之下，maya 和 arrow 这两个第三方库会方便很多。 不过第三方库并不是任何时候都可用，这里只介绍标准库 datetime 的用法，maya/arrow 请自行查找官方文档学习。 1. 获取当前时间 python3 import time import datetime as dt # 1. 获取当前时间的时间戳 time.time() # 直接调用 c api，因此速度很快: 1582315203.537061 utcnow = dt.datetime.utcnow() # 当前的世界标准时间: datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) utcnow.timestamp() # 将标准时转换成时间戳：datetime =\u003e 1582315203.537061 # 2. UTC 世界标准时间 time.gmtime() #输出为： time.struct_time(tm_year=2019, tm_mon=6, tm_mday=23, # tm_hour=3, tm_min=49, tm_sec=17, # tm_wday=6, tm_yday=174, tm_isdst=0) # 这实际上是一个命名元组 # 3. 构建一个指定的 datetime 实例 time_1997 = dt.datetime(year=1997, month=1, day=1) # =\u003e datetime.datetime(1997, 1, 1, 0, 0) dt.datetime(year=1997, month=1, day=1, minute=11) # =\u003e datetime.datetime(1997, 1, 1, 0, 11) 2. 时间日期的修改与运算 python3 # 0. 日期的修改（修改年月时分秒） utcnow.replace(day=11) # =\u003e datetime.datetime(2020, 2, 11, 4, 0, 3, 537061) 修改 day utcnow.replace(hour=11) # =\u003e datetime.datetime(2020, 2, 22, 11, 0, 3, 537061) 修改 hour # 1. 日期与时间 date_utcnow = utcnow.date() # =\u003e datetime.date(2020, 2, 22) 年月日 time_utcnow = utcnow.time() # =\u003e datetime.time(4, 0, 3, 537061) 时分秒 # 2. 联结时间和日期（date 和 time 不能用加法联结） dt.datetime.combine(date_utcnow, time_utcnow) # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 3. 日期的运算 # 3.1 datetime 之间只能计算时间差（减法），不能进行其他运算 utcnow - time_1997 # =\u003e datetime.timedelta(days=8452, seconds=14403, microseconds=537061) # 3.2 使用 timedelta 进行时间的增减 days_step = dt.timedelta(days=1) # 注意参数是复数形式 time_1997 + days_step # =\u003e datetime.datetime(1997, 1, 2, 0, 0) time_1997 - days_step # =\u003e datetime.datetime(1996, 12, 31, 0, 0) # 3.3 timedelta 之间也可以进行加减法 hours_step = dt.timedelta(hours=1) # =\u003e datetime.timedelta(seconds=3600) days_step + hours_step # =\u003e datetime.timedelta(days=1, seconds=3600) days_step - hours_step # =\u003e datetime.timedelta(seconds=82800) hours_step - days_step # =\u003e datetime.timedelta(days=-1, seconds=3600) # 3.4 timedelta 还可以按比例增减（与数字进行乘除法） hours_step * 2 # =\u003e datetime.timedelta(seconds=7200) days_step * -2 # =\u003e datetime.timedelta(days=-2) hours_step * 1.1 # =\u003e datetime.timedelta(seconds=3960) 3. 时间日期的格式化与解析先介绍下常用的格式化字符串： 普通格式 - ‘%Y-%m-%d %H:%M:%S’ =\u003e ‘2020-02-22 04:00:03’ ISO 格式 - ‘%Y-%m-%dT%H:%M:%S.%fZ’ =\u003e ‘2020-02-22T04:00:03.537061Z’ 带时区的格式 - ‘%Y-%m-%dT%H:%M:%S%Z’ =\u003e 2022-02-10T00:48:52UTC+08:00 需要时间对象自身有时区属性才行！否则格式化时会忽略 %Z 另外再介绍下 Python 两个时间格式化与解析函数的命名： strftime: 即 string format time strptime: 即 string parse time python3 # 1. 将时间格式化成字符串 # 1.1 将 datetime 格式化为 iso 标准格式 utcnow.isoformat() # =\u003e '2020-02-22T04:00:03.537061' utcnow.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T04:00:03.537061Z' utcnow.date().strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T00:00:00.000000Z' # 1.2 将 time.struct_time 格式化为日期字符串（貌似不支持 iso，可能是精度不够） time.strftime('%Y-%m-%dT%H:%M:%S', gm) # =\u003e '2020-02-22T04:00:03' # 1.3 将 datetime 格式化成指定格式 utcnow.strftime('%Y-%m-%d %H:%M:%S') # =\u003e '2020-02-22 04:00:03' # 2. 解析时间字符串 # 2.1 解析 iso 格式的时间字符串，手动指定格式（注意 %f 只对应六位小数，对9位小数它无能为力。。） dt.datetime.strptime('2020-02-22T04:00:03.537061Z', '%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 2.2 解析 iso 格式的时间字符串(需要 python 3.7+) dt.datetime.fromisoformat('2020-02-22T04:00:03.537061') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) dt.date.fromisoformat('2020-02-22') # =\u003e datetime.date(2020, 2, 22) dt.time.fromisoformat(\"04:00:03.537061\") # =\u003e datetime.time(4, 0, 3, 537061) # 2.3 解析指定格式的字符串 dt.datetime.strptime('2020-02-22 04:00:03', '%Y-%m-%d %H:%M:%S') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3) 4. 时区转换与日期格式化 python3 # 上海时区：东八区 utc+8 tz_shanghai = dt.timezone(dt.timedelta(hours=8)) now_shanghai = dt.datetime.now(tz=tz_shanghai) now_shanghai.strftime('%Y-%m-%dT%H:%M:%S%Z') # =\u003e 2022-02-10T00:48:52UTC+08:00 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-时间日期处理"},{"categories":["tech"],"content":" 2. 时间日期处理python3 在时间日期处理方面，有标准库 datetime 跟 calender，也有流行的第三方库 arrow 跟 maya. 标准库 datetime 有时候不太方便，比如没有提供解析 iso 格式的函数。另外就是用标准库时，经常需要自定义格式化串。相比之下，maya 和 arrow 这两个第三方库会方便很多。 不过第三方库并不是任何时候都可用，这里只介绍标准库 datetime 的用法，maya/arrow 请自行查找官方文档学习。 1. 获取当前时间 python3 import time import datetime as dt # 1. 获取当前时间的时间戳 time.time() # 直接调用 c api，因此速度很快: 1582315203.537061 utcnow = dt.datetime.utcnow() # 当前的世界标准时间: datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) utcnow.timestamp() # 将标准时转换成时间戳：datetime =\u003e 1582315203.537061 # 2. UTC 世界标准时间 time.gmtime() #输出为： time.struct_time(tm_year=2019, tm_mon=6, tm_mday=23, # tm_hour=3, tm_min=49, tm_sec=17, # tm_wday=6, tm_yday=174, tm_isdst=0) # 这实际上是一个命名元组 # 3. 构建一个指定的 datetime 实例 time_1997 = dt.datetime(year=1997, month=1, day=1) # =\u003e datetime.datetime(1997, 1, 1, 0, 0) dt.datetime(year=1997, month=1, day=1, minute=11) # =\u003e datetime.datetime(1997, 1, 1, 0, 11) 2. 时间日期的修改与运算 python3 # 0. 日期的修改（修改年月时分秒） utcnow.replace(day=11) # =\u003e datetime.datetime(2020, 2, 11, 4, 0, 3, 537061) 修改 day utcnow.replace(hour=11) # =\u003e datetime.datetime(2020, 2, 22, 11, 0, 3, 537061) 修改 hour # 1. 日期与时间 date_utcnow = utcnow.date() # =\u003e datetime.date(2020, 2, 22) 年月日 time_utcnow = utcnow.time() # =\u003e datetime.time(4, 0, 3, 537061) 时分秒 # 2. 联结时间和日期（date 和 time 不能用加法联结） dt.datetime.combine(date_utcnow, time_utcnow) # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 3. 日期的运算 # 3.1 datetime 之间只能计算时间差（减法），不能进行其他运算 utcnow - time_1997 # =\u003e datetime.timedelta(days=8452, seconds=14403, microseconds=537061) # 3.2 使用 timedelta 进行时间的增减 days_step = dt.timedelta(days=1) # 注意参数是复数形式 time_1997 + days_step # =\u003e datetime.datetime(1997, 1, 2, 0, 0) time_1997 - days_step # =\u003e datetime.datetime(1996, 12, 31, 0, 0) # 3.3 timedelta 之间也可以进行加减法 hours_step = dt.timedelta(hours=1) # =\u003e datetime.timedelta(seconds=3600) days_step + hours_step # =\u003e datetime.timedelta(days=1, seconds=3600) days_step - hours_step # =\u003e datetime.timedelta(seconds=82800) hours_step - days_step # =\u003e datetime.timedelta(days=-1, seconds=3600) # 3.4 timedelta 还可以按比例增减（与数字进行乘除法） hours_step * 2 # =\u003e datetime.timedelta(seconds=7200) days_step * -2 # =\u003e datetime.timedelta(days=-2) hours_step * 1.1 # =\u003e datetime.timedelta(seconds=3960) 3. 时间日期的格式化与解析先介绍下常用的格式化字符串： 普通格式 - ‘%Y-%m-%d %H:%M:%S’ =\u003e ‘2020-02-22 04:00:03’ ISO 格式 - ‘%Y-%m-%dT%H:%M:%S.%fZ’ =\u003e ‘2020-02-22T04:00:03.537061Z’ 带时区的格式 - ‘%Y-%m-%dT%H:%M:%S%Z’ =\u003e 2022-02-10T00:48:52UTC+08:00 需要时间对象自身有时区属性才行！否则格式化时会忽略 %Z 另外再介绍下 Python 两个时间格式化与解析函数的命名： strftime: 即 string format time strptime: 即 string parse time python3 # 1. 将时间格式化成字符串 # 1.1 将 datetime 格式化为 iso 标准格式 utcnow.isoformat() # =\u003e '2020-02-22T04:00:03.537061' utcnow.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T04:00:03.537061Z' utcnow.date().strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T00:00:00.000000Z' # 1.2 将 time.struct_time 格式化为日期字符串（貌似不支持 iso，可能是精度不够） time.strftime('%Y-%m-%dT%H:%M:%S', gm) # =\u003e '2020-02-22T04:00:03' # 1.3 将 datetime 格式化成指定格式 utcnow.strftime('%Y-%m-%d %H:%M:%S') # =\u003e '2020-02-22 04:00:03' # 2. 解析时间字符串 # 2.1 解析 iso 格式的时间字符串，手动指定格式（注意 %f 只对应六位小数，对9位小数它无能为力。。） dt.datetime.strptime('2020-02-22T04:00:03.537061Z', '%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 2.2 解析 iso 格式的时间字符串(需要 python 3.7+) dt.datetime.fromisoformat('2020-02-22T04:00:03.537061') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) dt.date.fromisoformat('2020-02-22') # =\u003e datetime.date(2020, 2, 22) dt.time.fromisoformat(\"04:00:03.537061\") # =\u003e datetime.time(4, 0, 3, 537061) # 2.3 解析指定格式的字符串 dt.datetime.strptime('2020-02-22 04:00:03', '%Y-%m-%d %H:%M:%S') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3) 4. 时区转换与日期格式化 python3 # 上海时区：东八区 utc+8 tz_shanghai = dt.timezone(dt.timedelta(hours=8)) now_shanghai = dt.datetime.now(tz=tz_shanghai) now_shanghai.strftime('%Y-%m-%dT%H:%M:%S%Z') # =\u003e 2022-02-10T00:48:52UTC+08:00 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-获取当前时间"},{"categories":["tech"],"content":" 2. 时间日期处理python3 在时间日期处理方面，有标准库 datetime 跟 calender，也有流行的第三方库 arrow 跟 maya. 标准库 datetime 有时候不太方便，比如没有提供解析 iso 格式的函数。另外就是用标准库时，经常需要自定义格式化串。相比之下，maya 和 arrow 这两个第三方库会方便很多。 不过第三方库并不是任何时候都可用，这里只介绍标准库 datetime 的用法，maya/arrow 请自行查找官方文档学习。 1. 获取当前时间 python3 import time import datetime as dt # 1. 获取当前时间的时间戳 time.time() # 直接调用 c api，因此速度很快: 1582315203.537061 utcnow = dt.datetime.utcnow() # 当前的世界标准时间: datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) utcnow.timestamp() # 将标准时转换成时间戳：datetime =\u003e 1582315203.537061 # 2. UTC 世界标准时间 time.gmtime() #输出为： time.struct_time(tm_year=2019, tm_mon=6, tm_mday=23, # tm_hour=3, tm_min=49, tm_sec=17, # tm_wday=6, tm_yday=174, tm_isdst=0) # 这实际上是一个命名元组 # 3. 构建一个指定的 datetime 实例 time_1997 = dt.datetime(year=1997, month=1, day=1) # =\u003e datetime.datetime(1997, 1, 1, 0, 0) dt.datetime(year=1997, month=1, day=1, minute=11) # =\u003e datetime.datetime(1997, 1, 1, 0, 11) 2. 时间日期的修改与运算 python3 # 0. 日期的修改（修改年月时分秒） utcnow.replace(day=11) # =\u003e datetime.datetime(2020, 2, 11, 4, 0, 3, 537061) 修改 day utcnow.replace(hour=11) # =\u003e datetime.datetime(2020, 2, 22, 11, 0, 3, 537061) 修改 hour # 1. 日期与时间 date_utcnow = utcnow.date() # =\u003e datetime.date(2020, 2, 22) 年月日 time_utcnow = utcnow.time() # =\u003e datetime.time(4, 0, 3, 537061) 时分秒 # 2. 联结时间和日期（date 和 time 不能用加法联结） dt.datetime.combine(date_utcnow, time_utcnow) # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 3. 日期的运算 # 3.1 datetime 之间只能计算时间差（减法），不能进行其他运算 utcnow - time_1997 # =\u003e datetime.timedelta(days=8452, seconds=14403, microseconds=537061) # 3.2 使用 timedelta 进行时间的增减 days_step = dt.timedelta(days=1) # 注意参数是复数形式 time_1997 + days_step # =\u003e datetime.datetime(1997, 1, 2, 0, 0) time_1997 - days_step # =\u003e datetime.datetime(1996, 12, 31, 0, 0) # 3.3 timedelta 之间也可以进行加减法 hours_step = dt.timedelta(hours=1) # =\u003e datetime.timedelta(seconds=3600) days_step + hours_step # =\u003e datetime.timedelta(days=1, seconds=3600) days_step - hours_step # =\u003e datetime.timedelta(seconds=82800) hours_step - days_step # =\u003e datetime.timedelta(days=-1, seconds=3600) # 3.4 timedelta 还可以按比例增减（与数字进行乘除法） hours_step * 2 # =\u003e datetime.timedelta(seconds=7200) days_step * -2 # =\u003e datetime.timedelta(days=-2) hours_step * 1.1 # =\u003e datetime.timedelta(seconds=3960) 3. 时间日期的格式化与解析先介绍下常用的格式化字符串： 普通格式 - ‘%Y-%m-%d %H:%M:%S’ =\u003e ‘2020-02-22 04:00:03’ ISO 格式 - ‘%Y-%m-%dT%H:%M:%S.%fZ’ =\u003e ‘2020-02-22T04:00:03.537061Z’ 带时区的格式 - ‘%Y-%m-%dT%H:%M:%S%Z’ =\u003e 2022-02-10T00:48:52UTC+08:00 需要时间对象自身有时区属性才行！否则格式化时会忽略 %Z 另外再介绍下 Python 两个时间格式化与解析函数的命名： strftime: 即 string format time strptime: 即 string parse time python3 # 1. 将时间格式化成字符串 # 1.1 将 datetime 格式化为 iso 标准格式 utcnow.isoformat() # =\u003e '2020-02-22T04:00:03.537061' utcnow.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T04:00:03.537061Z' utcnow.date().strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T00:00:00.000000Z' # 1.2 将 time.struct_time 格式化为日期字符串（貌似不支持 iso，可能是精度不够） time.strftime('%Y-%m-%dT%H:%M:%S', gm) # =\u003e '2020-02-22T04:00:03' # 1.3 将 datetime 格式化成指定格式 utcnow.strftime('%Y-%m-%d %H:%M:%S') # =\u003e '2020-02-22 04:00:03' # 2. 解析时间字符串 # 2.1 解析 iso 格式的时间字符串，手动指定格式（注意 %f 只对应六位小数，对9位小数它无能为力。。） dt.datetime.strptime('2020-02-22T04:00:03.537061Z', '%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 2.2 解析 iso 格式的时间字符串(需要 python 3.7+) dt.datetime.fromisoformat('2020-02-22T04:00:03.537061') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) dt.date.fromisoformat('2020-02-22') # =\u003e datetime.date(2020, 2, 22) dt.time.fromisoformat(\"04:00:03.537061\") # =\u003e datetime.time(4, 0, 3, 537061) # 2.3 解析指定格式的字符串 dt.datetime.strptime('2020-02-22 04:00:03', '%Y-%m-%d %H:%M:%S') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3) 4. 时区转换与日期格式化 python3 # 上海时区：东八区 utc+8 tz_shanghai = dt.timezone(dt.timedelta(hours=8)) now_shanghai = dt.datetime.now(tz=tz_shanghai) now_shanghai.strftime('%Y-%m-%dT%H:%M:%S%Z') # =\u003e 2022-02-10T00:48:52UTC+08:00 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-时间日期的修改与运算"},{"categories":["tech"],"content":" 2. 时间日期处理python3 在时间日期处理方面，有标准库 datetime 跟 calender，也有流行的第三方库 arrow 跟 maya. 标准库 datetime 有时候不太方便，比如没有提供解析 iso 格式的函数。另外就是用标准库时，经常需要自定义格式化串。相比之下，maya 和 arrow 这两个第三方库会方便很多。 不过第三方库并不是任何时候都可用，这里只介绍标准库 datetime 的用法，maya/arrow 请自行查找官方文档学习。 1. 获取当前时间 python3 import time import datetime as dt # 1. 获取当前时间的时间戳 time.time() # 直接调用 c api，因此速度很快: 1582315203.537061 utcnow = dt.datetime.utcnow() # 当前的世界标准时间: datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) utcnow.timestamp() # 将标准时转换成时间戳：datetime =\u003e 1582315203.537061 # 2. UTC 世界标准时间 time.gmtime() #输出为： time.struct_time(tm_year=2019, tm_mon=6, tm_mday=23, # tm_hour=3, tm_min=49, tm_sec=17, # tm_wday=6, tm_yday=174, tm_isdst=0) # 这实际上是一个命名元组 # 3. 构建一个指定的 datetime 实例 time_1997 = dt.datetime(year=1997, month=1, day=1) # =\u003e datetime.datetime(1997, 1, 1, 0, 0) dt.datetime(year=1997, month=1, day=1, minute=11) # =\u003e datetime.datetime(1997, 1, 1, 0, 11) 2. 时间日期的修改与运算 python3 # 0. 日期的修改（修改年月时分秒） utcnow.replace(day=11) # =\u003e datetime.datetime(2020, 2, 11, 4, 0, 3, 537061) 修改 day utcnow.replace(hour=11) # =\u003e datetime.datetime(2020, 2, 22, 11, 0, 3, 537061) 修改 hour # 1. 日期与时间 date_utcnow = utcnow.date() # =\u003e datetime.date(2020, 2, 22) 年月日 time_utcnow = utcnow.time() # =\u003e datetime.time(4, 0, 3, 537061) 时分秒 # 2. 联结时间和日期（date 和 time 不能用加法联结） dt.datetime.combine(date_utcnow, time_utcnow) # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 3. 日期的运算 # 3.1 datetime 之间只能计算时间差（减法），不能进行其他运算 utcnow - time_1997 # =\u003e datetime.timedelta(days=8452, seconds=14403, microseconds=537061) # 3.2 使用 timedelta 进行时间的增减 days_step = dt.timedelta(days=1) # 注意参数是复数形式 time_1997 + days_step # =\u003e datetime.datetime(1997, 1, 2, 0, 0) time_1997 - days_step # =\u003e datetime.datetime(1996, 12, 31, 0, 0) # 3.3 timedelta 之间也可以进行加减法 hours_step = dt.timedelta(hours=1) # =\u003e datetime.timedelta(seconds=3600) days_step + hours_step # =\u003e datetime.timedelta(days=1, seconds=3600) days_step - hours_step # =\u003e datetime.timedelta(seconds=82800) hours_step - days_step # =\u003e datetime.timedelta(days=-1, seconds=3600) # 3.4 timedelta 还可以按比例增减（与数字进行乘除法） hours_step * 2 # =\u003e datetime.timedelta(seconds=7200) days_step * -2 # =\u003e datetime.timedelta(days=-2) hours_step * 1.1 # =\u003e datetime.timedelta(seconds=3960) 3. 时间日期的格式化与解析先介绍下常用的格式化字符串： 普通格式 - ‘%Y-%m-%d %H:%M:%S’ =\u003e ‘2020-02-22 04:00:03’ ISO 格式 - ‘%Y-%m-%dT%H:%M:%S.%fZ’ =\u003e ‘2020-02-22T04:00:03.537061Z’ 带时区的格式 - ‘%Y-%m-%dT%H:%M:%S%Z’ =\u003e 2022-02-10T00:48:52UTC+08:00 需要时间对象自身有时区属性才行！否则格式化时会忽略 %Z 另外再介绍下 Python 两个时间格式化与解析函数的命名： strftime: 即 string format time strptime: 即 string parse time python3 # 1. 将时间格式化成字符串 # 1.1 将 datetime 格式化为 iso 标准格式 utcnow.isoformat() # =\u003e '2020-02-22T04:00:03.537061' utcnow.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T04:00:03.537061Z' utcnow.date().strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T00:00:00.000000Z' # 1.2 将 time.struct_time 格式化为日期字符串（貌似不支持 iso，可能是精度不够） time.strftime('%Y-%m-%dT%H:%M:%S', gm) # =\u003e '2020-02-22T04:00:03' # 1.3 将 datetime 格式化成指定格式 utcnow.strftime('%Y-%m-%d %H:%M:%S') # =\u003e '2020-02-22 04:00:03' # 2. 解析时间字符串 # 2.1 解析 iso 格式的时间字符串，手动指定格式（注意 %f 只对应六位小数，对9位小数它无能为力。。） dt.datetime.strptime('2020-02-22T04:00:03.537061Z', '%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 2.2 解析 iso 格式的时间字符串(需要 python 3.7+) dt.datetime.fromisoformat('2020-02-22T04:00:03.537061') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) dt.date.fromisoformat('2020-02-22') # =\u003e datetime.date(2020, 2, 22) dt.time.fromisoformat(\"04:00:03.537061\") # =\u003e datetime.time(4, 0, 3, 537061) # 2.3 解析指定格式的字符串 dt.datetime.strptime('2020-02-22 04:00:03', '%Y-%m-%d %H:%M:%S') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3) 4. 时区转换与日期格式化 python3 # 上海时区：东八区 utc+8 tz_shanghai = dt.timezone(dt.timedelta(hours=8)) now_shanghai = dt.datetime.now(tz=tz_shanghai) now_shanghai.strftime('%Y-%m-%dT%H:%M:%S%Z') # =\u003e 2022-02-10T00:48:52UTC+08:00 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-时间日期的格式化与解析"},{"categories":["tech"],"content":" 2. 时间日期处理python3 在时间日期处理方面，有标准库 datetime 跟 calender，也有流行的第三方库 arrow 跟 maya. 标准库 datetime 有时候不太方便，比如没有提供解析 iso 格式的函数。另外就是用标准库时，经常需要自定义格式化串。相比之下，maya 和 arrow 这两个第三方库会方便很多。 不过第三方库并不是任何时候都可用，这里只介绍标准库 datetime 的用法，maya/arrow 请自行查找官方文档学习。 1. 获取当前时间 python3 import time import datetime as dt # 1. 获取当前时间的时间戳 time.time() # 直接调用 c api，因此速度很快: 1582315203.537061 utcnow = dt.datetime.utcnow() # 当前的世界标准时间: datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) utcnow.timestamp() # 将标准时转换成时间戳：datetime =\u003e 1582315203.537061 # 2. UTC 世界标准时间 time.gmtime() #输出为： time.struct_time(tm_year=2019, tm_mon=6, tm_mday=23, # tm_hour=3, tm_min=49, tm_sec=17, # tm_wday=6, tm_yday=174, tm_isdst=0) # 这实际上是一个命名元组 # 3. 构建一个指定的 datetime 实例 time_1997 = dt.datetime(year=1997, month=1, day=1) # =\u003e datetime.datetime(1997, 1, 1, 0, 0) dt.datetime(year=1997, month=1, day=1, minute=11) # =\u003e datetime.datetime(1997, 1, 1, 0, 11) 2. 时间日期的修改与运算 python3 # 0. 日期的修改（修改年月时分秒） utcnow.replace(day=11) # =\u003e datetime.datetime(2020, 2, 11, 4, 0, 3, 537061) 修改 day utcnow.replace(hour=11) # =\u003e datetime.datetime(2020, 2, 22, 11, 0, 3, 537061) 修改 hour # 1. 日期与时间 date_utcnow = utcnow.date() # =\u003e datetime.date(2020, 2, 22) 年月日 time_utcnow = utcnow.time() # =\u003e datetime.time(4, 0, 3, 537061) 时分秒 # 2. 联结时间和日期（date 和 time 不能用加法联结） dt.datetime.combine(date_utcnow, time_utcnow) # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 3. 日期的运算 # 3.1 datetime 之间只能计算时间差（减法），不能进行其他运算 utcnow - time_1997 # =\u003e datetime.timedelta(days=8452, seconds=14403, microseconds=537061) # 3.2 使用 timedelta 进行时间的增减 days_step = dt.timedelta(days=1) # 注意参数是复数形式 time_1997 + days_step # =\u003e datetime.datetime(1997, 1, 2, 0, 0) time_1997 - days_step # =\u003e datetime.datetime(1996, 12, 31, 0, 0) # 3.3 timedelta 之间也可以进行加减法 hours_step = dt.timedelta(hours=1) # =\u003e datetime.timedelta(seconds=3600) days_step + hours_step # =\u003e datetime.timedelta(days=1, seconds=3600) days_step - hours_step # =\u003e datetime.timedelta(seconds=82800) hours_step - days_step # =\u003e datetime.timedelta(days=-1, seconds=3600) # 3.4 timedelta 还可以按比例增减（与数字进行乘除法） hours_step * 2 # =\u003e datetime.timedelta(seconds=7200) days_step * -2 # =\u003e datetime.timedelta(days=-2) hours_step * 1.1 # =\u003e datetime.timedelta(seconds=3960) 3. 时间日期的格式化与解析先介绍下常用的格式化字符串： 普通格式 - ‘%Y-%m-%d %H:%M:%S’ =\u003e ‘2020-02-22 04:00:03’ ISO 格式 - ‘%Y-%m-%dT%H:%M:%S.%fZ’ =\u003e ‘2020-02-22T04:00:03.537061Z’ 带时区的格式 - ‘%Y-%m-%dT%H:%M:%S%Z’ =\u003e 2022-02-10T00:48:52UTC+08:00 需要时间对象自身有时区属性才行！否则格式化时会忽略 %Z 另外再介绍下 Python 两个时间格式化与解析函数的命名： strftime: 即 string format time strptime: 即 string parse time python3 # 1. 将时间格式化成字符串 # 1.1 将 datetime 格式化为 iso 标准格式 utcnow.isoformat() # =\u003e '2020-02-22T04:00:03.537061' utcnow.strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T04:00:03.537061Z' utcnow.date().strftime('%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e '2020-02-22T00:00:00.000000Z' # 1.2 将 time.struct_time 格式化为日期字符串（貌似不支持 iso，可能是精度不够） time.strftime('%Y-%m-%dT%H:%M:%S', gm) # =\u003e '2020-02-22T04:00:03' # 1.3 将 datetime 格式化成指定格式 utcnow.strftime('%Y-%m-%d %H:%M:%S') # =\u003e '2020-02-22 04:00:03' # 2. 解析时间字符串 # 2.1 解析 iso 格式的时间字符串，手动指定格式（注意 %f 只对应六位小数，对9位小数它无能为力。。） dt.datetime.strptime('2020-02-22T04:00:03.537061Z', '%Y-%m-%dT%H:%M:%S.%fZ') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) # 2.2 解析 iso 格式的时间字符串(需要 python 3.7+) dt.datetime.fromisoformat('2020-02-22T04:00:03.537061') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3, 537061) dt.date.fromisoformat('2020-02-22') # =\u003e datetime.date(2020, 2, 22) dt.time.fromisoformat(\"04:00:03.537061\") # =\u003e datetime.time(4, 0, 3, 537061) # 2.3 解析指定格式的字符串 dt.datetime.strptime('2020-02-22 04:00:03', '%Y-%m-%d %H:%M:%S') # =\u003e datetime.datetime(2020, 2, 22, 4, 0, 3) 4. 时区转换与日期格式化 python3 # 上海时区：东八区 utc+8 tz_shanghai = dt.timezone(dt.timedelta(hours=8)) now_shanghai = dt.datetime.now(tz=tz_shanghai) now_shanghai.strftime('%Y-%m-%dT%H:%M:%S%Z') # =\u003e 2022-02-10T00:48:52UTC+08:00 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#4-时区转换与日期格式化"},{"categories":["tech"],"content":" 3. 排序常用库 - operatoroperator 模块包含四种类型的方法： 1. operator.itemgetter经常被用于 sorted/max/mix/itertools.groupby 等 使用方法： python3 # itemgetter f = itemgetter(2) f(r) # return r[2] # 还能一次获取多个值，像 numpy 那样索引 f2 = itemgetter(2,4,5) f2(r) # return (r[2], r[4], r[5]) # 或者使用 slice 切片 s = itemgetter(slice(2, None)) s[r] # return r[2:] # dict 索引也能用 d = itemgetter('rank', 'name') d[r] # return d['rank'], d['name'] 用途： python3 # 用于指定用于比较大小的属性 key = itemgetter(1) sorted(iterable, key=key) # 使用 iterable[1] 对 iterable 进行排序 max(iterable, key=key) # 找出最大的元素，使用 iterable[1] 做比较 # 用于高级切片（比如像 numpy 那样的，指定只获取某几列） s = itemgetter(1,3,4) matrix = [[0,1,2,3,4], [1,2,3,4,5]] map(s, matrix) # list 后得到 [(1, 3, 4), (2,4,5)] 2. operator.attrgetter可用于动态获取对象的属性，与直接用 getattr() 不同的是，它可以嵌套访问属性。 Python3 # 嵌套访问属性 att = attrgetter(\"a.b.c\") att(obj) # return obj.a.b.c # 和 itemgetter 一样，也可以一次获取多个属性 att = attrgetter(\"a.b.c\", \"x.y\") att(obj) # return (obj.a.b.c, obj.x.y) # 不嵌套的话，用 getattr 就行 getattr(obj, \"a\") # return obj.a 这里可以回顾一下类的两个魔法函数： __getattr__: 当被访问的属性不存在时，这个方法会被调用，它的返回值会成为对象的该属性。 用于动态生成实例的属性/函数 __getattribute__: 与 __getattr__ 唯一的差别在于，访问对象的任何属性，都会直接调用这个方法，不管属性存不存在。 3. operator.methodcaller可用于调用函数，它和 attrgetter 很像，差别在于 attrgetter 只是返回指定的属性，而 methodcaller 会直接把指定的属性当成函数调用，然后返回结果。 举例 python3 f = methodcaller('name', 'foo', bar=1) f(b) # returns b.name('foo', bar=1) 4. 各种操作符对应的函数operator.add、operator.sub、operator.mul、operator.div 等等，函数式编程有时需要用到。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-排序常用库---operator"},{"categories":["tech"],"content":" 3. 排序常用库 - operatoroperator 模块包含四种类型的方法： 1. operator.itemgetter经常被用于 sorted/max/mix/itertools.groupby 等 使用方法： python3 # itemgetter f = itemgetter(2) f(r) # return r[2] # 还能一次获取多个值，像 numpy 那样索引 f2 = itemgetter(2,4,5) f2(r) # return (r[2], r[4], r[5]) # 或者使用 slice 切片 s = itemgetter(slice(2, None)) s[r] # return r[2:] # dict 索引也能用 d = itemgetter('rank', 'name') d[r] # return d['rank'], d['name'] 用途： python3 # 用于指定用于比较大小的属性 key = itemgetter(1) sorted(iterable, key=key) # 使用 iterable[1] 对 iterable 进行排序 max(iterable, key=key) # 找出最大的元素，使用 iterable[1] 做比较 # 用于高级切片（比如像 numpy 那样的，指定只获取某几列） s = itemgetter(1,3,4) matrix = [[0,1,2,3,4], [1,2,3,4,5]] map(s, matrix) # list 后得到 [(1, 3, 4), (2,4,5)] 2. operator.attrgetter可用于动态获取对象的属性，与直接用 getattr() 不同的是，它可以嵌套访问属性。 Python3 # 嵌套访问属性 att = attrgetter(\"a.b.c\") att(obj) # return obj.a.b.c # 和 itemgetter 一样，也可以一次获取多个属性 att = attrgetter(\"a.b.c\", \"x.y\") att(obj) # return (obj.a.b.c, obj.x.y) # 不嵌套的话，用 getattr 就行 getattr(obj, \"a\") # return obj.a 这里可以回顾一下类的两个魔法函数： __getattr__: 当被访问的属性不存在时，这个方法会被调用，它的返回值会成为对象的该属性。 用于动态生成实例的属性/函数 __getattribute__: 与 __getattr__ 唯一的差别在于，访问对象的任何属性，都会直接调用这个方法，不管属性存不存在。 3. operator.methodcaller可用于调用函数，它和 attrgetter 很像，差别在于 attrgetter 只是返回指定的属性，而 methodcaller 会直接把指定的属性当成函数调用，然后返回结果。 举例 python3 f = methodcaller('name', 'foo', bar=1) f(b) # returns b.name('foo', bar=1) 4. 各种操作符对应的函数operator.add、operator.sub、operator.mul、operator.div 等等，函数式编程有时需要用到。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-operatoritemgetter"},{"categories":["tech"],"content":" 3. 排序常用库 - operatoroperator 模块包含四种类型的方法： 1. operator.itemgetter经常被用于 sorted/max/mix/itertools.groupby 等 使用方法： python3 # itemgetter f = itemgetter(2) f(r) # return r[2] # 还能一次获取多个值，像 numpy 那样索引 f2 = itemgetter(2,4,5) f2(r) # return (r[2], r[4], r[5]) # 或者使用 slice 切片 s = itemgetter(slice(2, None)) s[r] # return r[2:] # dict 索引也能用 d = itemgetter('rank', 'name') d[r] # return d['rank'], d['name'] 用途： python3 # 用于指定用于比较大小的属性 key = itemgetter(1) sorted(iterable, key=key) # 使用 iterable[1] 对 iterable 进行排序 max(iterable, key=key) # 找出最大的元素，使用 iterable[1] 做比较 # 用于高级切片（比如像 numpy 那样的，指定只获取某几列） s = itemgetter(1,3,4) matrix = [[0,1,2,3,4], [1,2,3,4,5]] map(s, matrix) # list 后得到 [(1, 3, 4), (2,4,5)] 2. operator.attrgetter可用于动态获取对象的属性，与直接用 getattr() 不同的是，它可以嵌套访问属性。 Python3 # 嵌套访问属性 att = attrgetter(\"a.b.c\") att(obj) # return obj.a.b.c # 和 itemgetter 一样，也可以一次获取多个属性 att = attrgetter(\"a.b.c\", \"x.y\") att(obj) # return (obj.a.b.c, obj.x.y) # 不嵌套的话，用 getattr 就行 getattr(obj, \"a\") # return obj.a 这里可以回顾一下类的两个魔法函数： __getattr__: 当被访问的属性不存在时，这个方法会被调用，它的返回值会成为对象的该属性。 用于动态生成实例的属性/函数 __getattribute__: 与 __getattr__ 唯一的差别在于，访问对象的任何属性，都会直接调用这个方法，不管属性存不存在。 3. operator.methodcaller可用于调用函数，它和 attrgetter 很像，差别在于 attrgetter 只是返回指定的属性，而 methodcaller 会直接把指定的属性当成函数调用，然后返回结果。 举例 python3 f = methodcaller('name', 'foo', bar=1) f(b) # returns b.name('foo', bar=1) 4. 各种操作符对应的函数operator.add、operator.sub、operator.mul、operator.div 等等，函数式编程有时需要用到。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-operatorattrgetter"},{"categories":["tech"],"content":" 3. 排序常用库 - operatoroperator 模块包含四种类型的方法： 1. operator.itemgetter经常被用于 sorted/max/mix/itertools.groupby 等 使用方法： python3 # itemgetter f = itemgetter(2) f(r) # return r[2] # 还能一次获取多个值，像 numpy 那样索引 f2 = itemgetter(2,4,5) f2(r) # return (r[2], r[4], r[5]) # 或者使用 slice 切片 s = itemgetter(slice(2, None)) s[r] # return r[2:] # dict 索引也能用 d = itemgetter('rank', 'name') d[r] # return d['rank'], d['name'] 用途： python3 # 用于指定用于比较大小的属性 key = itemgetter(1) sorted(iterable, key=key) # 使用 iterable[1] 对 iterable 进行排序 max(iterable, key=key) # 找出最大的元素，使用 iterable[1] 做比较 # 用于高级切片（比如像 numpy 那样的，指定只获取某几列） s = itemgetter(1,3,4) matrix = [[0,1,2,3,4], [1,2,3,4,5]] map(s, matrix) # list 后得到 [(1, 3, 4), (2,4,5)] 2. operator.attrgetter可用于动态获取对象的属性，与直接用 getattr() 不同的是，它可以嵌套访问属性。 Python3 # 嵌套访问属性 att = attrgetter(\"a.b.c\") att(obj) # return obj.a.b.c # 和 itemgetter 一样，也可以一次获取多个属性 att = attrgetter(\"a.b.c\", \"x.y\") att(obj) # return (obj.a.b.c, obj.x.y) # 不嵌套的话，用 getattr 就行 getattr(obj, \"a\") # return obj.a 这里可以回顾一下类的两个魔法函数： __getattr__: 当被访问的属性不存在时，这个方法会被调用，它的返回值会成为对象的该属性。 用于动态生成实例的属性/函数 __getattribute__: 与 __getattr__ 唯一的差别在于，访问对象的任何属性，都会直接调用这个方法，不管属性存不存在。 3. operator.methodcaller可用于调用函数，它和 attrgetter 很像，差别在于 attrgetter 只是返回指定的属性，而 methodcaller 会直接把指定的属性当成函数调用，然后返回结果。 举例 python3 f = methodcaller('name', 'foo', bar=1) f(b) # returns b.name('foo', bar=1) 4. 各种操作符对应的函数operator.add、operator.sub、operator.mul、operator.div 等等，函数式编程有时需要用到。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-operatormethodcaller"},{"categories":["tech"],"content":" 3. 排序常用库 - operatoroperator 模块包含四种类型的方法： 1. operator.itemgetter经常被用于 sorted/max/mix/itertools.groupby 等 使用方法： python3 # itemgetter f = itemgetter(2) f(r) # return r[2] # 还能一次获取多个值，像 numpy 那样索引 f2 = itemgetter(2,4,5) f2(r) # return (r[2], r[4], r[5]) # 或者使用 slice 切片 s = itemgetter(slice(2, None)) s[r] # return r[2:] # dict 索引也能用 d = itemgetter('rank', 'name') d[r] # return d['rank'], d['name'] 用途： python3 # 用于指定用于比较大小的属性 key = itemgetter(1) sorted(iterable, key=key) # 使用 iterable[1] 对 iterable 进行排序 max(iterable, key=key) # 找出最大的元素，使用 iterable[1] 做比较 # 用于高级切片（比如像 numpy 那样的，指定只获取某几列） s = itemgetter(1,3,4) matrix = [[0,1,2,3,4], [1,2,3,4,5]] map(s, matrix) # list 后得到 [(1, 3, 4), (2,4,5)] 2. operator.attrgetter可用于动态获取对象的属性，与直接用 getattr() 不同的是，它可以嵌套访问属性。 Python3 # 嵌套访问属性 att = attrgetter(\"a.b.c\") att(obj) # return obj.a.b.c # 和 itemgetter 一样，也可以一次获取多个属性 att = attrgetter(\"a.b.c\", \"x.y\") att(obj) # return (obj.a.b.c, obj.x.y) # 不嵌套的话，用 getattr 就行 getattr(obj, \"a\") # return obj.a 这里可以回顾一下类的两个魔法函数： __getattr__: 当被访问的属性不存在时，这个方法会被调用，它的返回值会成为对象的该属性。 用于动态生成实例的属性/函数 __getattribute__: 与 __getattr__ 唯一的差别在于，访问对象的任何属性，都会直接调用这个方法，不管属性存不存在。 3. operator.methodcaller可用于调用函数，它和 attrgetter 很像，差别在于 attrgetter 只是返回指定的属性，而 methodcaller 会直接把指定的属性当成函数调用，然后返回结果。 举例 python3 f = methodcaller('name', 'foo', bar=1) f(b) # returns b.name('foo', bar=1) 4. 各种操作符对应的函数operator.add、operator.sub、operator.mul、operator.div 等等，函数式编程有时需要用到。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#4-各种操作符对应的函数"},{"categories":["tech"],"content":" 4. itertoolsitertools 提供了许多针对可迭代对象的实用函数 方法很多，基本不可能一次全记住。还是要用到时多查吧。大致记住有提供哪些功能，需要用到时能想起可以查这个模块就行。 1. 无限迭代器 count(start=0, step=1): 从 start 开始，每次迭代时，返回值都加一个 step 默认返回序列为 0 1 2 3… cycle(iterable): 不断循环迭代 iterable repeat(element, times=None): 默认永远返回 element。（如果 times 不为 None，就迭代 times 后结束） 2. 排列组合迭代器 product(p1, p2, …, repeat=1)：p1, p2… 的元素的笛卡尔积，相当于多层 for 循环 repeat 指参数重复次数，比如 shell \u003e\u003e\u003e from itertools import product \u003e\u003e\u003e r = product([1, 2], [3, 4], [5, 6]) # 重复一次，也就是 (p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r)) [(1, 3, 5), (1, 3, 6), (1, 4, 5), (1, 4, 6), (2, 3, 5), (2, 3, 6), (2, 4, 5), (2, 4, 6)] \u003e\u003e\u003e r2 = product([1, 2], [3, 4], [5, 6], repeat=2) # 重复两次，即 (p1, p2, p3, p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r2)) [(1, 3, 5, 1, 3, 5), (1, 3, 5, 1, 3, 6), (1, 3, 5, 1, 4, 5), (1, 3, 5, 1, 4, 6), (1, 3, 5, 2, 3, 5), ... permutations(p[, r])：p 中元素，长度为 r 的所有可能的排列。相当于 product 去重后的结果。 combinations(p, r)：既然有排列，当然就有组合了。 3. 其他 zip_longest(*iterables, fillvalue=None)：和 zip 的差别在于，缺失的元素它会用 fillvalue 补全，而不是直接结束。 takewhile() dropwhile() groupby() 等等等，用得到的时候再查了。。。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:4","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#4-itertools"},{"categories":["tech"],"content":" 4. itertoolsitertools 提供了许多针对可迭代对象的实用函数 方法很多，基本不可能一次全记住。还是要用到时多查吧。大致记住有提供哪些功能，需要用到时能想起可以查这个模块就行。 1. 无限迭代器 count(start=0, step=1): 从 start 开始，每次迭代时，返回值都加一个 step 默认返回序列为 0 1 2 3… cycle(iterable): 不断循环迭代 iterable repeat(element, times=None): 默认永远返回 element。（如果 times 不为 None，就迭代 times 后结束） 2. 排列组合迭代器 product(p1, p2, …, repeat=1)：p1, p2… 的元素的笛卡尔积，相当于多层 for 循环 repeat 指参数重复次数，比如 shell \u003e\u003e\u003e from itertools import product \u003e\u003e\u003e r = product([1, 2], [3, 4], [5, 6]) # 重复一次，也就是 (p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r)) [(1, 3, 5), (1, 3, 6), (1, 4, 5), (1, 4, 6), (2, 3, 5), (2, 3, 6), (2, 4, 5), (2, 4, 6)] \u003e\u003e\u003e r2 = product([1, 2], [3, 4], [5, 6], repeat=2) # 重复两次，即 (p1, p2, p3, p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r2)) [(1, 3, 5, 1, 3, 5), (1, 3, 5, 1, 3, 6), (1, 3, 5, 1, 4, 5), (1, 3, 5, 1, 4, 6), (1, 3, 5, 2, 3, 5), ... permutations(p[, r])：p 中元素，长度为 r 的所有可能的排列。相当于 product 去重后的结果。 combinations(p, r)：既然有排列，当然就有组合了。 3. 其他 zip_longest(*iterables, fillvalue=None)：和 zip 的差别在于，缺失的元素它会用 fillvalue 补全，而不是直接结束。 takewhile() dropwhile() groupby() 等等等，用得到的时候再查了。。。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:4","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-无限迭代器"},{"categories":["tech"],"content":" 4. itertoolsitertools 提供了许多针对可迭代对象的实用函数 方法很多，基本不可能一次全记住。还是要用到时多查吧。大致记住有提供哪些功能，需要用到时能想起可以查这个模块就行。 1. 无限迭代器 count(start=0, step=1): 从 start 开始，每次迭代时，返回值都加一个 step 默认返回序列为 0 1 2 3… cycle(iterable): 不断循环迭代 iterable repeat(element, times=None): 默认永远返回 element。（如果 times 不为 None，就迭代 times 后结束） 2. 排列组合迭代器 product(p1, p2, …, repeat=1)：p1, p2… 的元素的笛卡尔积，相当于多层 for 循环 repeat 指参数重复次数，比如 shell \u003e\u003e\u003e from itertools import product \u003e\u003e\u003e r = product([1, 2], [3, 4], [5, 6]) # 重复一次，也就是 (p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r)) [(1, 3, 5), (1, 3, 6), (1, 4, 5), (1, 4, 6), (2, 3, 5), (2, 3, 6), (2, 4, 5), (2, 4, 6)] \u003e\u003e\u003e r2 = product([1, 2], [3, 4], [5, 6], repeat=2) # 重复两次，即 (p1, p2, p3, p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r2)) [(1, 3, 5, 1, 3, 5), (1, 3, 5, 1, 3, 6), (1, 3, 5, 1, 4, 5), (1, 3, 5, 1, 4, 6), (1, 3, 5, 2, 3, 5), ... permutations(p[, r])：p 中元素，长度为 r 的所有可能的排列。相当于 product 去重后的结果。 combinations(p, r)：既然有排列，当然就有组合了。 3. 其他 zip_longest(*iterables, fillvalue=None)：和 zip 的差别在于，缺失的元素它会用 fillvalue 补全，而不是直接结束。 takewhile() dropwhile() groupby() 等等等，用得到的时候再查了。。。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:4","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-排列组合迭代器"},{"categories":["tech"],"content":" 4. itertoolsitertools 提供了许多针对可迭代对象的实用函数 方法很多，基本不可能一次全记住。还是要用到时多查吧。大致记住有提供哪些功能，需要用到时能想起可以查这个模块就行。 1. 无限迭代器 count(start=0, step=1): 从 start 开始，每次迭代时，返回值都加一个 step 默认返回序列为 0 1 2 3… cycle(iterable): 不断循环迭代 iterable repeat(element, times=None): 默认永远返回 element。（如果 times 不为 None，就迭代 times 后结束） 2. 排列组合迭代器 product(p1, p2, …, repeat=1)：p1, p2… 的元素的笛卡尔积，相当于多层 for 循环 repeat 指参数重复次数，比如 shell \u003e\u003e\u003e from itertools import product \u003e\u003e\u003e r = product([1, 2], [3, 4], [5, 6]) # 重复一次，也就是 (p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r)) [(1, 3, 5), (1, 3, 6), (1, 4, 5), (1, 4, 6), (2, 3, 5), (2, 3, 6), (2, 4, 5), (2, 4, 6)] \u003e\u003e\u003e r2 = product([1, 2], [3, 4], [5, 6], repeat=2) # 重复两次，即 (p1, p2, p3, p1, p2, p3) 的笛卡尔积 \u003e\u003e\u003e pprint(list(r2)) [(1, 3, 5, 1, 3, 5), (1, 3, 5, 1, 3, 6), (1, 3, 5, 1, 4, 5), (1, 3, 5, 1, 4, 6), (1, 3, 5, 2, 3, 5), ... permutations(p[, r])：p 中元素，长度为 r 的所有可能的排列。相当于 product 去重后的结果。 combinations(p, r)：既然有排列，当然就有组合了。 3. 其他 zip_longest(*iterables, fillvalue=None)：和 zip 的差别在于，缺失的元素它会用 fillvalue 补全，而不是直接结束。 takewhile() dropwhile() groupby() 等等等，用得到的时候再查了。。。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:4","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-其他"},{"categories":["tech"],"content":" 5. collections提供了一些实用的高级数据结构（容器） defaultdict：这个感觉是最常用的，可以给定 key 的默认值 Counter：方便、快速的计数器。常用于分类统计 deque：一个线程安全的双端队列 OrderedDict：有时候会需要有序字典 namedtuple：命名元组，有时用于参数传递。与 tuple 的差别是它提供了关键字参数和通过名字访问属性的功能 ChainMap：将多个 map 连接（chain）在一起，提供一个统一的视图。因为是视图，所以原来的 map 不会被影响。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:5","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#5-collections"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__,__annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 python3 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： python3 from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 python3 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 python3 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#6-常用函数装饰器-functools"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__,__annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 python3 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： python3 from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 python3 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 python3 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-functoolswraps"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__,__annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 python3 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： python3 from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 python3 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 python3 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-functoolspartial"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__,__annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 python3 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： python3 from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 python3 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 python3 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-functoolslru_cachemaxsize128-typedfalse"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__,__annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 python3 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： python3 from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 python3 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 python3 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#4-functoolssingledispatch"},{"categories":["tech"],"content":" 6. 常用函数装饰器 functoolsfunctools 提供了几个有时很有用的函数和装饰器 1. @functools.wraps这个装饰器用于使装饰器 copy 被装饰的对象的 __module__, __name__, __qualname__,__annotations__ and __doc__ 属性，这样装饰器就显得更加透明。 python3 from functools import wraps def my_decorator(f): @wraps(f) def wrapper(*args, **kwds): print('Calling decorated function') return f(*args, **kwds) return wrapper # 用了 wraps，wrapper 会复制 f 的各种文档属性 @my_decorator def func(xx): \"\"\" this is func's docstring\"\"\" print(\"this is func~\") 如果不用 wraps 的话，因为实际上返回的是 wrapper，被装饰对象的这些文档属性都会丢失。（比如 docstring） 因此在使用 wrapper 装饰器时，添加 @wraps() 装饰器是个好习惯。 2. functools.partial这个感觉和高等数学的偏函数很像：比如函数 z = f(x, y) 有 x 和 y 两个变量，现在把 x 看作常数，就可以对 y 进行求导运算。而 python 的 partial 也差不多，不过它不是把 x 看作常数，而是先给定 x 的值。用法如下： python3 from functools import partial basetwo = partial(int, base=2) # 先给定 int 函数的 base 参数为 2 basetwo.__doc__ = 'Convert base 2 string to an int.' # 如果需要文档，可以添加 __doc__ 属性 basetwo('10010') # return 18 此外，还有个 partialmethod 函数，待了解 3. @functools.lru_cache(maxsize=128, typed=False)如果某方法可能被频繁调用（使用相同的参数），而且它的结果在一定时间内不会改变。可以用 lru_cache 装饰它，减少运算量或 IO 操作。 python3 from functools import lru_cache # 缓存最近的（least recently used，lru） 64 次参数不同的调用结果。 @lru_cache(maxsize=64) def my_sum(x): # 后续的调用中，如果参数能匹配到缓存，就直接返回缓存结果 return sum(x) 比如用递归计算斐波那契数列，数值较低的参数会被频繁使用，于是可以用 lru_cache 来缓存它们。或者爬取网页，可能会需要频繁爬取一个变化不快的网页，这时完全可以用 cache 缓存。 但是它不能控制缓存失效时间，因此不能用于 Web 系统的缓存。还是得自己写个简单的装饰器，把缓存存到 redis 里并设置 expires。或者直接用 Flask 或 Django 的 caching 插件。 4. @functools.singledispatch单重派发，即根据函数的第一个参数的类型，来决定调用哪一个同名函数。 python3 @singledispatch def parse(arg): # 首先定义一个默认函数 print('没有合适的类型被调用') # 如果参数类型没有匹配上，就调用这个默认函数 @parse.register(type(None)) # 第一个参数为 None def _(arg): print('出现 None 了') @parse.register(int) # 第一个参数为整数 def _(arg): print('这次输入的是整数') @parse.register def _(arg: list): # python3.7 开始，可以直接用类型注解来标注第一个参数的类型 print('这次输入的是列表') 画外：有单重派发，自然就有多重派发，Julia 语言就支持多重派发，即根据函数所有参数的类型，来决定调用哪一个同名函数。Julia 语言根本没有类这个定义，类型的所有方法都是通过多重派发来定义的。 其他 @functools.total_ordering：用于自动生成比较函数。 functools.cmp_to_key(func)：用于将老式的比较函数，转换成新式的 key 函数。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#其他"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： python3 class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： python3 from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 python3 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： python3 from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） python3 f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#7-上下文管理---contextlib"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： python3 class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： python3 from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 python3 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： python3 from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） python3 f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-使用-__enter__-和-__exit__"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： python3 class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： python3 from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 python3 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： python3 from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） python3 f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-推荐contextlib"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： python3 class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： python3 from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 python3 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： python3 from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） python3 f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#21-contextlibcontextmanager"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： python3 class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： python3 from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 python3 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： python3 from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） python3 f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#22-contextlibclosingthing"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： python3 class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： python3 from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 python3 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： python3 from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） python3 f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#23-contextlibsuppressexceptions"},{"categories":["tech"],"content":" 7. 上下文管理 - contextlib即实现使用 with 语句进行自定义的上下文管理。 1. 使用 __enter__ 和 __exit__Java 使用 try 来自动管理资源，只要实现了 AutoCloseable 接口，就可以部分摆脱手动 colse 的地狱了。 而 Python，则是定义了两个 Protocol：__enter__ 和 __exit__. 下面是一个 open 的模拟实现： python3 class OpenContext(object): def __init__(self, filename, mode): # 调用 open(filename, mode) 返回一个实例 self.fp = open(filename, mode) def __enter__(self): # 用 with 管理 __init__ 返回的实例时，with 会自动调用这个方法 return self.fp # 退出 with 代码块时，会自动调用这个方法。 def __exit__(self, exc_type, exc_value, traceback): self.fp.close() # 这里先构造了 OpenContext 实例，然后用 with 管理该实例 with OpenContext('/tmp/a', 'a') as f: f.write('hello world') 这里唯一有点复杂的，就是 __exit__ 方法。和 Java 一样，__exit__ 相当于try - catch - finally 的 finally 代码块，在发生异常时，它也会被调用。 当没有异常发生时，__exit__ 的三个参数 exc_type, exc_value, traceback 都为 None，而当发生异常时，它们就对应异常的详细信息。发生异常时， __exit__ 的返回值将被用于决定是否向外层抛出该异常，返回 True 则抛出，返回 False 则抑制（swallow it）。 Note 1：Python 3.6 提供了 async with 异步上下文管理器，它的 Protocol 和同步的 with 完全类似，是 __aenter__ 和 __aexit__ 两个方法。Note 2：与 Java 相同，with 支持同时管理多个资源，因此可以直接写 with open(x) as a, open(y) as b: 这样的形式。 2. 推荐：contextlib 2.1 @contextlib.contextmanager对于简单的 with 资源管理，编写一个类可能会显得比较繁琐，为此 contextlib 提供了一个方便的装饰器 @contextlib.contextmanager 用来简化代码。 使用它，上面的 OpenContext 可以改写成这样： python3 from contextlib import contextmanager @contextmanager def make_open_context(filename, mode): fp = open(filename, mode) try: yield fp # 没错，这是一个生成器函数 finally: fp.close() with make_open_context('/tmp/a', 'a') as f: f.write('hello world') 使用 contextmanager 装饰一个生成器函数，yield 之前的代码对应 __enter__，finally 代码块就对应 __exit__. Note：同样，也有异步版本的装饰器 @contextlib.asynccontextmanager 2.2 contextlib.closing(thing)用于将原本不支持 with 管理的资源，包装成一个 Context 对象。 python3 from contextlib import closing from urllib.request import urlopen with closing(urlopen('http://www.python.org')) as page: for line in page: print(line) # closing 等同于 from contextlib import contextmanager @contextmanager def closing(thing): try: yield thing finally: thing.close() # 就是添加了一个自动 close 的功能 2.3 contextlib.suppress(*exceptions)使 with 管理器抑制代码块内任何被指定的异常： python3 from contextlib import suppress with suppress(FileNotFoundError): os.remove('somefile.tmp') # 等同于 try: os.remove('somefile.tmp') except FileNotFoundError: pass 2.4 contextlib.redirect_stdout(new_target)将 with 代码块内的 stdout 重定向到指定的 target（可用于收集 stdout 的输出） python3 f = io.StringIO() with redirect_stdout(f): # 将输出直接写入到 StringIO help(pow) s = f.getvalue() # 或者直接写入到文件 with open('help.txt', 'w') as f: with redirect_stdout(f): help(pow) redirect_stdout 函数返回的 Context 是可重入的（ reentrant），可以重复使用。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:1:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#24-contextlibredirect_stdoutnew_target"},{"categories":["tech"],"content":" 二、实用代码片段","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#二实用代码片段"},{"categories":["tech"],"content":" 1. 元素分组/group数据处理中一个常见的操作，是将列表中的元素，依次每 k 个分作一组。 下面的函数使用非常简洁的代码实现了元素分组的功能： python3 from itertools import zip_longest def group_each(a, size: int, longest=False): \"\"\" 将一个可迭代对象 a 内的元素, 每 size 个分为一组 group_each([1,2,3,4], 2) -\u003e [(1,2), (3,4)] \"\"\" iterators = [iter(a)] * size # 将新构造的 iterator 复制 size 次（浅复制） func_zip = zip_longest if longest else zip return func_zip(*iterators) # 然后 zip a = \"abcdefghijk\" list(group_each(a, 3)) # =\u003e [('a', 'b', 'c'), ('d', 'e', 'f'), ('g', 'h', 'i')] list(group_each(a, 3, longest=True)) # =\u003e [('a', 'b', 'c'), ('d', 'e', 'f'), ('g', 'h', 'i'), ('j', 'k', None)] 这个函数还可以进一步简化为 zip(*[iter(a)] * 3)，如果没想到浅复制（Shallow Copy）特性的话，会很难理解它的逻辑。 此外，如果某个 size 比较常用（比如 2），还可以用 partial 封装一下： python3 from functools import partial # 每两个分一组 group_each_2 = partial(group_each, size=2) # 等同于 group_each_2 = lambda a: group_each(a, 2) a = \"abcde\" list(group_each_2(a)) # =\u003e [('a', 'b'), ('c', 'd')] list(group_each_2(a, longest=True)) # =\u003e [('a', 'b'), ('c', 'd'), ('e', None)] ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#group_size"},{"categories":["tech"],"content":" 2. 扁平版本的 map稍微接触过函数式应该都知道 flat_map，可 Python 标准库却没有提供。下面是我在 stackoverflow 上找到的实现，其实很简单 python3 from itertools import chain def flat_map(f, items): return chain.from_iterable(map(f, items)) 它和 map 的差别在于是不是扁平(flat) 的（废话。。），举个例子 ipython \u003e\u003e\u003e list(map(list, ['123', '456'])) [['1', '2', '3'], ['4', '5', '6']] \u003e\u003e\u003e list(flat_map(list, ['123', '456'])) ['1', '2', '3', '4', '5', '6'] ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-扁平版本的-map"},{"categories":["tech"],"content":" 3. 轮流迭代多个迭代器假设我有多个可迭代对象（迭代器、列表等），现在我需要每次从每个对象中取一个值，直到某个对象为空。如果用循环写会比较繁琐，但是用 itertools 可以这样写： python from itertools import chain def iter_one_by_one(items): return chain.from_iterable(zip(*items)) a = [1,2,3] b = [4,5,6] c = [7,8,9,10] list(iter_one_by_one([a,b,c])) # =\u003e [1, 4, 7, 2, 5, 8, 3, 6, 9] ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:3","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#3-轮流迭代多个迭代器"},{"categories":["tech"],"content":" 4. 多 dict 的去重假设我们有一个 dict 的列表，里面可能有内容一模一样的 dict，我们需要对它做去重。容易想到的方法就是使用 set，可是 set 中的元素必须是 hashable 的，而 dict 是 unhashable 的，因此不能直接放进 set 里。 ipython \u003e\u003e\u003e a = [{'a': 1}, {'a': 1}, {'b': 2}] \u003e\u003e\u003e set(a) Traceback (most recent call last): File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File \"\u003cipython-input-5-5b4c643a6feb\u003e\", line 1, in \u003cmodule\u003e set(a) TypeError: unhashable type: 'dict' 难道就必须手写递归了么？未必，我在 stackoverflow 看到这样一个小技巧 python3 import json def unique_dicts(data_list: list): \"\"\"unique a list of dict dict 是 unhashable 的，不能放入 set 中，所以先转换成 str unique_dicts([{'a': 1}, {'a': 1}, {'b': 2}]) -\u003e [{'a': 1}, {'b': 2}] \"\"\" data_json_set = set(json.dumps(item) for item in data_list) return [json.loads(item) for item in data_json_set] ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:4","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#4-多-dict-的去重"},{"categories":["tech"],"content":" 5. str 的 startswith 和 endswith 的参数可以是元组 ipython3 In[7]: a = \"bb.gif\" In[8]: b = 'a.jpg' In[9]: a.endswith(('.jpg', '.gif')) Out[9]: True In[10]: b.startswith(('bb', 'a')) Out[10]: True ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:5","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#5-str-的-startswith-和-endswith-的参数可以是元组"},{"categories":["tech"],"content":" 6. 判断两个对象的所有属性都相同python 和 java 一样，直接用 == 做判断，默认是比较的引用，相当于 is。对自定义的类，你需要重写 __eq__ 函数。判断值相等的方法很简单，一行代码： python3 class A: ... def __eq__(self, obj): return self.__dict__ == obj.__dict__ # 转成 __dict__ 再比较 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:6","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#6-判断两个对象的所有属性都相同"},{"categories":["tech"],"content":" 7. 案例 7.1 html table 元素的处理在做爬虫工作时，有时会遇到这样的 table 元素： 对这种 html 元素，我一般会直接把它转换成 list，结果如下： python3 table = [['label1', 'value1', 'label2', 'value2'], ['label3', 'value3'], ['label4', 'value4', 'label5', 'value5'], ... ] 为了方便索引，现在我需要把上面的数据转换成下面这个样子的 dict python { 'label1': 'value1', 'label2': 'value2', 'label3': 'value3', 'label4': 'value4', 'label5': 'value5' } 如果是平常，大概需要写循环了。不过如果用刚刚说到的几个函数的话，会变得异常简单 text # 1. 分组 groups = flat_map(group_each_2, table) # 1.1 flat_map 返回的是迭代器，list 后内容如下： # [('label1', 'value1'), # ('label2', 'value2'), # ('label3', 'value3'), # ('label4', 'value4'), # ('label5', 'value5')] # 2. 转换成 dict key_values = dict(groups) # 得到的 key_values 与上面需要的 dict 别无二致。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#7-案例"},{"categories":["tech"],"content":" 7. 案例 7.1 html table 元素的处理在做爬虫工作时，有时会遇到这样的 table 元素： 对这种 html 元素，我一般会直接把它转换成 list，结果如下： python3 table = [['label1', 'value1', 'label2', 'value2'], ['label3', 'value3'], ['label4', 'value4', 'label5', 'value5'], ... ] 为了方便索引，现在我需要把上面的数据转换成下面这个样子的 dict python { 'label1': 'value1', 'label2': 'value2', 'label3': 'value3', 'label4': 'value4', 'label5': 'value5' } 如果是平常，大概需要写循环了。不过如果用刚刚说到的几个函数的话，会变得异常简单 text # 1. 分组 groups = flat_map(group_each_2, table) # 1.1 flat_map 返回的是迭代器，list 后内容如下： # [('label1', 'value1'), # ('label2', 'value2'), # ('label3', 'value3'), # ('label4', 'value4'), # ('label5', 'value5')] # 2. 转换成 dict key_values = dict(groups) # 得到的 key_values 与上面需要的 dict 别无二致。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:2:7","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#71-html-table-元素的处理"},{"categories":["tech"],"content":" 三、常见错误","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:3:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#三常见错误"},{"categories":["tech"],"content":" 1. 浅复制导致错误利用好浅复制，可以非常简洁的实现前面提到的元素分组/group功能，但是如果不注意，也会导致非常隐晦的错误！ 比如在使用 * 作为重复运算符时，如果目标是一个嵌套的可变对象，就会产生令人费解的问题： python \u003e\u003e\u003e a = [1,2,3] \u003e\u003e\u003e b = a * 3 \u003e\u003e\u003e b [1, 2, 3, 1, 2, 3, 1, 2, 3] \u003e\u003e\u003e b = [a] * 3 # nested \u003e\u003e\u003e b [[1, 2, 3], [1, 2, 3], [1, 2, 3]] \u003e\u003e\u003e b[1][1] = 4 \u003e\u003e\u003e b [[1, 4, 3], [1, 4, 3], [1, 4, 3]] 因为 _ 并不是深拷贝，它只是简单地复制了 [a] 这个列表，里面的 [1,2,3] 都是同一个对象，所以改了一个，所有的都会改变。**解决方法是不要使用 _ 号，改用[a.copy() for i in range(3)] 执行深拷贝。如果不需要修改，请直接使用不可变对象**。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:3:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-浅复制导致错误"},{"categories":["tech"],"content":" 2. 变量作用域 Python 中只有模块，类以及函数才会引入新的作用域，其它的代码块是不会引入新的作用域的。（而在 C/Java 中，任何一个 {} 块就构成一个局部作用域。另外 Julia 中 for/while/try-catch 都是局部作用域，但 if-else 又不是局部作用域。总之这些小差别要注意。） 局部变量可以与外部变量同名，并且在其作用域中，局部变量会覆盖掉外部变量。不知是出于实现简单或是性能，还是其他的原因，好像所有的语言都是这样的。其实我更希望变量的作用域覆盖会报错。 如果有函数与其他函数或变量（甚至某些保留字）同名，后定义的会覆盖掉先定义的。（这是因为 Python 中函数也是对象。而在 C/Java 中这是会报错的） 此外，还有一个小问题，先看一个例子： python3 \u003e\u003e\u003e i = 4 \u003e\u003e\u003e def f(): # 单纯的从函数作用域访问外部作用域是没问题的 ... print(i) ... \u003e\u003e\u003e f() 4 再看一个问题举例： python3 \u003e\u003e\u003e i = 3 \u003e\u003e\u003e def f(): ... print(i) # 这里应该是访问外部作用域 ... i = 5 # 可这里又定义了一个同名局部变量 i ... \u003e\u003e\u003e f() # 于是就出错了 Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e File \"\u003cstdin\u003e\", line 2, in f UnboundLocalError: local variable 'i' referenced before assignment 如果在内部作用域先访问外部作用域，再定义一个同名的局部变量，解释器就懵逼了。如果你其实想做的是改变全局变量 i 的值，就应该在开头声明 global i. 而如果 外部变量 i 不是存在于全局作用域，而是在某个闭合作用域内的话，就该用 nonlocal i ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:3:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-变量作用域"},{"categories":["tech"],"content":" 四、自定义装饰器装饰器有两种：用函数定义的装饰器，还有用类定义的装饰器。函数装饰器最常用。 装饰器可用于装饰函数，修改函数/类的某些行为，或者将函数注册到别的地方。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:4:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#四自定义装饰器"},{"categories":["tech"],"content":" 1. 函数定义装饰器 python @decc def gg(xx): ... # 等同于 def gg(xx) gg = decc(gg) 带参的装饰器 python @decorator(A, B) def F(arg): ... F(99) # 等同于 def F(arg): ... F = decorator(A, B)(F) # Rebind F to result of decorator's return value F(99) # Essentially calls decorator(A, B)(F)(99) 上面演示的是用函数定义的装饰器，也是最常用的装饰器。装饰器接收的参数可以是各种各样的，下面是一个带参的装饰器： python @on_command(\"info\") def get_info(): return \"这就是你需要的 info\" def on_command(name: str): # 调用此函数获得装饰器，这样就实现了带参装饰器 def deco(func: Callable) -\u003e Callable: # 这个才是真正的装饰器 # 将命令处理器注册到命令列表内 return func # 直接返回原函数，这样的话，多个装饰器就不会相互影响了。 return deco # 上面的等同于： get_info = on_command(\"info\")(get_info) # on_command(\"info\") 返回真正的装饰器 如果你的 on_command 有通用的部分，还可以将通用的部分抽离出来复用： python def _deco_maker(event_type: str) -\u003e Callable: # 调用这个，获取 on_xxx 的 deco_deco， def deco_deco(self) -\u003e Callable: # 这个对应 on_xxx def deco(func: Callable) -\u003e Callable: # 这个才是真正的装饰器 # do something return func # 返回原函数 return deco return deco_deco 我们知道 Python 的类实际上是可以很方便的修改的，因此函数装饰器也能用于装饰类，修改类的某些行为。 python def log_getattribute(cls): # Get the original implementation orig_getattribute = cls.__getattribute__ # Make a new definition def new_getattribute(self, name): print('getting:', name) return orig_getattribute(self, name) # Attach to the class and return cls.__getattribute__ = new_getattribute # 修改了被装饰类 cls 的 __getattribute__ return cls # Example use @log_getattribute class A: def __init__(self,x): self.x = x def spam(self): pass ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:4:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#1-函数定义装饰器"},{"categories":["tech"],"content":" 1. 函数定义装饰器 python @decc def gg(xx): ... # 等同于 def gg(xx) gg = decc(gg) 带参的装饰器 python @decorator(A, B) def F(arg): ... F(99) # 等同于 def F(arg): ... F = decorator(A, B)(F) # Rebind F to result of decorator's return value F(99) # Essentially calls decorator(A, B)(F)(99) 上面演示的是用函数定义的装饰器，也是最常用的装饰器。装饰器接收的参数可以是各种各样的，下面是一个带参的装饰器： python @on_command(\"info\") def get_info(): return \"这就是你需要的 info\" def on_command(name: str): # 调用此函数获得装饰器，这样就实现了带参装饰器 def deco(func: Callable) -\u003e Callable: # 这个才是真正的装饰器 # 将命令处理器注册到命令列表内 return func # 直接返回原函数，这样的话，多个装饰器就不会相互影响了。 return deco # 上面的等同于： get_info = on_command(\"info\")(get_info) # on_command(\"info\") 返回真正的装饰器 如果你的 on_command 有通用的部分，还可以将通用的部分抽离出来复用： python def _deco_maker(event_type: str) -\u003e Callable: # 调用这个，获取 on_xxx 的 deco_deco， def deco_deco(self) -\u003e Callable: # 这个对应 on_xxx def deco(func: Callable) -\u003e Callable: # 这个才是真正的装饰器 # do something return func # 返回原函数 return deco return deco_deco 我们知道 Python 的类实际上是可以很方便的修改的，因此函数装饰器也能用于装饰类，修改类的某些行为。 python def log_getattribute(cls): # Get the original implementation orig_getattribute = cls.__getattribute__ # Make a new definition def new_getattribute(self, name): print('getting:', name) return orig_getattribute(self, name) # Attach to the class and return cls.__getattribute__ = new_getattribute # 修改了被装饰类 cls 的 __getattribute__ return cls # Example use @log_getattribute class A: def __init__(self,x): self.x = x def spam(self): pass ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:4:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#带参的装饰器"},{"categories":["tech"],"content":" 2. 类定义装饰器类定义装饰器和函数定义装饰器的使用方式完全一致。它也可以用于装饰函数或者类。 那么为啥还需要类定义装饰器呢？它的优势在于类是可以继承的，这样的话，就能用继承的方式定义装饰器，将通用部分定义成超类。 类定义装饰器的定义方法如下： python # PythonDecorators/entry_exit_class.py class entry_exit(object): def __init__(self, f): self.f = f def __call__(self): #关键在于这个函数，它使此类的对象变成 Callable print(\"Entering\", self.f.__name__) self.f() print(\"Exited\", self.f.__name__) @entry_exit def func1(): print(\"inside func1()\") # 上面的装饰器相当于 func1 = entry_exit(func1) # 从这里看的话，装饰器的行为完全一致 # 接下来调用该函数（实际上是调用了 entry_exit 对象的 call 函数） func1() 输出结果如下： text Entering func1 inside func1() Exited func1 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:4:2","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#2-类定义装饰器"},{"categories":["tech"],"content":" 五、OOP 调用超类方法：- 直接通过超类名.__init__(self,xx)调用 - 通过super(__class__, self).__init__()调用。（Python3 可直接用 super().__init__() 但是要搞清楚，super() 方法返回的是一个代理类。另外被代理的类也不一定是其超类。如果不清楚这些差别，最好还是显式用方法一最好。） 抽象超类：@abstractmethod @staticmethod @classmethod 与 Java 的 static 方法对比 python的类方法、静态方法，与 java的静态方法： 1. java 中 constants、utils 这样的静态类，对应的是python的一个模块（文件），类属性对应模块的全局属性，静态方法对应模块的函数 2. 对于 java 中需要访问类属性的静态方法，如果它不属于第一类，应该用 `@classmethod` 实现它。classmethod最大的特点就是一定有一个 cls 传入。这种方法的主要用途是实现工厂函数。 3. 对于不需要访问任何类属性，也不属于第一类的方法，应该用 `@staticmathod` 实现。这种方法其实完全不需要放到类里面，它就是一个独立的函数。（仍然放里面，是为了把功能类似的函数组织到一起而已。） __slots__: 属性导出，不在该列表内的属性，若存在则为只读。不存在的话，就不存在。。6. __getattr__: 拦截对不存在的属性的访问，可用于实现动态分配属性。 __getattribute__: 和上面相同，但是它拦截对所有属性的访问，包括对已存在的属性的访问。 @property: 提供对属性访问的安全检查 descriptor: get set delete 控制对类的访问。（上面的 getattr 等是控制对类的属性的访问） 类构造器 __new__：在 __init__ 之前运行，它接收一个 cls 参数，然后使用它构造并返回类实例 self。 类方法的 cls 即是当前类，是 type 的实例，cls.xxx 和 \u003c类名\u003e.xxx 调用结果是一致的。而 self 由 __new__ 构造，是 cls 的实例。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:5:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#五oop"},{"categories":["tech"],"content":" 元类 metaclasses元类，也就是用于创建class 的 class，算是很高级的话题了（If you wonder whether you need metaclasses, you don’t ）元类的工作流程： 拦截类的创建 修改类 返回修改之后的类 详细直接看 http://blog.jobbole.com/21351/ 吧。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:5:1","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#元类-metaclasses"},{"categories":["tech"],"content":" 六、查看 Python 源码对一般的标准库的模块，要查看其具体的 Python 实现是很简单的：直接通过 __file__ 属性就能看到 .py 文件的位置。 但是 Python 很多功能是 C 写的，对于这类函数/类，__file__ 就没啥用了。 如果是需要查看builtins 模块 的具体实现，直接查看Python/bltinmodule.c 就行。 其他 C 模块的源码，待补充具体的查看方法。 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:6:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#六查看-python-源码"},{"categories":["tech"],"content":" 七、参考文档 Python中一些不为人知的基础技巧总结 Python3 官方文档 ","date":"2022-02-13","objectID":"/posts/python-tips-and-tricks/:7:0","series":null,"tags":["Python","Tips","Tricks","常见错误"],"title":"Python 实用技巧与常见错误集锦","uri":"/posts/python-tips-and-tricks/#七参考文档"},{"categories":null,"content":" 此页面受极客兔兔 - 博客折腾记(七) - Gitalk Plus 的启发而创建，其核心诉求是「帮助访客发现本站的优质文章」。 此页面的数据由Github Action 自动从 Google Analytics 获取，更新间隔为 6 小时，目前由博主看心情更新。 更新时间: 2025-06-12T23:50UTC+08:00 ","date":"2022-02-07","objectID":"/statistics/:0:0","series":null,"tags":null,"title":"本站统计数据","uri":"/statistics/#"},{"categories":null,"content":" 一、全站统计 总字数 总访客数 UV 总访问量 PV 总阅读时长 人均阅读时长 401786 154975 342786 180 days, 7h 59m 34s 01m 40s ","date":"2022-02-07","objectID":"/statistics/:1:0","series":null,"tags":null,"title":"本站统计数据","uri":"/statistics/#一全站统计"},{"categories":null,"content":" 二、90 天阅读排行 默认按「人均阅读时长」排序，可点击表头切换排序方式。 序号 标题 人均阅读时长 访客数 访问量 1 2021 年年终总结 02m 52s 28 34 2 个人数据安全不完全指南 02m 50s 155 221 3 我的 2023 - 认识更多有趣的人，见识更宽广的世界 02m 28s 53 61 4 两岸猿声啼不住，轻舟已过万重山——我的四分之一人生 02m 26s 83 164 5 OS as Code - 我的 NixOS 使用体会 02m 22s 357 483 6 写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 02m 03s 90 153 7 EE 入门（一） - 电子电路基础知识 02m 01s 574 869 8 写给开发人员的实用密码学（七）—— 非对称密钥加密算法 RSA/ECC 02m 00s 148 191 9 NAT 网关、NAT 穿越以及虚拟网络 01m 55s 59 79 10 在回声中重历 01m 54s 5 6 11 写给开发人员的实用密码学（三）—— MAC 与密钥派生函数 KDF 01m 51s 116 159 12 进程线程协程与并发并行 01m 48s 5 5 13 写给开发人员的实用密码学（二）—— 哈希函数 01m 46s 98 119 14 Linux 上的 WireGuard 网络分析（一） 01m 45s 125 162 15 我的 2024 - 稳中求进、热爱生活 01m 44s 224 295 16 Learn English Again 01m 44s 42 52 17 FinOps for Kubernetes - 如何拆分 Kubernetes 成本 01m 42s 55 77 18 云原生流水线 Argo Workflows 的安装、使用以及个人体验 01m 41s 101 160 19 SQLAlchemy 学习笔记（三）：ORM 中的关系构建 01m 39s 16 29 20 iptables 及 docker 容器网络分析 01m 33s 82 123 21 secrets 管理工具 Vault 的介绍、安装及使用 01m 32s 212 349 22 我在创业公司做技术一年多的一点体会 01m 29s 37 38 23 KubeCon China 2024 之旅 01m 28s 65 78 24 写给开发人员的实用密码学（六）—— 对称密钥加密算法 01m 24s 98 161 25 2024 年香港徒步旅行记录（一） 01m 24s 66 69 26 部署一个 Kubernetes 集群 01m 24s 19 24 27 OS as Code - My Experience of NixOS 01m 23s 334 406 28 WebSocket、HTTP/2 与 gRPC 01m 22s 38 57 29 欧几里得算法求最大公约数(GCD)的数学原理 01m 22s 13 15 30 SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言 01m 20s 29 31 31 NixOS 在 Lichee Pi 4A 上是如何启动的 01m 16s 58 86 32 为什么我折腾这些小众技术？ 01m 09s 94 107 33 SQLAlchemy 学习笔记（二）：ORM 基础 01m 09s 9 11 34 写给开发人员的实用密码学（五）—— 密钥交换 DHKE 与完美前向保密 PFS 01m 07s 129 161 35 写给开发人员的实用密码学（四）—— 安全随机数生成器 CSPRNG 01m 07s 107 129 36 Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例 01m 07s 58 63 37 Python 并发编程：PoolExecutor 篇 01m 07s 16 16 38 Base64 编码并不唯一 01m 07s 12 17 39 Proxmox Virtual Environment 使用指南 01m 06s 476 611 40 EE 入门（二） - 使用 ESP32 与 SPI 显示屏绘图、显示图片、跑贪吃蛇 01m 03s 414 530 41 Kubernetes 集群伸缩组件 - Karpenter 01m 03s 69 81 42 写给开发人员的实用密码学（一）—— 概览 01m 01s 107 142 43 使用 Istio 进行 JWT 身份验证（充当 API 网关） 01m 00s 28 61 44 openSUSE 使用指南 59s 40 50 45 2020 年年终总结 59s 13 15 46 QEMU/KVM 虚拟化环境的搭建与使用 57s 203 248 47 Linux 中的虚拟网络接口 57s 82 131 48 Pulumi 使用体验 - 基础设施代码化 57s 52 69 49 2019 年年终总结 57s 14 18 50 Linux 网络工具中的瑞士军刀 - socat \u0026 netcat 56s 118 137 51 「转」且看有思想的年轻人 56s 12 14 52 Summary of My 2021 55s 8 8 53 常见压缩格式的区别，及 Linux 下的压缩相关指令 53s 18 17 54 Why do I explore these niche technologies? 52s 59 75 55 Kubernetes 常见错误、原因及处理方法 49s 70 95 56 Kubernetes 中的证书管理工具 - cert-manager 48s 91 108 57 Kubernetes 微服务最佳实践 47s 54 79 58 2022 年年终总结 47s 18 19 59 使用 tcpdump 和 Wireshark 进行远程实时抓包分析 46s 282 387 60 KubeCon China 2024 Adventure 41s 21 23 61 Death Is But a Dream 40s 8 8 62 逃离我的大学 38s 24 26 63 Python 实用技巧与常见错误集锦 38s 20 28 64 Python 视频转字符动画（一）60 行代码 36s 9 9 65 刻意练习 32s 24 25 66 分布式数据库的一致性问题与共识算法 31s 58 72 67 Linux/Windows/MacOSX 系统常用命令集锦 30s 35 40 68 瘾的退却 30s 6 6 69 浮生若梦，为欢几何？ 29s 15 17 70 MacOS 窗口管理器 yabai 玩耍笔记 26s 194 251 71 学英语啊学英语 26s 15 15 72 JWT 签名算法 HS256、RS256 及 ES256 及密钥生成 25s 105 127 73 脚踏实地，仰望星空 23s 16 16 74 我患上了阅读焦虑症 20s 16 17 75 通过 systemctl 设置自定义 Service 20s 9 9 ","date":"2022-02-07","objectID":"/statistics/:2:0","series":null,"tags":null,"title":"本站统计数据","uri":"/statistics/#二90-天阅读排行"},{"categories":null,"content":" 三、文章发布周期与数量统计 ","date":"2022-02-07","objectID":"/statistics/:3:0","series":null,"tags":null,"title":"本站统计数据","uri":"/statistics/#三文章发布周期与数量统计"},{"categories":["tech"],"content":" 本文完成于 2022-01-25，其中部分内容已经过时，仅供参考。 本文由个人笔记ryan4yin/knowledge 整理而来，不保证正确 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:0:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#"},{"categories":["tech"],"content":" 本地 Kubernetes 集群安装工具 云上的 Kubernetes 集群，基本上各云厂商都支持一键部署。这里主要关注本地部署，或者叫做裸机 (baremetal)部署 本文介绍的方法适合开发测试使用，安全性、稳定性、长期可用性等方案都可能还有问题。 kubernetes 是一个组件化的系统，安装过程有很大的灵活性，很多组件都有多种实现，这些实现各有特点，让初学者眼花缭乱。 而且要把这些组件一个个安装配置好并且能协同工作，也是很不容易的。 因此社区出现了各种各样的安装方案，下面介绍下几种支持裸机（Baremetal）部署的工具： kubeadm: 社区的集群安装工具，目前已经很成熟了。 使用难度：简单 k3s: 轻量级 kubernetes，资源需求小，部署非常简单，适合开发测试用或者边缘环境 支持 airgap 离线部署 使用难度：超级简单 alibaba/sealer: 支持将整个 kubernetes 打包成一个镜像进行交付，而且部署也非常简单。 使用难度：超级简单 这个项目目前还在发展中，不过貌似已经有很多 toB 的公司在使用它进行 k8s 应用的交付了。 kubespray: 适合自建生产级别的集群，是一个大而全的 kubernetes 安装方案，自动安装容器运行时、k8s、网络插件等组件，而且各组件都有很多方案可选，但是感觉有点复杂。 使用难度：中等 支持 airgap 离线部署，但是以前我试用过是有坑，现在不知道咋样了 底层使用了 kubeadm 部署集群 sealos: 也很方便，一行命令部署 其他社区部署方案 自己写脚本，使用各组件的二进制文件进行部署。 笔者为了学习 Kubernetes，下面采用官方的 kubeadm 进行部署，容器运行时使用 containerd，网络插件则使用目前最潮的基于 eBPF 的 Cilium. kubernetes 官方介绍了两种高可用集群的拓扑结构：「堆叠 Etcd 拓扑（Stacked Etcd Topology）」和「外部 Etcd 拓扑（External Etcd Topology）」。「堆叠 Etcd 拓扑」是指 Etcd 跟 Kubernetes Master 的其他组件部署在同一节点上，而「外部 Etcd 拓扑（External Etcd Topology）」则是指 Etcd 单独部署，与 Kubernetes Master 分开。 简单起见，本文使用「堆叠 Etcd 拓扑」结构，创建一个 3 master 的高可用集群。 参考： Kubernetes Docs - Installing kubeadm Kubernetes Docs - Creating Highly Available clusters with kubeadm ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:1:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#本地-kubernetes-集群安装工具"},{"categories":["tech"],"content":" 0. 网络环境的准备本文行文未考虑国内网络环境，但是 Kubernetes 用到的很多镜像都在 gcr.io 上，在国内访问会有困难，这里提供三个解决办法： 在家庭路由器上整个科学代理，实现全局科学上网。（我就是这么干的） 使用 liangyuanpeng 大佬在评论区提供的 gcr 国内镜像地址，这需要进行如下替换： k8s.gcr.io—\u003e lank8s.cn 自己维护一个国内镜像仓库（或私有镜像仓库如 harbor），使用 skopeo 等工具或脚本将上述镜像列表拷贝到你的私有仓库 如果对可靠性要求高，最好是选择第三个方案——自建私有镜像仓库，把镜像推送到私有仓库。可以通过如下命令列出所有 kubeadm 需要用到的镜像地址（需要先安装好 kubeadm，建议读完本篇文章全文后再尝试）： shell ❯ kubeadm config images list --kubernetes-version v1.22.1 k8s.gcr.io/kube-apiserver:v1.22.1 k8s.gcr.io/kube-controller-manager:v1.22.1 k8s.gcr.io/kube-scheduler:v1.22.1 k8s.gcr.io/kube-proxy:v1.22.1 k8s.gcr.io/pause:3.5 k8s.gcr.io/etcd:3.5.0-0 k8s.gcr.io/coredns/coredns:v1.8.4 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:2:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#0-网络环境的准备"},{"categories":["tech"],"content":" 1. 节点的环境准备首先准备三台 Linux 虚拟机，系统按需选择，然后调整这三台机器的设置： 节点配置： master：不低于 2c/3g，硬盘 20G 主节点性能也受集群 Pods 个数的影响，上述配置应该可以支撑到每个 Worker 节点跑 100 个 Pod. worker：看需求，建议不低于 2c/4g，硬盘不小于 20G，资源充分的话建议 40G 以上。 处于同一网络内并可互通（通常是同一局域网） 各主机的 hostname 和 mac/ip 地址以及 /sys/class/dmi/id/product_uuid，都必须唯一 这里新手最容易遇到的问题，是 hostname 冲突 必须关闭 swap 交换内存，kubelet 才能正常工作 方便起见，我直接使用ryan4yin/pulumi-libvirt 自动创建了五个 opensuse leap 15.3 虚拟机，并设置好了 ip/hostname. ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:3:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#1-节点的环境准备"},{"categories":["tech"],"content":" 1.1 iptables 设置目前 kubernetes 的容器网络，默认使用的是 bridge 模式，这种模式下，需要使 iptables 能够接管 bridge 上的流量。 配置如下： shell sudo modprobe br_netfilter cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:3:1","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#11-iptables-设置"},{"categories":["tech"],"content":" 1.2 开放节点端口 局域网环境的话，建议直接关闭防火墙。这样所有端口都可用，方便快捷。 通常我们的云上集群，也是关闭防火墙的，只是会通过云服务提供的「安全组」来限制客户端 ip Control-plane 节点，也就是 master，需要开放如下端口： Protocol Direction Port Range Purpose Used By TCP Inbound 6443* Kubernetes API server All TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd TCP Inbound 10250 kubelet API Self, Control plane TCP Inbound 10251 kube-scheduler Self TCP Inbound 10252 kube-controller-manager Self Worker 节点需要开发如下端口： Protocol Direction Port Range Purpose Used By TCP Inbound 10250 kubelet API Self, Control plane TCP Inbound 30000-32767 NodePort Services† All 另外通常我们本地测试的时候，可能更想直接在 80 443 8080 等端口上使用 NodePort，就需要修改 kube-apiserver 的 --service-node-port-range 参数来自定义 NodePort 的端口范围，相应的 Worker 节点也得开放这些端口。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:3:2","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#12-开放节点端口"},{"categories":["tech"],"content":" 2. 安装容器运行时 containerd首先是环境配置： shell cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter nf_conntrack EOF sudo modprobe overlay sudo modprobe br_netfilter sudo modprobe nf_conntrack # Setup required sysctl params, these persist across reboots. cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # Apply sysctl params without reboot sudo sysctl --system 安装 containerd+nerdctl: shell wget https://github.com/containerd/nerdctl/releases/download/v0.11.1/nerdctl-full-0.11.1-linux-amd64.tar.gz tar -axvf nerdctl-full-0.11.1-linux-amd64.tar.gz # 这里简单起见，rootless 相关的东西也一起装进去了，测试嘛就无所谓了... mv bin/* /usr/local/bin/ mv lib/systemd/system/containerd.service /usr/lib/systemd/system/ systemctl enable containerd systemctl start containerd nerdctl 是一个 containerd 的命令行工具，但是它的容器、镜像与 Kubernetes 的容器、镜像是完全隔离的，不能互通！ 目前只能通过 crictl 来查看、拉取 Kubernetes 的容器、镜像，下一节会介绍 crictl 的安装。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:4:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#2-安装容器运行时-containerd"},{"categories":["tech"],"content":" 3. 安装 kubelet/kubeadm/kubectl shell # 一些全局都需要用的变量 CNI_VERSION=\"v0.8.2\" CRICTL_VERSION=\"v1.17.0\" # kubernetes 的版本号 # RELEASE=\"$(curl -sSL https://dl.k8s.io/release/stable.txt)\" RELEASE=\"1.22.1\" # kubelet 配置文件的版本号 RELEASE_VERSION=\"v0.4.0\" # 架构 ARCH=\"amd64\" # 安装目录 DOWNLOAD_DIR=/usr/local/bin # CNI 插件 sudo mkdir -p /opt/cni/bin curl -L \"https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-${ARCH}-${CNI_VERSION}.tgz\" | sudo tar -C /opt/cni/bin -xz # crictl 相关工具 curl -L \"https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-${ARCH}.tar.gz\" | sudo tar -C $DOWNLOAD_DIR -xz # kubelet/kubeadm/kubectl cd $DOWNLOAD_DIR sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet,kubectl} sudo chmod +x {kubeadm,kubelet,kubectl} # kubelet/kubeadm 配置 curl -sSL \"https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service\" | sed \"s:/usr/bin:${DOWNLOAD_DIR}:g\" | sudo tee /etc/systemd/system/kubelet.service sudo mkdir -p /etc/systemd/system/kubelet.service.d curl -sSL \"https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf\" | sed \"s:/usr/bin:${DOWNLOAD_DIR}:g\" | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf systemctl enable --now kubelet # 验证 kubelet 启动起来了，但是目前还没有初始化配置，过一阵就会重启一次 systemctl status kubelet 试用 crictl: shell export CONTAINER_RUNTIME_ENDPOINT='unix:///var/run/containerd/containerd.sock' # 列出所有 pods，现在应该啥也没 crictl pods # 列出所有镜像 crictl images ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:5:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#3-安装-kubeletkubeadmkubectl"},{"categories":["tech"],"content":" 4. 为 master 的 kube-apiserver 创建负载均衡实现高可用根据 kubeadm 官方文档Kubeadm Docs - High Availability Considerations 介绍，要实现 kube-apiserver 的高可用，目前最知名的负载均衡方式是 keepalived+haproxy，另外也可以考虑使用 kube-vip 等更简单的工具。 简单起见，我们直接用 kube-vip 吧，参考了 kube-vip 的官方文档：Kube-vip as a Static Pod with Kubelet. P.S. 我也见过有的安装工具会直接抛弃 keepalived，直接在每个节点上跑一个 nginx 做负载均衡，配置里写死了所有 master 的地址… 首先使用如下命令生成 kube-vip 的配置文件，以 ARP 为例（生产环境建议换成 BGP）： shell cat \u003c\u003cEOF | sudo tee add-kube-vip.sh # 你的虚拟机网卡，opensuse/centos 等都是 eth0，但是 ubuntu 可能是 ens3 export INTERFACE=eth0 # 用于实现高可用的 vip，需要和前面的网络接口在同一网段内，否则就无法路由了。 export VIP=192.168.122.200 # 生成 static-pod 的配置文件 mkdir -p /etc/kubernetes/manifests nerdctl run --rm --network=host --entrypoint=/kube-vip ghcr.io/kube-vip/kube-vip:v0.3.8 \\ manifest pod \\ --interface $INTERFACE \\ --vip $VIP \\ --controlplane \\ --services \\ --arp \\ --leaderElection | tee /etc/kubernetes/manifests/kube-vip.yaml EOF bash add-kube-vip.sh 三个 master 节点都需要跑下上面的命令（worker 不需要），创建好 kube-vip 的 static-pod 配置文件。在完成 kubeadm 初始化后，kubelet 会自动把它们拉起为 static pod. ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:6:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#4-为-master-的-kube-apiserver-创建负载均衡实现高可用"},{"categories":["tech"],"content":" 5. 使用 kubeadm 创建集群其实需要运行的就是这条命令： shell # 极简配置： cat \u003c\u003cEOF | sudo tee kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration nodeRegistration: criSocket: \"/var/run/containerd/containerd.sock\" imagePullPolicy: IfNotPresent --- kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta3 kubernetesVersion: v1.22.1 clusterName: kubernetes certificatesDir: /etc/kubernetes/pki imageRepository: k8s.gcr.io controlPlaneEndpoint: \"192.168.122.200:6443\" # 填 apiserver 的 vip 地址，或者整个域名也行，但是就得加 /etc/hosts 或者内网 DNS 解析 networking: serviceSubnet: \"10.96.0.0/16\" podSubnet: \"10.244.0.0/16\" etcd: local: dataDir: /var/lib/etcd --- apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration cgroupDriver: systemd # 让 kubelet 从 certificates.k8s.io 申请由集群 CA Root 签名的 tls 证书，而非直接使用自签名证书 # 如果不启用这个， 安装 metrics-server 时就会遇到证书报错，后面会详细介绍。 serverTLSBootstrap: true EOF # 查看 kubeadm 默认的完整配置，供参考 kubeadm config print init-defaults \u003e init.default.yaml # 执行集群的初始化，这会直接将当前节点创建为 master # 成功运行的前提：前面该装的东西都装好了，而且 kubelet 已经在后台运行了 # `--upload-certs` 会将生成的集群证书上传到 kubeadm 服务器，在两小时内加入集群的 master 节点会自动拉证书，主要是方便集群创建。 kubeadm init --config kubeadm-config.yaml --upload-certs kubeadm 应该会报错，提示你有些依赖不存在，下面先安装好依赖项。 shell sudo zypper in -y socat ebtables conntrack-tools 再重新运行前面的 kubeadm 命令，应该就能正常执行了，它做的操作有： 拉取控制面的容器镜像 生成 ca 根证书 使用根证书为 etcd/apiserver 等一票工具生成 tls 证书 为控制面的各个组件生成 kubeconfig 配置 生成 static pod 配置，kubelet 会根据这些配置自动拉起 kube-proxy 以及其他所有的 k8s master 组件 运行完会给出三部分命令： 将 kubeconfig 放到 $HOME/.kube/config 下，kubectl 需要使用该配置文件连接 kube-apiserver control-plane 节点加入集群的命令: 这里由于我们提前添加了 kube-vip 的 static-pod 配置，这里的 preflight-check 会报错，需要添加此参数忽略该报错 ---ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests shell kubeadm join 192.168.122.200:6443 --token \u003ctoken\u003e \\ --discovery-token-ca-cert-hash sha256:\u003chash\u003e \\ --control-plane --certificate-key \u003ckey\u003e \\ --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests worker 节点加入集群的命令: shell kubeadm join 192.168.122.200:6443 --token \u003ctoken\u003e \\ --discovery-token-ca-cert-hash sha256:\u003chash\u003e 跑完第一部分 kubeconfig 的处理命令后，就可以使用 kubectl 查看集群状况了： shell k8s-master-0:~/kubeadm # kubectl get no NAME STATUS ROLES AGE VERSION k8s-master-0 NotReady control-plane,master 79s v1.22.1 k8s-master-0:~/kubeadm # kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcd69978-6tlnw 0/1 Pending 0 83s kube-system coredns-78fcd69978-hxtvs 0/1 Pending 0 83s kube-system etcd-k8s-master-0 1/1 Running 6 90s kube-system kube-apiserver-k8s-master-0 1/1 Running 4 90s kube-system kube-controller-manager-k8s-master-0 1/1 Running 4 90s kube-system kube-proxy-6w2bx 1/1 Running 0 83s kube-system kube-scheduler-k8s-master-0 1/1 Running 7 97s 现在在其他节点运行前面打印出的加入集群的命令，就可以搭建好一个高可用的集群了。 所有节点都加入集群后，通过 kubectl 查看，应该是三个控制面 master，两个 worker： shell k8s-master-0:~/kubeadm # kubectl get node NAME STATUS ROLES AGE VERSION k8s-master-0 NotReady control-plane,master 26m v1.22.1 k8s-master-1 NotReady control-plane,master 7m2s v1.22.1 k8s-master-2 NotReady control-plane,master 2m10s v1.22.1 k8s-worker-0 NotReady \u003cnone\u003e 97s v1.22.1 k8s-worker-1 NotReady \u003cnone\u003e 86s v1.22.1 现在它们都还处于 NotReady 状态，需要等到我们把网络插件安装好，才会 Ready. 现在再看下集群的证书签发状态： shell ❯ kubectl get csr --sort-by='{.spec.username}' NAME AGE SIGNERNAME REQUESTER REQUESTEDDURATION CONDITION csr-95hll 6m58s kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:q8ivnz \u003cnone\u003e Approved,Issued csr-tklnr 7m5s kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:q8ivnz \u003cnone\u003e Approved,Issued csr-w92jv 9m15s kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:q8ivnz \u003cnone\u003e Approved,Issued csr-rv7sj 8m11s kubernetes.io/kube-apiserver-client-kubelet system:bootstrap:q8ivnz \u003cnone\u003e Approved,Issued csr-nxkgx 10m kubernetes.io/kube-apiserver-client-kubelet system:node:k8s-master-0 \u003cnone\u003e Approved,Issued csr-cd22c 10m kubernetes.io/kubelet-serving system:node:k8s-master-0 \u003cnone\u003e Pending csr-wjrnr 9m53s kubernetes.io/kubelet-serving system:node:k8s-master-0 \u003cnone\u003e Pending ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:7:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#5-使用-kubeadm-创建集群"},{"categories":["tech"],"content":" 5.1 常见问题 5.1.1 重置集群配置创建集群的过程中出现任何问题，都可以通过在所有节点上运行 kubeadm reset 来还原配置，然后重新走 kubeadm 的集群创建流程。 但是要注意几点： kubeadm reset 会清除包含 kube-vip 配置在内的所有 static-pod 配置文件，所以 master 节点需要重新跑下前面给的 kube-vip 命令，生成下 kube-vip 配置。 kubeadm reset 不会重置网络接口的配置，master 节点需要手动清理下 kube-vip 添加的 vip:ip addr del 192.168.122.200/32 dev eth0. 如果你在安装了网络插件之后希望重装集群，顺序如下： 通过 kubectl delete -f xxx.yaml/helm uninstall 删除所有除网络之外的其他应用配置 删除网络插件 先重启一遍所有节点，或者手动重置所有节点的网络配置 建议重启，因为我不知道该怎么手动重置… 试了 systemctl restart network 并不会清理所有虚拟网络接口。 如此操作后，再重新执行集群安装，应该就没啥毛病了。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:7:1","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#51-常见问题"},{"categories":["tech"],"content":" 5.1 常见问题 5.1.1 重置集群配置创建集群的过程中出现任何问题，都可以通过在所有节点上运行 kubeadm reset 来还原配置，然后重新走 kubeadm 的集群创建流程。 但是要注意几点： kubeadm reset 会清除包含 kube-vip 配置在内的所有 static-pod 配置文件，所以 master 节点需要重新跑下前面给的 kube-vip 命令，生成下 kube-vip 配置。 kubeadm reset 不会重置网络接口的配置，master 节点需要手动清理下 kube-vip 添加的 vip:ip addr del 192.168.122.200/32 dev eth0. 如果你在安装了网络插件之后希望重装集群，顺序如下： 通过 kubectl delete -f xxx.yaml/helm uninstall 删除所有除网络之外的其他应用配置 删除网络插件 先重启一遍所有节点，或者手动重置所有节点的网络配置 建议重启，因为我不知道该怎么手动重置… 试了 systemctl restart network 并不会清理所有虚拟网络接口。 如此操作后，再重新执行集群安装，应该就没啥毛病了。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:7:1","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#511-重置集群配置"},{"categories":["tech"],"content":" 6. 验证集群的高可用性虽然网络插件还没装导致集群所有节点都还没 ready，但是我们已经可以通过 kubectl 命令来简单验证集群的高可用性了。 首先，我们将前面放置在 k8s-master-0 的认证文件 $HOME/.kube/config 以及 kunbectl 安装在另一台机器上，比如我直接放我的宿主机。 然后在宿主机上跑 kubectl get node 命令验证集群的高可用性： 三个主节点都正常运行时，kubectl 命令也正常 pause 或者 stop 其中一个 master，kubectl 命令仍然能正常运行 再 pause 第二个 master，kubectl 命令应该就会卡住，并且超时，无法使用了 resume 恢复停掉的两个 master 之一，会发现 kubectl 命令又能正常运行了 到这里 kubeadm 的工作就完成了，接下来再安装网络插件，集群就可用了。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:8:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#6-验证集群的高可用性"},{"categories":["tech"],"content":" 7. 安装网络插件社区有很多种网络插件可选，比较知名且性能也不错的，应该是 Calico 和 Cilium，其中 Cilium 主打基于 eBPF 的高性能与高可观测性。 下面分别介绍这两个插件的安装方法。（注意只能安装其中一个网络插件，不能重复安装。） 需要提前在本机安装好 helm，我这里使用宿主机，因此只需要在宿主机安装: shell # 一行命令安装，也可以自己手动下载安装包，都行 curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash # 或者 opensuse 直接用包管理器安装 sudo zypper in helm ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:9:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#7-安装网络插件"},{"categories":["tech"],"content":" 7.1 安装 Cilium 官方文档：https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-kubeadm/ cilium 通过 eBPF 提供了高性能与高可观测的 k8s 集群网络，另外 cilium 还提供了比 kube-proxy 更高效的实现，可以完全替代 kube-proxy. 这里我们还是先使用 kube-proxy 模式，先熟悉下 cilium 的使用： shell helm repo add cilium https://helm.cilium.io/ helm search repo cilium/cilium -l | head helm install cilium cilium/cilium --version 1.10.4 --namespace kube-system 可以通过 kubectl get pod -A 查看 cilium 的安装进度，当所有 pod 都 ready 后，集群就 ready 了~ cilium 也提供了专用的客户端： shell curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz{,.sha256sum} 然后使用 cilium 客户端检查网络插件的状态： shell $ cilium status --wait /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Hubble: disabled \\__/¯¯\\__/ ClusterMesh: disabled \\__/ DaemonSet cilium Desired: 5, Ready: 5/5, Available: 5/5 Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 Containers: cilium Running: 5 cilium-operator Running: 2 Cluster Pods: 2/2 managed by Cilium Image versions cilium quay.io/cilium/cilium:v1.10.4@sha256:7d354052ccf2a7445101d78cebd14444c7c40129ce7889f2f04b89374dbf8a1d: 5 cilium-operator quay.io/cilium/operator-generic:v1.10.4@sha256:c49a14e34634ff1a494c84b718641f27267fb3a0291ce3d74352b44f8a8d2f93: 2 cilium 还提供了命令，自动创建 pod 进行集群网络的连接性测试： shell ❯ cilium connectivity test ℹ️ Monitor aggregation detected, will skip some flow validation steps ✨ [kubernetes] Creating namespace for connectivity check... ✨ [kubernetes] Deploying echo-same-node service... ✨ [kubernetes] Deploying same-node deployment... ✨ [kubernetes] Deploying client deployment... ✨ [kubernetes] Deploying client2 deployment... ✨ [kubernetes] Deploying echo-other-node service... ✨ [kubernetes] Deploying other-node deployment... ... ℹ️ Expose Relay locally with: cilium hubble enable cilium status --wait cilium hubble port-forward\u0026 🏃 Running tests... ... --------------------------------------------------------------------------------------------------------------------- ✅ All 11 tests (134 actions) successful, 0 tests skipped, 0 scenarios skipped. 通过 kubectl get po -A 能观察到，这个测试命令会自动创建一个 cilium-test 名字空间，并在启动创建若干 pod 进行详细的测试。 整个测试流程大概会持续 5 分多钟，测试完成后，相关 Pod 不会自动删除，使用如下命令手动删除： shell kubectl delete namespace cilium-test ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:9:1","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#71-安装-cilium"},{"categories":["tech"],"content":" 7.2 安装 Calico 官方文档：https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises 也就两三行命令。安装确实特别简单，懒得介绍了，看官方文档吧。 但是实际上 calico 的细节还蛮多的，建议通读下它的官方文档，了解下 calico 的架构。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:9:2","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#72-安装-calico"},{"categories":["tech"],"content":" 8. 查看集群状态官方的 dashboard 个人感觉不太好用，建议直接在本地装个 k9s 用，特别爽。 shell sudo zypper in k9s 然后就可以愉快地玩耍了。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:10:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#8-查看集群状态"},{"categories":["tech"],"content":" 9. 安装 metrics-server 这一步可能遇到的问题：Enabling signed kubelet serving certificates 如果需要使用 HPA 以及简单的集群监控，那么 metrics-server 是必须安装的，现在我们安装一下它。 首先，跑 kubectl 的监控命令应该会报错： shell ❯ kubectl top node error: Metrics API not available k9s 里面应该也看不到任何监控指标。 现在通过 helm 安装它： shell helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ helm search repo metrics-server/metrics-server -l | head helm upgrade --install metrics-server metrics-server/metrics-server --version 3.5.0 --namespace kube-system metrics-server 默认只会部署一个实例，如果希望高可用，请参考官方配置：metrics-server - high-availability manifests 等 metrics-server 启动好后，就可以使用 kubectl top 命令啦： shell ❯ kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master-0 327m 16% 1465Mi 50% k8s-master-1 263m 13% 1279Mi 44% k8s-master-2 289m 14% 1282Mi 44% k8s-worker-0 62m 3% 518Mi 13% k8s-worker-1 115m 2% 659Mi 8% ❯ kubectl top pod No resources found in default namespace. ❯ kubectl top pod -A NAMESPACE NAME CPU(cores) MEMORY(bytes) kube-system cilium-45nw4 9m 135Mi kube-system cilium-5x7jf 6m 154Mi kube-system cilium-84sr2 7m 160Mi kube-system cilium-operator-78f45675-dp4b6 2m 30Mi kube-system cilium-operator-78f45675-fpm5g 1m 30Mi kube-system cilium-tkhl4 6m 141Mi kube-system cilium-zxbvm 5m 138Mi kube-system coredns-78fcd69978-dpxxk 3m 16Mi kube-system coredns-78fcd69978-pdf9p 1m 18Mi kube-system etcd-k8s-master-0 61m 88Mi kube-system etcd-k8s-master-1 50m 85Mi kube-system etcd-k8s-master-2 55m 83Mi kube-system kube-apiserver-k8s-master-0 98m 462Mi kube-system kube-apiserver-k8s-master-1 85m 468Mi kube-system kube-apiserver-k8s-master-2 85m 423Mi kube-system kube-controller-manager-k8s-master-0 22m 57Mi kube-system kube-controller-manager-k8s-master-1 2m 23Mi kube-system kube-controller-manager-k8s-master-2 2m 23Mi kube-system kube-proxy-j2s76 1m 24Mi kube-system kube-proxy-k6d6z 1m 18Mi kube-system kube-proxy-k85rx 1m 23Mi kube-system kube-proxy-pknsc 1m 20Mi kube-system kube-proxy-xsq4m 1m 15Mi kube-system kube-scheduler-k8s-master-0 3m 25Mi kube-system kube-scheduler-k8s-master-1 4m 21Mi kube-system kube-scheduler-k8s-master-2 5m 21Mi kube-system kube-vip-k8s-master-0 4m 17Mi kube-system kube-vip-k8s-master-1 2m 16Mi kube-system kube-vip-k8s-master-2 2m 17Mi kube-system metrics-server-559f85484-5b6xf 7m 27Mi ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:11:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#9-安装-metrics-server"},{"categories":["tech"],"content":" 10. 为 etcd 添加定期备份能力请移步etcd 的备份与恢复 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:12:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#10-为-etcd-添加定期备份能力"},{"categories":["tech"],"content":" 11. 安装 Volume Provisioner在我们学习使用 Prometheus/MinIO/Tekton 等有状态应用时，它们默认情况下会通过 PVC 声明需要的数据卷。 为了支持这个能力，我们需要在集群中部署一个 Volume Provisioner. 对于云上环境，直接接入云服务商提供的 Volume Provisioner 就 OK 了，方便省事而且足够可靠。 而对于 bare-metal 环境，比较有名的应该是 rook-ceph，但是这个玩意部署复杂，维护难度又高，不适合用来测试学习。 对于开发、测试环境，或者个人集群，建议使用： local 数据卷，适合数据可丢失，且不要求分布式的场景，如开发测试环境 https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner https://github.com/rancher/local-path-provisioner NFS 数据卷，适合数据可丢失，对性能要求不高，并且要求分布式的场景。比如开发测试环境、或者线上没啥压力的应用 https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner https://github.com/kubernetes-csi/csi-driver-nfs NFS 数据的可靠性依赖于外部 NFS 服务器，企业通常使用群晖等 NAS 来做 NFS 服务器 如果外部 NFS 服务器出问题，应用就会崩。 直接使用云上的对象存储，适合希望数据不丢失、对性能要求不高的场景。 直接使用 https://github.com/rclone/rclone mount 模式来保存数据，或者直接同步文件夹数据到云端（可能会有一定数据丢失）。 ","date":"2022-01-25","objectID":"/posts/kubernetes-deployment-using-kubeadm/:13:0","series":["云原生相关"],"tags":["Kubernetes","云原生"],"title":"部署一个 Kubernetes 集群","uri":"/posts/kubernetes-deployment-using-kubeadm/#11-安装-volume-provisioner"},{"categories":["tech"],"content":" 本文由个人笔记ryan4yin/knowledge 整理而来 本文主要介绍我个人在使用 Kubernetes 的过程中，总结出的一套「Kubernetes 配置」，是我个人的「最佳实践」。其中大部分内容都经历过线上环境的考验，但是也有少部分还只在我脑子里模拟过，请谨慎参考。 阅读前的几个注意事项： 这份文档比较长，囊括了很多内容，建议当成参考手册使用，先参照目录简单读一读，有需要再细读相关内容。 这份文档需要一定的 Kubernetes 基础才能理解，而且如果没有过实践经验的话，看上去可能会比较枯燥。 而有过实践经验的大佬，可能会跟我有不同的见解，欢迎各路大佬评论~ 我会视情况不定期更新这份文档。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:0:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#"},{"categories":["tech"],"content":" 零、示例首先，这里给出一些本文遵守的前提，这些前提只是契合我遇到的场景，可灵活变通： 这里只讨论无状态服务，有状态服务不在讨论范围内 我们不使用 Deployment 的滚动更新能力，而是为每个服务的每个版本，都创建不同的Deployment + HPA + PodDisruptionBudget，这是为了方便做金丝雀/灰度发布 我们的服务可能会使用 IngressController / Service Mesh 来进行服务的负载均衡、流量切分 下面先给出一个 Deployment + HPA + PodDisruptionBudget 的 demo，后面再拆开详细说下： yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app-v3 namespace: prod # 建议按业务逻辑划分名字空间，prod 仅为示例 labels: app: my-app spec: replicas: 3 strategy: type: RollingUpdate # 因为服务的每个版本都使用各自的 Deployment，服务更新时其实是用不上这里的滚动更新策略的 # 这个配置应该只在 SRE 手动修改 Deployment 配置时才会生效（通常不应该发生这种事） rollingUpdate: maxSurge: 10% # 滚动更新时，每次最多更新 10% 的 Pods maxUnavailable: 0 # 滚动更新时，不允许出现不可用的 Pods，也就是说始终要维持 3 个可用副本 selector: matchLabels: app: my-app version: v3 template: metadata: labels: app: my-app version: v3 spec: affinity: # 注意，podAffinity/podAntiAffinity 可能不是最佳方案，这部分配置待更新 # topologySpreadConstraints 可能是更好的选择 podAffinity: preferredDuringSchedulingIgnoredDuringExecution: # 非强制性条件 - weight: 100 # weight 用于为节点评分，会优先选择评分最高的节点（只有一条规则的情况下，这个值没啥意义） podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - my-app - key: version operator: In values: - v3 # pod 尽量使用同一种节点类型，也就是尽量保证节点的性能一致 topologyKey: node.kubernetes.io/instance-type podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: # 非强制性条件 - weight: 100 # weight 用于为节点评分，会优先选择评分最高的节点（只有一条规则的情况下，这个值没啥意义） podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - my-app - key: version operator: In values: - v3 # 将 pod 尽量打散在多个可用区 topologyKey: topology.kubernetes.io/zone requiredDuringSchedulingIgnoredDuringExecution: # 强制性要求（这个建议按需添加） # 注意这个没有 weights，必须满足列表中的所有条件 - labelSelector: matchExpressions: - key: app operator: In values: - my-app - key: version operator: In values: - v3 # Pod 必须运行在不同的节点上 topologyKey: kubernetes.io/hostname securityContext: # runAsUser: 1000 # 设定用户 # runAsGroup: 1000 # 设定用户组 runAsNonRoot: true # Pod 必须以非 root 用户运行 seccompProfile: # security compute mode type: RuntimeDefault nodeSelector: nodegroup: common # 使用专用节点组，如果希望使用多个节点组，可改用节点亲和性 volumes: - name: tmp-dir emptyDir: {} containers: - name: my-app-v3 image: my-app:v3 # 建议使用私有镜像仓库，规避 docker.io 的镜像拉取限制 imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /tmp name: tmp-dir lifecycle: preStop: # 在容器被 kill 之前执行 exec: command: - /bin/sh - -c - \"while [ $(netstat -plunt | grep tcp | wc -l | xargs) -ne 0 ]; do sleep 1; done\" resources: # 资源请求与限制 # 对于核心服务，建议设置 requests = limits，避免资源竞争 requests: # HPA 会使用 requests 计算资源利用率 # 建议将 requests 设为服务正常状态下的 CPU 使用率，HPA 的目前指标设为 80% # 所有容器的 requests 总量不建议为 2c/4G 4c/8G 等常见值，因为节点通常也是这个配置，这会导致 Pod 只能调度到更大的节点上，适当调小 requests 等扩充可用的节点类型，从而扩充节点池。 cpu: 1000m memory: 1Gi limits: # limits - requests 为允许超卖的资源量，建议为 requests 的 1 到 2 倍，酌情配置。 cpu: 1000m memory: 1Gi securityContext: # 将容器层设为只读，防止容器文件被篡改 ## 如果需要写入临时文件，建议额外挂载 emptyDir 来提供可读写的数据卷 readOnlyRootFilesystem: true # 禁止 Pod 做任何权限提升 allowPrivilegeEscalation: false capabilities: # drop ALL 的权限比较严格，可按需修改 drop: - ALL startupProbe: # 要求 kubernetes 1.18+ httpGet: path: /actuator/health # 直接使用健康检查接口即可 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 20 # 最多提供给服务 5s * 20 的启动时间 successThreshold: 1 livenessProbe: httpGet: path: /actuator/health # spring 的通用健康检查路径 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 # Readiness probes are very important for a RollingUpdate to work properly, readinessProbe: httpGet: path: /actuator/health # 简单起见可直接使用 livenessProbe 相同的接口，当然也可额外定义 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 --- apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: labels: app: my-app name: my-app-v3 namespace: prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: my-app-v3 maxReplicas: 50 minReplicas: 3 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 --- apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: my-app-v3 namespac","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:1:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#零示例"},{"categories":["tech"],"content":" 一、优雅停止（Gracful Shutdown）与 502/504 报错如果 Pod 正在处理大量请求（比如 1000 QPS+）时，因为节点故障或「竞价节点」被回收等原因被重新调度，你可能会观察到在容器被 terminate 的一段时间内出现少量 502/504。 为了搞清楚这个问题，需要先理解清楚 terminate 一个 Pod 的流程： Pod 的状态被设为 Terminating（几乎）同时该 Pod 被从所有关联的 Service Endpoints 中移除 如果配置了 preStop 参数，开始执行如下步骤 执行 preStop 钩子 它的执行阶段很好理解：在容器被 stop 之前执行 它可以是一个命令，或者一个对 Pod 中容器的 http 调用 如果在收到 SIGTERM 信号时，无法优雅退出，要支持优雅退出比较麻烦的话，用 preStop 实现优雅退出是一个非常好的方式 preStop 的定义位置：https://github.com/kubernetes/api/blob/master/core/v1/types.go#L2515 preStop 执行完毕后，SIGTERM 信号被发送给 Pod 中的所有容器 如果未配置 preStop，则 SIGTERM 信号将被立即发送给 Pod 中的所有容器 继续等待，直到容器停止或者超时。 spec.terminationGracePeriodSeconds 为超时时间，默认为 30s 需要注意的是，这个优雅退出的等待计时是与 preStop 同步开始的！ 如果超过了 spec.terminationGracePeriodSeconds 容器仍然没有停止，k8s 将会发送 SIGKILL 信号给容器 进程全部终止后，整个 Pod 完全被清理掉 注意：「从 Service Endpoints 中移除 Pod IP」跟后续的步骤是异步发生的，所以在未设置preStop 时，可能会出现「Pod 还在 Service Endpoints 中，但是 SIGTERM 已经被发送给 Pod 导致容器都挂掉」的情况，我们需要考虑到这种状况的发生。 了解了上面的流程后，我们就能分析出两种错误码出现的原因： 502：应用程序在收到 SIGTERM 信号后直接终止了运行，导致部分还没有被处理完的请求直接中断， 代理层返回 502 表示这种情况 504：Service Endpoints 移除不够及时，在 Pod 已经被终止后，仍然有个别请求被路由到了该 Pod，得不到响应导致 504 通常的解决方案是，在 Pod 的 preStop 步骤加一个 15s 的等待时间。其原理是：在 Pod 处理 terminating 状态的时候，就会被从 Service Endpoints 中移除，也就不会再有新的请求过来了。在preStop 等待 15s，基本就能保证所有的请求都在容器死掉之前被处理完成（一般来说，绝大部分请求的处理时间都在 300ms 以内吧）。 一个简单的示例如下，它使 Pod 被 Terminate 时，总是在 stop 前先等待 15s，再发送 SIGTERM 信号给容器： yaml containers: - name: my-app # 添加下面这部分 lifecycle: preStop: exec: command: - /bin/sleep - \"15\" 更好的解决办法，是直接等待所有 tcp 连接都关闭（需要镜像中有 netstat）： yaml containers: - name: my-app # 添加下面这部分 lifecycle: preStop: exec: command: - /bin/sh - -c - \"while [ $(netstat -plunt | grep tcp | wc -l | xargs) -ne 0 ]; do sleep 1; done\" ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:2:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#一优雅停止gracful-shutdown与-502504-报错"},{"categories":["tech"],"content":" 如果我的服务还使用了 Sidecar 代理网络请求，该怎么处理？以服务网格 Istio 为例，在 Envoy 代理了 Pod 流量的情况下，502/504 的问题会变得更复杂一点—— 还需要考虑 Sidecar 与主容器的关闭顺序： 如果在 Envoy 已关闭后，有新的请求再进来，将会导致 504（没人响应这个请求了） 所以 Envoy 最好在 Terminating 至少 3s 后才能关，确保 Istio 网格配置已完全更新 如果在 Envoy 还没停止时，主容器先关闭，然后又有新的请求再进来，Envoy 将因为无法连接到 upstream 导致 503 所以主容器也最好在 Terminating 至少 3s 后，才能关闭。 如果主容器处理还未处理完遗留请求时，Envoy 或者主容器的其中一个停止了，会因为 tcp 连接直接断开连接导致 502 因此 Envoy 必须在主容器处理完遗留请求后（即没有 tcp 连接时），才能关闭 所以总结下：Envoy 及主容器的 preStop 都至少得设成 3s，并且在「没有 tcp 连接」时，才能关闭，避免出现 502/503/504. 主容器的修改方法在前文中已经写过了，下面介绍下 Envoy 的修改方法。 和主容器一样，Envoy 也能直接加 preStop，修改 istio-sidecar-injector 这个 configmap， 在 sidecar 里添加 preStop sleep 命令: yaml containers: - name: istio-proxy # 添加下面这部分 lifecycle: preStop: exec: command: - /bin/sh - -c - \"while [ $(netstat -plunt | grep tcp | grep -v envoy | wc -l | xargs) -ne 0 ]; do sleep 1; done\" ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:2:1","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-istio-pod-prestop"},{"categories":["tech"],"content":" 参考 Kubernetes best practices: terminating with grace Graceful shutdown in Kubernetes is not always trivial ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:2:2","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#参考"},{"categories":["tech"],"content":" 二、服务的伸缩配置 - HPAKubernetes 官方主要支持基于 Pod CPU 的伸缩，这是应用最为广泛的伸缩指标，需要部署metrics-server 才可使用。 先回顾下前面给出的，基于 Pod CPU 使用率进行伸缩的示例： yaml apiVersion: autoscaling/v2beta2 # k8s 1.23+ 此 API 已经 GA kind: HorizontalPodAutoscaler metadata: labels: app: my-app name: my-app-v3 namespace: prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: my-app-v3 maxReplicas: 50 minReplicas: 3 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-hpa"},{"categories":["tech"],"content":" 1. 当前指标值的计算方式提前总结：每个 Pod 的指标是其中所有容器指标之和，如果计算百分比，就再除以 Pod 的 requests. HPA 默认使用 Pod 的当前指标进行计算，以 CPU 使用率为例，其计算公式为： text 「Pod 的 CPU 使用率」= 100% * 「所有 Container 的 CPU 用量之和」/「所有 Container 的 CPU requests 之和」 注意分母是总的 requests 量，而不是 limits. 1.1 存在的问题与解决方法在 Pod 只有一个容器时这没啥问题，但是当 Pod 注入了 envoy 等 sidecar 时，这就会有问题了。 因为 Istio 的 Sidecar requests 默认为 100m 也就是 0.1 核。在未 tuning 的情况下，服务负载一高，sidecar 的实际用量很容易就能涨到 0.2-0.4 核。把这两个值代入前面的公式，会发现 对于 QPS 较高的服务，添加 Sidecar 后，「Pod 的 CPU 利用率」可能会高于「应用容器的 CPU 利用率」，造成不必要的扩容。主容器的 requests 与 limits 差距越小，这样的扩容造成的资源浪费就越大。 而且还有个问题是，不同应用的 Pod，数据流特征、应用负载特征等都有区别（请求/响应的数据量、处理时长等），这会造成 sidecar 与主容器的 cpu 利用率不一，加大了优化 HPA 机制的困难度。 解决方法： 方法一：HPA 改用绝对指标进行扩缩容，即 Pod 的总 CPU 用量。这使 HPA 不受任何容器 requests 设置的影响。 但是因为不同服务负载的区别，需要根据实际负载为每个服务调整 HPA 的期望指标。 方法二：HPA 仍然使用 Pod 利用率进行扩缩容，但是针对每个服务的 CPU 使用情况，为每个服务的 sidecar 设置不同的 requests/limits，降低 sidecar 对扩缩容的影响。 方法三：使用 KEDA 等第三方组件，获取到应用容器的 CPU 利用率（排除掉 Sidecar），使用它进行扩缩容 方法四：使用 k8s 1.20 提供的 alpha 特性：Container Resource Metrics. 这种方式可以将 Pod 的不同容器的指标区分看待，算是最佳的处理方法了，但是该特性仍未进入 beta 阶段，慎用。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:1","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#1-当前指标值的计算方式"},{"categories":["tech"],"content":" 1. 当前指标值的计算方式提前总结：每个 Pod 的指标是其中所有容器指标之和，如果计算百分比，就再除以 Pod 的 requests. HPA 默认使用 Pod 的当前指标进行计算，以 CPU 使用率为例，其计算公式为： text 「Pod 的 CPU 使用率」= 100% * 「所有 Container 的 CPU 用量之和」/「所有 Container 的 CPU requests 之和」 注意分母是总的 requests 量，而不是 limits. 1.1 存在的问题与解决方法在 Pod 只有一个容器时这没啥问题，但是当 Pod 注入了 envoy 等 sidecar 时，这就会有问题了。 因为 Istio 的 Sidecar requests 默认为 100m 也就是 0.1 核。在未 tuning 的情况下，服务负载一高，sidecar 的实际用量很容易就能涨到 0.2-0.4 核。把这两个值代入前面的公式，会发现 对于 QPS 较高的服务，添加 Sidecar 后，「Pod 的 CPU 利用率」可能会高于「应用容器的 CPU 利用率」，造成不必要的扩容。主容器的 requests 与 limits 差距越小，这样的扩容造成的资源浪费就越大。 而且还有个问题是，不同应用的 Pod，数据流特征、应用负载特征等都有区别（请求/响应的数据量、处理时长等），这会造成 sidecar 与主容器的 cpu 利用率不一，加大了优化 HPA 机制的困难度。 解决方法： 方法一：HPA 改用绝对指标进行扩缩容，即 Pod 的总 CPU 用量。这使 HPA 不受任何容器 requests 设置的影响。 但是因为不同服务负载的区别，需要根据实际负载为每个服务调整 HPA 的期望指标。 方法二：HPA 仍然使用 Pod 利用率进行扩缩容，但是针对每个服务的 CPU 使用情况，为每个服务的 sidecar 设置不同的 requests/limits，降低 sidecar 对扩缩容的影响。 方法三：使用 KEDA 等第三方组件，获取到应用容器的 CPU 利用率（排除掉 Sidecar），使用它进行扩缩容 方法四：使用 k8s 1.20 提供的 alpha 特性：Container Resource Metrics. 这种方式可以将 Pod 的不同容器的指标区分看待，算是最佳的处理方法了，但是该特性仍未进入 beta 阶段，慎用。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:1","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#11-存在的问题与解决方法"},{"categories":["tech"],"content":" 2. HPA 的扩缩容算法HPA 什么时候会扩容，这一点是很好理解的。但是 HPA 的缩容策略，会有些迷惑，下面简单分析下。 HPA 的「目标指标」可以使用两种形式：绝对度量指标和资源利用率。 绝对度量指标：比如 CPU，就是指 CPU 的使用量 资源利用率（资源使用量/资源请求 * 100%）：在 Pod 设置了资源请求时，可以使用资源利用率进行 Pod 伸缩 HPA 的「当前指标」是一段时间内所有 Pods 的平均值，不是峰值。 HPA 的扩缩容算法为： text 期望副本数 = ceil[当前副本数 * ( 当前指标 / 目标指标 )] 从上面的参数可以看到： 只要「当前指标」超过了目标指标，就一定会发生扩容。 当前指标 / 目标指标要小到一定的程度，才会触发缩容。 比如双副本的情况下，上述比值要小于等于 1/2，才会缩容到单副本。 三副本的情况下，上述比值的临界点是 2/3。 五副本时临界值是 4/5，100副本时临界值是 99/100，依此类推。 如果 当前指标 / 目标指标 从 1 降到 0.5，副本的数量将会减半。（虽然说副本数越多，发生这么大变化的可能性就越小。） 当前副本数 / 目标指标的值越大，「当前指标」的波动对「期望副本数」的影响就越大。 为了防止扩缩容过于敏感，HPA 有几个相关参数： Hardcoded 参数 HPA Loop 延时：默认 15 秒，每 15 秒钟进行一次 HPA 扫描。 缩容冷却时间：默认 5 分钟。 对于 K8s 1.18+，HPA 通过 spec.behavior 提供了多种控制扩缩容行为的参数，后面会具体介绍。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:2","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#2-hpa-的扩缩容算法"},{"categories":["tech"],"content":" 3. HPA 的期望值设成多少合适？如何兼顾资源利用率与服务稳定性？这个需要针对每个服务的具体情况，具体分析。 以最常用的按 CPU 值伸缩为例， 核心服务 requests/limits 值: 建议设成相等的，保证服务质量等级为 Guaranteed 需要注意 CPU 跟 Memory 的 limits 限制策略是不同的，CPU 是真正地限制了上限，而 Memory 是用超了就干掉容器（OOMKilled） k8s 一直使用 cgroups v1 (cpu_shares/memory.limit_in_bytes)来限制 cpu/memory，但是对于 Guaranteed 的 Pods 而言，内存并不能完全预留，资源竞争总是有可能发生的。1.22 有 alpha 特性改用 cgroups v2，可以关注下。 HPA: 一般来说，期望值设为 60% 到 70% 可能是比较合适的，最小副本数建议设为 2 - 5. （仅供参考） PodDisruptionBudget: 建议按服务的健壮性与 HPA 期望值，来设置 PDB，后面会详细介绍，这里就先略过了 非核心服务 requests/limits 值: 建议 requests 设为 limits 的 0.6 - 0.9 倍（仅供参考），对应的服务质量等级为 Burstable 也就是超卖了资源，这样做主要的考量点是，很多非核心服务负载都很低，根本跑不到 limits 这么高，降低 requests 可以提高集群资源利用率，也不会损害服务稳定性。 HPA: 因为 requests 降低了，而 HPA 是以 requests 为 100% 计算使用率的，我们可以提高 HPA 的期望值（如果使用百分比为期望值的话），比如 80% ~ 90%，最小副本数建议设为 1 - 3. （仅供参考） PodDisruptionBudget: 非核心服务嘛，保证最少副本数为 1 就行了。 相关资料： 最佳实践｜Kubernetes集群利用率提升的思路和实现方式 - 腾讯云原生 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:3","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#3-hpa-的期望值设成多少合适如何兼顾资源利用率与服务稳定性"},{"categories":["tech"],"content":" 4. HPA 的常见问题 4.1. Pod 扩容 - VM 预热陷阱Java/C# 这类运行在 VM 上的语言，在启动阶段与第一次执行请求时，往往需要初始化一些资源。这导致在容器刚启动的一段时间，它需要消耗更多的 CPU 去将字节码编译成机器码并缓存起来。这就是 VM 语言的「预热」问题。 相关文档： How to Warm Up the JVM How to cold start fast a java service on k8s (EKS) 因为上述「预热」问题在使用 HPA 扩缩容 Java/C# 等运行在 VM 上的程序时，HPA 一扩容，服务可用率就会抖动，甚至引发雪崩。 举例说明，在有大量用户访问的时候，只要请求被转发到新建的 Java Pod 上，这个请求就会「卡住」。如果请求速度太快，Pod 启动的瞬间「卡住」的请求就越多，这可能会导致新建 Pod 因为压力过大而垮掉。然后 Pod 每次刚重启就被打垮，进入 CrashLoopBackoff 循环。 如果是在使用多线程做负载测试时，效果更明显：50 个线程在不间断地请求，别的 Pod 响应时间是「毫秒级」，而新建的 Pod 的首次响应是「秒级」。几乎是一瞬间，50 个线程就会全部陷在新建的 Pod 这里。而新建的 Pod 在启动的瞬间可能特别脆弱，瞬间的 50 个并发请求就可以将它压垮。然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。 解决方法： 可以在「应用层面」解决： 大 Pod 策略：JVM 预热阶段需要使用更多的 CPU，假设在 HPA 为 60% CPU 的情况下，越大的 Pod 它的可超卖 CPU （40%）显然就更多，这就能缓解 JVM 预热问题。 实测将 Pod 的 requests/limits 从 3.7c/7.5G 扩容到 7.4c/15G，服务的扩容明显更平滑了。 JVM 参数调优：不完全举例如下 提前分配 G1New 内存：-XX:+UnlockExperimentalVMOptions -XX:G1NewSizePercent=60，避免 JVM 启动阶段因为内存不足频繁 GC 使用「AOT 预编译」技术：预热通常都是因为「JIT 即时编译」导致的问题，在需要用到时它才编译。而 AOT 是预先编译，在使用前完成编译，因此 AOT 能解决预热的问题。比如说 Java 的 GraalVM. Sprint Native 是使用 GralalVM 实现 的，但还在 beta 阶段，而且有些兼容性问题 其他资料 steinsag/warm-me-up How to Warm Up the JVM 也可以在「基础设施层面」解决： 像 AWS ALB TargetGroup 以及其他云服务商的 ALB 服务，通常都可以设置 slow_start 时长， 即对新加入的实例，使用一定时间慢慢地把流量切过去，最终达到预期的负载均衡状态。这个可以解决服务预热问题。 Envoy 也已经支持 slow_start 模式，支持在一个设置好的时间窗口内，把流量慢慢负载到新加入的实例上，达成预热效果。 4.2. HPA 扩缩容过于敏感，导致 Pod 数量震荡通常来讲，K8s 上绝大部分负载都应该选择使用 CPU 进行扩缩容。因为 CPU 通常能很好的反映服务的负载情况 但是有些服务会存在其他影响 CPU 使用率的因素，导致使用 CPU 扩缩容变得不那么可靠，比如： 有些 Java 服务堆内存设得很大，GC pause 也设得比较长，因此内存 GC 会造成 CPU 间歇性飙升，CPU 监控会有大量的尖峰。 有些服务有定时任务，定时任务一运行 CPU 就涨，但是这跟服务的 QPS 是无关的 有些服务可能一运行 CPU 就会立即处于一个高位状态，它可能希望使用别的业务侧指标来进行扩容，而不是 CPU. 因为上述问题存在，使用 CPU 扩缩容，就可能会造成服务频繁的扩容然后缩容，或者无限扩容。而有些服务（如我们的「推荐服务」），对「扩容」和「缩容」都是比较敏感的，每次扩缩都会造成服务可用率抖动。 对这类服务而言，HPA 有这几种调整策略： 选择使用 QPS 等相对比较平滑，没有 GC 这类干扰的指标来进行扩缩容，这需要借助 KEDA 等社区组件。 对 kubernetes 1.18+，可以直接使用 HPA 的 behavior.scaleDown 和 behavior.scaleUp 两个参数，控制每次扩缩容的最多 pod 数量或者比例。 示例如下： yaml --- apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: podinfo namespace: default spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: podinfo minReplicas: 3 maxReplicas: 50 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 # 期望的 CPU 平均值 behavior: scaleUp: stabilizationWindowSeconds: 0 # 默认为 0，只使用当前值进行扩缩容 policies: - periodSeconds: 180 # 每 3 分钟最多扩容 5% 的 Pods type: Percent value: 5 - periodSeconds: 60 # 每分钟最多扩容 1 个 Pod，扩的慢一点主要是为了一个个地预热，避免一次扩容太多未预热的 Pods 导致服务可用率剧烈抖动 type: Pods value: 1 selectPolicy: Min # 选择最小的策略 # 以下的一切配置，都是为了更平滑地缩容 scaleDown: stabilizationWindowSeconds: 600 # 使用过去 10 mins 的最大 cpu 值进行缩容计算，避免过快缩容 policies: - type: Percent # 每 3 mins 最多缩容 `ceil[当前副本数 * 5%]` 个 pod（20 个 pod 以内，一次只缩容 1 个 pod） value: 5 periodSeconds: 180 - type: Pods # 每 1 mins 最多缩容 1 个 pod value: 1 periodSeconds: 60 selectPolicy: Min # 上面的 policies 列表，只生效其中最小的值作为缩容限制（保证平滑缩容） 而对于扩容不够平滑这个问题，可以考虑提供类似 AWS ALB TargetGroup slow_start 的功能，在扩容时缓慢将流量切到新 Pod 上，以实现预热服务（JVM 预热以及本地缓存预热），这样就能达到比较好的平滑扩容效果。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:4","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#4-hpa-的常见问题"},{"categories":["tech"],"content":" 4. HPA 的常见问题 4.1. Pod 扩容 - VM 预热陷阱Java/C# 这类运行在 VM 上的语言，在启动阶段与第一次执行请求时，往往需要初始化一些资源。这导致在容器刚启动的一段时间，它需要消耗更多的 CPU 去将字节码编译成机器码并缓存起来。这就是 VM 语言的「预热」问题。 相关文档： How to Warm Up the JVM How to cold start fast a java service on k8s (EKS) 因为上述「预热」问题在使用 HPA 扩缩容 Java/C# 等运行在 VM 上的程序时，HPA 一扩容，服务可用率就会抖动，甚至引发雪崩。 举例说明，在有大量用户访问的时候，只要请求被转发到新建的 Java Pod 上，这个请求就会「卡住」。如果请求速度太快，Pod 启动的瞬间「卡住」的请求就越多，这可能会导致新建 Pod 因为压力过大而垮掉。然后 Pod 每次刚重启就被打垮，进入 CrashLoopBackoff 循环。 如果是在使用多线程做负载测试时，效果更明显：50 个线程在不间断地请求，别的 Pod 响应时间是「毫秒级」，而新建的 Pod 的首次响应是「秒级」。几乎是一瞬间，50 个线程就会全部陷在新建的 Pod 这里。而新建的 Pod 在启动的瞬间可能特别脆弱，瞬间的 50 个并发请求就可以将它压垮。然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。 解决方法： 可以在「应用层面」解决： 大 Pod 策略：JVM 预热阶段需要使用更多的 CPU，假设在 HPA 为 60% CPU 的情况下，越大的 Pod 它的可超卖 CPU （40%）显然就更多，这就能缓解 JVM 预热问题。 实测将 Pod 的 requests/limits 从 3.7c/7.5G 扩容到 7.4c/15G，服务的扩容明显更平滑了。 JVM 参数调优：不完全举例如下 提前分配 G1New 内存：-XX:+UnlockExperimentalVMOptions -XX:G1NewSizePercent=60，避免 JVM 启动阶段因为内存不足频繁 GC 使用「AOT 预编译」技术：预热通常都是因为「JIT 即时编译」导致的问题，在需要用到时它才编译。而 AOT 是预先编译，在使用前完成编译，因此 AOT 能解决预热的问题。比如说 Java 的 GraalVM. Sprint Native 是使用 GralalVM 实现 的，但还在 beta 阶段，而且有些兼容性问题 其他资料 steinsag/warm-me-up How to Warm Up the JVM 也可以在「基础设施层面」解决： 像 AWS ALB TargetGroup 以及其他云服务商的 ALB 服务，通常都可以设置 slow_start 时长， 即对新加入的实例，使用一定时间慢慢地把流量切过去，最终达到预期的负载均衡状态。这个可以解决服务预热问题。 Envoy 也已经支持 slow_start 模式，支持在一个设置好的时间窗口内，把流量慢慢负载到新加入的实例上，达成预热效果。 4.2. HPA 扩缩容过于敏感，导致 Pod 数量震荡通常来讲，K8s 上绝大部分负载都应该选择使用 CPU 进行扩缩容。因为 CPU 通常能很好的反映服务的负载情况 但是有些服务会存在其他影响 CPU 使用率的因素，导致使用 CPU 扩缩容变得不那么可靠，比如： 有些 Java 服务堆内存设得很大，GC pause 也设得比较长，因此内存 GC 会造成 CPU 间歇性飙升，CPU 监控会有大量的尖峰。 有些服务有定时任务，定时任务一运行 CPU 就涨，但是这跟服务的 QPS 是无关的 有些服务可能一运行 CPU 就会立即处于一个高位状态，它可能希望使用别的业务侧指标来进行扩容，而不是 CPU. 因为上述问题存在，使用 CPU 扩缩容，就可能会造成服务频繁的扩容然后缩容，或者无限扩容。而有些服务（如我们的「推荐服务」），对「扩容」和「缩容」都是比较敏感的，每次扩缩都会造成服务可用率抖动。 对这类服务而言，HPA 有这几种调整策略： 选择使用 QPS 等相对比较平滑，没有 GC 这类干扰的指标来进行扩缩容，这需要借助 KEDA 等社区组件。 对 kubernetes 1.18+，可以直接使用 HPA 的 behavior.scaleDown 和 behavior.scaleUp 两个参数，控制每次扩缩容的最多 pod 数量或者比例。 示例如下： yaml --- apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: podinfo namespace: default spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: podinfo minReplicas: 3 maxReplicas: 50 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 # 期望的 CPU 平均值 behavior: scaleUp: stabilizationWindowSeconds: 0 # 默认为 0，只使用当前值进行扩缩容 policies: - periodSeconds: 180 # 每 3 分钟最多扩容 5% 的 Pods type: Percent value: 5 - periodSeconds: 60 # 每分钟最多扩容 1 个 Pod，扩的慢一点主要是为了一个个地预热，避免一次扩容太多未预热的 Pods 导致服务可用率剧烈抖动 type: Pods value: 1 selectPolicy: Min # 选择最小的策略 # 以下的一切配置，都是为了更平滑地缩容 scaleDown: stabilizationWindowSeconds: 600 # 使用过去 10 mins 的最大 cpu 值进行缩容计算，避免过快缩容 policies: - type: Percent # 每 3 mins 最多缩容 `ceil[当前副本数 * 5%]` 个 pod（20 个 pod 以内，一次只缩容 1 个 pod） value: 5 periodSeconds: 180 - type: Pods # 每 1 mins 最多缩容 1 个 pod value: 1 periodSeconds: 60 selectPolicy: Min # 上面的 policies 列表，只生效其中最小的值作为缩容限制（保证平滑缩容） 而对于扩容不够平滑这个问题，可以考虑提供类似 AWS ALB TargetGroup slow_start 的功能，在扩容时缓慢将流量切到新 Pod 上，以实现预热服务（JVM 预热以及本地缓存预热），这样就能达到比较好的平滑扩容效果。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:4","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#41-pod-扩容---vm-预热陷阱"},{"categories":["tech"],"content":" 4. HPA 的常见问题 4.1. Pod 扩容 - VM 预热陷阱Java/C# 这类运行在 VM 上的语言，在启动阶段与第一次执行请求时，往往需要初始化一些资源。这导致在容器刚启动的一段时间，它需要消耗更多的 CPU 去将字节码编译成机器码并缓存起来。这就是 VM 语言的「预热」问题。 相关文档： How to Warm Up the JVM How to cold start fast a java service on k8s (EKS) 因为上述「预热」问题在使用 HPA 扩缩容 Java/C# 等运行在 VM 上的程序时，HPA 一扩容，服务可用率就会抖动，甚至引发雪崩。 举例说明，在有大量用户访问的时候，只要请求被转发到新建的 Java Pod 上，这个请求就会「卡住」。如果请求速度太快，Pod 启动的瞬间「卡住」的请求就越多，这可能会导致新建 Pod 因为压力过大而垮掉。然后 Pod 每次刚重启就被打垮，进入 CrashLoopBackoff 循环。 如果是在使用多线程做负载测试时，效果更明显：50 个线程在不间断地请求，别的 Pod 响应时间是「毫秒级」，而新建的 Pod 的首次响应是「秒级」。几乎是一瞬间，50 个线程就会全部陷在新建的 Pod 这里。而新建的 Pod 在启动的瞬间可能特别脆弱，瞬间的 50 个并发请求就可以将它压垮。然后 Pod 一重启就被压垮，进入 CrashLoopBackoff 循环。 解决方法： 可以在「应用层面」解决： 大 Pod 策略：JVM 预热阶段需要使用更多的 CPU，假设在 HPA 为 60% CPU 的情况下，越大的 Pod 它的可超卖 CPU （40%）显然就更多，这就能缓解 JVM 预热问题。 实测将 Pod 的 requests/limits 从 3.7c/7.5G 扩容到 7.4c/15G，服务的扩容明显更平滑了。 JVM 参数调优：不完全举例如下 提前分配 G1New 内存：-XX:+UnlockExperimentalVMOptions -XX:G1NewSizePercent=60，避免 JVM 启动阶段因为内存不足频繁 GC 使用「AOT 预编译」技术：预热通常都是因为「JIT 即时编译」导致的问题，在需要用到时它才编译。而 AOT 是预先编译，在使用前完成编译，因此 AOT 能解决预热的问题。比如说 Java 的 GraalVM. Sprint Native 是使用 GralalVM 实现 的，但还在 beta 阶段，而且有些兼容性问题 其他资料 steinsag/warm-me-up How to Warm Up the JVM 也可以在「基础设施层面」解决： 像 AWS ALB TargetGroup 以及其他云服务商的 ALB 服务，通常都可以设置 slow_start 时长， 即对新加入的实例，使用一定时间慢慢地把流量切过去，最终达到预期的负载均衡状态。这个可以解决服务预热问题。 Envoy 也已经支持 slow_start 模式，支持在一个设置好的时间窗口内，把流量慢慢负载到新加入的实例上，达成预热效果。 4.2. HPA 扩缩容过于敏感，导致 Pod 数量震荡通常来讲，K8s 上绝大部分负载都应该选择使用 CPU 进行扩缩容。因为 CPU 通常能很好的反映服务的负载情况 但是有些服务会存在其他影响 CPU 使用率的因素，导致使用 CPU 扩缩容变得不那么可靠，比如： 有些 Java 服务堆内存设得很大，GC pause 也设得比较长，因此内存 GC 会造成 CPU 间歇性飙升，CPU 监控会有大量的尖峰。 有些服务有定时任务，定时任务一运行 CPU 就涨，但是这跟服务的 QPS 是无关的 有些服务可能一运行 CPU 就会立即处于一个高位状态，它可能希望使用别的业务侧指标来进行扩容，而不是 CPU. 因为上述问题存在，使用 CPU 扩缩容，就可能会造成服务频繁的扩容然后缩容，或者无限扩容。而有些服务（如我们的「推荐服务」），对「扩容」和「缩容」都是比较敏感的，每次扩缩都会造成服务可用率抖动。 对这类服务而言，HPA 有这几种调整策略： 选择使用 QPS 等相对比较平滑，没有 GC 这类干扰的指标来进行扩缩容，这需要借助 KEDA 等社区组件。 对 kubernetes 1.18+，可以直接使用 HPA 的 behavior.scaleDown 和 behavior.scaleUp 两个参数，控制每次扩缩容的最多 pod 数量或者比例。 示例如下： yaml --- apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: podinfo namespace: default spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: podinfo minReplicas: 3 maxReplicas: 50 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 # 期望的 CPU 平均值 behavior: scaleUp: stabilizationWindowSeconds: 0 # 默认为 0，只使用当前值进行扩缩容 policies: - periodSeconds: 180 # 每 3 分钟最多扩容 5% 的 Pods type: Percent value: 5 - periodSeconds: 60 # 每分钟最多扩容 1 个 Pod，扩的慢一点主要是为了一个个地预热，避免一次扩容太多未预热的 Pods 导致服务可用率剧烈抖动 type: Pods value: 1 selectPolicy: Min # 选择最小的策略 # 以下的一切配置，都是为了更平滑地缩容 scaleDown: stabilizationWindowSeconds: 600 # 使用过去 10 mins 的最大 cpu 值进行缩容计算，避免过快缩容 policies: - type: Percent # 每 3 mins 最多缩容 `ceil[当前副本数 * 5%]` 个 pod（20 个 pod 以内，一次只缩容 1 个 pod） value: 5 periodSeconds: 180 - type: Pods # 每 1 mins 最多缩容 1 个 pod value: 1 periodSeconds: 60 selectPolicy: Min # 上面的 policies 列表，只生效其中最小的值作为缩容限制（保证平滑缩容） 而对于扩容不够平滑这个问题，可以考虑提供类似 AWS ALB TargetGroup slow_start 的功能，在扩容时缓慢将流量切到新 Pod 上，以实现预热服务（JVM 预热以及本地缓存预热），这样就能达到比较好的平滑扩容效果。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:4","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#42-hpa-扩缩容过于敏感导致-pod-数量震荡"},{"categories":["tech"],"content":" 5. HPA 注意事项注意 kubectl 1.23 以下的版本，默认使用 hpa.v1.autoscaling 来查询 HPA 配置，v2beta2 相关的参数会被编码到 metadata.annotations 中。 比如 behavior 就会被编码到 autoscaling.alpha.kubernetes.io/behavior 这个 key 所对应的值中。 因此如果使用了 v2beta2 的 HPA，一定要明确指定使用 v2beta2 版本的 HPA： shell kubectl get hpa.v2beta2.autoscaling 否则不小心动到 annotations 中编码的某些参数，可能会产生意料之外的效果，甚至直接把控制面搞崩… 比如这个 issue:Nil pointer dereference in KCM after v1 HPA patch request ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:5","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#5-hpa-注意事项"},{"categories":["tech"],"content":" 6. 参考 Pod 水平自动伸缩 - Kubernetes Docs Horizontal Pod Autoscaler演练 - Kubernetes Docs ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:3:6","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#6-参考"},{"categories":["tech"],"content":" 三、节点维护与Pod干扰预算 https://kubernetes.io/zh/docs/tasks/run-application/configure-pdb/ 在我们通过 kubectl drain 将某个节点上的容器驱逐走的时候，kubernetes 会依据 Pod 的「PodDistruptionBuget」来进行 Pod 的驱逐。 如果不设置任何明确的 PodDistruptionBuget，Pod 将会被直接杀死，然后在别的节点重新调度，这可能导致服务中断！ PDB 是一个单独的 CR 自定义资源，示例如下： yaml apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: podinfo-pdb spec: # 如果不满足 PDB，Pod 驱逐将会失败！ minAvailable: 1 # 最少也要维持一个 Pod 可用 # maxUnavailable: 1 # 最大不可用的 Pod 数，与 minAvailable 不能同时配置！二选一 selector: matchLabels: app: podinfo 如果在进行节点维护时(kubectl drain)，Pod 不满足 PDB，drain 将会失败，示例： shell \u003e kubectl drain node-205 --ignore-daemonsets --delete-local-data node/node-205 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-nfhj7, kube-system/kube-proxy-94dz5 evicting pod default/podinfo-7c84d8c94d-h9brq evicting pod default/podinfo-7c84d8c94d-gw6qf error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq pod/podinfo-7c84d8c94d-gw6qf evicted pod/podinfo-7c84d8c94d-h9brq evicted node/node-205 evicted 上面的示例中，podinfo 一共有两个副本，都运行在 node-205 上面。我给它设置了干扰预算 PDBminAvailable: 1。 然后使用 kubectl drain 驱逐 Pod 时，其中一个 Pod 被立即驱逐走了，而另一个 Pod 大概在 15 秒内一直驱逐失败。因为第一个 Pod 还没有在新的节点上启动完成，它不满足干扰预算 PDBminAvailable: 1 这个条件。 大约 15 秒后，最先被驱逐走的 Pod 在新节点上启动完成了，另一个 Pod 满足了 PDB 所以终于也被驱逐了。这才完成了一个节点的 drain 操作。 ClusterAutoscaler 等集群节点伸缩组件，在缩容节点时也会考虑 PodDisruptionBudget. 如果你的集群使用了 ClusterAutoscaler 等动态扩缩容节点的组件，强烈建议设置为所有服务设置 PodDisruptionBudget. 在 PDB 中使用百分比的注意事项在使用百分比时，计算出的实例数都会被向上取整，这会造成两个现象： 如果使用 minAvailable，实例数较少的情况下，可能会导致 ALLOWED DISRUPTIONS 为 0，所有实例都无法被驱逐了。 如果使用 maxUnavailable，因为是向上取整，ALLOWED DISRUPTIONS 的值一定不会低于 1，至少有 1 个实例可以被驱逐。 因此从「便于驱逐」的角度看，如果你的服务至少有 2-3 个实例，建议在 PDB 中使用百分比配置maxUnavailable，而不是 minAvailable. 相对的从「确保服务稳定性」的角度看，我们则应该使用 minAvailable，确保至少有 1 个实例可用。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:4:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-PodDistruptionBuget"},{"categories":["tech"],"content":" 三、节点维护与Pod干扰预算 https://kubernetes.io/zh/docs/tasks/run-application/configure-pdb/ 在我们通过 kubectl drain 将某个节点上的容器驱逐走的时候，kubernetes 会依据 Pod 的「PodDistruptionBuget」来进行 Pod 的驱逐。 如果不设置任何明确的 PodDistruptionBuget，Pod 将会被直接杀死，然后在别的节点重新调度，这可能导致服务中断！ PDB 是一个单独的 CR 自定义资源，示例如下： yaml apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: podinfo-pdb spec: # 如果不满足 PDB，Pod 驱逐将会失败！ minAvailable: 1 # 最少也要维持一个 Pod 可用 # maxUnavailable: 1 # 最大不可用的 Pod 数，与 minAvailable 不能同时配置！二选一 selector: matchLabels: app: podinfo 如果在进行节点维护时(kubectl drain)，Pod 不满足 PDB，drain 将会失败，示例： shell \u003e kubectl drain node-205 --ignore-daemonsets --delete-local-data node/node-205 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-nfhj7, kube-system/kube-proxy-94dz5 evicting pod default/podinfo-7c84d8c94d-h9brq evicting pod default/podinfo-7c84d8c94d-gw6qf error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq error when evicting pod \"podinfo-7c84d8c94d-h9brq\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod default/podinfo-7c84d8c94d-h9brq pod/podinfo-7c84d8c94d-gw6qf evicted pod/podinfo-7c84d8c94d-h9brq evicted node/node-205 evicted 上面的示例中，podinfo 一共有两个副本，都运行在 node-205 上面。我给它设置了干扰预算 PDBminAvailable: 1。 然后使用 kubectl drain 驱逐 Pod 时，其中一个 Pod 被立即驱逐走了，而另一个 Pod 大概在 15 秒内一直驱逐失败。因为第一个 Pod 还没有在新的节点上启动完成，它不满足干扰预算 PDBminAvailable: 1 这个条件。 大约 15 秒后，最先被驱逐走的 Pod 在新节点上启动完成了，另一个 Pod 满足了 PDB 所以终于也被驱逐了。这才完成了一个节点的 drain 操作。 ClusterAutoscaler 等集群节点伸缩组件，在缩容节点时也会考虑 PodDisruptionBudget. 如果你的集群使用了 ClusterAutoscaler 等动态扩缩容节点的组件，强烈建议设置为所有服务设置 PodDisruptionBudget. 在 PDB 中使用百分比的注意事项在使用百分比时，计算出的实例数都会被向上取整，这会造成两个现象： 如果使用 minAvailable，实例数较少的情况下，可能会导致 ALLOWED DISRUPTIONS 为 0，所有实例都无法被驱逐了。 如果使用 maxUnavailable，因为是向上取整，ALLOWED DISRUPTIONS 的值一定不会低于 1，至少有 1 个实例可以被驱逐。 因此从「便于驱逐」的角度看，如果你的服务至少有 2-3 个实例，建议在 PDB 中使用百分比配置maxUnavailable，而不是 minAvailable. 相对的从「确保服务稳定性」的角度看，我们则应该使用 minAvailable，确保至少有 1 个实例可用。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:4:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#在-pdb-中使用百分比的注意事项"},{"categories":["tech"],"content":" 最佳实践 Deployment + HPA + PodDisruptionBudget一般而言，一个服务的每个版本，都应该包含如下三个资源： Deployment: 管理服务自身的 Pods 嘛 HPA: 负责 Pods 的扩缩容，通常使用 CPU 指标进行扩缩容 PodDisruptionBudget(PDB): 建议按照 HPA 的目标值，来设置 PDB. 比如 HPA CPU 目标值为 60%，就可以考虑设置 PDB minAvailable=65%，保证至少有 65% 的 Pod 可用。这样理论上极限情况下 QPS 均摊到剩下 65% 的 Pods 上也不会造成雪崩（这里假设 QPS 和 CPU 是完全的线性关系） ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:4:1","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#最佳实践-deployment--hpa--poddisruptionbudget"},{"categories":["tech"],"content":" 四、节点亲和性与节点组我们一个集群，通常会使用不同的标签为节点组进行分类，比如 kubernetes 自动生成的一些节点标签： kubernetes.io/os: 通常都用 linux kubernetes.io/arch: amd64, arm64 topology.kubernetes.io/region 和 topology.kubernetes.io/zone: 云服务的区域及可用区 我们使用得比较多的，是「节点亲和性」以及「Pod 反亲和性」，另外两个策略视情况使用。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:5:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-affinity"},{"categories":["tech"],"content":" 1. 节点亲和性如果你使用的是 aws，那 aws 有一些自定义的节点标签： eks.amazonaws.com/nodegroup: aws eks 节点组的名称，同一个节点组使用同样的 aws ec2 实例模板 比如 arm64 节点组、amd64/x64 节点组 内存比例高的节点组如 m 系实例，计算性能高的节点组如 c 系列 竞价实例节点组：这个省钱啊，但是动态性很高，随时可能被回收 按量付费节点组：这类实例贵，但是稳定。 假设你希望优先选择竞价实例跑你的 Pod，如果竞价实例暂时跑满了，就选择按量付费实例。那nodeSelector 就满足不了你的需求了，你需要使用 nodeAffinity，示例如下: yaml apiVersion: apps/v1 kind: Deployment metadata: name: xxx namespace: xxx spec: # ... template: # ... spec: affinity: nodeAffinity: # 优先选择 spot-group-c 的节点 preferredDuringSchedulingIgnoredDuringExecution: - preference: matchExpressions: - key: eks.amazonaws.com/nodegroup operator: In values: - spot-group-c weight: 80 # weight 用于为节点评分，会优先选择评分最高的节点 - preference: matchExpressions: # 优先选择 aws c6i 的机器 - key: node.kubernetes.io/instance-type operator: In values: - \"c6i.xlarge\" - \"c6i.2xlarge\" - \"c6i.4xlarge\" - \"c6i.8xlarge\" weight: 70 - preference: matchExpressions: # 其次选择 aws c5 的机器 - key: node.kubernetes.io/instance-type operator: In values: - \"c5.xlarge\" - \"c5.2xlarge\" - \"c5.4xlarge\" - \"c5.9xlarge\" weight: 60 # 如果没 spot-group-c 可用，也可选择 ondemand-group-c 的节点跑 requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: eks.amazonaws.com/nodegroup operator: In values: - spot-group-c - ondemand-group-c containers: # ... ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:5:1","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#1-节点亲和性"},{"categories":["tech"],"content":" 2. Pod 反亲和性 Pod 亲和性与反亲和性可能不是最佳的实现手段，这部分内容待更新 相关 Issue: https://github.com/kubernetes/kubernetes/issues/72479 相关替代方案：https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/ 通常建议为每个 Deployment 的 template 配置 Pod 反亲和性，把 Pods 打散在所有节点上： yaml apiVersion: apps/v1 kind: Deployment metadata: name: xxx namespace: xxx spec: # ... template: # ... spec: replicas: 3 affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: # 非强制性条件 - weight: 100 # weight 用于为节点评分，会优先选择评分最高的节点 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - xxx - key: version operator: In values: - v12 # 将 pod 尽量打散在多个可用区 topologyKey: topology.kubernetes.io/zone requiredDuringSchedulingIgnoredDuringExecution: # 强制性要求 # 注意这个没有 weights，必须满足列表中的所有条件 - labelSelector: matchExpressions: - key: app operator: In values: - xxx - key: version operator: In values: - v12 # Pod 必须运行在不同的节点上 topologyKey: kubernetes.io/hostname ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:5:2","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#2-pod-反亲和性"},{"categories":["tech"],"content":" 五、Pod 的就绪探针、存活探针与启动探针Pod 提供如下三种探针，均支持使用 Command、HTTP API、TCP Socket 这三种手段来进行服务可用性探测。 startupProbe 启动探针（Kubernetes v1.18 [beta]）: 此探针通过后，「就绪探针」与「存活探针」才会进行存活性与就绪检查 用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉 startupProbe 显然比 livenessProbe 的 initialDelaySeconds 参数更灵活。 同时它也能延迟 readinessProbe 的生效时间，这主要是为了避免无意义的探测。容器都还没 startUp，显然是不可能就绪的。 程序将最多有 failureThreshold * periodSeconds 的时间用于启动，比如设置failureThreshold=20、periodSeconds=5，程序启动时间最长就为 100s，如果超过 100s 仍然未通过「启动探测」，容器会被杀死。 readinessProbe 就绪探针: 就绪探针失败次数超过 failureThreshold 限制（默认三次），服务将被暂时从 Service 的 Endpoints 中踢出，直到服务再次满足 successThreshold. livenessProbe 存活探针: 检测服务是否存活，它可以捕捉到死锁等情况，及时杀死这种容器。 存活探针失败可能的原因： 服务发生死锁，对所有请求均无响应 服务线程全部卡在对外部 redis/mysql 等外部依赖的等待中，导致请求无响应 存活探针失败次数超过 failureThreshold 限制（默认三次），容器将被杀死，随后根据重启策略执行重启。 kubectl describe pod 会显示重启原因为State.Last State.Reason = Error, Exit Code=137，同时 Events 中会有Liveness probe failed: ... 这样的描述。 上述三类探测器的参数都是通用的，五个时间相关的参数列举如下： yaml # 下面的值就是 k8s 的默认值 initialDelaySeconds: 0 # 默认没有 delay 时间 periodSeconds: 10 timeoutSeconds: 1 failureThreshold: 3 successThreshold: 1 示例： yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app-v3 spec: # ... template: # ... spec: containers: - name: my-app-v3 image: xxx.com/app/my-app:v3 imagePullPolicy: IfNotPresent # ... 省略若干配置 startupProbe: httpGet: path: /actuator/health # 直接使用健康检查接口即可 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 20 # 最多提供给服务 5s * 20 的启动时间 successThreshold: 1 livenessProbe: httpGet: path: /actuator/health # spring 的通用健康检查路径 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 # Readiness probes are very important for a RollingUpdate to work properly, readinessProbe: httpGet: path: /actuator/health # 简单起见可直接使用 livenessProbe 相同的接口，当然也可额外定义 port: 8080 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 在 Kubernetes 1.18 之前，通用的手段是为「就绪探针」添加较长的 initialDelaySeconds 来实现类似「启动探针」的功能动，避免容器因为启动太慢，存活探针失败导致容器被重启。示例如下： yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app-v3 spec: # ... template: # ... spec: containers: - name: my-app-v3 image: xxx.com/app/my-app:v3 imagePullPolicy: IfNotPresent # ... 省略若干配置 livenessProbe: httpGet: path: /actuator/health # spring 的通用健康检查路径 port: 8080 initialDelaySeconds: 120 # 前两分钟，都假设服务健康，避免 livenessProbe 失败导致服务重启 periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 # 容器一启动，Readiness probes 就会不断进行检测 readinessProbe: httpGet: path: /actuator/health port: 8080 initialDelaySeconds: 3 # readiness probe 不需要设太长时间，使 Pod 尽快加入到 Endpoints. periodSeconds: 5 timeoutSeconds: 1 failureThreshold: 5 successThreshold: 1 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:6:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-container-probe"},{"categories":["tech"],"content":" 六、Pod 安全这里只介绍 Pod 中安全相关的参数，其他诸如集群全局的安全策略，不在这里讨论。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:7:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#k8s-pod-security"},{"categories":["tech"],"content":" 1. Pod SecurityContext https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ 通过设置 Pod 的 SecurityContext，可以为每个 Pod 设置特定的安全策略。 SecurityContext 有两种类型： spec.securityContext: 这是一个PodSecurityContext 对象 顾名思义，它对 Pod 中的所有 containers 都有效。 spec.containers[*].securityContext: 这是一个SecurityContext 对象 container 私有的 SecurityContext 这两个 SecurityContext 的参数只有部分重叠，重叠的部分 spec.containers[*].securityContext 优先级更高。 我们比较常遇到的一些提升权限的安全策略： 特权容器：spec.containers[*].securityContext.privileged 添加（Capabilities）可选的系统级能力:spec.containers[*].securityContext.capabilities.add 只有 ntp 同步服务等少数容器，可以开启这项功能。请注意这非常危险。 Sysctls: 系统参数: spec.securityContext.sysctls 权限限制相关的安全策略有（强烈建议在所有 Pod 上按需配置如下安全策略！）： spec.volumes: 所有的数据卷都可以设定读写权限 spec.securityContext.runAsNonRoot: true Pod 必须以非 root 用户运行 spec.containers[*].securityContext.readOnlyRootFileSystem:true 将容器层设为只读，防止容器文件被篡改。 如果微服务需要读写文件，建议额外挂载 emptydir 类型的数据卷。 spec.containers[*].securityContext.allowPrivilegeEscalation: false 不允许 Pod 做任何权限提升！ spec.containers[*].securityContext.capabilities.drop: 移除（Capabilities）可选的系统级能力 还有其他诸如指定容器的运行用户(user)/用户组(group)等功能未列出，请自行查阅 Kubernetes 相关文档。 一个无状态的微服务 Pod 配置举例： yaml apiVersion: v1 kind: Pod metadata: name: \u003cPod name\u003e spec: containers: - name: \u003ccontainer name\u003e image: \u003cimage\u003e imagePullPolicy: IfNotPresent # ......此处省略 500 字 securityContext: readOnlyRootFilesystem: true # 将容器层设为只读，防止容器文件被篡改。 allowPrivilegeEscalation: false # 禁止 Pod 做任何权限提升 capabilities: drop: # 禁止容器使用 raw 套接字，通常只有 hacker 才会用到 raw 套接字。 # raw_socket 可自定义网络层数据，避开 tcp/udp 协议栈，直接操作底层的 ip/icmp 数据包。可实现 ip 伪装、自定义协议等功能。 # 去掉 net_raw 会导致 tcpdump 无法使用，无法进行容器内抓包。需要抓包时可临时去除这项配置 - NET_RAW # 更好的选择：直接禁用所有 capabilities # - ALL securityContext: # runAsUser: 1000 # 设定用户 # runAsGroup: 1000 # 设定用户组 runAsNonRoot: true # Pod 必须以非 root 用户运行 seccompProfile: # security compute mode type: RuntimeDefault ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:7:1","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#1-pod-securitycontext"},{"categories":["tech"],"content":" 2. seccomp: security compute modeseccomp 和 seccomp-bpf 允许对系统调用进行过滤，可以防止用户的二进制文对主机操作系统件执行通常情况下并不需要的危险操作。它和 Falco 有些类似，不过 Seccomp 没有为容器提供特别的支持。 视频: Seccomp: What Can It Do For You? - Justin Cormack, Docker ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:7:2","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#2-seccomp-security-compute-mode"},{"categories":["tech"],"content":" 七、业务服务的权限管控 基于 K8s RBAC 给予业务服务合适的云资源访问权限 给每个业务服务自动化地创建各自的云服务商 Role（如 AWS IAM Role）以及 Service Account，实现细粒度的权限分配。 有条件可以上 vault 之类的工具，管理数据库密码等敏感信息 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:8:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#七业务服务的权限管控"},{"categories":["tech"],"content":" 八、隔离性这个我的了解暂时有限，不过有几个建议应该是值得参考的： 推荐按业务线或者业务团队进行名字空间划分，方便对每个业务线/业务团队分别进行资源限制 推荐使用 network policy 对服务实施强力的网络管控，避免长期发展过程中，业务服务之间出现混乱的跨业务线相互调用关系，也避免服务被黑后，往未知地址发送数据。 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:9:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#八隔离性"},{"categories":["tech"],"content":" 其他问题 不同节点类型的性能有差距，导致 QPS 均衡的情况下，CPU 负载不均衡 解决办法（未验证）： 尽量使用性能相同的实例类型：通过 podAffinity 及 nodeAffinity 添加节点类型的亲和性 ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:10:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#其他问题"},{"categories":["tech"],"content":" 参考 istio 实践指南 - imroc.cc Kubernetes 实践指南 - imroc.cc ","date":"2022-01-25","objectID":"/posts/kubernetes-best-practices/:11:0","series":["云原生相关"],"tags":["Kubernetes","最佳实践","云原生","Cloud-Native"],"title":"Kubernetes 微服务最佳实践","uri":"/posts/kubernetes-best-practices/#参考-1"},{"categories":["life","tech"],"content":" 更新：2022/1/22 ","date":"2022-01-03","objectID":"/posts/2021-summary/:0:0","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#"},{"categories":["life","tech"],"content":" 闲言碎语一晃一年又是过去了，这个新年，全球疫情再创新高，圣诞节后美国单日新增更是直接突破 50 万直逼 60 万大关❌ 100 万✅，国内也有西安管理不力导致民众忍饥挨饿。 新冠已经两年多了啊。 言归正传，我今年年初从 W 公司离职后，非常幸运地进了现在的公司——大宇无限，在融入大宇的过程中也是五味杂陈。不过总体结果还是挺满意的，目前工作已经步入正轨，也发现了非常多的机会，大宇的基础设施领域仍然大有可为。 一些重要事情还是没怎么想通，不过毕竟风口上的猪都能飞，今年小小努力了一把，大部分时间仍然随波逐流，却也渐入佳境。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:1:0","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#闲言碎语"},{"categories":["life","tech"],"content":" 生活 1 月的时候从博客园迁移到这个独立博客，还认识了 @芝士，芝士帮我调整好了博客「友链」页面的样式，超级感谢~ 2 月的时候从 W 公司离职，然后怎么说呢，瞬间感觉海阔天空，心态 180 度转变，好得不得了，但是其实也很担心自己各方面的不足。总之心里有好多的想法，跟@是格子啊、@芝士 以及前同事聊了好多，非常感谢这几位朋友跟同事帮我梳理思路，给我肯定。也是这个时间点，我被@芝士 拉进了中文 twitter 的圈子。 过年响应号召没回家（其实是嫌核酸检测麻烦，家里也建议先别回），每天爬爬山看看风景，买了个吊床去公园午睡，练习口琴竹笛，就这样玩了一个月。 到了 3 月份的时候我开始找工作，面了几家公司后，非常幸运地进入了大宇无限，成为了一名 SRE 萌新。在大宇一年的感受，就放在后面的「工作」部分写了，这里先略过。 加入大宇后，全年都有定期的团建，跟 SRE 小伙伴公款吃喝，我 2021 年下馆子次数估计是上一年的七八倍 3 月底，看了电影——《寻龙传说》（2021 年看的唯一一部电影），片尾曲超好听。 4 月份，各种巧合下，意外发现初中同学住得离我 1km 不到，在他家吃了顿家乡菜，还有杨梅酒， 味道非常棒！还有回甘强烈的城步青钱柳茶，让我念念不忘。 8 月份，堂弟来深圳暑期实习，跟两个堂弟一起穿越深圳东西冲海岸线，风景非常棒，不过路上也是又热又渴 10 月份 加入了大宇的冲浪小分队，第一次冲浪、海边烧烤 买了双轮滑鞋，学会了倒滑、压步转向，复习了大学时学过的若干基础技巧 12 月，买了台云米泉先净饮机后，有了随时随地的矿物质热水，就想起了 4 月份在初中同学家喝过的青钱柳，然后就喝茶上瘾了，一桌子的滇红、祁门红茶、安吉白茶、黄山毛峰、青钱柳、莓茶、梅子菁…目前感觉滇红跟祁门红茶最好喝，安吉白茶跟黄山毛峰都非常清香，青钱柳回甘最强烈，莓茶怎么说呢味道感觉不太好（也可能是泡的手法不对？） 我的云米净饮机 桌面上的各种茶叶 2022 年 1 月，第一次买动漫手办，妆点后感觉房间都增色不少~ 我的房间-挂画-手办 ","date":"2022-01-03","objectID":"/posts/2021-summary/:2:0","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#生活"},{"categories":["life","tech"],"content":" 读书 年初辞职后游山玩水，心思稍微安定了些，看了大半本《走出荒野》。 6 月份社区组织打新冠疫苗时，在等候室看了本《青春驿站——深圳打工妹写真》，讲述八九十年代打工妹的生活。很真实，感情很细腻。 年末二爷爷去世，参加完葬礼后，心态有些变化，看完了大一时买下的《月宫 Moon Palace》，讲述主角的悲剧人生。 其余大部分业余时间，无聊，又不想学点东西，也不想运动，于是看了非常多的网络小说打发时间。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:3:0","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#读书"},{"categories":["life","tech"],"content":" 音乐年初辞职后，练了一段时间的竹笛跟蓝调口琴，但后来找到工作后就基本沉寂了。 总的来说还是原地踏步吧。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:4:0","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#音乐"},{"categories":["life","tech"],"content":" 工作 - 我在大宇无限的这一年3 月份刚进大宇的我充满好奇，但也小心谨慎，甚至有点不敢相信自己能进到一家这么棒的公司，感觉自己运气爆棚。毕竟大宇无论是同事水平还是工作氛围，亦或是用户体量，相比我上家公司都是质的差别。 我在大宇的第一个工位 之后慢慢熟悉工作的内容与方法，leader 尽力把最匹配我兴趣的工作安排给我，帮我排疑解难，同时又给我极大的自主性，真的是棒极了。 然而自主性高带来的也是更高的工作难度，遇到困难时也曾手忙脚乱、迷茫、甚至自我怀疑，很担心是不是隔天就得跑路了… 但好在我终究还是能调节好心态，负起责任，一步步把工作完成。中间有几次工作有延误时，leader 还陪我加班，事情干完后又带我去吃大餐犒劳自己，真的超级感谢他的帮助与支持。 换座位后的新工位，落地窗风景很棒 这样经历了几个项目的洗礼后，现在我终于能说自己是脚踏实地了，心态从「明天是不是得提桶跑路」转变成了「哇还有这个可以搞，那个 ROI 也很高，有好多有趣的事可以做啊」，我终于能说自己真正融入了大宇无限这家公司，成为了它的一员。 回看下了 2020 年的总结与展望，今年实际的进步，跟去年期望的差别很大。最初的目标大概只实现了 10%，但是接触到了许多意料之外的东西，总体还是满意的： 熟悉了新公司的文化与工作方式，这感觉是个很大的收获，我的工作方式有了很大的改善 接触并且熟悉了新公司的 AWS 线上环境 负责维护线上 Kubernetes 管理平台，第一次接触到的线上集群峰值 QPS 就有好几万。从一开始的小心翼翼，到现在也转变成了老手，这算是意义重大吧 使用 python 写了几个 Kubernetes 管理平台的服务，这也是我第一次写线上服务，很有些成就感 花了比预期多三倍的时间，还加了好几次班才把问题解决。因此对 leader 的感叹印象深刻：虽然你早就强调过你写代码比较慢，但是没想到会这么慢… 下半年在 AWS 成本的分析与管控上花了很多精力，也有了一些不错的成果，受益匪浅 学会了 Nginx 的简单使用，刚好够用于维护公司先有的 Nginx 代理配置 主导完成了「新建 K8s 集群，将服务迁移到新集群」。虽然并不是一件很难的事，但这应该算是我 2021 年最大的成就了。 升级过程中也是遇到了各种问题，第一次升级迁移时我准备了好久，慌的不行，结果升级时部分服务还是出了问题，当时脑子真的是个懵的，跟 leader 搞到半夜 1 点多后还是没解决，回退到了旧集群，升级失败。 升级失败后有一段时间心灰意冷，非常丧气，都不敢面对各位同事，感觉这事情我是搞不定要提桶跑路了。不过后面还是调整好了心情，毕竟事情到手上了肯定还是要做。 之后通过各种测试分析，确认到是某个服务扩缩容震荡导致可用率无法恢复，尝试通过 HPA 的 behavior 来控制扩缩容速率，又意外触发了 K8s HPA 的 bug 把集群控制面搞崩了… 再之后把问题都确认了，第二次尝试升级，又是有个别服务可用率抖动，调试了好几天。那几天神经一直紧绷，每天早上都是被服务可用率的告警吵醒的。 跨年的那天晚上业务量上涨，业务侧指标又开始抖动，业务侧的同事领导压力也很大，建议我把服务回滚到旧集群。我跟业务同事说再试一次参数调整（改 HPA 的 behavior 参数限制扩缩容速率），改完这个参数如果还有问题，我就立即回滚。然后我就在观察服务可用率的过程中跨年了， 可用率的每一次波动都让我提心吊胆，不过终于还是基本稳定了。这样才终于完成了 K8s 集群的升级，期间各位同事也有参与帮忙分析排查各种问题，领导跟业务侧同事也足够信任我（业务侧也跟我一样忍受了好多天的连续告警），非常感谢他们，还有努力的我自己。 随便写了几个 go 的 demo，基本没啥进步 学了一个星期的 rust 语言，快速看完了 the book，用 rust 重写了个 video2chars 学习了 Linux 容器的底层原理：cgroups/namespace 技术，并且用 go/rust 实现了个 demo 学习了 Linux 的各种网络接口、Iptables 熟悉了 PromQL/Grafana，现在也能拷贝些 PromQL 查各种数据了 如果要给自己打分的话，那就是「良好」吧。因为并没有很强的进取心，所以出来的结果也并不能称之为「优秀」。 顺便公司的新办公区真的超赞，详情见我的 twitter： 新办公区真好呐～ 值此良辰美景，好想整个榻榻米坐垫，坐在角落的落地窗边工作🤣 那种使用公共设施工（mo）作（yu）的乐趣，以及平常工位见不到的景色交相辉映，是不太好表述的奇妙体验 pic.twitter.com/FASffzw8N3 — ryan4yin | 於清樂 (@ryan4yin) January 17, 2022 ","date":"2022-01-03","objectID":"/posts/2021-summary/:5:0","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#工作---我在大宇无限的这一年"},{"categories":["life","tech"],"content":" 技术方面的感受 Istio 服务网格：体会到了它有点重，而且它的发展跟我们的需求不一定匹配 Sidecar 模式的成本比较高，在未调优的情况下，它会给服务带来 1/3 到 1/4 的成本提升，以及延迟上升 比如切量权重固定为 100（新版本将会放宽限制），不支持 pod 的 warm up（社区已经有 PR，持续观望吧） 而它重点发展的虚拟机支持我们却完全不需要 一直在思考是持续往 Istio 投入，还是换其他的方案 服务网格仍然在快速发展，未来的趋势应该是 eBPF + Envoy + WASM Cilium 推出的基于 eBPF 的 Service Mesh 是一个新趋势（它使用高级特性时会退化成 Per Node Proxy 模式），成本、延迟方面都有望吊打 Sidecar 模式的其他服务网格，是今年服务网格领域的大新闻。 我们曾尝试使用中心化网关来替代 Sidecar 以降低成本。但是跨区流量成本、HTTP/gRPC 多协议共存，这些都是挑战。而且这也并不是社区的最佳实践，现在我觉得维持 Sidecar 其实反而能提升资源利用率，我们的集群资源利用率目前很低。如果能把控好，这部分成本或许是可以接受的。 K8s 集群的日志方面，我们目前是使用自研的基于 gelf 协议的系统，但是问题挺多的 从提升系统的可维护性、易用性等角度来说，loki 是值得探索下的 K8s 集群管理方面，觉得集群的升级迭代，可以做得更自动化、更可靠。明年可以在多集群管理这个方向上多探索下。 Pod 服务质量： 对非核心服务，可以适当调低 requests 的资源量，而不是完全预留(Guaranteed)，以提升资源利用率。 官方的 HPA 能力是不够用的，业务侧可能会需要基于 QPS/Queue 或者业务侧的其他参数来进行扩缩容 推广基于 KEDA 的扩缩容能力 关注Container resource metrics 的进展 成本控制方面，体会到了 ARM 架构以及 Spot 竞价实例的好处 2022-02-17 更新：数据库等中间件可以切换到 ARM。EKS 服务目前都是 Spot 实例，它的 ARM 化 ROI 并不高。 跨区流量成本有很大的潜在优化空间 跨区流量成本是进出该可用区都会收费，而且不仅涉及 Kubernetes 集群内服务间的调用，还会涉及对 RDS/ES/ElastiCache/EC2 等其他资源的调用。 今年各云厂商故障频发，没有跨 region 的服务迁移就会很难受，需要持续关注下karmada 这类多集群管理方案。 Google 账号系统宕机 Fastly CDN 故障 Facebook 故障 AWS 更是各种可用区故障，12/7 的故障导致 AWS 大部分服务都崩了。因此我们 SRE 今年经常是救各种大火小火… Rust/Go/WASM 蓬勃发展，未来可期。 AI 落地到各个领域，影响到了我们日常使用的语音导航、歌声合成、语音合成等多个领域，当然也包括与 SRE 工作相关的场景：AIOps ","date":"2022-01-03","objectID":"/posts/2021-summary/:6:0","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#技术方面的感受"},{"categories":["life","tech"],"content":" 2022 年的展望","date":"2022-01-03","objectID":"/posts/2021-summary/:7:0","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#2022-年的展望"},{"categories":["life","tech"],"content":" 技术侧今年的展望写得更聚焦一些，争取能实现 50%，就是很大的突破了。 重点仍然是网络技术与 Kubernetes 技术，Redis/Search/Database 等技术还得靠后排，或许明年吧哈哈。 熟练掌握 Go 语言，并分别用于至少两个项目中 打铁还需自身硬，编码能力是基础中的基础 Kubernetes 相关 以 kubebuilder 为代表的 k8s 开发、拓展技术 阅读 k8s 及相关生态的源码，了解其实现逻辑 网络技术 服务网格 Istio 代理工具 Envoy/APISIX 网络插件 Cilium + eBPF AWS K8s 成本与服务稳定性优化 通过拓扑感知的请求转发，节约跨可用区/跨域的流量成本 K8s 新特性：Topology Aware Hints Istio:Locality Load Balancing 推广 gRPC 协议 通过亲和性与反亲和性 +descheduler，实现合理调度 Pods 减少跨域流量、也提升服务容灾能力 提升本地开发效率： nocalhost 多集群的应用部署、容灾 karmada 探索新技术与可能性（优先级低） 基于 Kubernetes 的服务平台，未来的发展方向 kubevela buildpack 是否应该推进 gitops openkruise Serverless 平台的进展 Knative OpenFunction 机器学习、深度学习技术：想尝试下将 AI 应用在音乐、语音、SRE 等我感兴趣的领域，即使是调包也行啊，总之想出点成果… 可以预料到明年 SRE 团队有超多的机会，这其中我具体能负责哪些部分，又能做出怎样的成果，真的相当期待~ ","date":"2022-01-03","objectID":"/posts/2021-summary/:7:1","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#技术侧"},{"categories":["life","tech"],"content":" 生活侧 运动： 把轮滑练好，学会点花样吧，每个月至少两次。 进行三次以上的次短途旅行，东西冲穿越可以再来一次。 音乐： 再一次学习乐理… midi 键盘买了一直吃灰，多多练习吧 买了个 Synthesizer V Stduio Pro + 「青溯 AI」，新的一年想学下调教，翻唱些自己喜欢的歌。 阅读：清单如下，一个月至少读完其中一本。 文学类： 《人间失格》：久仰大名的一本书，曾经有同学力荐，但是一直没看。 《生命最后的读书会》：或许曾经看过，但是一点印象都没了 《百年孤独》：高中的时候读过一遍，但是都忘差不多了 《霍乱时期的爱情》 《苏菲的世界》：据说是哲学启蒙读物，曾经看过，但是对内容完全没印象了。 《你一生的故事》：我也曾是个科幻迷 《沈从文的后半生》 《我与地坛》 《将饮茶》 《吾国与吾民 - 林语堂》 《房思琪的初恋乐园》 人文社科 《在生命的尽头拥抱你-临终关怀医生手记》：今年想更多地了解下「死亡」 《怎样征服美丽少女》：哈哈 《爱的艺术》 《社会心理学》 《被讨厌的勇气》 《人体简史》 《科学革命的结构》 《邓小平时代》 《论中国》 《刘擎西方现代思想讲义》 《时间的秩序》 《极简宇宙史》 《圆圈正义-作为自由前提的信念》 《人生脚本》 技术类 《复杂》 《SRE - Google 运维解密》 《凤凰项目：一个 IT 运维的传奇故事》 《人月神话》 《绩效使能：超越 OKR》 《奈飞文化手册》 《幕后产品-打造突破式思维》 《深入 Linux 内核架构》 《Linux/UNIX 系统编程手册》 《重构 - 改善既有代码的设计》 《网络是怎样连接的》：曾经学习过《计算机网络：自顶向下方法》，不过只学到网络层。就从这本书开始重新学习吧。 ","date":"2022-01-03","objectID":"/posts/2021-summary/:7:2","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#生活侧"},{"categories":["life","tech"],"content":" 结语2021 年初朋友与我给自己的期许是「拆破玉笼飞彩凤，顿开金锁走蛟龙」，感觉确实应验了。 今年我希望不论是在生活上还是在工作上，都能「更上一层楼」~ 更多有趣的、有深度的 2021 年度总结：https://github.com/saveweb/review-2021 ","date":"2022-01-03","objectID":"/posts/2021-summary/:8:0","series":["年终总结"],"tags":["总结"],"title":"2021 年年终总结","uri":"/posts/2021-summary/#结语"},{"categories":["life"],"content":"大雪，沙雪。 到晴岚桥等送葬队伍时，非常冷。 转头一望，发现送葬的几位师傅在渠渡庙门口就地取材生起了火堆取暖。这样寒冷的天气下，很有种惊喜的感觉。 送葬路上又是风又是雪，像是老天也在哀伤。辉辉说这还是他第一次在风雪天里送葬，我也有同感。到了山上，雨伞上已经结了薄薄一层冰，老爸跟老妈衣服也冻上了冰晶，辉辉更是头发都冻上了。风雪之中，二爷爷被葬在我家后山。 我们就这样送走了二爷爷。 这次的送葬，对我而言像是一个仪式——多年前的高考备战、以及后来的异乡求学，使我失去了一些一生只有一次的告别机会，我在尝试弥补这些曾经的遗憾。 事情都办完后，我到洞口赶高铁，结果不论是高铁还是火车都晚点，就连只隔一个站的 K809 都晚点 99 分钟。漫长的等车时间里，我又看起了《月宫》这本小说。这是我刚上大学时买的书，因为看到说主角想把自己逼到极限，这引起了当时苦行僧般的我的共鸣，于是就想买来读一读，但我始终没有看完它，因为越读内心就越压抑。 不知道该如何描述这种心态的变化，我意识到我现在终于能够沉下心去读这本书了。 我边读边回忆多年前读过的故事情节。在记起是 Kitty 救了自我放逐中的 Fogg，并且重新获得希望之后，我发觉自己目前的状态可能有些问题。业余时间沉迷在自我中心的网络小说中，其他时间只关注技术，人就渐渐变得跟人脱节。 终于上了高铁，在车上我同样用《月宫》打发时间。在晚点两个半小时后，一点半，到达了深圳北，这时候我刚好看到书中 Kitty 对 Fogg 说：「已经太晚了，我不能再一次冒险。再见，请你好好对自己。」心里突然就空落落的，我意识到这是一个彻头彻尾的悲剧，我居然想在悲剧中期许一个美好的转折，真的是有些妄想了。下了车，站在站台上，眼泪就涌了出来。为书中的悲剧哭泣，也再一次意识到，那些记忆中满脸皱纹的身影，是真的永别了。 从 2015 年 11 月到 2021 年 12 月 27 日的凌晨，二爷爷下葬的翌日，我借着火车站路边昏黄的灯光，看完了保罗·奥斯特的《月宫》。 ","date":"2021-12-27","objectID":"/posts/moon-palace/:0:0","series":null,"tags":["生死"],"title":"月宫","uri":"/posts/moon-palace/#"},{"categories":["life"],"content":" 已过了立冬，却没想象中的那么冷。 忽闻堂弟打算去河南，而且后天就走。 一瞬间感觉生活有点梦幻，惶惶然又脱离了掌控。 又想到今年找到的新工作，梦幻般的待遇，不限量的三餐供应，窗明几净的落地窗工位，这一切都像是在做梦。 即使如此，我一边担心自己工作搞不定要提桶跑路，一边却又还不满足。 浮生若梦，为欢几何？ 嘿，又想要喝点酒了，梦里或许有好酒呢。 恍惚间，又回到了那年大二开学，我拖着个旧皮箱，在凌晨薄雾的校园里走着，耳边只有皮箱轮子的滚动声和几声鸟鸣。耳机里放着一首《遥远的歌》。 ","date":"2021-11-16","objectID":"/posts/life-is-just-like-a-dream/:0:0","series":null,"tags":[],"title":"浮生若梦，为欢几何？","uri":"/posts/life-is-just-like-a-dream/#"},{"categories":["音乐","life"],"content":" 「此岸弃草，彼岸繁花。」取自前永动机主唱「河津樱/白金」的个人简介 今天想推几首歌 emmmm 音乐插件出了点毛病，直接上链接了： https://music.163.com/#/playlist?id=901077788 ","date":"2021-08-28","objectID":"/posts/weeds-on-this-side-flowers-on-the-other/:0:0","series":null,"tags":["后摇"],"title":"此岸弃草，彼岸繁花","uri":"/posts/weeds-on-this-side-flowers-on-the-other/#"},{"categories":["tech"],"content":" 本文仅针对 ipv4 网络 本文先介绍 iptables 的基本概念及常用命令，然后分析 docker/podman 是如何利用 iptables 和 Linux 虚拟网络接口实现的单机容器网络。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:0:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#"},{"categories":["tech"],"content":" 一、iptablesiptables 提供了包过滤、NAT 以及其他的包处理能力，iptables 应用最多的两个场景是 firewall 和 NAT iptables 及新的 nftables 都是基于 netfilter 开发的，是 netfilter 的子项目。 但是 eBPF 社区目前正在开发旨在取代 netfilter 的新项目 bpfilter，他们的目标之一是兼容 iptables/nftables 规则，让我们拭目以待吧。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#一iptables"},{"categories":["tech"],"content":" 1. iptables 基础概念 - 四表五链 实际上还有张 SELinux 相关的 security 表（应该是较新的内核新增的，但是不清楚是哪个版本加的），但是我基本没接触过，就略过了。 这里只对 iptables 做简短介绍，详细的教程参见iptables 详解（1）：iptables 概念 - 朱双印，这篇文章写得非常棒！把 iptables 讲清楚了。 默认情况下，iptables 提供了四张表（不考虑 security 的话）和五条链，数据在这四表五链中的处理流程如下图所示： 在这里的介绍中，可以先忽略掉图中 link layer 层的链路，它属于 ebtables 的范畴。另外conntrack 也暂时忽略，在下一小节会详细介绍 conntrack 的功能。 netfilter 数据包处理流程，来自 wikipedia 对照上图，对于发送到某个用户层程序的数据而言，流量顺序如下： 首先进入 PREROUTING 链，依次经过这三个表： raw -\u003e mangle -\u003e nat 然后通过路由决策，发现目标 IP 为本机地址，于是进入 INPUT 链，这个链上也有三个表，处理顺序是：mangle -\u003e nat -\u003e filter 过了 INPUT 链后，数据才会进入内核协议栈，最终到达用户层程序。 用户层程序发出的报文，则依次经过这几个表：OUTPUT -\u003e POSTROUTING 在路由决策时，如果目标 IP 不是本机，就得看内核是否开启了 ip_forward 功能，如果没开启数据包就扔掉了。如果开了转发，就会进入 FORWARD 链处理，然后直接进入 POSTROUTING 链，也就是说这类流量不会过 INPUT 链！ 从图中也很容易看出，如果数据 dst ip 不是本机任一接口的 ip，那它通过的几个链依次是：PREROUTEING -\u003e FORWARD -\u003e POSTROUTING 五链的功能和名称完全一致，应该很容易理解。除了默认的五条链外，用户也可以创建自定义的链，自定义的链需要被默认链引用才能生效，我们后面要介绍的 Docker 实际上就定义了好几条自定义链。 除了「链」外，iptables 还有「表」的概念，四个表的优先级顺序如下： raw: 对收到的数据包在连接跟踪前进行处理。一般用不到，可以忽略 一旦用户使用了 raw 表，raw 表处理完后，将跳过 nat 表和 ip_conntrack 处理，即不再做地址转换和数据包的链接跟踪处理了 mangle: 用于修改报文、给报文打标签，用得也较少。 nat: 主要用于做网络地址转换，SNAT 或者 DNAT filter: 主要用于过滤数据包 数据在按优先级经过四个表的处理时，一旦在某个表中匹配到一条规则 A,下一条处理规则就由规则 A 的 target 参数指定，后续的所有表都会被忽略。target 有如下几种类型： ACCEPT: 直接允许数据包通过 DROP: 直接丢弃数据包，对程序而言就是 100% 丢包 REJECT: 丢弃数据包，但是会给程序返回 RESET。这个对程序更友好，但是存在安全隐患，通常不使用。 MASQUERADE: （伪装）将 src ip 改写为网卡 ip，和 SNAT 的区别是它会自动读取网卡 ip。路由设备必备。 SNAT/DNAT: 顾名思义，做网络地址转换 REDIRECT: 在本机做端口映射 LOG: 在 /var/log/messages 文件中记录日志信息，然后将数据包传递给下一条规则，也就是说除了记录以外不对数据包做任何其他操作，仍然让下一条规则去匹配。 只有这个 target 特殊一些，匹配它的数据仍然可以匹配后续规则，不会直接跳过。 其他自定义链的名称：表示将数据包交给该链进行下一步处理。 RETURN: 如果是在子链（自定义链）遇到 RETURN，则返回父链的下一条规则继续进行条件的比较。如果是在默认链 RETURN 则直接使用默认的动作（ACCEPT/DROP） 其他类型，可以用到的时候再查 理解了上面这张图，以及四个表的用途，就很容易理解 iptables 的命令了。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:1","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#1-iptables-基础概念---四表五链"},{"categories":["tech"],"content":" 2. 常用命令 注意: 下面提供的 iptables 命令做的修改是未持久化的，重启就会丢失！在下一节会简单介绍持久化配置的方法。 命令格式： shell iptables [-t table] {-A|-C|-D} chain [-m matchname [per-match-options]] -j targetname [per-target-options] 其中 table 默认为 filter 表（可通过 -t xxx 指定别的表名），其中系统管理员实际使用最多的是 INPUT 链，用于设置防火墙。 先介绍下 iptables 的查询指令： shell # --list-rules 以命令的形式查看所有规则 iptables -S # --list-rules 查看 INPUT 表中的所有规则 iptables -S INPUT # -L 表示查看当前表的所有规则，相比 -S 它的显示效果要更 human-readable # -n 表示不对 IP 地址进行反查，一般都不需要反查 iptables -nL # 查看其他表的规则，如 nat 表 iptables -t nat -S iptables -t nat -nL 以下简单介绍在 INPUT 链上添加、修改规则，来设置防火墙（默认 filter 表，可通过 -t xxx 指定别的表名）： shell # --add 允许 80 端口通过 iptables -A INPUT -p tcp --dport 80 -j ACCEPT # ---delete 通过编号删除规则 iptables -D 1 # 或者通过完整的规则参数来删除规则 iptables -D INPUT -p tcp --dport 80 -j ACCEPT # --replace 通过编号来替换规则内容 iptables -R INPUT 1 -s 192.168.0.1 -j DROP # --insert 在指定的位置插入规则，可类比链表的插入 iptables -I INPUT 1 -p tcp --dport 80 -j ACCEPT # 在匹配条件前面使用感叹号表示取反 # 如下规则表示接受所有来自 docker0，但是目标接口不是 docker0 的流量 iptables -A FORWARD -i docker0 ! -o docker0 -j ACCEPT # --policy 设置某个链的默认规则 # 很多系统管理员会习惯将连接公网的服务器，默认规则设为 DROP，提升安全性，避免错误地开放了端口。 # 但是也要注意，默认规则设为 DROP 前，一定要先把允许 ssh 端口的规则加上，否则就尴尬了。 iptables -P INPUT DROP # --flush 清空 INPUT 表上的所有规则 iptables -F INPUT 本文后续分析时，假设用户已经清楚 linux bridge、veth 等虚拟网络接口相关知识。如果你还缺少这些前置知识，请先阅读文章Linux 中的虚拟网络接口。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:2","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#2-常用命令"},{"categories":["tech"],"content":" 3. conntrack 连接跟踪与 NAT在讲 conntrack 之间，我们再回顾下前面给出过的 netfilter 数据处理流程图： netfilter 数据包处理流程，来自 wikipedia 上一节中我们忽略了图中的 conntrack，它就是本节的主角——netfilter 的连接跟踪（connection tracking）模块。 netfilter/conntrack 是 iptables 实现 SNAT/DNAT/MASQUERADE 的前提条件，上面的流程图显示， conntrack 在 PREROUTEING 和 OUTPUT 链的 raw 表之后生效。 下面以 docker 默认的 bridge 网络为例详细介绍下 conntrack 的功能。 首先，这是我在「Linux 的虚拟网络接口」文中给出过的 docker0 网络架构图: text +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container A | Container B | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) docker 会在 iptables 中为 docker0 网桥添加如下规则： shell -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -t filter -P DROP -t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT 这几行规则使 docker 容器能正常访问外部网络。MASQUERADE 在请求出网时，会自动做 SNAT，将 src ip 替换成出口网卡的 ip. 这样数据包能正常出网，而且对端返回的数据包现在也能正常回到出口网卡。 现在问题就来了：出口网卡收到返回的数据包后，还能否将数据包转发到数据的初始来源端——某个 docker 容器？难道 docker 还额外添加了与 MASQUERADE 对应的 dst ip 反向转换规则？ 实际上这一步依赖的是本节的主角——iptables 提供的 conntrack 连接跟踪功能（在「参考」中有一篇文章详细介绍了此功能）。 连接跟踪对 NAT 的贡献是：在做 NAT 转换时，无需手动添加额外的规则来执行反向转换以实现数据的双向传输。netfilter/conntrack 系统会记录 NAT 的连接状态，NAT 地址的反向转换是根据这个状态自动完成的。 比如上图中的 Container A 通过 bridge 网络向 baidu.com 发起了 N 个连接，这时数据的处理流程如下： 首先 Container A 发出的数据包被 MASQUERADE 规则处理，将 src ip 替换成 eth0 的 ip，然后发送到物理网络 192.168.31.0/24。 conntrack 系统记录此连接被 NAT 处理前后的状态信息，并将其状态设置为 NEW，表示这是新发起的一个连接 对端 baidu.com 返回数据包后，会首先到达 eth0 网卡 conntrack 查表，发现返回数据包的连接已经记录在表中并且状态为 NEW，于是它将连接的状态修改为 ESTABLISHED，并且将 dst_ip 改为 172.17.0.2 然后发送出去 注意，这个和 tcp 的 ESTABLISHED 没任何关系 经过路由匹配，数据包会进入到 docker0，然后匹配上 iptables 规则：-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT， 数据直接被放行 数据经过 veth 后，最终进入到 Container A 中，交由容器的内核协议栈处理。 数据被 Container A 的内核协议栈发送到「发起连接的应用程序」。 1. 支持哪些协议conntrack 连接跟踪模块目前只支持以下六种协议：TCP、UDP、ICMP、DCCP、SCTP、GRE 要注意的一点是，conntrack 跟踪的「连接」，跟「TCP 连接」不是一个层面的概念，可以看到 conntrack 也支持 UDP 这种无连接通讯协议。 2. 实际测试 conntrack现在我们来实际测试一下，看看是不是这么回事： shell # 使用 tcpdump 分别在出口网卡 wlp4s0 （相当于 eth0）和 docker0 网桥上抓包，后面会用来分析 ❯ sudo tcpdump -i wlp4s0 -n \u003e wlp4s0.dump # 窗口一，抓 wlp4s0 的包 ❯ sudo tcpdump -i docker0 -n \u003e docker0.dump # 窗口二，抓 docker0 的包 现在新建窗口三，启动一个容器，通过 curl 命令低速下载一个视频文件： text ❯ docker run --rm --name curl -it curlimages/curl \"https://media.w3.org/2010/05/sintel/trailer.mp4\" -o /tmp/video.mp4 --limit-rate 100k 然后新建窗口四，在宿主机查看 conntrack 状态 shell ❯ sudo zypper in conntrack-tools # 这个记得先提前安装好 ❯ sudo conntrack -L | grep 172.17 # curl 通过 NAT 网络发起了一个 dns 查询请求，DNS 服务器是网关上的 192.168.31.1 udp 17 22 src=172.17.0.4 dst=192.168.31.1 sport=59423 dport=53 src=192.168.31.1 dst=192.168.31.228 sport=53 dport=59423 [ASSURED] mark=0 use=1 # curl 通过 NAT 网络向 media.w3.org 发起了 tcp 连接 tcp 6 298 ESTABLISHED src=172.17.0.4 dst=198.18.5.130 sport=54636 dport=443 src=198.18.5.130 dst=","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:3","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#3-conntrack-连接跟踪与-nat"},{"categories":["tech"],"content":" 3. conntrack 连接跟踪与 NAT在讲 conntrack 之间，我们再回顾下前面给出过的 netfilter 数据处理流程图： netfilter 数据包处理流程，来自 wikipedia 上一节中我们忽略了图中的 conntrack，它就是本节的主角——netfilter 的连接跟踪（connection tracking）模块。 netfilter/conntrack 是 iptables 实现 SNAT/DNAT/MASQUERADE 的前提条件，上面的流程图显示， conntrack 在 PREROUTEING 和 OUTPUT 链的 raw 表之后生效。 下面以 docker 默认的 bridge 网络为例详细介绍下 conntrack 的功能。 首先，这是我在「Linux 的虚拟网络接口」文中给出过的 docker0 网络架构图: text +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container A | Container B | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) docker 会在 iptables 中为 docker0 网桥添加如下规则： shell -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -t filter -P DROP -t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT 这几行规则使 docker 容器能正常访问外部网络。MASQUERADE 在请求出网时，会自动做 SNAT，将 src ip 替换成出口网卡的 ip. 这样数据包能正常出网，而且对端返回的数据包现在也能正常回到出口网卡。 现在问题就来了：出口网卡收到返回的数据包后，还能否将数据包转发到数据的初始来源端——某个 docker 容器？难道 docker 还额外添加了与 MASQUERADE 对应的 dst ip 反向转换规则？ 实际上这一步依赖的是本节的主角——iptables 提供的 conntrack 连接跟踪功能（在「参考」中有一篇文章详细介绍了此功能）。 连接跟踪对 NAT 的贡献是：在做 NAT 转换时，无需手动添加额外的规则来执行反向转换以实现数据的双向传输。netfilter/conntrack 系统会记录 NAT 的连接状态，NAT 地址的反向转换是根据这个状态自动完成的。 比如上图中的 Container A 通过 bridge 网络向 baidu.com 发起了 N 个连接，这时数据的处理流程如下： 首先 Container A 发出的数据包被 MASQUERADE 规则处理，将 src ip 替换成 eth0 的 ip，然后发送到物理网络 192.168.31.0/24。 conntrack 系统记录此连接被 NAT 处理前后的状态信息，并将其状态设置为 NEW，表示这是新发起的一个连接 对端 baidu.com 返回数据包后，会首先到达 eth0 网卡 conntrack 查表，发现返回数据包的连接已经记录在表中并且状态为 NEW，于是它将连接的状态修改为 ESTABLISHED，并且将 dst_ip 改为 172.17.0.2 然后发送出去 注意，这个和 tcp 的 ESTABLISHED 没任何关系 经过路由匹配，数据包会进入到 docker0，然后匹配上 iptables 规则：-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT， 数据直接被放行 数据经过 veth 后，最终进入到 Container A 中，交由容器的内核协议栈处理。 数据被 Container A 的内核协议栈发送到「发起连接的应用程序」。 1. 支持哪些协议conntrack 连接跟踪模块目前只支持以下六种协议：TCP、UDP、ICMP、DCCP、SCTP、GRE 要注意的一点是，conntrack 跟踪的「连接」，跟「TCP 连接」不是一个层面的概念，可以看到 conntrack 也支持 UDP 这种无连接通讯协议。 2. 实际测试 conntrack现在我们来实际测试一下，看看是不是这么回事： shell # 使用 tcpdump 分别在出口网卡 wlp4s0 （相当于 eth0）和 docker0 网桥上抓包，后面会用来分析 ❯ sudo tcpdump -i wlp4s0 -n \u003e wlp4s0.dump # 窗口一，抓 wlp4s0 的包 ❯ sudo tcpdump -i docker0 -n \u003e docker0.dump # 窗口二，抓 docker0 的包 现在新建窗口三，启动一个容器，通过 curl 命令低速下载一个视频文件： text ❯ docker run --rm --name curl -it curlimages/curl \"https://media.w3.org/2010/05/sintel/trailer.mp4\" -o /tmp/video.mp4 --limit-rate 100k 然后新建窗口四，在宿主机查看 conntrack 状态 shell ❯ sudo zypper in conntrack-tools # 这个记得先提前安装好 ❯ sudo conntrack -L | grep 172.17 # curl 通过 NAT 网络发起了一个 dns 查询请求，DNS 服务器是网关上的 192.168.31.1 udp 17 22 src=172.17.0.4 dst=192.168.31.1 sport=59423 dport=53 src=192.168.31.1 dst=192.168.31.228 sport=53 dport=59423 [ASSURED] mark=0 use=1 # curl 通过 NAT 网络向 media.w3.org 发起了 tcp 连接 tcp 6 298 ESTABLISHED src=172.17.0.4 dst=198.18.5.130 sport=54636 dport=443 src=198.18.5.130 dst=","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:3","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#1-支持哪些协议"},{"categories":["tech"],"content":" 3. conntrack 连接跟踪与 NAT在讲 conntrack 之间，我们再回顾下前面给出过的 netfilter 数据处理流程图： netfilter 数据包处理流程，来自 wikipedia 上一节中我们忽略了图中的 conntrack，它就是本节的主角——netfilter 的连接跟踪（connection tracking）模块。 netfilter/conntrack 是 iptables 实现 SNAT/DNAT/MASQUERADE 的前提条件，上面的流程图显示， conntrack 在 PREROUTEING 和 OUTPUT 链的 raw 表之后生效。 下面以 docker 默认的 bridge 网络为例详细介绍下 conntrack 的功能。 首先，这是我在「Linux 的虚拟网络接口」文中给出过的 docker0 网络架构图: text +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container A | Container B | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) docker 会在 iptables 中为 docker0 网桥添加如下规则： shell -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -t filter -P DROP -t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT 这几行规则使 docker 容器能正常访问外部网络。MASQUERADE 在请求出网时，会自动做 SNAT，将 src ip 替换成出口网卡的 ip. 这样数据包能正常出网，而且对端返回的数据包现在也能正常回到出口网卡。 现在问题就来了：出口网卡收到返回的数据包后，还能否将数据包转发到数据的初始来源端——某个 docker 容器？难道 docker 还额外添加了与 MASQUERADE 对应的 dst ip 反向转换规则？ 实际上这一步依赖的是本节的主角——iptables 提供的 conntrack 连接跟踪功能（在「参考」中有一篇文章详细介绍了此功能）。 连接跟踪对 NAT 的贡献是：在做 NAT 转换时，无需手动添加额外的规则来执行反向转换以实现数据的双向传输。netfilter/conntrack 系统会记录 NAT 的连接状态，NAT 地址的反向转换是根据这个状态自动完成的。 比如上图中的 Container A 通过 bridge 网络向 baidu.com 发起了 N 个连接，这时数据的处理流程如下： 首先 Container A 发出的数据包被 MASQUERADE 规则处理，将 src ip 替换成 eth0 的 ip，然后发送到物理网络 192.168.31.0/24。 conntrack 系统记录此连接被 NAT 处理前后的状态信息，并将其状态设置为 NEW，表示这是新发起的一个连接 对端 baidu.com 返回数据包后，会首先到达 eth0 网卡 conntrack 查表，发现返回数据包的连接已经记录在表中并且状态为 NEW，于是它将连接的状态修改为 ESTABLISHED，并且将 dst_ip 改为 172.17.0.2 然后发送出去 注意，这个和 tcp 的 ESTABLISHED 没任何关系 经过路由匹配，数据包会进入到 docker0，然后匹配上 iptables 规则：-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT， 数据直接被放行 数据经过 veth 后，最终进入到 Container A 中，交由容器的内核协议栈处理。 数据被 Container A 的内核协议栈发送到「发起连接的应用程序」。 1. 支持哪些协议conntrack 连接跟踪模块目前只支持以下六种协议：TCP、UDP、ICMP、DCCP、SCTP、GRE 要注意的一点是，conntrack 跟踪的「连接」，跟「TCP 连接」不是一个层面的概念，可以看到 conntrack 也支持 UDP 这种无连接通讯协议。 2. 实际测试 conntrack现在我们来实际测试一下，看看是不是这么回事： shell # 使用 tcpdump 分别在出口网卡 wlp4s0 （相当于 eth0）和 docker0 网桥上抓包，后面会用来分析 ❯ sudo tcpdump -i wlp4s0 -n \u003e wlp4s0.dump # 窗口一，抓 wlp4s0 的包 ❯ sudo tcpdump -i docker0 -n \u003e docker0.dump # 窗口二，抓 docker0 的包 现在新建窗口三，启动一个容器，通过 curl 命令低速下载一个视频文件： text ❯ docker run --rm --name curl -it curlimages/curl \"https://media.w3.org/2010/05/sintel/trailer.mp4\" -o /tmp/video.mp4 --limit-rate 100k 然后新建窗口四，在宿主机查看 conntrack 状态 shell ❯ sudo zypper in conntrack-tools # 这个记得先提前安装好 ❯ sudo conntrack -L | grep 172.17 # curl 通过 NAT 网络发起了一个 dns 查询请求，DNS 服务器是网关上的 192.168.31.1 udp 17 22 src=172.17.0.4 dst=192.168.31.1 sport=59423 dport=53 src=192.168.31.1 dst=192.168.31.228 sport=53 dport=59423 [ASSURED] mark=0 use=1 # curl 通过 NAT 网络向 media.w3.org 发起了 tcp 连接 tcp 6 298 ESTABLISHED src=172.17.0.4 dst=198.18.5.130 sport=54636 dport=443 src=198.18.5.130 dst=","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:3","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#2-实际测试-conntrack"},{"categories":["tech"],"content":" 3. conntrack 连接跟踪与 NAT在讲 conntrack 之间，我们再回顾下前面给出过的 netfilter 数据处理流程图： netfilter 数据包处理流程，来自 wikipedia 上一节中我们忽略了图中的 conntrack，它就是本节的主角——netfilter 的连接跟踪（connection tracking）模块。 netfilter/conntrack 是 iptables 实现 SNAT/DNAT/MASQUERADE 的前提条件，上面的流程图显示， conntrack 在 PREROUTEING 和 OUTPUT 链的 raw 表之后生效。 下面以 docker 默认的 bridge 网络为例详细介绍下 conntrack 的功能。 首先，这是我在「Linux 的虚拟网络接口」文中给出过的 docker0 网络架构图: text +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container A | Container B | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) docker 会在 iptables 中为 docker0 网桥添加如下规则： shell -t nat -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -t filter -P DROP -t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT 这几行规则使 docker 容器能正常访问外部网络。MASQUERADE 在请求出网时，会自动做 SNAT，将 src ip 替换成出口网卡的 ip. 这样数据包能正常出网，而且对端返回的数据包现在也能正常回到出口网卡。 现在问题就来了：出口网卡收到返回的数据包后，还能否将数据包转发到数据的初始来源端——某个 docker 容器？难道 docker 还额外添加了与 MASQUERADE 对应的 dst ip 反向转换规则？ 实际上这一步依赖的是本节的主角——iptables 提供的 conntrack 连接跟踪功能（在「参考」中有一篇文章详细介绍了此功能）。 连接跟踪对 NAT 的贡献是：在做 NAT 转换时，无需手动添加额外的规则来执行反向转换以实现数据的双向传输。netfilter/conntrack 系统会记录 NAT 的连接状态，NAT 地址的反向转换是根据这个状态自动完成的。 比如上图中的 Container A 通过 bridge 网络向 baidu.com 发起了 N 个连接，这时数据的处理流程如下： 首先 Container A 发出的数据包被 MASQUERADE 规则处理，将 src ip 替换成 eth0 的 ip，然后发送到物理网络 192.168.31.0/24。 conntrack 系统记录此连接被 NAT 处理前后的状态信息，并将其状态设置为 NEW，表示这是新发起的一个连接 对端 baidu.com 返回数据包后，会首先到达 eth0 网卡 conntrack 查表，发现返回数据包的连接已经记录在表中并且状态为 NEW，于是它将连接的状态修改为 ESTABLISHED，并且将 dst_ip 改为 172.17.0.2 然后发送出去 注意，这个和 tcp 的 ESTABLISHED 没任何关系 经过路由匹配，数据包会进入到 docker0，然后匹配上 iptables 规则：-t filter -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT， 数据直接被放行 数据经过 veth 后，最终进入到 Container A 中，交由容器的内核协议栈处理。 数据被 Container A 的内核协议栈发送到「发起连接的应用程序」。 1. 支持哪些协议conntrack 连接跟踪模块目前只支持以下六种协议：TCP、UDP、ICMP、DCCP、SCTP、GRE 要注意的一点是，conntrack 跟踪的「连接」，跟「TCP 连接」不是一个层面的概念，可以看到 conntrack 也支持 UDP 这种无连接通讯协议。 2. 实际测试 conntrack现在我们来实际测试一下，看看是不是这么回事： shell # 使用 tcpdump 分别在出口网卡 wlp4s0 （相当于 eth0）和 docker0 网桥上抓包，后面会用来分析 ❯ sudo tcpdump -i wlp4s0 -n \u003e wlp4s0.dump # 窗口一，抓 wlp4s0 的包 ❯ sudo tcpdump -i docker0 -n \u003e docker0.dump # 窗口二，抓 docker0 的包 现在新建窗口三，启动一个容器，通过 curl 命令低速下载一个视频文件： text ❯ docker run --rm --name curl -it curlimages/curl \"https://media.w3.org/2010/05/sintel/trailer.mp4\" -o /tmp/video.mp4 --limit-rate 100k 然后新建窗口四，在宿主机查看 conntrack 状态 shell ❯ sudo zypper in conntrack-tools # 这个记得先提前安装好 ❯ sudo conntrack -L | grep 172.17 # curl 通过 NAT 网络发起了一个 dns 查询请求，DNS 服务器是网关上的 192.168.31.1 udp 17 22 src=172.17.0.4 dst=192.168.31.1 sport=59423 dport=53 src=192.168.31.1 dst=192.168.31.228 sport=53 dport=59423 [ASSURED] mark=0 use=1 # curl 通过 NAT 网络向 media.w3.org 发起了 tcp 连接 tcp 6 298 ESTABLISHED src=172.17.0.4 dst=198.18.5.130 sport=54636 dport=443 src=198.18.5.130 dst=","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:3","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#3-nat-如何分配端口"},{"categories":["tech"],"content":" 4. 如何持久化 iptables 配置首先需要注意的是，centos7/opensuse 15 都已经切换到了 firewalld 作为防火墙配置软件，而 ubuntu18.04 lts 也换成了 ufw 来配置防火墙。 包括 docker 应该也是在启动的时候动态添加 iptables 配置。 对于上述新系统，还是建议直接使用 firewalld/ufw 配置防火墙吧，或者网上搜下关闭 ufw/firewalld、启用 iptables 持久化的解决方案。 本文主要目的在于理解 docker 容器网络的原理，以及为后面理解 kubernetes 网络插件 calico/flannel 打好基础，因此就不多介绍持久化了。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:1:4","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#4-如何持久化-iptables-配置"},{"categories":["tech"],"content":" 二、容器网络实现原理 - iptables + bridge + vethDocker/Podman 默认使用的都是 bridge 网络，它们的底层实现完全类似。下面以 docker 为例进行分析（Podman 的分析流程也基本一样）。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:2:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#二容器网络实现原理---iptables--bridge--veth"},{"categories":["tech"],"content":" 1. 简单分析 docker0 网桥的原理首先，使用 docker run 运行几个容器，检查下网络状况： shell # 运行一个 debian 容器和一个 nginx ❯ docker run -d --name debian --rm debian:buster sleep 1000000 ❯ docker run -d --name nginx --rm nginx:1.19-alpine # 查看网络接口，有两个 veth 接口（而且都没设 ip 地址），分别连接到两个容器的 eth0（docker0 网络架构图前面给过了，可以往前面翻翻对照下） ❯ ip addr ls ... 5: docker0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:42:c7:12:ba brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:42ff:fec7:12ba/64 scope link valid_lft forever preferred_lft forever 100: veth16b37ea@if99: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 42:af:34:ae:74:ae brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::40af:34ff:feae:74ae/64 scope link valid_lft forever preferred_lft forever 102: veth4b4dada@if101: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 9e:f1:58:1a:cf:ae brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::9cf1:58ff:fe1a:cfae/64 scope link valid_lft forever preferred_lft forever # 两个 veth 接口都连接到了 docker0 上面，说明两个容器都使用了 docker 默认的 bridge 网络 ❯ sudo brctl show bridge name bridge id STP enabled interfaces docker0 8000.024242c712ba no veth16b37ea veth4b4dada # 查看路由规则 ❯ ip route ls default via 192.168.31.1 dev wlp4s0 proto dhcp metric 600 #下列路由规则将 `172.17.0.0/16` 网段的所有流量转发到 docker0 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.31.0/24 dev wlp4s0 proto kernel scope link src 192.168.31.228 metric 600 # 查看 iptables 规则 # nat 表 ❯ sudo iptables -t nat -S -P PREROUTING ACCEPT -P INPUT ACCEPT -P OUTPUT ACCEPT -P POSTROUTING ACCEPT # 在 nat 表中新建一条自定义链 DOCKER -N DOCKER # 所有目的地址在本机的，都先交给 DOCKER 链处理一波 -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER # （容器访问外部网络）所有出口不为 docker0 的流量，都做下 SNAT，把 src ip 换成出口接口的 ip 地址 -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE # DOCKER 链目前没任何内容，单纯直接返回父链进行进一步匹配 -A DOCKER -i docker0 -j RETURN # filter 表 ❯ sudo iptables -t filter -S -P INPUT ACCEPT -P FORWARD DROP -P OUTPUT ACCEPT # 在 filter 表中新建四条自定义链 -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER # 所有流量都必须先经过如下两个自定义链的处理，没问题才能继续往下走 -A FORWARD -j DOCKER-ISOLATION-STAGE-1 -A FORWARD -j DOCKER-USER # （容器访问外部网络）出去的流量走了 MASQUERADE，回来的流量会被 conntrack 识别并转发回来，这里允许返回的数据包通过。 # 这里直接 ACCEPT 被 conntrack 识别到的流量 -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT # 将所有访问 docker0 的流量都转给自定义链 DOCKER 处理 -A FORWARD -o docker0 -j DOCKER # 允许所有来自 docker0 的流量通过，不论下一跳是否是 docker0 -A FORWARD -i docker0 ! -o docker0 -j ACCEPT -A FORWARD -i docker0 -o docker0 -j ACCEPT # 下面三个链目前啥规则也没有，就是简单的 RETURN，直接返回父链进行进一步匹配 -A DOCKER-ISOLATION-STAGE-1 -j RETURN -A DOCKER-ISOLATION-STAGE-2 -j RETURN -A DOCKER-USER -j RETURN ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:2:1","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#1-简单分析-docker0-网桥的原理"},{"categories":["tech"],"content":" 2. docker0 禁止容器间通信docker 可以通过为 dockerd 启动参数添加 --icc=false 来禁用容器间通信 （inter-container-networking 的缩写），这里来验证下它是如何实现这个功能的。 首先验证下前面创建的 debian 容器目前是能访问 nginx 容器的： shell # 查到 nginx 容器的 ip 地址 ❯ docker inspect nginx | grep \\\"IPAddress \"IPAddress\": \"172.17.0.3\", ❯ docker exec -it debian bash # 首先跑 `apt update \u0026\u0026 apt install -y curl` 安装 curl 工具，这里略过相关日志 ...... # 访问 nginx 容器，返回数据正常 root@499fbc07b79c:/# curl -s -v 172.17.0.3:80 -o /dev/null * Expire in 0 ms for 6 (transfer 0x5556f6dfd110) * Trying 172.17.0.3... * TCP_NODELAY set * Expire in 200 ms for 4 (transfer 0x5556f6dfd110) * Connected to 172.17.0.3 (172.17.0.3) port 80 (#0) \u003e GET / HTTP/1.1 \u003e Host: 172.17.0.3 \u003e User-Agent: curl/7.64.0 \u003e Accept: */* \u003e \u003c HTTP/1.1 200 OK \u003c Server: nginx/1.19.10 \u003c Date: Sat, 04 Mar 2023 14:00:09 GMT \u003c Content-Type: text/html \u003c Content-Length: 612 ... 接着查找下 docker 的 systemd 配置位置： shell ❯ sudo systemctl disable docker Removed \"/etc/systemd/system/multi-user.target.wants/docker.service\". ❯ sudo systemctl enable docker Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service. 根据日志可定位到我的 docker.service 配置位于 /usr/lib/systemd/system/docker.service，修改此配置，在 ExecStart 一行的末尾添加参数 --icc=false，然后重启 docker 服务： shell ❯ sudo systemctl daemon-reload ❯ sudo systemctl restart docker 现在再走一遍前面的测试，会发现 debian 无法访问 nginx 容器了。查看 iptables 规则会发现所有 docker0 网桥的内部通信数据全部被 drop 掉了： shell # nat 表 ❯ sudo iptables -t nat -S # 内容没有任何变化，这里略过 ... # filter 表 ❯ sudo iptables -t filter -S -P INPUT ACCEPT -P FORWARD DROP -P OUTPUT ACCEPT -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER -A FORWARD -j DOCKER-USER -A FORWARD -j DOCKER-ISOLATION-STAGE-1 -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT # 将所有访问 docker0 的流量都转给自定义链 DOCKER 处理 -A FORWARD -o docker0 -j DOCKER # 放行 docker0 网桥与外网通信的数据（下一跳不为 docker） -A FORWARD -i docker0 ! -o docker0 -j ACCEPT # 丢弃所有 docker0 网桥的内部通信流量（即禁止 docker0 上的容器互相访问） -A FORWARD -i docker0 -o docker0 -j DROP -A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2 -A DOCKER-ISOLATION-STAGE-1 -j RETURN -A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP -A DOCKER-ISOLATION-STAGE-2 -j RETURN -A DOCKER-USER -j RETURN ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:2:2","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#2-docker0-禁止容器间通信"},{"categories":["tech"],"content":" 3. 使用 docker-compose 自定义网桥与端口映射接下来使用如下 docker-compose 配置启动一个 caddy 容器，添加自定义 network 和端口映射，待会就能验证 docker 是如何实现这两种网络的了。 docker-compose.yml 内容： yaml version: \"3.3\" services: caddy: image: \"caddy:2.2.1-alpine\" container_name: \"caddy\" restart: always command: caddy file-server --browse --root / ports: - \"8081:80\" networks: - caddy-1 networks: caddy-1: 现在先用上面的配置启动 caddy 容器，然后再查看网络状况： shell # 启动 caddy ❯ docker-compose up -d # 查下 caddy 容器的 ip ❯ docker inspect caddy | grep IPAddress ... \"IPAddress\": \"172.18.0.2\", # 查看网络接口，可以看到多了一个网桥 br-ac3e0514d837 ，它就是上一行命令创建的 caddy-1 网络 # 还多了一个 veth0c25c6f@if104 ，它实际连接到了 caddy 容器的 eth0(veth) 接口 ❯ ip addr ls ... 5: docker0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:42:c7:12:ba brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:42ff:fec7:12ba/64 scope link valid_lft forever preferred_lft forever 100: veth16b37ea@if99: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 42:af:34:ae:74:ae brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::40af:34ff:feae:74ae/64 scope link valid_lft forever preferred_lft forever 102: veth4b4dada@if101: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 9e:f1:58:1a:cf:ae brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::9cf1:58ff:fe1a:cfae/64 scope link valid_lft forever preferred_lft forever 103: br-ac3e0514d837: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:7d:95:ba:7e brd ff:ff:ff:ff:ff:ff inet 172.18.0.1/16 brd 172.18.255.255 scope global br-ac3e0514d837 valid_lft forever preferred_lft forever inet6 fe80::42:7dff:fe95:ba7e/64 scope link valid_lft forever preferred_lft forever 105: veth0c25c6f@if104: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue master br-ac3e0514d837 state UP group default link/ether 9a:03:e1:f0:26:ea brd ff:ff:ff:ff:ff:ff link-netnsid 2 inet6 fe80::9803:e1ff:fef0:26ea/64 scope link valid_lft forever preferred_lft forever # 查看网桥，能看到 caddy 容器的 veth0c25c6f 接口连在了 br-ac3e0514d837 也就是 caddy-1 网桥上，没有加入到 docker0 网络 ❯ sudo brctl show bridge name bridge id STP enabled interfaces br-ac3e0514d837 8000.02427d95ba7e no veth0c25c6f docker0 8000.024242c712ba no veth16b37ea veth4b4dada # 查看路由，能看到新网桥使用的地址段是 172.18.0.0/16，是 docker0 递增上来的 ❯ ip route ls default via 192.168.31.1 dev wlp4s0 proto dhcp metric 600 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 # 多了一个网桥的 172.18.0.0/16 dev br-ac3e0514d837 proto kernel scope link src 172.18.0.1 192.168.31.0/24 dev wlp4s0 proto kernel scope link src 192.168.31.228 metric 600 # iptables 中也多了 caddy-1 网桥的 MASQUERADE 规则，以及端口映射的规则，下面重点给这些新增规则加了注释 ❯ sudo iptables -t nat -S -P PREROUTING ACCEPT -P INPUT ACCEPT -P OUTPUT ACCEPT -P POSTROUTING ACCEPT -N DOCKER -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER -A POSTROUTING -s 172.18.0.0/16 ! -o br-ac3e0514d837 -j MASQUERADE -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE # 源地址与目标地址都是 172.18.0.2/32，说明是 caddy 容器在请求它自己，为什么自己请求自己还要做 NAT(MASQUERADE) 呢？？？ # 我表示也觉得有点离谱，只要容器中的协议栈实现没毛病，请求它自己应该根本不会走到网桥来... # 查了一波资料发现老外也同样觉得很离谱: https://www.ipspace.net/kb/DockerSvc/30-nat-iptables.html -A POSTROUTING -s 172.18.0.2/32 -d 172.18.0.2/32 -p tcp -m tcp --dport 80 -j MASQUERADE -A DOCKER -i br-ac3e0514d837 -j RETURN -A DOCKER -i docker0 -j RETURN # 所有从非 br-ac3e0514d837(caddy-1) 网桥进来的 tcp 流量，只要目标端口是 8081，就转发到 caddy 容器去并且目标端口改为 80（端口映射） # DOCKER 链处理的流量目标地址不是宿主机 IP，因此在路由决策时它会走 FORWARD 链，直接绕过了通常设置在 INPUT 链的主机防火墙规则，这就是 Docker 端口映射能使防火墙配置失效的原因。 -A DOCKER ! -i br-ac3e0514d837 -p tcp -m tcp --dport 8081 -j DNAT --to-destination 172.18.0.2:80 ❯ sudo iptables -t filter -S -P INPUT ACCEPT -P FORWARD DROP -P OUTPUT ACCEPT -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER -A FORWARD -j D","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:2:3","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#docker-publish-ports"},{"categories":["tech"],"content":" 三、Docker/Podman 的 macvlan/ipvlan 模式 注意：macvlan 和 wifi 好像不兼容，测试时不要使用无线网络的接口！ 我在前面介绍 Linux 虚拟网络接口的文章中，有介绍过 macvlan 和 ipvlan 两种新的虚拟接口。 目前 Podman/Docker 都支持使用 macvlan 来构建容器网络，这种模式下创建的容器直连外部网络，容器可以拥有独立的外部 IP，不需要端口映射，也不需要借助 iptables. 这和虚拟机的 Bridge 模式就很类似，主要适用于希望容器拥有独立外部 IP 的情况。 下面详细分析下 Docker 的 macvlan 网络（Podman 应该也完全类似）。 shell # 首先创建一个 macvlan 网络 # subnet/gateway 的参数需要和物理网络一致 # 通过 -o parent 设定父接口，我本机的以太网口名称为 eno1 $ docker network create -d macvlan \\ --subnet=192.168.31.0/24 \\ --gateway=192.168.31.1 \\ -o parent=eno1 \\ macnet0 # 现在使用 macvlan 启动一个容器试试 # 建议和我一样，通过 --ip 手动配置静态 ip 地址，当然不配也可以，DHCP 会自动分配 IP $ docker run --network macnet0 --ip=192.168.31.233 --rm -it buildpack-deps:buster-curl /bin/bash # 在容器中查看网络接口状况，能看到 eth0 是一个 macvlan 接口 root@4319488cb5e7:/# ip -d addr ls 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 minmtu 0 maxmtu 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 8: eth0@if2: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:c0:a8:1f:e9 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 0 minmtu 68 maxmtu 9194 macvlan mode bridge numtxqueues 1 numrxqueues 1 gso_max_size 64000 gso_max_segs 64 inet 192.168.31.233/24 brd 192.168.31.255 scope global eth0 valid_lft forever preferred_lft forever # 路由表，默认 gateway 被自动配置进来了 root@4319488cb5e7:/# ip route ls default via 192.168.31.1 dev eth0 192.168.31.0/24 dev eth0 proto kernel scope link src 192.168.31.233 # 可以正常访问 baidu root@4319488cb5e7:/# curl baidu.com \u003chtml\u003e \u003cmeta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"\u003e \u003c/html\u003e Docker 支持的另一种网络模式是 ipvlan（ipvlan 和 macvlan 的区别我在前一篇文章中已经介绍过， 不再赘言），创建命令和 macvlan 几乎一样： shell # 首先创建一个 macvlan 网络 # subnet/gateway 的参数需要和物理网络一致 # 通过 -o parent 设定父接口，我本机的以太网口名称为 eno1 # ipvlan_mode 默认为 l2，表示工作在数据链路层。 $ docker network create -d ipvlan \\ --subnet=192.168.31.0/24 \\ --gateway=192.168.31.1 \\ -o parent=eno1 \\ -o ipvlan_mode=l2 \\ ipvnet0 # 现在使用 macvlan 启动一个容器试试 # 建议和我一样，通过 --ip 手动配置静态 ip 地址，当然不配也可以，DHCP 会自动分配 IP $ docker run --network ipvnet0 --ip=192.168.31.234 --rm -it buildpack-deps:buster-curl /bin/bash # 在容器中查看网络接口状况，能看到 eth0 是一个 ipvlan 接口 root@d0764ebbbf42:/# ip -d addr ls 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 minmtu 0 maxmtu 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 12: eth0@if2: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UNKNOWN group default link/ether 38:f3:ab:a3:e6:71 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 0 minmtu 68 maxmtu 65535 ipvlan mode l2 bridge numtxqueues 1 numrxqueues 1 gso_max_size 64000 gso_max_segs 64 inet 192.168.31.234/24 brd 192.168.31.255 scope global eth0 valid_lft forever preferred_lft forever # 路由表，默认 gateway 被自动配置进来了 root@d0764ebbbf42:/# ip route ls default via 192.168.31.1 dev eth0 192.168.31.0/24 dev eth0 proto kernel scope link src 192.168.31.234 # 可以正常访问 baidu root@d0764ebbbf42:/# curl baidu.com \u003chtml\u003e \u003cmeta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"\u003e \u003c/html\u003e ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:3:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#三dockerpodman-的-macvlanipvlan-模式"},{"categories":["tech"],"content":" 四、Rootless 容器的网络实现如果容器运行时也在 Rootless 模式下运行，那它就没有权限在宿主机添加 bridge/veth 等虚拟网络接口，这种情况下，我们前面描述的容器网络就无法设置了。 那么 podman/containerd(nerdctl) 目前是如何在 Rootless 模式下构建容器网络的呢？ 查看文档，发现它们都用到了 rootlesskit 相关的东西，而 rootlesskit 提供了 rootless 网络的几个实现，文档参见rootlesskit/docs/network.md 其中目前推荐使用，而且 podman/containerd(nerdctl) 都默认使用的方案，是rootless-containers/slirp4netns 以 containerd(nerdctl) 为例，按官方文档安装好后，随便启动几个容器，然后在宿主机查iptables/ip addr ls，会发现啥也没有。这显然是因为 rootless 模式下 containerd 改不了宿主机的 iptables 配置和虚拟网络接口。但是可以查看到宿主机 slirp4netns 在后台运行： shell ❯ ps aux | grep tap ryan 11644 0.0 0.0 5288 3312 ? S 00:01 0:02 slirp4netns --mtu 65520 -r 3 --disable-host-loopback --enable-sandbox --enable-seccomp 11625 tap0 但是我看半天文档，只看到怎么使用 rootlesskit/slirp4netns 创建新的名字空间，没看到有介绍如何进入一个已存在的 slirp4netns 名字空间… 使用 nsenter -a -t 11644 也一直报错，任何程序都是 no such binary… 以后有空再重新研究一波… 总之能确定的是，它通过在虚拟的名字空间中创建了一个 tap 虚拟接口来实现容器网络，性能相比前面介绍的网络多少是要差一点的。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:4:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#四rootless-容器的网络实现"},{"categories":["tech"],"content":" 五、nftables前面介绍了 iptables 以及其在 docker 和防火墙上的应用。但是实际上目前各大 Linux 发行版都已经不建议使用 iptables 了，甚至把 iptables 重命名为了 iptables-legacy. 目前 opensuse/debian/opensuse 都已经预装了并且推荐使用 nftables，而且 firewalld 已经默认使用 nftables 作为它的后端了。 我在 opensuse tumbleweed 上实测，firewalld 添加的是 nftables 配置，而 docker 仍然在用旧的 iptables，也就是说我现在的机器上有两套 netfilter 工具并存： text # 查看 iptables 数据 \u003e iptables -S -P INPUT ACCEPT -P FORWARD DROP -P OUTPUT ACCEPT -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER -A FORWARD -j DOCKER-ISOLATION-STAGE-1 -A FORWARD -o br-e3fbbb7a1b3a -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A FORWARD -o br-e3fbbb7a1b3a -j DOCKER ... # 确认下是否使用了 nftables 的兼容层，结果提示请我使用 iptables-legacy \u003e iptables-nft -S # Warning: iptables-legacy tables present, use iptables-legacy to see them -P INPUT ACCEPT -P FORWARD ACCEPT -P OUTPUT ACCEPT # 查看 nftables 规则，能看到三张 firewalld 生成的 table \u003e nft list ruleset table inet firewalld { ... } table ip firewalld { ... } table ip6 firewalld { ... } 但是现在 kubernetes/docker 都还是用的 iptables，nftables 我学了用处不大，以后有空再补充。 ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:5:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#五nftables"},{"categories":["tech"],"content":" 参考 iptables 详解（1）：iptables 概念 连接跟踪（conntrack）：原理、应用及 Linux 内核实现 网络地址转换（NAT）之报文跟踪 容器安全拾遗 - Rootless Container 初探 netfilter - wikipedia ","date":"2021-08-15","objectID":"/posts/iptables-and-container-networks/:6:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器","iptables","conntrack"],"title":"iptables 及 docker 容器网络分析","uri":"/posts/iptables-and-container-networks/#参考"},{"categories":["tech"],"content":" 本文用到的字符画工具：vscode-asciiflow2 注意: 本文中使用 ip 命令创建或修改的任何网络配置，都是未持久化的，主机重启即消失。 Linux 具有强大的虚拟网络能力，这也是 openstack 网络、docker 容器网络以及 kubernetes 网络等虚拟网络的基础。 这里介绍 Linux 常用的虚拟网络接口类型：TUN/TAP、bridge、veth、ipvlan/macvlan、vlan 以及 vxlan/geneve. ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:0:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#"},{"categories":["tech"],"content":" 一、tun/tap 虚拟网络接口tun/tap 是操作系统内核中的虚拟网络设备，他们为用户层程序提供数据的接收与传输。 普通的物理网络接口如 eth0，它的两端分别是内核协议栈和外面的物理网络。 而对于 TUN/TAP 虚拟接口如 tun0，它的一端一定是连接的用户层程序，另一端则视配置方式的不同而变化，可以直连内核协议栈，也可以是某个 bridge（后面会介绍）。Linux 通过内核模块 TUN 提供 tun/tap 功能，该模块提供了一个设备接口 /dev/net/tun 供用户层程序读写，用户层程序通过/dev/net/tun 读写主机内核协议栈的数据。 text \u003e modinfo tun filename: /lib/modules/5.13.6-1-default/kernel/drivers/net/tun.ko.xz alias: devname:net/tun alias: char-major-10-200 license: GPL author: (C) 1999-2004 Max Krasnyansky \u003cmaxk@qualcomm.com\u003e description: Universal TUN/TAP device driver ... \u003e ls /dev/net/tun /dev/net/tun 一个 TUN 设备的示例图如下： text +----------------------------------------------------------------------+ | | | +--------------------+ +--------------------+ | | | User Application A | | User Application B +\u003c-----+ | | +------------+-------+ +-------+------------+ | | | | 1 | 5 | | |...............+......................+...................|...........| | ↓ ↓ | | | +----------+ +----------+ | | | | socket A | | socket B | | | | +-------+--+ +--+-------+ | | | | 2 | 6 | | |.................+.................+......................|...........| | ↓ ↓ | | | +------------------------+ +--------+-------+ | | | Network Protocol Stack | | /dev/net/tun | | | +--+-------------------+-+ +--------+-------+ | | | 7 | 3 ^ | |................+...................+.....................|...........| | ↓ ↓ | | | +----------------+ +----------------+ 4 | | | | eth0 | | tun0 | | | | +-------+--------+ +-----+----------+ | | | 10.32.0.11 | | 192.168.3.11 | | | | 8 +---------------------+ | | | | +----------------+-----------------------------------------------------+ ↓ Physical Network 因为 TUN/TAP 设备的一端是内核协议栈，显然流入 tun0 的数据包是先经过本地的路由规则匹配的。 路由匹配成功，数据包被发送到 tun0 后，tun0 发现另一端是通过 /dev/net/tun 连接到应用程序 B，就会将数据丢给应用程序 B。 应用程序对数据包进行处理后，可能会构造新的数据包，通过物理网卡发送出去。比如常见的 VPN 程序就是把原来的数据包封装/加密一遍，再发送给 VPN 服务器。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:1:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#一tuntap-虚拟网络接口"},{"categories":["tech"],"content":" C 语言编程测试 TUN 设备为了使用 tun/tap 设备，用户层程序需要通过系统调用打开 /dev/net/tun 获得一个读写该设备的文件描述符(FD)，并且调用 ioctl() 向内核注册一个 TUN 或 TAP 类型的虚拟网卡(实例化一个 tun/tap 设备)，其名称可能是 tun0/tap0 等。 此后，用户程序可以通过该 TUN/TAP 虚拟网卡与主机内核协议栈（或者其他网络设备）交互。当用户层程序关闭后，其注册的 TUN/TAP 虚拟网卡以及自动生成的路由表相关条目都会被内核释放。 可以把用户层程序看做是网络上另一台主机，他们通过 tun/tap 虚拟网卡相连。 一个简单的 C 程序示例如下，它每次收到数据后，都只单纯地打印一下收到的字节数： c #include \u003clinux/if.h\u003e #include \u003clinux/if_tun.h\u003e #include \u003csys/ioctl.h\u003e #include \u003cfcntl.h\u003e #include \u003cstring.h\u003e #include \u003cunistd.h\u003e #include\u003cstdlib.h\u003e #include\u003cstdio.h\u003e int tun_alloc(int flags) { struct ifreq ifr; int fd, err; char *clonedev = \"/dev/net/tun\"; // 打开 tun 文件，获得 fd if ((fd = open(clonedev, O_RDWR)) \u003c 0) { return fd; } memset(\u0026ifr, 0, sizeof(ifr)); ifr.ifr_flags = flags; // 向内核注册一个 TUN 网卡，并与前面拿到的 fd 关联起来 // 程序关闭时，注册的 tun 网卡及自动生成的相关路由策略，会被自动释放 if ((err = ioctl(fd, TUNSETIFF, (void *) \u0026ifr)) \u003c 0) { close(fd); return err; } printf(\"Open tun/tap device: %s for reading...\\n\", ifr.ifr_name); return fd; } int main() { int tun_fd, nread; char buffer[1500]; /* Flags: IFF_TUN - TUN device (no Ethernet headers) * IFF_TAP - TAP device * IFF_NO_PI - Do not provide packet information */ tun_fd = tun_alloc(IFF_TUN | IFF_NO_PI); if (tun_fd \u003c 0) { perror(\"Allocating interface\"); exit(1); } while (1) { nread = read(tun_fd, buffer, sizeof(buffer)); if (nread \u003c 0) { perror(\"Reading from interface\"); close(tun_fd); exit(1); } printf(\"Read %d bytes from tun/tap device\\n\", nread); } return 0; } 接下来开启三个终端窗口来测试上述程序，分别运行上面的 tun 程序、tcpdump 和 iproute2 指令。 首先通过编译运行上述 c 程序，程序会阻塞住，等待数据到达： text # 编译，请忽略部分 warning \u003e gcc mytun.c -o mytun # 创建并监听 tun 设备需要 root 权限 \u003e sudo mytun Open tun/tap device: tun0 for reading... 现在使用 iproute2 查看下链路层设备： text # 能发现最后面有列出名为 tun0 的接口，但是状态为 down ❯ ip addr ls ...... 3: wlp4s0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether c0:3c:59:36:a4:16 brd ff:ff:ff:ff:ff:ff inet 192.168.31.228/24 brd 192.168.31.255 scope global dynamic noprefixroute wlp4s0 valid_lft 41010sec preferred_lft 41010sec inet6 fe80::4ab0:130f:423b:5d37/64 scope link noprefixroute valid_lft forever preferred_lft forever 7: tun0: \u003cPOINTOPOINT,MULTICAST,NOARP\u003e mtu 1500 qdisc noop state DOWN group default qlen 500 link/none # 为 tun0 设置 ip 地址，注意不要和其他接口在同一网段，会导致路由冲突 \u003e sudo ip addr add 172.21.22.23/24 dev tun0 # 启动 tun0 这个接口，这一步会自动向路由表中添加将 172.21.22.23/24 路由到 tun0 的策略 \u003e sudo ip link set tun0 up #确认上一步添加的路由策略是否存在 ❯ ip route ls default via 192.168.31.1 dev wlp4s0 proto dhcp metric 600 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 172.21.22.0/24 dev tun0 proto kernel scope link src 172.21.22.23 192.168.31.0/24 dev wlp4s0 proto kernel scope link src 192.168.31.228 metric 600 # 此时再查看接口，发现 tun0 状态为 unknown \u003e ip addr ls ...... 8: tun0: \u003cPOINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 500 link/none inet 172.21.22.23/24 scope global tun0 valid_lft forever preferred_lft forever inet6 fe80::3d52:49b5:1cf3:38fd/64 scope link stable-privacy valid_lft forever preferred_lft forever # 使用 tcpdump 尝试抓下 tun0 的数据，会阻塞在这里，等待数据到达 \u003e tcpdump -i tun0 现在再启动第三个窗口发点数据给 tun0，持续观察前面 tcpdump 和 mytun 的日志: text # 直接 ping tun0 的地址，貌似有问题，数据没进 mytun 程序，而且还有响应 ❯ ping -c 4 172.21.22.23 PING 172.21.22.23 (172.21.22.23) 56(84) bytes of data. 64 bytes from 172.21.22.23: icmp_seq=1 ttl=64 time=0.167 ms 64 bytes from 172.21.22.23: icmp_seq=2 ttl=64 time=0.180 ms 64 bytes from 172.21.22.23: icmp_seq=3 ttl=64 time=0.126 ms 64 bytes from 172.21.22.23: icmp_seq=4 ttl=64 time=0.141 ms --- 172.21.22.23 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3060ms rtt min/avg/max/mdev = 0.126/0.153/0.180/0.021 ms # 但是 ping 该网段下的其他地址，流量就会被转发给 mytun 程序，因为 mytun 啥数据也没回，自然丢包率 100% # tcpdump 和 mytun 都会打印出相关日志 ❯ ping -c 4 172.21.22.26 PING 172.21.22.26 (172.21.22.26) 56(84) bytes of data. --- 172.21.22.26 ping statistics --- 4 packets transmitted, 0 received, 100% packet loss, time 3055ms 下面给出 mytun 的输出： text Read 84 bytes from tun/ta","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:1:1","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#c-语言编程测试-tun-设备"},{"categories":["tech"],"content":" TUN 与 TAP 的区别TUN 和 TAP 的区别在于工作的网络层次不同，用户程序通过 TUN 设备只能读写网络层的 IP 数据包， 而 TAP 设备则支持读写链路层的数据包（通常是以太网数据包，带有 Ethernet headers）。 TUN 与 TAP 的关系，就类似于 socket 和 raw socket. TUN/TAP 应用最多的场景是 VPN 代理，比如: clash: 一个支持各种规则的隧道，也支持 TUN 模式 tun2socks: 一个全局透明代理，和 VPN 的工作模式一样，它通过创建虚拟网卡+修改路由表，在第三层网络层代理系统流量。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:1:2","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#tun-与-tap-的区别"},{"categories":["tech"],"content":" 二、vethveth 接口总是成对出现，一对 veth 接口就类似一根网线，从一端进来的数据会从另一端出去。 同时 veth 又是一个虚拟网络接口，因此它和 TUN/TAP 或者其他物理网络接口一样，也都能配置 mac/ip 地址（但是并不是一定得配 mac/ip 地址）。 其主要作用就是连接不同的网络，比如在容器网络中，用于将容器的 namespace 与 root namespace 的网桥 br0 相连。容器网络中，容器侧的 veth 自身设置了 ip/mac 地址并被重命名为 eth0，作为容器的网络接口使用，而主机侧的 veth 则直接连接在 docker0/br0 上面。 使用 veth 实现容器网络，需要结合下一小节介绍的 bridge，在下一小节将给出容器网络结构图。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:2:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#二veth"},{"categories":["tech"],"content":" 三、bridgeLinux Bridge 是工作在链路层的网络交换机，由 Linux 内核模块 bridge 提供，它负责在所有连接到它的接口之间转发链路层数据包。 添加到 Bridge 上的设备被设置为只接受二层数据帧并且转发所有收到的数据包到 Bridge 中。在 Bridge 中会进行一个类似物理交换机的查MAC端口映射表、转发、更新MAC端口映射表这样的处理逻辑，从而数据包可以被转发到另一个接口/丢弃/广播/发往上层协议栈，由此 Bridge 实现了数据转发的功能。 如果使用 tcpdump 在 Bridge 接口上抓包，可以抓到网桥上所有接口进出的包，因为这些数据包都要通过网桥进行转发。 与物理交换机不同的是，Bridge 本身可以设置 IP 地址，可以认为当使用 brctl addbr br0 新建一个 br0 网桥时，系统自动创建了一个同名的隐藏 br0 网络接口。br0 一旦设置 IP 地址，就意味着这个隐藏的 br0 接口可以作为路由接口设备，参与 IP 层的路由选择(可以使用 route -n 查看最后一列 Iface)。因此只有当 br0 设置 IP 地址时，Bridge 才有可能将数据包发往上层协议栈。 但被添加到 Bridge 上的网卡是不能配置 IP 地址的，他们工作在数据链路层，对路由系统不可见。 它常被用于在虚拟机、主机上不同的 namespaces 之间转发数据。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:3:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#三bridge"},{"categories":["tech"],"content":" 虚拟机场景（桥接模式）以 qemu-kvm 为例，在虚拟机的桥接模式下，qemu-kvm 会为每个虚拟机创建一个 tun/tap 虚拟网卡并连接到 br0 网桥。虚拟机内部的网络接口 eth0 是 qemu-kvm 软件模拟的，实际上虚拟机内网络数据的收发都会被 qemu-kvm 转换成对 /dev/net/tun 的读写。 以发送数据为例，整个流程如下： 虚拟机发出去的数据包先到达 qemu-kvm 程序 数据被用户层程序 qemu-kvm 写入到 /dev/net/tun，到达 tap 设备 tap 设备把数据传送到 br0 网桥 br0 把数据交给 eth0 发送出去 整个流程跑完，数据包都不需要经过宿主机的协议栈，效率高。 text +------------------------------------------------+-----------------------------------+-----------------------------------+ | Host | VirtualMachine1 | VirtualMachine2 | | | | | | +--------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +--------------------------------------+ | +-------------------------+ | +-------------------------+ | | ↑ | ↑ | ↑ | |.......................|........................|................|..................|.................|.................| | ↓ | ↓ | ↓ | | +--------+ | +-------+ | +-------+ | | | .3.101 | | | .3.102| | | .3.103| | | +------+ +--------+ +-------+ | +-------+ | +-------+ | | | eth0 |\u003c---\u003e| br0 |\u003c---\u003e|tun/tap| | | eth0 | | | eth0 | | | +------+ +--------+ +-------+ | +-------+ | +-------+ | | ↑ ↑ ↑ +--------+ ↑ | ↑ | | | | +------|qemu-kvm|-----------+ | | | | | ↓ +--------+ | | | | | +-------+ | | | | | | |tun/tap| | | | | | | +-------+ | | | | | | ↑ | +--------+ | | | | | +-------------------------------------|qemu-kvm|-------------|-----------------+ | | | | +--------+ | | | | | | | +---------|--------------------------------------+-----------------------------------+-----------------------------------+ ↓ Physical Network (192.168.3.0/24) ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:3:1","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#虚拟机场景桥接模式"},{"categories":["tech"],"content":" 跨 namespace 通信场景（容器网络，NAT 模式） docker/podman 提供的 bridge 网络模式，就是使用 veth+bridge+iptalbes 实现的。我会在下一篇文章详细介绍「容器网络」。 由于容器运行在自己单独的 network namespace 里面，所以和虚拟机一样，它们也都有自己单独的协议栈。 容器网络的结构和虚拟机差不多，但是它改用了 NAT 网络，并把 tun/tap 换成了 veth，导致 docker0 过来的数据，要先经过宿主机协议栈，然后才进入 veth 接口。 多了一层 NAT，以及多走了一层宿主机协议栈，都会导致性能下降。 示意图如下： text +-----------------------------------------------+-----------------------------------+-----------------------------------+ | Host | Container 1 | Container 2 | | | | | | +---------------------------------------+ | +-------------------------+ | +-------------------------+ | | | Network Protocol Stack | | | Network Protocol Stack | | | Network Protocol Stack | | | +----+-------------+--------------------+ | +-----------+-------------+ | +------------+------------+ | | ^ ^ | ^ | ^ | |........|.............|........................|................|..................|.................|.................| | v v ↓ | v | v | | +----+----+ +-----+------+ | +-----+-------+ | +-----+-------+ | | | .31.101 | | 172.17.0.1 | +------+ | | 172.17.0.2 | | | 172.17.0.3 | | | +---------+ +-------------\u003c----\u003e+ veth | | +-------------+ | +-------------+ | | | eth0 | | docker0 | +--+---+ | | eth0(veth) | | | eth0(veth) | | | +----+----+ +-----+------+ ^ | +-----+-------+ | +-----+-------+ | | ^ ^ | | ^ | ^ | | | | +------------------------+ | | | | | v | | | | | | +--+---+ | | | | | | | veth | | | | | | | +--+---+ | | | | | | ^ | | | | | | +------------------------------------------------------------------------------+ | | | | | | | | | | | +-----------------------------------------------+-----------------------------------+-----------------------------------+ v Physical Network (192.168.31.0/24) 每创建一个新容器，都会在容器的 namespace 里新建一个 veth 接口并命令为 eth0，同时在主 namespace 创建一个 veth，将容器的 eth0 与 docker0 连接。 可以在容器中通过 iproute2 查看到， eth0 的接口类型为 veth： shell ❯ docker run -it --rm debian:buster bash root@5facbe4ddc1e:/# ip --details addr ls 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0 minmtu 0 maxmtu 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 20: eth0@if21: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 0 minmtu 68 maxmtu 65535 veth numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever 同时在宿主机中能看到对应的 veth 设备是绑定到了 docker0 网桥的： shell ❯ sudo brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242fce99ef5 no vethea4171a ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:3:2","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#跨-namespace-通信场景容器网络nat-模式"},{"categories":["tech"],"content":" 四、macvlan 目前 docker/podman 都支持创建基于 macvlan 的 Linux 容器网络。 注意 macvlan 和 WiFi 存在兼容问题，如果使用笔记本测试，可能会遇到麻烦。 参考文档：linux 网络虚拟化： macvlan macvlan 是比较新的 Linux 特性，需要内核版本 \u003e= 3.9，它被用于在主机的网络接口（父接口）上配置多个虚拟子接口，这些子接口都拥有各自独立的 mac 地址，也可以配上 ip 地址进行通讯。 macvlan 下的虚拟机或者容器网络和主机在同一个网段中，共享同一个广播域。macvlan 和 bridge 比较相似，但因为它省去了 bridge 的存在，所以配置和调试起来比较简单，而且效率也相对高。除此之外，macvlan 自身也完美支持 VLAN。 如果希望容器或者虚拟机放在主机相同的网络中，享受已经存在网络栈的各种优势，可以考虑 macvlan。 我会在下一篇文章对 docker 的 macvlan/ipvlan 做个分析，这里先略过了… ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:4:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#四macvlan"},{"categories":["tech"],"content":" 五、ipvlan linux 网络虚拟化： ipvlan cilium 1.9 已经提供了基于 ipvlan 的网络（beta 特性），用于替换传统的 veth+bridge 容器网络。详见IPVLAN based Networking (beta) - Cilium 1.9 Docs ipvlan 和 macvlan 的功能很类似，也是用于在主机的网络接口（父接口）上配置出多个虚拟的子接口。但不同的是，ipvlan 的各子接口没有独立的 mac 地址，它们和主机的父接口共享 mac 地址。 因为 mac 地址共享，所以如果使用 DHCP，就要注意不能使用 mac 地址做 DHCP，需要额外配置唯一的 clientID. 如果你遇到以下的情况，请考虑使用 ipvlan： 父接口对 mac 地址数目有限制，或者在 mac 地址过多的情况下会造成严重的性能损失 工作在 802.11(wireless)无线网络中（macvlan 无法和无线网络共同工作） 希望搭建比较复杂的网络拓扑（不是简单的二层网络和 VLAN），比如要和 BGP 网络一起工作 基于 ipvlan/macvlan 的容器网络，比 veth+bridge+iptables 的性能要更高。 我会在下一篇文章对 docker 的 macvlan/ipvlan 做个分析，这里先略过了… ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:5:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#五ipvlan"},{"categories":["tech"],"content":" 六、vlanvlan 即虚拟局域网，是一个链路层的广播域隔离技术，可以用于切分局域网，解决广播泛滥和安全性问题。被隔离的广播域之间需要上升到第三层才能完成通讯。 常用的企业路由器如 ER-X 基本都可以设置 vlan，Linux 也直接支持了 vlan. 以太网数据包有一个专门的字段提供给 vlan 使用，vlan 数据包会在该位置记录它的 VLAN ID，交换机通过该 ID 来区分不同的 VLAN，只将该以太网报文广播到该 ID 对应的 VLAN 中。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:6:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#六vlan"},{"categories":["tech"],"content":" 七、vxlan/geneve rfc8926 - Geneve: Generic Network Virtualization Encapsulation \u003erfc7348 - Virtual eXtensible Local Area Network (VXLAN) linux 上实现 vxlan 网络 在介绍 vxlan 前，先说明下两个名词的含义： underlay 网络：即物理网络 overlay 网络：指在现有的物理网络之上构建的虚拟网络。其实就是一种隧道技术，将原生态的二层数据帧报文进行封装后通过隧道进行传输。 vxlan 与 geneve 都是 overlay 网络协议，它俩都是使用 UDP 包来封装链路层的以太网帧。 vxlan 在 2014 年标准化，而 geneve 在 2020 年底才通过草案阶段，目前尚未形成最终标准。但是目前 linux/cilium 都已经支持了 geneve. geneve 相对 vxlan 最大的变化，是它更灵活——它的 header 长度是可变的。 目前所有 overlay 的跨主机容器网络方案，几乎都是基于 vxlan 实现的（例外：cilium 也支持 geneve）。 我们在学习单机的容器网络时，不需要接触到 vxlan，但是在学习跨主机容器网络方案如 flannel/calico/cilium 时，那 vxlan(overlay) 及 BGP(underlay) 就不可避免地要接触了。 先介绍下 vxlan 的数据包结构： VXLAN 栈帧结构 在创建 vxlan 的 vtep 虚拟设备时，我们需要手动设置图中的如下属性： VXLAN 目标端口：即接收方 vtep 使用的端口，这里 IANA 定义的端口是 4789，但是只有 calico 的 vxlan 模式默认使用该端口 calico，而 cilium/flannel 的默认端口都是 Linux 默认的 8472. VNID: 每个 VXLAN 网络接口都会被分配一个独立的 VNID 一个点对点的 vxlan 网络架构图如下: VXLAN 点对点网络架构 可以看到每台虚拟机 VM 都会被分配一个唯一的 VNID，然后两台物理机之间通过 VTEP 虚拟网络设备建立了 VXLAN 隧道，所有 VXLAN 网络中的虚拟机，都通过 VTEP 来互相通信。 有了上面这些知识，我们就可以通过如下命令在两台 Linux 机器间建立一个点对点的 VXLAN 隧道： shell # 在主机 A 上创建 VTEP 设备 vxlan0 # 与另一个 vtep 接口 B（192.168.8.101）建立隧道 # 将 vxlan0 自身的 IP 地址设为 192.168.8.100 # 使用的 VXLAN 目标端口为 4789(IANA 标准) ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ remote 192.168.8.101 \\ local 192.168.8.100 \\ dev enp0s8 # 为我们的 VXLAN 网络设置虚拟网段，vxlan0 就是默认网关 ip addr add 10.20.1.2/24 dev vxlan0 # 启用我们的 vxlan0 设备，这会自动生成路由规则 ip link set vxlan0 up # 现在在主机 B 上运行如下命令，同样创建一个 VTEP 设备 vxlan0，remote 和 local 的 ip 与前面用的命令刚好相反。 # 注意 VNID 和 dstport 必须和前面完全一致 ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ remote 192.168.8.100 \\ local 192.168.8.101 \\ dev enp0s8 # 为我们的 VXLAN 网络设置虚拟网段，vxlan0 就是默认网关 ip addr add 10.20.1.3/24 dev vxlan0 ip link set vxlan0 up # 到这里，两台机器就完成连接，可以通信了。可以在主机 B 上 ping 10.20.1.2 试试，应该能收到主机 A 的回应。 ping 10.20.1.2 点对点的 vxlan 隧道实际用处不大，如果集群中的每个节点都互相建 vxlan 隧道，代价太高了。 一种更好的方式，是使用 「组播模式」的 vxlan 隧道，这种模式下一个 vtep 可以一次与组内的所有 vtep 建立隧道。示例命令如下（这里略过了如何设置组播地址 239.1.1.1 的信息）： shell ip link add vxlan0 type vxlan \\ id 42 \\ dstport 4789 \\ group 239.1.1.1 \\ dev enp0s8 ip addr add 10.20.1.2/24 dev vxlan0 ip link set vxlan0 up 可以看到，只需要简单地把 local_ip/remote_ip 替换成一个组播地址就行。组播功能会将收到的数据包发送给组里的所有 vtep 接口，但是只有 VNID 能对上的 vtep 会处理该报文，其他 vtep 会直接丢弃数据。 接下来，为了能让所有的虚拟机/容器，都通过 vtep 通信，我们再添加一个 bridge 网络，充当 vtep 与容器间的交换机。架构如下： VXLAN 多播网络架构 使用 ip 命令创建网桥、网络名字空间、veth pairs 组成上图中的容器网络： shell # 创建 br0 并将 vxlan0 绑定上去 ip link add br0 type bridge ip link set vxlan0 master br0 ip link set vxlan0 up ip link set br0 up # 模拟将容器加入到网桥中的操作 ip netns add container1 ## 创建 veth pair，并把一端加到网桥上 ip link add veth0 type veth peer name veth1 ip link set dev veth0 master br0 ip link set dev veth0 up ## 配置容器内部的网络和 IP ip link set dev veth1 netns container1 ip netns exec container1 ip link set lo up ip netns exec container1 ip link set veth1 name eth0 ip netns exec container1 ip addr add 10.20.1.11/24 dev eth0 ip netns exec container1 ip link set eth0 up 然后在另一台机器上做同样的操作，并创建新容器，两个容器就能通过 vxlan 通信啦~ ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:7:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#vxlan-geneve"},{"categories":["tech"],"content":" 比组播更高效的 vxlan 实现组播最大的问题在于，因为它不知道数据的目的地，所以每个 vtep 都发了一份。如果每次发数据时， 如果能够精确到对应的 vtep，就能节约大量资源。 另一个问题是 ARP 查询也会被组播，要知道 vxlan 本身就是个 overlay 网络，ARP 的成本也很高。 上述问题都可以通过一个中心化的注册中心（如 etcd）来解决，所有容器、网络的注册与变更，都写入到这个注册中心，然后由程序自动维护 vtep 之间的隧道、fdb 表及 ARP 表. ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:7:1","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#比组播更高效的-vxlan-实现"},{"categories":["tech"],"content":" 八、虚拟网络接口的速率Loopback 和本章讲到的其他虚拟网络接口一样，都是一种软件模拟的网络设备。他们的速率是不是也像物理链路一样，存在链路层（比如以太网）协议的带宽限制呢？ 比如目前很多老旧的网络设备，都是只支持到百兆以太网，这就决定了它的带宽上限。即使是较新的设备，目前基本也都只支持到千兆，也就是 1GbE 以太网标准，那本文提到的虚拟网络接口单纯在本机内部通信，是否也存在这样的制约呢？是否也只能跑到 1GbE? 另外物理网络还存在链路层协议协商机制，将一个千兆接口与一个百兆接口连接，它们会自动协商使用百兆以太网标准进行通讯。虚拟网络接口是否也存在这样的机制呢？ 先使用 ethtool 检查看看： text # docker 容器的 veth 接口速率 \u003e ethtool vethe899841 | grep Speed Speed: 10000Mb/s # 网桥看起来没有固定的速率 \u003e ethtool docker0 | grep Speed Speed: Unknown! # tun0 设备的默认速率貌似是 10Mb/s ? \u003e ethtool tun0 | grep Speed Speed: 10Mb/s # 此外 ethtool 无法检查 lo 以及 wifi 的速率，先略过不提 从上面的输出能看到，虚拟接口的 Speed 属性都有点离谱，veth 接口显示 10Gb/s，tun0 更是离谱的 10Mb/s. 那么事实真的如此么？话不多说，先实测一波。 ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:8:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#八虚拟网络接口的速率"},{"categories":["tech"],"content":" 网络性能实测接下来实际测试一下，受先给出测试机的配置： text ❯ cat /etc/os-release NAME=\"openSUSE Tumbleweed\" # VERSION=\"20210810\" ... ❯ uname -a Linux legion-book 5.13.8-1-default #1 SMP Thu Aug 5 08:56:22 UTC 2021 (967c6a8) x86_64 x86_64 x86_64 GNU/Linux ❯ lscpu Architecture: x86_64 CPU(s): 16 Model name: AMD Ryzen 7 5800H with Radeon Graphics ... # 内存，单位 MB ❯ free -m total used free shared buff/cache available Mem: 27929 4482 17324 249 6122 22797 Swap: 2048 0 2048 好了，现在使用 iperf3 测试： shell # 启动服务端 iperf3 -s ------------- # 新窗口启动客户端，通过 loopback 接口访问 iperf3-server，大概 49Gb/s ❯ iperf3 -c 127.0.0.1 Connecting to host 127.0.0.1, port 5201 [ 5] local 127.0.0.1 port 48656 connected to 127.0.0.1 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 4.46 GBytes 38.3 Gbits/sec 0 1.62 MBytes [ 5] 1.00-2.00 sec 4.61 GBytes 39.6 Gbits/sec 0 1.62 MBytes [ 5] 2.00-3.00 sec 5.69 GBytes 48.9 Gbits/sec 0 1.62 MBytes [ 5] 3.00-4.00 sec 6.11 GBytes 52.5 Gbits/sec 0 1.62 MBytes [ 5] 4.00-5.00 sec 6.04 GBytes 51.9 Gbits/sec 0 1.62 MBytes [ 5] 5.00-6.00 sec 6.05 GBytes 52.0 Gbits/sec 0 1.62 MBytes [ 5] 6.00-7.00 sec 6.01 GBytes 51.6 Gbits/sec 0 1.62 MBytes [ 5] 7.00-8.00 sec 6.05 GBytes 52.0 Gbits/sec 0 1.62 MBytes [ 5] 8.00-9.00 sec 6.34 GBytes 54.5 Gbits/sec 0 1.62 MBytes [ 5] 9.00-10.00 sec 5.91 GBytes 50.8 Gbits/sec 0 1.62 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 57.3 GBytes 49.2 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 57.3 GBytes 49.2 Gbits/sec receiver # 客户端通过 wlp4s0 wifi 网卡(192.168.31.228)访问 iperf3-server，实际还是走的本机，但是速度要比 loopback 快一点，可能是默认设置的问题 ❯ iperf3 -c 192.168.31.228 Connecting to host 192.168.31.228, port 5201 [ 5] local 192.168.31.228 port 43430 connected to 192.168.31.228 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 5.12 GBytes 43.9 Gbits/sec 0 1.25 MBytes [ 5] 1.00-2.00 sec 5.29 GBytes 45.5 Gbits/sec 0 1.25 MBytes [ 5] 2.00-3.00 sec 5.92 GBytes 50.9 Gbits/sec 0 1.25 MBytes [ 5] 3.00-4.00 sec 6.00 GBytes 51.5 Gbits/sec 0 1.25 MBytes [ 5] 4.00-5.00 sec 5.98 GBytes 51.4 Gbits/sec 0 1.25 MBytes [ 5] 5.00-6.00 sec 6.05 GBytes 52.0 Gbits/sec 0 1.25 MBytes [ 5] 6.00-7.00 sec 6.16 GBytes 52.9 Gbits/sec 0 1.25 MBytes [ 5] 7.00-8.00 sec 6.08 GBytes 52.2 Gbits/sec 0 1.25 MBytes [ 5] 8.00-9.00 sec 6.00 GBytes 51.6 Gbits/sec 0 1.25 MBytes [ 5] 9.00-10.00 sec 6.01 GBytes 51.6 Gbits/sec 0 1.25 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 58.6 GBytes 50.3 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 58.6 GBytes 50.3 Gbits/sec receiver # 从容器中访问宿主机的 iperf3-server，速度几乎没区别 ❯ docker run -it --rm --name=iperf3-server networkstatic/iperf3 -c 192.168.31.228 Connecting to host 192.168.31.228, port 5201 [ 5] local 172.17.0.2 port 43436 connected to 192.168.31.228 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 4.49 GBytes 38.5 Gbits/sec 0 403 KBytes [ 5] 1.00-2.00 sec 5.31 GBytes 45.6 Gbits/sec 0 544 KBytes [ 5] 2.00-3.00 sec 6.14 GBytes 52.8 Gbits/sec 0 544 KBytes [ 5] 3.00-4.00 sec 5.85 GBytes 50.3 Gbits/sec 0 544 KBytes [ 5] 4.00-5.00 sec 6.14 GBytes 52.7 Gbits/sec 0 544 KBytes [ 5] 5.00-6.00 sec 5.99 GBytes 51.5 Gbits/sec 0 544 KBytes [ 5] 6.00-7.00 sec 5.86 GBytes 50.4 Gbits/sec 0 544 KBytes [ 5] 7.00-8.00 sec 6.05 GBytes 52.0 Gbits/sec 0 544 KBytes [ 5] 8.00-9.00 sec 5.99 GBytes 51.5 Gbits/sec 0 544 KBytes [ 5] 9.00-10.00 sec 6.12 GBytes 52.5 Gbits/sec 0 544 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 58.0 GBytes 49.8 Gbits/sec 0 sender [ 5] 0.00-10.00 sec 58.0 GBytes 49.8 Gbits/sec receiver 把 iperf3-server 跑在容器里再测一遍： shell # 在容器中启动 iperf3-server，并映射到宿主机端口 6201 \u003e docker run -it --rm --name=iperf3-server -p 6201:5201 networkstatic/iperf3 -s \u003e docker inspect --format \"{{ .NetworkSettings.IPAddress }}\" iperf3-server 172.17.0.2 ----------------------------- # 测试容器之间互访的速度，ip 为 iperf3-server 的容器 ip，速度要慢一些。 # 毕竟过了 veth -","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:8:1","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#网络性能实测"},{"categories":["tech"],"content":" 参考 Linux虚拟网络设备之tun/tap Linux虚拟网络设备之veth 云计算底层技术-虚拟网络设备(Bridge,VLAN) 云计算底层技术-虚拟网络设备(tun/tap,veth) Universal TUN/TAP device driver - Kernel Docs Tun/Tap interface tutorial Linux Loopback performance with TCP_NODELAY enabled ","date":"2021-08-14","objectID":"/posts/linux-virtual-network-interfaces/:9:0","series":["计算机网络相关"],"tags":["Linux","网络","虚拟化","容器"],"title":"Linux 中的虚拟网络接口","uri":"/posts/linux-virtual-network-interfaces/#参考"},{"categories":["tech"],"content":" 文中的命令均在 macOS Big Sure 和 openSUSE Tumbleweed 上测试通过 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:0:0","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#"},{"categories":["tech"],"content":" socat \u0026 netcatnetcat(network cat) 是一个历史悠久的网络工具包，被称作 TCP/IP 的瑞士军刀，各大 Linux 发行版都有默认安装 openbsd 版本的 netcat，它的命令行名称为 nc. 而 socat(socket cat)，官方文档描述它是\"netcat++\" (extended design, new implementation)，项目比较活跃，kubernetes-client(kubectl) 底层就是使用的它做各种流量转发。 在不方便安装 socat 的环境中，我们可以使用系统自带的 netcat. 而在其他环境，可以考虑优先使用 socat. ","date":"2021-04-11","objectID":"/posts/socat-netcat/:1:0","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#socat--netcat"},{"categories":["tech"],"content":" 一、简介socat 的基本命令格式： shell socat [参数] 地址1 地址2 给 socat 提供两个地址，socat 干的活就是把两个地址的流对接起来。左边地址的输出传给右边，同时又把右边地址的输出传给左边，也就是一个双向的数据管道。 听起来好像没啥特别的，但是实际上计算机网络干的活也就是数据传输而已，却影响了整个世界，不可小觑它的功能。 socat 支持非常多的地址类型：-/stdio，TCP, TCP-LISTEN, UDP, UDP-LISTEN, OPEN, EXEC, SOCKS, PROXY 等等，可用于端口监听、链接，文件和进程读写，代理桥接等等。 socat 的功能就是这么简单，命令行参数也很简洁，唯一需要花点精力学习的就是它各种地址的定义和搭配写法。 而 netcat 定义貌似没这么严谨，可以简单的理解为网络版的 cat 命令 2333 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:2:0","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#一简介"},{"categories":["tech"],"content":" 二、安装方法各发行版都自带 netcat，包名通常为 nc-openbsd，因此这里只介绍 socat 的安装方法： shell # Debian/Ubuntu sudo apt install socat # CentOS/RedHat sudo yum install socat # macOS brew install socat 其他发行版基本也都可以使用包管理器安装 socat ","date":"2021-04-11","objectID":"/posts/socat-netcat/:3:0","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#二安装方法"},{"categories":["tech"],"content":" 三、常用命令","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:0","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#三常用命令"},{"categories":["tech"],"content":" 1. 检测远程端口的可连接性 很多人会用 telnet 来做这项测试，不过现在很多发行版基本都不自带 telnet 了，还需要额外安装。telnet 差不多已经快寿终正寝了，还是建议使用更专业的 socat/netcat 使用 socat/netcat 检测远程端口的可连接性： shell # -d[ddd] 增加日志详细程度，-dd Prints fatal, error, warning, and notice messages. socat -dd - TCP:192.168.1.252:3306 # -v 显示详细信息 # -z 不发送数据，效果为立即关闭连接，快速得出结果 nc -vz 192.168.1.2 8080 # -vv 显示更详细的内容 # -w2 超时时间设为 2 秒 # 使用 nc 做简单的端口扫描 nc -vv -w2 -z 192.168.1.2 20-500 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:1","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#1-检测远程端口的可连接性"},{"categories":["tech"],"content":" 2. 测试本机端口是否能被外部访问在本机监听一个 TCP 端口，接收到的内容传到 stdout，同时将 stdin 的输入传给客户端： shell # 服务端启动命令，socat/nc 二选一 socat TCP-LISTEN:7000 - # -l --listening nc -l 7000 # 当然也可以使用 python3（注意文件安全性） # 此命令在 7000 端口启用一个文件服务器，绑定到 0.0.0.0，以当前目录为根目录 python3 -m http.server 7000 # 或者在较老的机器上可以用 python2（注意文件安全性） python -m SimpleHTTPServer 8000 # 客户端连接命令，socat/nc 二选一 socat TCP:192.168.31.123:7000 - nc 192.168.11.123 7000 UDP 协议的测试也非常类似，使用 netcat 的示例如下： shell # 服务端，只监听 ipv4 nc -u -l 8080 # 客户端 nc -u 192.168.31.123 8080 # 客户端本机测试，注意 localhost 会被优先解析为 ipv6! 这会导致服务端(ipv4)的 nc 接收不到数据！ nc -u localhost 8080 使用 socat 的 UDP 测试示例如下： shell socat UDP-LISTEN:7000 - socat UDP:192.168.31.123:7000 - ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:2","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#2-测试本机端口是否能被外部访问"},{"categories":["tech"],"content":" 3. 调试 TLS 协议 参考 socat 官方文档：Securing Traffic Between two Socat Instances Using SSL 测试证书及私钥的生成参见写给开发人员的实用密码学（八）—— 数字证书与 TLS 协议 模拟一个 mTLS 服务器，监听 4433 端口，接收到的数据同样输出到 stdout： shell # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem cat client.key client.crt \u003e client.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,cafile=client.crt - # 客户端连接命令（使用系统的 ca.crt 或者你自己的私有 ca.cert 来验证服务端证书） socat - openssl-connect:192.168.31.123:4433,cert=client.pem,cafile=ca.crt # 或者使用 curl 连接（ca.crt 证书来源同上） curl -v --cacert ca.crt --cert client.crt --key client.key --tls-max 1.2 https://192.168.31.123:4433 上面的命令使用了 mTLS 双向认证的协议，可通过设定 verify=0 来关掉客户端认证，示例如下： shell # socat 需要使用同时包含证书和私钥的 pem 文件，生成方法如下 cat server.key server.crt \u003e server.pem # 服务端启动命令 socat openssl-listen:4433,reuseaddr,cert=server.pem,verify=0 - # 客户端连接命令，如果 ip/域名不受证书保护，就也需要添加 verify=0 # （使用系统的 ca.crt 或者你自己的私有 ca.cert 来验证服务端证书） socat - openssl-connect:192.168.31.123:4433,cafile=ca.crt # 或者使用 curl 连接（ca.crt 证书来源同上） curl -v --cacert ca.crt https://192.168.31.123:4433 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:3","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#3-调试-tls-协议"},{"categories":["tech"],"content":" 4. 数据传输通常传输文件时，我都习惯使用 scp/ssh/rsync，但是 socat 其实也可以传输文件。 以将 demo.tar.gz 从主机 A 发送到主机 B 为例，首先在数据发送方 A 执行如下命令： shell # -u 表示数据只从左边的地址单向传输给右边（socat 默认是一个双向管道） # -U 和 -u 相反，数据只从右边单向传输给左边 socat -u open:demo.tar.gz tcp-listen:2000,reuseaddr 然后在数据接收方 B 执行如下命令，就能把文件接收到： shell socat -u tcp:192.168.1.252:2000 open:demo.tar.gz,create # 如果觉得太繁琐，也可以直接通过 stdout 重定向 socat -u tcp:192.168.1.252:2000 - \u003e demo.tar.gz 使用 netcat 也可以实现数据传输： shell # 先在接收方启动服务端 nc -l -p 8080 \u003e demo.tar.gz # 再在发送方启动客户端发送数据 nc 192.168.1.2 8080 \u003c demo.tar.gz ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:4","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#4-数据传输"},{"categories":["tech"],"content":" 5. 担当临时的 web 服务器使用 fork reuseaddr SYSTEM 三个命令，再用 systemd/supervisor 管理一下，就可以用几行命令实现一个简单的后台服务器。 下面的命令将监听 8080 端口，并将数据流和 web.py 的 stdio 连接起来，可以直接使用浏览器访问http://\u003cip\u003e:8080 来查看效果。 shell socat TCP-LISTEN:8080,reuseaddr,fork SYSTEM:\"python3 web.py\" 假设 web.py 的内容为（注意 print 的内容要与 HTTP 协议格式一致）： python print(\"\"\"HTTP/1.1 200 OK Content-Type: text/plain hello world \"\"\") 那 curl localhost:8080 就应该会输出 hello world 当然，如果你仅希望快速提供一个文件服务器，也可直接使用 python 命令： shell # https://docs.python.org/3/library/http.server.html#http.server.SimpleHTTPRequestHandler.do_GET python3 -m http.server 8000 --directory /tmp/ # 或者在旧机器上可以直接使用 python2 提供文件服务器，默认以当前文件夹为根目录 python -m SimpleHTTPServer 8000 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:5","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#5-担当临时的-web-服务器"},{"categories":["tech"],"content":" 6. 端口转发监听 8080 端口，建立该端口与 baidu.com:80 之间的双向管道: shell socat TCP-LISTEN:8080,fork,reuseaddr TCP:baidu.com:80 拿 curl 命令测试一下，应该能正常访问到百度： shell # 注意指定 Host curl -v -H 'Host: baidu.com' localhost:8080 其他用法，比如为一个仅监听了 127.0.0.1 loopback 网卡的服务，允许通过外部网络访问（注意安全性）： shell socat TCP-LISTEN:5432,fork,reuseaddr TCP:localhost:3658 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:6","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#6-端口转发"},{"categories":["tech"],"content":" 7. 端口扫描netcat 支持简单的端口扫描功能，如下示例扫描 192.168.1.2 从 8000 到 9000 的所有端口号： text # -w 指定连接超时时间（秒） # -z 只扫描正在监听的 daemons，不向其发送任何数据 nc -vv -w3 -z 192.168.1.2 8000-9000 socat 貌似不支持这项功能，估计是更建议使用专业的 nmap 吧。 ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:7","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#7-端口扫描"},{"categories":["tech"],"content":" 其他socat 还提供了丰富的 examples 与 tutorials，介绍了许多其他用法，包括： Building TUN based virtual networks with socat: 构造 TUN 虚拟网卡 IP Multicasting with Socat: 支持 IP 包广播，将管道另一端设为一个 CIDR 网段 etc… 详见官方文档： socat - Multipurpose relay nc-openbsd man page ","date":"2021-04-11","objectID":"/posts/socat-netcat/:4:8","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#其他"},{"categories":["tech"],"content":" 参考 新版瑞士军刀：socat - 韦易笑 - 知乎 用好你的瑞士军刀/netcat - 韦易笑 - 知乎 socat - Multipurpose relay ","date":"2021-04-11","objectID":"/posts/socat-netcat/:5:0","series":["计算机网络相关"],"tags":["网络","Linux","网络调试"],"title":"Linux 网络工具中的瑞士军刀 - socat \u0026 netcat","uri":"/posts/socat-netcat/#参考"},{"categories":["life"],"content":" 本文可能充斥着学生型思维，请谨慎阅读… ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:0:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#"},{"categories":["life"],"content":" 年轻真好最近看了些前辈们的博客，很多是在计算机行业工作几十年的前辈，还有许嵩的文章。 我更深刻地认识到了一件事：我当下的很多文章，都能看得出我在很认真的思考、总结，但是总是有很明显的稚嫩的感觉在里面——我自认为这是「学生型思维」。 我总是喜欢讲「且行且寻」、「自己的眼界还太狭窄了，我对世界还很缺乏了解」、「根本看不清好坏，就无法独立做出决策」诸如此类。 我把这样的文章写出来，前辈们给我留言「博主只是沉淀的时间还远远不够。憋着急，年轻就是最大的资本。」、「只想说年轻真好，使劲折腾才知道要什么东西」。 嗯，我理解到了，因为我「年轻」，所以写出这样的文章没问题，可以使劲去折腾、去探索、去思考。 ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:1:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#年轻真好"},{"categories":["life"],"content":" 三十而立 子曰：吾，十有五，而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲，不逾矩。 孔子说：“我十五岁立志学习，三十岁在人生道路上站稳脚跟，四十岁心中不再迷惘，五十岁知道上天给我安排的命运，六十岁听到别人说话就能分辨是非真假，七十岁能随心所欲地说话做事，又不会超越规矩。” 「四十而不惑」对我而言可能还太远，但「三十而立」却是已经能预见到了的，没几年了。 三十而立，人到了三十岁，就应该知道自己如何立身处世，尘世滚滚中能守住自己的一点本真不失。 三十岁，已不是一个年轻的年纪了。 如果我到了三十岁，还去写些「自己的眼界还太狭窄了，我对世界还很缺乏了解」、「我根本看不清好坏，很多时候无法独立做出决策」，那就贻笑大方了。 所以即使说「年轻就是最大的资本」，也不是能随意挥霍的。 人生这条道路上我们踽踽独行，道阻且长，眼光要放长远一点、多看一点，不要把自己限制住了，更不应该原地踏步。 ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:2:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#三十而立"},{"categories":["life"],"content":" 许嵩——我没有梦想这两天看多了前辈们的博客，就想找点非虚构的书藉看看，补充点阅历。 昨天向朋友们讨书看，@rea.ink 就给我推荐许嵩的《海上灵光》。意外地发现了许嵩的新浪博客。 博客的内容都很老了，最新的一篇是 2013 年。但是这并不妨碍其中见解的价值 海上灵光——许嵩 以前媒体问我接下来有什么计划或梦想时我总是很愣的回答，我没有梦想。 真的，一个年过半百的人还把梦想这种字眼挂在嘴上是很乏味的。 睁大眼看看眼前的生活，周遭的一切吧。 脚踏实地认真过好每一天的生活吧。 至于心底的信念——是决计不必拿出来高谈阔论的。 出离心——许嵩 这几个月，走过了不少地方。 每到一处，采访我的媒体通常会有这么一问：你的音乐理想是什么？ 而当答案是“我从来没有理想”时，我迎接那些错愕的眼神。 年轻的时候，拥有一些世俗的念想（比如声名远播？）、一些物质上的期待（比如大房子好车子？）、一些精神上的憧憬（比如寻得佳偶？）、一些相对崇高的目标（比如造福子孙？！），似乎的确能让一些人更有动力的过活每一天。 但如果，岁月在你脸上已然留下不少年轮——你坐船的动机仍然只是到达一座岛，别人把岛上的一切美妙和宝藏说给你听就可以让你划船划的更带劲儿——那我能对你说些什么呢？ ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:3:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#许嵩我没有梦想"},{"categories":["life"],"content":" 池建强——你老了这两天读到了一篇池建强写的《你老了》，作者是极客时间创始人， 真的是年过半百的技术人了。 你老了 - 池建强的随想录 40 以后，不惑是不可能的，恐慌是与日俱增的。四十不惑，说得不是你想明白了，而是你想不明白的，可能就想不明白了，生日变成另一种仪式，它严肃的告诉你，同学，不要有任何幻想了，接受这个现实，你已经不再年轻了。再卖萌也改变不了这个事实。 人们总会长大，成熟，衰老，一如万事万物。今何在说，人从一出生开始，就踏上了自己的西游路，一路向西，到了尽头，就是虚无，人就没了。所有人都不可避免要奔向那个归宿，你没办法选择，没办法回头。 你老了 - 池建强的随想录 你跳不出这个世界，是因为你不知道这个世界有多大，一旦你知道了，你就超出了它。 年龄也是如此。 ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:4:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#池建强你老了"},{"categories":["life"],"content":" 梦想不要多的，想看世界也不是靠说的既然说了要多走走看看，那就多看多想。 就像许嵩写的那样，不必去高谈阔论什么理想与信念，实际行动才是最有力的证明。 Keep eyes on the stars, and feet on the ground. ","date":"2021-02-13","objectID":"/posts/no-more-dreams/:5:0","series":null,"tags":[],"title":"脚踏实地，仰望星空","uri":"/posts/no-more-dreams/#梦想不要多的想看世界也不是靠说的"},{"categories":["life"],"content":" 2022-02-09 更新：2022 年再回看这篇文章，明显感觉到我的进步很大很大，不论是工作文化与环境、薪资、吃喝玩乐、还是接触到的线上环境规模都有了质的变化，详情见2021 年总结。目前对自己的认知更清晰了，期待 2022 年我能「更上一层楼」哈哈~ 2021-09-04 更新：在新公司认识到了自己技术、方法论、思维模式等多方面的不足。这篇文章的部分内容让我觉得有点羞耻…不过就这样吧，毕竟这确实是我当时的所思所想… ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:0:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#"},{"categories":["life"],"content":" 人有悲欢离合，月有阴晴圆缺今年年底的时候，自己心思摇摆不定，这影响到了我的工作，顺势就向公司提出了离职。 这两三天和老板、技术经理，还包括公司比较强的同事们，都做了一番沟通。这一是公司希望我能够认同公司的路线和理念，跟着一条道往前走。二呢我也很想知道，老板、技术经理、还有技术骨干们，为什么能这么坚定不疑？为什么这么拼？ 结果是我和公司都发现，我们不是一路人，观念存在冲突。公司的技术骨干们都是创业思维，他们或者乐在其中，或者愿意为了老板描述的未来忍一时痛苦。他们都愿意为了产品付出更多。但是我发现对公司，我不愿意付出太多。在这里，我一直就是个普通上班族的想法，高点工资，多点个人时间，做着自己喜欢的事情。 于是我火速离职。当天办完交接，签完离职协议，拿着离职协议和离职证明，光速撤退。 这是我毕业后的第一份工作，2019 年 6 月底入职，在公司呆了一年多，学到了很多东西，绝不仅仅只是技术。因此我觉得自己有必要做一个技术以外的总结。 任何一家公司都有好有坏，但是按照惯例，这篇文章会避而不谈公司不好的东西。公司的名称呢， 这里就用 W 来代替吧。 因为有前辈在博客园评论里为 W 公司感到可惜，在开始正题之前，还是先说下离职原因。其实说来也简单，基本都能猜到：工资超低、画饼充饥、鼓励无意义加班、技术能力到了瓶颈，以及技术能力提升使我信心膨胀。 ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:1:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#人有悲欢离合月有阴晴圆缺"},{"categories":["life"],"content":" 自我认识我刚进 W 公司时，是一个刚毕业的小白，只是兴趣使然喜欢技术。因为专业不同，周围也接触不到多少搞技术的，就比较「独」。 在 W 公司我获得了和一群有上进心的人们互相协作的机会，大家在一步一步往前走的感觉，让我在职期间一直非常快乐。 我们的技术经理也给了 DevOps 团队足够的自由，甚至是鼓励我们去探索、尝试新技术。这是我这一年多 DevOps 这个方向进步这么快的最大原因。 然后在和技术经理、同事们坦诚沟通的时候，我也了解到了自己的能力，不仅仅在技术。我对公司的价值，也绝不仅仅在技术。这是我以前从来没有想过的，我喜欢技术，而且找工作发现职位要求也都是写的技术，我真的就一直以为技术就是一切。 这里我尤其要感谢技术经理，是他帮我把自己对公司的价值和不足分析得如此透彻。 下面是我结合经理和同事对我的评价，对我个人能力做个评估（三人行必有我师，仅供参考）： 理解能力、洞察能力：在公司，我这方面的能力是拔尖的。和人交流，我经常能很快地把握住核心。 表达能力：我的表达能力也是公司里拔尖的。同事跟我讲，听我描述一条鱼，他能清晰地看到鱼的骨头。 其实我日常写博客时，经常觉得自己表达地不够好，很多人的文章就比我写得更好。不同的角度看到的东西真的区别很大，感谢我的同事们。 探索能力：我日常喜欢逛 github，翻 CNCF Landscape，我的兴趣驱动着自己去探索各种新技术，思考它们的优劣。 但是我的大部分同事们都不是这样的，很多同事只读中文文档和博客，英文也必须依赖不怎么靠谱的翻译。另外他们工期紧业务多，也没我们 DevOps 这边这么多的时间去探索试用新技术。 因此，我的探索能力要强于大多数同事。 全局思考能力: 放眼全局、思考未来，在众多选择中能够并且敢于做出决策。我目前还很缺乏这样的能力。 说到底我目前还是个普通人的思路，没有把自己放在决策者的位置上去思考。 其次呢，我的知识面还太窄，导致我根本看不清好坏，很多时候就无法独立做出决策。我需要扩大自己的知识面。 技术能力：我的技术能力在公司里能评到 80 分吧。我技术不算好，基础薄弱，但是在我们一个小创业公司内部比较，能到 80 分。 管理能力：DevOps 就两三人，因为我具有上面这些能力，矮个子里拔高的，理所当然地我成了领头的。但是性格使然，我管理能力是公司最差的… 创业思维：公司是创业公司，技术骨干们都是创业思维。但是说实话我从来没想过要去创业，不愿意投入太多。这也是我离职的原因。 因此，技术经理认为，我可能更适合当讲师哈哈。 在公司也确实给同事们讲过几次课，能够看到同事们高兴地鼓掌，告诉我「讲得可以」，我就很开心。这种心情就和有人在我博客里评论「感谢，很有帮助」是一样的。 ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:2:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#自我认识"},{"categories":["life"],"content":" 我的收获首先技术就不用说了，从我这一年多的博客文章就能看到，我的技术进步相当大。 还有就是提升了对自我的认识，这一点前面也已经阐述过了。 那其他方面我收获了啥呢？大概有下面这些： 我发现，技术经理几十年的技术经验和生活经历，能让他不了解的技术领域中，也能快速找出真正有价值的东西。——经验和阅历给了他强大的洞察能力。 技术产品中最有价值的东西，也最难看透的东西，并不是技术本身，而是理念、抽象。比如 DevOps、基础设施即代码、云计算、开源。 这很难，但是能领先所有人，最先发现这些宏观概念的价值，并押注的人，就能获得巨大的先手优势。 但是「世人大都愚昧」，或者说「太过聪明」，导致这类创业团队可能和社会格格不入。不论成功失败，这类永远是少数人。 理性的沟通是好的，但是有时候情绪化的沟通反而更有效果。 我们技术经理是一个超级理性的人，但是我和他沟通，他的想法并不能很好的传达给我。反而情绪化的老板跟我沟通，我更能感同身受。 我认识了形形色色的人，公司的同事、领导，很多都有值得我学习的地方。有些感悟 富二代不在意钱，没普通人这么斤斤计较，只在意公司氛围，以及自己能做什么。反而更愿意付出更多，能够乐在其中，也更容易成功。创业公司大概很喜欢这类人。 世界上大部分人都是普通人。大众认同的观点，不一定就是正确的观点。大众观点的变化也能体现出社会的变迁。 比如当年大跃进全民的狂热，和现在公司倡导 996，社会舆论则积极反抗。公司和民众站在了对立面。 除了上面这些虚的，还有更实在的： 快乐：经理为人相当好，同事之间合作大都也很愉快，人事小姐姐超级专业，无微不至地照顾我们。在职期间我收获了相当多的快乐。 自信心：我进公司之前，作为一个跨专业的小白，非常没有底气，而且在学校的时候整个人非常颓废。但是在 W 公司，我学到了技术，工作乐在其中，还收获了同事和领导的肯定。我建立起了自信心。 Money: 虽然不多，但是我好歹也是个有些闲钱了的人hhh ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:3:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#我的收获"},{"categories":["life"],"content":" 未来毕业后，第一份工作就这样结束了。有点仓促，因为很出乎意料，但细想下来也是情理之中。 下面就是过年了，过年呢，就照着既有的计划来吧，继续提升下技术能力。至少对目前的我而言，技术还是我的能力基础和找工作的最大依仗，其他能力目前还是在围绕技术成长。 年后准备找下一份工作。这一次，我希望能多走一走，看一看，不着急做决定。 我觉得自己的眼界还太狭窄了，我对世界还很缺乏了解。以至于很多东西，我根本无法作出评判。 既然现在跳出了一座我的「围城」，自然要去多看看，外面的世界是个啥样子。 或许也没什么区别？那也得看过才能下结论啊（笑 ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:4:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#未来"},{"categories":["life"],"content":" 文末文章的最后，祝大家、也祝我自己在 2021 年里—— 拆破玉笼飞彩凤，顿开金锁走蛟龙。 ","date":"2021-02-06","objectID":"/posts/end-of-the-first-round/:5:0","series":null,"tags":["总结","心得"],"title":"我在创业公司做技术一年多的一点体会","uri":"/posts/end-of-the-first-round/#文末"},{"categories":null,"content":" 在这个信息爆炸的时代，更需要能够放慢脚步，沉下心，系统性的学习。 虽然这一页列了这么多东西，但我实际学习还是从心，当前对啥感兴趣就学啥。我也会尝试找到感兴趣的应用场景，做点小玩意或者写点学习感悟，毕竟学而不用，那一是兴趣难以持续，二是纸上谈兵学不到真本事。 ","date":"2021-02-01","objectID":"/now/:0:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#"},{"categories":null,"content":" 零、2025 年学习计划 首先仍然是每年固定的目标：每月读一本书、写一篇博客。 技术上今年的主要学习目标： 学学 Go 语言、eBPF/istio ambient mode、Linux 系统编程与性能优化 旅游与户外运动 TODO 心理学 TODO ","date":"2021-02-01","objectID":"/now/:1:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#零2025-年学习计划"},{"categories":null,"content":" 一、我正在研究这些 按优先级排序 《深入理解 Linux 进程与内存》 《深入理解 Linux 网络》 《BPF Performance Tools（英文版）》 - 35/740 《Educated - A Memoir（中文名：你当像鸟飞往你的山）》 户外徒步、露营 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 ","date":"2021-02-01","objectID":"/now/:2:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#一我正在研究这些"},{"categories":null,"content":" 二、我今年还想搞搞这些 操作系统 课程：操作系统：设计与实现 - 南大 jyy 老师的课程 资料： 操作系统：设计与实现 (2024 春季学期) 南京大学 计算机科学与技术系 计算机系统基础 课程实验 (PA) 书 Operating Systems - Three Easy Pieces ","date":"2021-02-01","objectID":"/now/:3:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#二我今年还想搞搞这些"},{"categories":null,"content":" 三、今年的阅读进展 电子版都可以在 z-library 上很方便地下载到，实体书的话可以在多抓鱼等二手书平台碰碰运气。 已读： 《Go 高级编程》 Linux/Unix 系统编程手册（下册） 正在读： 《深入理解 Linux 网络》 《深入理解 Linux 进程与内存》 《BPF Performance Tools（英文版）》 - 35/740 《Educated - A Memoir（中文名：你当像鸟飞往你的山）》 The Great Gatsby - 10/41 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 想读，但是没啥计划（大致按感兴趣程度排序）： 第一梯队 《界限：通往个人自由的实践指南》 《置身事内：中国政府与经济发展》 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 《复杂 - 梅拉尼 米歇尔》 Psychology and Life, 20th edition, by Richard J. Gerrig 第二梯队 Mountaineering: The Freedom of the Hills（登山圣经） 《分心也有好人生》 《刘擎西方现代思想讲义》 《五四运动史：现代中国的知识革命》 《八次危机：中国的真实经验》 《我们今天怎样做父亲》：梁启超的教育观 《中国国家治理的制度逻辑》 《江村经济》 《党员、党权与党争：1924 - 1949 年中国国民党的组织形态》 《我的前半生——爱新觉罗·溥仪》 《为什么学生不喜欢上学》 ","date":"2021-02-01","objectID":"/now/:4:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#三今年的阅读进展"},{"categories":null,"content":" 四、备选学习路线","date":"2021-02-01","objectID":"/now/:5:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#四备选学习路线"},{"categories":null,"content":" 1. 高优先级 操作系统理解 Linux 操作系统也是我继续精进技术必不可少的技能。 核心课程：课程 6.S081 + 书Operating Systems - Three Easy Pieces 课程相关资源 0xFFFF - MIT6.S081 Operating System Engineering (Fall 2020) 操作系统：设计与实现 - 南大 jyy 老师的课程 OSTEP 学习指南：https://github.com/ryan4yin/computer-science/tree/master/coursepages/ostep 学到 xv6 时可结合这份资料啃源码：xv6-annotated Systems Performance: Enterprise and the Cloud, 2nd Edition (2020): 进阶读物，搞系统性能优化的 《BPF Performance Tools（英文版）》：进阶读物，Linux 内核技术，主要用于搞 Linux 网络数据包处理、性能分析、系统监控的。 Linux 系统 Linux 经典书目(按阅读顺序排列) TLPI - 正在读 APUE(Advanced Programming in the UNIX Environment) UNP(Unix Network Programming) Understanding the Linux Kernel, 3rd Edition 极客时间 《Linux 内核技术实战课》 flash-linux0.11-talk 极客时间《容器实战高手课》 极客时间《eBPF 核心技术与实战》 C 语言 / Rust 语言 极客时间《深入 C 语言和程序运行原理》 极客时间《Rust 编程第一课》 Linux 性能调优与 Linux 网络技术 《深入理解 Linux 网络 - 张彦飞》 - 14/320 《深入理解 Linux 进程与内存 - 张彦飞》 极客时间《网络排查案例课》 极客时间 《Linux 性能优化实战》 计算机网络计算机网络可算是我的老本行了，用来吃饭的家伙事，技艺不能落下。 计算机网络 TCP/IP Illustrated, Volume 1, 2nd Edition - 进度 31/920 课程CS 144: Introduction to Computer Networking 以前学过一次《Computer Networking - A Top-Down Approach, 7e》，这次算是重学吧。 课程主要使用 C++，我或许可以考虑用 rust/go 实现下协议栈？ TCP/IP 协议栈的实现：可以参考 google/gvisor ","date":"2021-02-01","objectID":"/now/:5:1","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#1-高优先级"},{"categories":null,"content":" 1. 高优先级 操作系统理解 Linux 操作系统也是我继续精进技术必不可少的技能。 核心课程：课程 6.S081 + 书Operating Systems - Three Easy Pieces 课程相关资源 0xFFFF - MIT6.S081 Operating System Engineering (Fall 2020) 操作系统：设计与实现 - 南大 jyy 老师的课程 OSTEP 学习指南：https://github.com/ryan4yin/computer-science/tree/master/coursepages/ostep 学到 xv6 时可结合这份资料啃源码：xv6-annotated Systems Performance: Enterprise and the Cloud, 2nd Edition (2020): 进阶读物，搞系统性能优化的 《BPF Performance Tools（英文版）》：进阶读物，Linux 内核技术，主要用于搞 Linux 网络数据包处理、性能分析、系统监控的。 Linux 系统 Linux 经典书目(按阅读顺序排列) TLPI - 正在读 APUE(Advanced Programming in the UNIX Environment) UNP(Unix Network Programming) Understanding the Linux Kernel, 3rd Edition 极客时间 《Linux 内核技术实战课》 flash-linux0.11-talk 极客时间《容器实战高手课》 极客时间《eBPF 核心技术与实战》 C 语言 / Rust 语言 极客时间《深入 C 语言和程序运行原理》 极客时间《Rust 编程第一课》 Linux 性能调优与 Linux 网络技术 《深入理解 Linux 网络 - 张彦飞》 - 14/320 《深入理解 Linux 进程与内存 - 张彦飞》 极客时间《网络排查案例课》 极客时间 《Linux 性能优化实战》 计算机网络计算机网络可算是我的老本行了，用来吃饭的家伙事，技艺不能落下。 计算机网络 TCP/IP Illustrated, Volume 1, 2nd Edition - 进度 31/920 课程CS 144: Introduction to Computer Networking 以前学过一次《Computer Networking - A Top-Down Approach, 7e》，这次算是重学吧。 课程主要使用 C++，我或许可以考虑用 rust/go 实现下协议栈？ TCP/IP 协议栈的实现：可以参考 google/gvisor ","date":"2021-02-01","objectID":"/now/:5:1","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#操作系统"},{"categories":null,"content":" 1. 高优先级 操作系统理解 Linux 操作系统也是我继续精进技术必不可少的技能。 核心课程：课程 6.S081 + 书Operating Systems - Three Easy Pieces 课程相关资源 0xFFFF - MIT6.S081 Operating System Engineering (Fall 2020) 操作系统：设计与实现 - 南大 jyy 老师的课程 OSTEP 学习指南：https://github.com/ryan4yin/computer-science/tree/master/coursepages/ostep 学到 xv6 时可结合这份资料啃源码：xv6-annotated Systems Performance: Enterprise and the Cloud, 2nd Edition (2020): 进阶读物，搞系统性能优化的 《BPF Performance Tools（英文版）》：进阶读物，Linux 内核技术，主要用于搞 Linux 网络数据包处理、性能分析、系统监控的。 Linux 系统 Linux 经典书目(按阅读顺序排列) TLPI - 正在读 APUE(Advanced Programming in the UNIX Environment) UNP(Unix Network Programming) Understanding the Linux Kernel, 3rd Edition 极客时间 《Linux 内核技术实战课》 flash-linux0.11-talk 极客时间《容器实战高手课》 极客时间《eBPF 核心技术与实战》 C 语言 / Rust 语言 极客时间《深入 C 语言和程序运行原理》 极客时间《Rust 编程第一课》 Linux 性能调优与 Linux 网络技术 《深入理解 Linux 网络 - 张彦飞》 - 14/320 《深入理解 Linux 进程与内存 - 张彦飞》 极客时间《网络排查案例课》 极客时间 《Linux 性能优化实战》 计算机网络计算机网络可算是我的老本行了，用来吃饭的家伙事，技艺不能落下。 计算机网络 TCP/IP Illustrated, Volume 1, 2nd Edition - 进度 31/920 课程CS 144: Introduction to Computer Networking 以前学过一次《Computer Networking - A Top-Down Approach, 7e》，这次算是重学吧。 课程主要使用 C++，我或许可以考虑用 rust/go 实现下协议栈？ TCP/IP 协议栈的实现：可以参考 google/gvisor ","date":"2021-02-01","objectID":"/now/:5:1","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#计算机网络"},{"categories":null,"content":" 2. 以后可能会感兴趣的 AI infra 方向这是一个相当新的方向，因为近两年 AI 开始落地而兴起，它既要求熟悉 K8s/Istio 等传统 infra 组件，又要求对 AI 训练推理相关的技术有足够了解，例如： Operator: training-operator 调度：volcano 框架：Kubeflow, RayTrain, Argo Workflows 性能优化：Triton/BentoML/vLLM/PyTorch/CUDA 感觉是一个转型的方向，业余可以看看。 相关资料： https://github.com/stas00/ml-engineering/ 心理学与认知神经科学学习路线： 入门： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig, 2012 Educational Psychology, 14th Global Edition (Anita Woolfolk, 2021) Development Across the Life Span, 10th edition (Robert S. Feldman, 2023) 进阶到认知神经科学 Neuroscience: Exploring the Brain, 4th edition (2015) Cognitive Neuroscience: The Biology of the Mind, 5th edition (2019) 其他方向 《Intimate Relationships》 - 进度 14/449 《Social Psychology, 14e, David Myers》 嵌入式/物联网嵌入式跟 IoT 是我 2022 年底开的新坑，目前兴趣强烈。 之前制定的 FPGA 学习路线： 从数字电路到 FPGA 再到 RISC-V 当前目标：用 FPGA 实现些小功能 先学点数字电路基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 为了快速上手，直接跳过模拟电路部分，看第 12 到第 13 章 再学点 FPGA 基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 开发板：矽速荔枝糖系列，主要用 verilog 语言开发 阅读第 14 章，简单入门 FPGA verilog 语言，直接用这个站点就够了，是非常好的教程 + 练习场： https://hdlbits.01xz.net/wiki/Main_Page 更有趣的练习题： https://www.fpga4fun.com/ 从 LED 点灯到 RISCV CPU（循序渐进）:https://github.com/BrunoLevy/learn-fpga/blob/master/FemtoRV/TUTORIALS/FROM_BLINKER_TO_RISCV/README.md 进阶：学习 RISCV 与处理器微架构 书籍：Digital Design and Computer Architecture RISC-V Edition 此书从第六章开始讲 RISCV 微架构。 我目前收集的相关内容（仅是一个资料合集，内容有重复的）： 嵌入式 Linux 系列 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 Linux Driver Development for Embedded Processors 2nd Edition: 这本是 2018 年出的，写得没上面那本好、内容也没那么新，但是看评价也不错，特点是有许多的 Lab 可做。 Linux Kernel Programming: A comprehensive guide to kernel internals: Linux 内核编程领域的新书，适合入门 Linux 内核，amazon 上评价挺好，先收藏一个 Understanding the Linux Kernel, 3rd Edition：Linux 内核技术进阶。 linux-insides: 从 bootloader 开始讲解了 Linux 内核的许多重要的功能模块，看 stars 很高所以也在这里列一下。 电路原理 Practical Electronics for Inventors, Fourth Edition 芯片 ARM64: STM32 ESP32 RK3588s RISCV: milkv mars/duo, licheepi4a FPGA / 电路设计:FPGA 玩耍之旅 目前的学习目标 DIY 无人机编队飞行！要达成这个目标需要学习的东西有点多，慢慢努力吧~ 其他杂项 Go 语言进阶 《Go 学习笔记（第六版下卷）》 基于 go 1.10，详细分析 go 的实现机制：内存分配、垃圾回收、并发调度等等 Go语言动手写Web框架 - 进度 20% Go 语言高性能编程 英语 找外教练口语 再多背点单词，现在我技术英文能无障碍阅读，但是生活英语方面词汇量相当低。 《英语语法新思维——初级教程》 《English Grammar In Use》语法书 《Key words for fluency》口语表达 生活： 娱乐+运动： 轮滑：倒滑后压步 游泳：学会蛙泳并且提升速度 徒步：夏、冬两季各完成一次麦理浩径全程 其他资料 这个列表中的内容没啥优先级，反正先列着，什么时候有兴趣可以玩玩。 附一份屌炸天的 CS 自学指南：https://github.com/pkuflyingpig/cs-self-learning/ 写几个小项目（使用 rust/go） 实现一个文本编辑器 https://viewsourcecode.org/snaptoken/kilo/ 实现一个简单的 Linux 容器 https://blog.lizzie.io/linux-containers-in-500-loc.html 网络代理（不到 2000 行的 TUN 库） https://github.com/songgao/water Go 语言 Web 编程 7天用Go从零实现分布式缓存GeeCache 7天用Go从零实现ORM框架GeeORM 7天用Go从零实现RPC框架GeeRPC balancer: 源码阅读，如何使用 go 实现常见 balancer 算法 容器与 Kubernetes（其实好像也没啥兴趣） Hacking Kubernetes: Threat-Driven Analysis and Defense: Kubernetes 安全，威胁模型以及如何防护。 Container Security: Fundamental Technology Concepts that Protect Containerized Applications: 容器安全，这书在亚马逊上评价很好。 分布式数据库： 学习路线 极客时间《分布式协议与算法实战》 - 学习进度 50% 分布式系统：课程 MIT 6.824 + 书Designing Data-Intensive Applications 数据库系统：课程CMU 15-445 参加 tidb 的 talent-plan，完成 tinykv 项目 其他参考书籍 《Distributed Systems, 3rd Edition, 2017》 《Distributed Algorithms, 2nd Edition, 2018》 SQL 进阶教程 ","date":"2021-02-01","objectID":"/now/:5:2","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#2-以后可能会感兴趣的"},{"categories":null,"content":" 2. 以后可能会感兴趣的 AI infra 方向这是一个相当新的方向，因为近两年 AI 开始落地而兴起，它既要求熟悉 K8s/Istio 等传统 infra 组件，又要求对 AI 训练推理相关的技术有足够了解，例如： Operator: training-operator 调度：volcano 框架：Kubeflow, RayTrain, Argo Workflows 性能优化：Triton/BentoML/vLLM/PyTorch/CUDA 感觉是一个转型的方向，业余可以看看。 相关资料： https://github.com/stas00/ml-engineering/ 心理学与认知神经科学学习路线： 入门： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig, 2012 Educational Psychology, 14th Global Edition (Anita Woolfolk, 2021) Development Across the Life Span, 10th edition (Robert S. Feldman, 2023) 进阶到认知神经科学 Neuroscience: Exploring the Brain, 4th edition (2015) Cognitive Neuroscience: The Biology of the Mind, 5th edition (2019) 其他方向 《Intimate Relationships》 - 进度 14/449 《Social Psychology, 14e, David Myers》 嵌入式/物联网嵌入式跟 IoT 是我 2022 年底开的新坑，目前兴趣强烈。 之前制定的 FPGA 学习路线： 从数字电路到 FPGA 再到 RISC-V 当前目标：用 FPGA 实现些小功能 先学点数字电路基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 为了快速上手，直接跳过模拟电路部分，看第 12 到第 13 章 再学点 FPGA 基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 开发板：矽速荔枝糖系列，主要用 verilog 语言开发 阅读第 14 章，简单入门 FPGA verilog 语言，直接用这个站点就够了，是非常好的教程 + 练习场： https://hdlbits.01xz.net/wiki/Main_Page 更有趣的练习题： https://www.fpga4fun.com/ 从 LED 点灯到 RISCV CPU（循序渐进）:https://github.com/BrunoLevy/learn-fpga/blob/master/FemtoRV/TUTORIALS/FROM_BLINKER_TO_RISCV/README.md 进阶：学习 RISCV 与处理器微架构 书籍：Digital Design and Computer Architecture RISC-V Edition 此书从第六章开始讲 RISCV 微架构。 我目前收集的相关内容（仅是一个资料合集，内容有重复的）： 嵌入式 Linux 系列 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 Linux Driver Development for Embedded Processors 2nd Edition: 这本是 2018 年出的，写得没上面那本好、内容也没那么新，但是看评价也不错，特点是有许多的 Lab 可做。 Linux Kernel Programming: A comprehensive guide to kernel internals: Linux 内核编程领域的新书，适合入门 Linux 内核，amazon 上评价挺好，先收藏一个 Understanding the Linux Kernel, 3rd Edition：Linux 内核技术进阶。 linux-insides: 从 bootloader 开始讲解了 Linux 内核的许多重要的功能模块，看 stars 很高所以也在这里列一下。 电路原理 Practical Electronics for Inventors, Fourth Edition 芯片 ARM64: STM32 ESP32 RK3588s RISCV: milkv mars/duo, licheepi4a FPGA / 电路设计:FPGA 玩耍之旅 目前的学习目标 DIY 无人机编队飞行！要达成这个目标需要学习的东西有点多，慢慢努力吧~ 其他杂项 Go 语言进阶 《Go 学习笔记（第六版下卷）》 基于 go 1.10，详细分析 go 的实现机制：内存分配、垃圾回收、并发调度等等 Go语言动手写Web框架 - 进度 20% Go 语言高性能编程 英语 找外教练口语 再多背点单词，现在我技术英文能无障碍阅读，但是生活英语方面词汇量相当低。 《英语语法新思维——初级教程》 《English Grammar In Use》语法书 《Key words for fluency》口语表达 生活： 娱乐+运动： 轮滑：倒滑后压步 游泳：学会蛙泳并且提升速度 徒步：夏、冬两季各完成一次麦理浩径全程 其他资料 这个列表中的内容没啥优先级，反正先列着，什么时候有兴趣可以玩玩。 附一份屌炸天的 CS 自学指南：https://github.com/pkuflyingpig/cs-self-learning/ 写几个小项目（使用 rust/go） 实现一个文本编辑器 https://viewsourcecode.org/snaptoken/kilo/ 实现一个简单的 Linux 容器 https://blog.lizzie.io/linux-containers-in-500-loc.html 网络代理（不到 2000 行的 TUN 库） https://github.com/songgao/water Go 语言 Web 编程 7天用Go从零实现分布式缓存GeeCache 7天用Go从零实现ORM框架GeeORM 7天用Go从零实现RPC框架GeeRPC balancer: 源码阅读，如何使用 go 实现常见 balancer 算法 容器与 Kubernetes（其实好像也没啥兴趣） Hacking Kubernetes: Threat-Driven Analysis and Defense: Kubernetes 安全，威胁模型以及如何防护。 Container Security: Fundamental Technology Concepts that Protect Containerized Applications: 容器安全，这书在亚马逊上评价很好。 分布式数据库： 学习路线 极客时间《分布式协议与算法实战》 - 学习进度 50% 分布式系统：课程 MIT 6.824 + 书Designing Data-Intensive Applications 数据库系统：课程CMU 15-445 参加 tidb 的 talent-plan，完成 tinykv 项目 其他参考书籍 《Distributed Systems, 3rd Edition, 2017》 《Distributed Algorithms, 2nd Edition, 2018》 SQL 进阶教程 ","date":"2021-02-01","objectID":"/now/:5:2","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#ai-infra-方向"},{"categories":null,"content":" 2. 以后可能会感兴趣的 AI infra 方向这是一个相当新的方向，因为近两年 AI 开始落地而兴起，它既要求熟悉 K8s/Istio 等传统 infra 组件，又要求对 AI 训练推理相关的技术有足够了解，例如： Operator: training-operator 调度：volcano 框架：Kubeflow, RayTrain, Argo Workflows 性能优化：Triton/BentoML/vLLM/PyTorch/CUDA 感觉是一个转型的方向，业余可以看看。 相关资料： https://github.com/stas00/ml-engineering/ 心理学与认知神经科学学习路线： 入门： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig, 2012 Educational Psychology, 14th Global Edition (Anita Woolfolk, 2021) Development Across the Life Span, 10th edition (Robert S. Feldman, 2023) 进阶到认知神经科学 Neuroscience: Exploring the Brain, 4th edition (2015) Cognitive Neuroscience: The Biology of the Mind, 5th edition (2019) 其他方向 《Intimate Relationships》 - 进度 14/449 《Social Psychology, 14e, David Myers》 嵌入式/物联网嵌入式跟 IoT 是我 2022 年底开的新坑，目前兴趣强烈。 之前制定的 FPGA 学习路线： 从数字电路到 FPGA 再到 RISC-V 当前目标：用 FPGA 实现些小功能 先学点数字电路基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 为了快速上手，直接跳过模拟电路部分，看第 12 到第 13 章 再学点 FPGA 基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 开发板：矽速荔枝糖系列，主要用 verilog 语言开发 阅读第 14 章，简单入门 FPGA verilog 语言，直接用这个站点就够了，是非常好的教程 + 练习场： https://hdlbits.01xz.net/wiki/Main_Page 更有趣的练习题： https://www.fpga4fun.com/ 从 LED 点灯到 RISCV CPU（循序渐进）:https://github.com/BrunoLevy/learn-fpga/blob/master/FemtoRV/TUTORIALS/FROM_BLINKER_TO_RISCV/README.md 进阶：学习 RISCV 与处理器微架构 书籍：Digital Design and Computer Architecture RISC-V Edition 此书从第六章开始讲 RISCV 微架构。 我目前收集的相关内容（仅是一个资料合集，内容有重复的）： 嵌入式 Linux 系列 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 Linux Driver Development for Embedded Processors 2nd Edition: 这本是 2018 年出的，写得没上面那本好、内容也没那么新，但是看评价也不错，特点是有许多的 Lab 可做。 Linux Kernel Programming: A comprehensive guide to kernel internals: Linux 内核编程领域的新书，适合入门 Linux 内核，amazon 上评价挺好，先收藏一个 Understanding the Linux Kernel, 3rd Edition：Linux 内核技术进阶。 linux-insides: 从 bootloader 开始讲解了 Linux 内核的许多重要的功能模块，看 stars 很高所以也在这里列一下。 电路原理 Practical Electronics for Inventors, Fourth Edition 芯片 ARM64: STM32 ESP32 RK3588s RISCV: milkv mars/duo, licheepi4a FPGA / 电路设计:FPGA 玩耍之旅 目前的学习目标 DIY 无人机编队飞行！要达成这个目标需要学习的东西有点多，慢慢努力吧~ 其他杂项 Go 语言进阶 《Go 学习笔记（第六版下卷）》 基于 go 1.10，详细分析 go 的实现机制：内存分配、垃圾回收、并发调度等等 Go语言动手写Web框架 - 进度 20% Go 语言高性能编程 英语 找外教练口语 再多背点单词，现在我技术英文能无障碍阅读，但是生活英语方面词汇量相当低。 《英语语法新思维——初级教程》 《English Grammar In Use》语法书 《Key words for fluency》口语表达 生活： 娱乐+运动： 轮滑：倒滑后压步 游泳：学会蛙泳并且提升速度 徒步：夏、冬两季各完成一次麦理浩径全程 其他资料 这个列表中的内容没啥优先级，反正先列着，什么时候有兴趣可以玩玩。 附一份屌炸天的 CS 自学指南：https://github.com/pkuflyingpig/cs-self-learning/ 写几个小项目（使用 rust/go） 实现一个文本编辑器 https://viewsourcecode.org/snaptoken/kilo/ 实现一个简单的 Linux 容器 https://blog.lizzie.io/linux-containers-in-500-loc.html 网络代理（不到 2000 行的 TUN 库） https://github.com/songgao/water Go 语言 Web 编程 7天用Go从零实现分布式缓存GeeCache 7天用Go从零实现ORM框架GeeORM 7天用Go从零实现RPC框架GeeRPC balancer: 源码阅读，如何使用 go 实现常见 balancer 算法 容器与 Kubernetes（其实好像也没啥兴趣） Hacking Kubernetes: Threat-Driven Analysis and Defense: Kubernetes 安全，威胁模型以及如何防护。 Container Security: Fundamental Technology Concepts that Protect Containerized Applications: 容器安全，这书在亚马逊上评价很好。 分布式数据库： 学习路线 极客时间《分布式协议与算法实战》 - 学习进度 50% 分布式系统：课程 MIT 6.824 + 书Designing Data-Intensive Applications 数据库系统：课程CMU 15-445 参加 tidb 的 talent-plan，完成 tinykv 项目 其他参考书籍 《Distributed Systems, 3rd Edition, 2017》 《Distributed Algorithms, 2nd Edition, 2018》 SQL 进阶教程 ","date":"2021-02-01","objectID":"/now/:5:2","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#心理学与认知神经科学"},{"categories":null,"content":" 2. 以后可能会感兴趣的 AI infra 方向这是一个相当新的方向，因为近两年 AI 开始落地而兴起，它既要求熟悉 K8s/Istio 等传统 infra 组件，又要求对 AI 训练推理相关的技术有足够了解，例如： Operator: training-operator 调度：volcano 框架：Kubeflow, RayTrain, Argo Workflows 性能优化：Triton/BentoML/vLLM/PyTorch/CUDA 感觉是一个转型的方向，业余可以看看。 相关资料： https://github.com/stas00/ml-engineering/ 心理学与认知神经科学学习路线： 入门： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig, 2012 Educational Psychology, 14th Global Edition (Anita Woolfolk, 2021) Development Across the Life Span, 10th edition (Robert S. Feldman, 2023) 进阶到认知神经科学 Neuroscience: Exploring the Brain, 4th edition (2015) Cognitive Neuroscience: The Biology of the Mind, 5th edition (2019) 其他方向 《Intimate Relationships》 - 进度 14/449 《Social Psychology, 14e, David Myers》 嵌入式/物联网嵌入式跟 IoT 是我 2022 年底开的新坑，目前兴趣强烈。 之前制定的 FPGA 学习路线： 从数字电路到 FPGA 再到 RISC-V 当前目标：用 FPGA 实现些小功能 先学点数字电路基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 为了快速上手，直接跳过模拟电路部分，看第 12 到第 13 章 再学点 FPGA 基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 开发板：矽速荔枝糖系列，主要用 verilog 语言开发 阅读第 14 章，简单入门 FPGA verilog 语言，直接用这个站点就够了，是非常好的教程 + 练习场： https://hdlbits.01xz.net/wiki/Main_Page 更有趣的练习题： https://www.fpga4fun.com/ 从 LED 点灯到 RISCV CPU（循序渐进）:https://github.com/BrunoLevy/learn-fpga/blob/master/FemtoRV/TUTORIALS/FROM_BLINKER_TO_RISCV/README.md 进阶：学习 RISCV 与处理器微架构 书籍：Digital Design and Computer Architecture RISC-V Edition 此书从第六章开始讲 RISCV 微架构。 我目前收集的相关内容（仅是一个资料合集，内容有重复的）： 嵌入式 Linux 系列 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 Linux Driver Development for Embedded Processors 2nd Edition: 这本是 2018 年出的，写得没上面那本好、内容也没那么新，但是看评价也不错，特点是有许多的 Lab 可做。 Linux Kernel Programming: A comprehensive guide to kernel internals: Linux 内核编程领域的新书，适合入门 Linux 内核，amazon 上评价挺好，先收藏一个 Understanding the Linux Kernel, 3rd Edition：Linux 内核技术进阶。 linux-insides: 从 bootloader 开始讲解了 Linux 内核的许多重要的功能模块，看 stars 很高所以也在这里列一下。 电路原理 Practical Electronics for Inventors, Fourth Edition 芯片 ARM64: STM32 ESP32 RK3588s RISCV: milkv mars/duo, licheepi4a FPGA / 电路设计:FPGA 玩耍之旅 目前的学习目标 DIY 无人机编队飞行！要达成这个目标需要学习的东西有点多，慢慢努力吧~ 其他杂项 Go 语言进阶 《Go 学习笔记（第六版下卷）》 基于 go 1.10，详细分析 go 的实现机制：内存分配、垃圾回收、并发调度等等 Go语言动手写Web框架 - 进度 20% Go 语言高性能编程 英语 找外教练口语 再多背点单词，现在我技术英文能无障碍阅读，但是生活英语方面词汇量相当低。 《英语语法新思维——初级教程》 《English Grammar In Use》语法书 《Key words for fluency》口语表达 生活： 娱乐+运动： 轮滑：倒滑后压步 游泳：学会蛙泳并且提升速度 徒步：夏、冬两季各完成一次麦理浩径全程 其他资料 这个列表中的内容没啥优先级，反正先列着，什么时候有兴趣可以玩玩。 附一份屌炸天的 CS 自学指南：https://github.com/pkuflyingpig/cs-self-learning/ 写几个小项目（使用 rust/go） 实现一个文本编辑器 https://viewsourcecode.org/snaptoken/kilo/ 实现一个简单的 Linux 容器 https://blog.lizzie.io/linux-containers-in-500-loc.html 网络代理（不到 2000 行的 TUN 库） https://github.com/songgao/water Go 语言 Web 编程 7天用Go从零实现分布式缓存GeeCache 7天用Go从零实现ORM框架GeeORM 7天用Go从零实现RPC框架GeeRPC balancer: 源码阅读，如何使用 go 实现常见 balancer 算法 容器与 Kubernetes（其实好像也没啥兴趣） Hacking Kubernetes: Threat-Driven Analysis and Defense: Kubernetes 安全，威胁模型以及如何防护。 Container Security: Fundamental Technology Concepts that Protect Containerized Applications: 容器安全，这书在亚马逊上评价很好。 分布式数据库： 学习路线 极客时间《分布式协议与算法实战》 - 学习进度 50% 分布式系统：课程 MIT 6.824 + 书Designing Data-Intensive Applications 数据库系统：课程CMU 15-445 参加 tidb 的 talent-plan，完成 tinykv 项目 其他参考书籍 《Distributed Systems, 3rd Edition, 2017》 《Distributed Algorithms, 2nd Edition, 2018》 SQL 进阶教程 ","date":"2021-02-01","objectID":"/now/:5:2","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#嵌入式物联网"},{"categories":null,"content":" 2. 以后可能会感兴趣的 AI infra 方向这是一个相当新的方向，因为近两年 AI 开始落地而兴起，它既要求熟悉 K8s/Istio 等传统 infra 组件，又要求对 AI 训练推理相关的技术有足够了解，例如： Operator: training-operator 调度：volcano 框架：Kubeflow, RayTrain, Argo Workflows 性能优化：Triton/BentoML/vLLM/PyTorch/CUDA 感觉是一个转型的方向，业余可以看看。 相关资料： https://github.com/stas00/ml-engineering/ 心理学与认知神经科学学习路线： 入门： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig, 2012 Educational Psychology, 14th Global Edition (Anita Woolfolk, 2021) Development Across the Life Span, 10th edition (Robert S. Feldman, 2023) 进阶到认知神经科学 Neuroscience: Exploring the Brain, 4th edition (2015) Cognitive Neuroscience: The Biology of the Mind, 5th edition (2019) 其他方向 《Intimate Relationships》 - 进度 14/449 《Social Psychology, 14e, David Myers》 嵌入式/物联网嵌入式跟 IoT 是我 2022 年底开的新坑，目前兴趣强烈。 之前制定的 FPGA 学习路线： 从数字电路到 FPGA 再到 RISC-V 当前目标：用 FPGA 实现些小功能 先学点数字电路基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 为了快速上手，直接跳过模拟电路部分，看第 12 到第 13 章 再学点 FPGA 基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 开发板：矽速荔枝糖系列，主要用 verilog 语言开发 阅读第 14 章，简单入门 FPGA verilog 语言，直接用这个站点就够了，是非常好的教程 + 练习场： https://hdlbits.01xz.net/wiki/Main_Page 更有趣的练习题： https://www.fpga4fun.com/ 从 LED 点灯到 RISCV CPU（循序渐进）:https://github.com/BrunoLevy/learn-fpga/blob/master/FemtoRV/TUTORIALS/FROM_BLINKER_TO_RISCV/README.md 进阶：学习 RISCV 与处理器微架构 书籍：Digital Design and Computer Architecture RISC-V Edition 此书从第六章开始讲 RISCV 微架构。 我目前收集的相关内容（仅是一个资料合集，内容有重复的）： 嵌入式 Linux 系列 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 Linux Driver Development for Embedded Processors 2nd Edition: 这本是 2018 年出的，写得没上面那本好、内容也没那么新，但是看评价也不错，特点是有许多的 Lab 可做。 Linux Kernel Programming: A comprehensive guide to kernel internals: Linux 内核编程领域的新书，适合入门 Linux 内核，amazon 上评价挺好，先收藏一个 Understanding the Linux Kernel, 3rd Edition：Linux 内核技术进阶。 linux-insides: 从 bootloader 开始讲解了 Linux 内核的许多重要的功能模块，看 stars 很高所以也在这里列一下。 电路原理 Practical Electronics for Inventors, Fourth Edition 芯片 ARM64: STM32 ESP32 RK3588s RISCV: milkv mars/duo, licheepi4a FPGA / 电路设计:FPGA 玩耍之旅 目前的学习目标 DIY 无人机编队飞行！要达成这个目标需要学习的东西有点多，慢慢努力吧~ 其他杂项 Go 语言进阶 《Go 学习笔记（第六版下卷）》 基于 go 1.10，详细分析 go 的实现机制：内存分配、垃圾回收、并发调度等等 Go语言动手写Web框架 - 进度 20% Go 语言高性能编程 英语 找外教练口语 再多背点单词，现在我技术英文能无障碍阅读，但是生活英语方面词汇量相当低。 《英语语法新思维——初级教程》 《English Grammar In Use》语法书 《Key words for fluency》口语表达 生活： 娱乐+运动： 轮滑：倒滑后压步 游泳：学会蛙泳并且提升速度 徒步：夏、冬两季各完成一次麦理浩径全程 其他资料 这个列表中的内容没啥优先级，反正先列着，什么时候有兴趣可以玩玩。 附一份屌炸天的 CS 自学指南：https://github.com/pkuflyingpig/cs-self-learning/ 写几个小项目（使用 rust/go） 实现一个文本编辑器 https://viewsourcecode.org/snaptoken/kilo/ 实现一个简单的 Linux 容器 https://blog.lizzie.io/linux-containers-in-500-loc.html 网络代理（不到 2000 行的 TUN 库） https://github.com/songgao/water Go 语言 Web 编程 7天用Go从零实现分布式缓存GeeCache 7天用Go从零实现ORM框架GeeORM 7天用Go从零实现RPC框架GeeRPC balancer: 源码阅读，如何使用 go 实现常见 balancer 算法 容器与 Kubernetes（其实好像也没啥兴趣） Hacking Kubernetes: Threat-Driven Analysis and Defense: Kubernetes 安全，威胁模型以及如何防护。 Container Security: Fundamental Technology Concepts that Protect Containerized Applications: 容器安全，这书在亚马逊上评价很好。 分布式数据库： 学习路线 极客时间《分布式协议与算法实战》 - 学习进度 50% 分布式系统：课程 MIT 6.824 + 书Designing Data-Intensive Applications 数据库系统：课程CMU 15-445 参加 tidb 的 talent-plan，完成 tinykv 项目 其他参考书籍 《Distributed Systems, 3rd Edition, 2017》 《Distributed Algorithms, 2nd Edition, 2018》 SQL 进阶教程 ","date":"2021-02-01","objectID":"/now/:5:2","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#其他杂项"},{"categories":null,"content":" 2. 以后可能会感兴趣的 AI infra 方向这是一个相当新的方向，因为近两年 AI 开始落地而兴起，它既要求熟悉 K8s/Istio 等传统 infra 组件，又要求对 AI 训练推理相关的技术有足够了解，例如： Operator: training-operator 调度：volcano 框架：Kubeflow, RayTrain, Argo Workflows 性能优化：Triton/BentoML/vLLM/PyTorch/CUDA 感觉是一个转型的方向，业余可以看看。 相关资料： https://github.com/stas00/ml-engineering/ 心理学与认知神经科学学习路线： 入门： 《这才是心理学 - 看穿伪科学的批判性思维 第 11 版》 Psychology and Life, 20th edition, by Richard J. Gerrig, 2012 Educational Psychology, 14th Global Edition (Anita Woolfolk, 2021) Development Across the Life Span, 10th edition (Robert S. Feldman, 2023) 进阶到认知神经科学 Neuroscience: Exploring the Brain, 4th edition (2015) Cognitive Neuroscience: The Biology of the Mind, 5th edition (2019) 其他方向 《Intimate Relationships》 - 进度 14/449 《Social Psychology, 14e, David Myers》 嵌入式/物联网嵌入式跟 IoT 是我 2022 年底开的新坑，目前兴趣强烈。 之前制定的 FPGA 学习路线： 从数字电路到 FPGA 再到 RISC-V 当前目标：用 FPGA 实现些小功能 先学点数字电路基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 为了快速上手，直接跳过模拟电路部分，看第 12 到第 13 章 再学点 FPGA 基础知识 书籍：Practical Electronics for Inventors, Fourth Edition 开发板：矽速荔枝糖系列，主要用 verilog 语言开发 阅读第 14 章，简单入门 FPGA verilog 语言，直接用这个站点就够了，是非常好的教程 + 练习场： https://hdlbits.01xz.net/wiki/Main_Page 更有趣的练习题： https://www.fpga4fun.com/ 从 LED 点灯到 RISCV CPU（循序渐进）:https://github.com/BrunoLevy/learn-fpga/blob/master/FemtoRV/TUTORIALS/FROM_BLINKER_TO_RISCV/README.md 进阶：学习 RISCV 与处理器微架构 书籍：Digital Design and Computer Architecture RISC-V Edition 此书从第六章开始讲 RISCV 微架构。 我目前收集的相关内容（仅是一个资料合集，内容有重复的）： 嵌入式 Linux 系列 Linux Device Driver Development - Second Edition: Linux 驱动编程入门，2022 年出的新书，基于 Linux 5.10，amazon 上评价不错，目前只有英文版，写的很好，对新手很友好。 Linux Driver Development for Embedded Processors 2nd Edition: 这本是 2018 年出的，写得没上面那本好、内容也没那么新，但是看评价也不错，特点是有许多的 Lab 可做。 Linux Kernel Programming: A comprehensive guide to kernel internals: Linux 内核编程领域的新书，适合入门 Linux 内核，amazon 上评价挺好，先收藏一个 Understanding the Linux Kernel, 3rd Edition：Linux 内核技术进阶。 linux-insides: 从 bootloader 开始讲解了 Linux 内核的许多重要的功能模块，看 stars 很高所以也在这里列一下。 电路原理 Practical Electronics for Inventors, Fourth Edition 芯片 ARM64: STM32 ESP32 RK3588s RISCV: milkv mars/duo, licheepi4a FPGA / 电路设计:FPGA 玩耍之旅 目前的学习目标 DIY 无人机编队飞行！要达成这个目标需要学习的东西有点多，慢慢努力吧~ 其他杂项 Go 语言进阶 《Go 学习笔记（第六版下卷）》 基于 go 1.10，详细分析 go 的实现机制：内存分配、垃圾回收、并发调度等等 Go语言动手写Web框架 - 进度 20% Go 语言高性能编程 英语 找外教练口语 再多背点单词，现在我技术英文能无障碍阅读，但是生活英语方面词汇量相当低。 《英语语法新思维——初级教程》 《English Grammar In Use》语法书 《Key words for fluency》口语表达 生活： 娱乐+运动： 轮滑：倒滑后压步 游泳：学会蛙泳并且提升速度 徒步：夏、冬两季各完成一次麦理浩径全程 其他资料 这个列表中的内容没啥优先级，反正先列着，什么时候有兴趣可以玩玩。 附一份屌炸天的 CS 自学指南：https://github.com/pkuflyingpig/cs-self-learning/ 写几个小项目（使用 rust/go） 实现一个文本编辑器 https://viewsourcecode.org/snaptoken/kilo/ 实现一个简单的 Linux 容器 https://blog.lizzie.io/linux-containers-in-500-loc.html 网络代理（不到 2000 行的 TUN 库） https://github.com/songgao/water Go 语言 Web 编程 7天用Go从零实现分布式缓存GeeCache 7天用Go从零实现ORM框架GeeORM 7天用Go从零实现RPC框架GeeRPC balancer: 源码阅读，如何使用 go 实现常见 balancer 算法 容器与 Kubernetes（其实好像也没啥兴趣） Hacking Kubernetes: Threat-Driven Analysis and Defense: Kubernetes 安全，威胁模型以及如何防护。 Container Security: Fundamental Technology Concepts that Protect Containerized Applications: 容器安全，这书在亚马逊上评价很好。 分布式数据库： 学习路线 极客时间《分布式协议与算法实战》 - 学习进度 50% 分布式系统：课程 MIT 6.824 + 书Designing Data-Intensive Applications 数据库系统：课程CMU 15-445 参加 tidb 的 talent-plan，完成 tinykv 项目 其他参考书籍 《Distributed Systems, 3rd Edition, 2017》 《Distributed Algorithms, 2nd Edition, 2018》 SQL 进阶教程 ","date":"2021-02-01","objectID":"/now/:5:2","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#其他资料"},{"categories":null,"content":" 五、备选书单 我的豆瓣 如下是我目前想读的书单，如果决定读，就把对应的书移到「计划读」中。 家庭教育（教育也要讲究科学，凭直觉做事往往会错得很离谱） 《教育与美好生活》：大名鼎鼎的思想家罗素的教育观。 《孩子：挑战（Children - The Challenge）》：美国人写的书，如何在尊重孩子、给孩子平等自由的同时，让孩子尊重规则、承担责任、赢得合作。阿德勒心理学。 《P.E.T.父母效能训练 - 让亲子沟通如此高效而简单》 《高压年代：如何帮助孩子在大学渡过难关、顺利成人（The Stressed Years of Their Lives）》：作者之一的儿子杰森在上大学的第一年出现了严重的心理问题：无法返校继续学业， 情绪濒于崩溃，甚至产生自杀冲动。本书总结了如何去发现并解决青少年的心理问题，帮助他们完成从家庭到大学、从青春期到真正成年的顺利过渡。 《享受孩子成长 - 留美教育博士十八年教育手记》：这书主要是个流水账，既包含作者主观的教育理念、也援引了许多教育、心理学等领域的重要科学发现。正在读，目前评个 7 分吧。 《游戏力（Playful Parenting）》：游戏力的游戏，特指亲子间的互动。它是思考方式，是互动方式，也是大人与孩子在一起轻松开心的状态。本书的重点是解决孩子常见的行为问题，激发孩子内在的自信力，重建父母与孩子间亲密沟通的桥梁。 《真希望我父母读过这本书》 《为什么学生不喜欢上学?（Why Don’t Students Like School?）》：用认知心理学的原理，详细分析了学生学习的过程和教师在课堂教学中必须注意的一些问题。 《学习的本质》：法国人安德烈·焦尔当的书 经济 / 管理 / 社会 《Principles Of Economics, 9e, N. Gregory Mankiw》 《圆圈正义-作为自由前提的信念》 《投资中最简单的事》 《债务危机 - 我的应对原则》 《分析与思考 - 黄奇帆的复旦经济课》：这本书会需要一定的经济学基础知识，打算在入门经济学后再看 Animal Farm - 一本政治讽刺小书 《手把手教你读财报》 《原则 - 应对变化中的世界秩序》 《探路之役 - 1978-1922 年的中国经济改革》 《筚路维艰 - 中国社会主义路径的五次选择》 《邓小平时代》 《论中国》 《中国国家治理的制度逻辑》 《江村经济》 《八次危机：中国的真实经验》 《中国经济：适应与增长》 《中国为什么有前途：对外经济关系的战略潜能》 《置身事内：中国政府与经济发展》 《党员、党权与党争：1924 - 1949 年中国国民党的组织形态》 人物传记或者与名人相关的书籍（从历史上的成功者，以及历史中学习） 《史蒂夫·乔布斯传》 《埃隆·马斯克传》 《维特根斯坦传》 《李光耀观天下》 《沈从文的后半生》：这本书更偏研究性质，有点难读 《陆征祥评传》：从中国外交家陆征祥的史料出发考察清末、北洋到国民政府时期，近代中国与世界的互动历程。 《第一圈》：不是自传，胜似自传。诺奖得主索尔仁尼琴以自己的亲身经历为原型，再现了斯大林时期的独裁制度对人性的摧残和破坏。 《别闹了，费曼先生：科学顽童的故事》 《我的前半生——爱新觉罗·溥仪》：看完电影《末代皇帝》后，对清朝历史产生了兴趣，打算看看。 历史与纪实作品 《万历十五年》 《跨越边界的社区（修订版）》：持续至今的真实“北漂”史。转型中的中国城市、流动人口、经济与社会。北京“浙江村”与“浙江村人”三十年生活记录研究。 《天朝的崩溃：鸦片战争再研究（修订版）》 《甲午战争前后之晚清战局》 《晚期帝制中国的科举文化史》 《剑桥中国晚清史》：1400 页的一本大部头 《五四运动史：现代中国的知识革命》 《中西文化回眸》：这几本中西文化对比的书，主要是想用于学习以及对比中西文化的差异。 《对岸的诱惑：中西文化交流记》 《中西文化的精神分野》 《血殇：埃博拉的过去、现在和未来》 《向您告知，明天我们一家就要被杀：卢旺达大屠杀纪事》 《东京贫困女子》 其他人文社科 《跨学科：人文学科的诞生、危机与未来》 《枪炮、病菌与钢铁》 《西线无战事》 《人类群星闪耀时》 《人体简史》 《时间的秩序》 《极简宇宙史》 《人生脚本》 《语言学的邀请》- 进度 68/288 对语言学有点兴趣，同时听说这本书对表达（沟通、写作）也大有帮助~ 《步天歌研究》 公益慈善 / NGO 《如何改变世界 - 社会企业家与新思想的威力》：据评社会企业的概念即源自此书 《离开微软 改变世界》 《穷人的银行家》：穷人知道该怎么摆脱贫困，只要你给予平等的借贷的权力。相信并支持每个独立人自己的选择。 《撬动公益 - 慈善和社会投资新前沿导论》 《表达的力量 - 当中国公益组织遇上媒体》 《财富的责任与资本主义演变 - 美国百年公益发展的启示》 《为公益而共和 - 阿拉善SEE生态协会治理之路》 《公益創業 - 青年創業與中年專業的新選擇》 《蓝毛衣》：如何成为一个社会企业家，有作者的亲身经历，走过的失败教训与成功经验 虚构类 Majo no Tabitabi（魔女之旅）Vol.1 Tasogare-iro no Uta Tsukai（黄昏色的咏使）Vol.1 Moon Palace, by Pual Auster - 读过中文版，但是看英文版词汇量也不高，可以一读 文学类： 《百年孤独》：高中的时候读过一遍，但是都忘差不多了 《霍乱时期的爱情》 《苏菲的世界（Sophie’s World）》：据说是哲学启蒙读物，曾经看过，但是对内容完全没印象了。 《你一生的故事》：我也曾是个科幻迷 《房思琪的初恋乐园》 《月光落在左手上》 《了不起的盖茨比》 《The Windup Girl》：高中时读过中文版，刷新我三观，现在想再读一遍英文原版。 技术类 《人月神话》 《绩效使能：超越 OKR》 《奈飞文化手册》 《幕后产品-打造突破式思维》 《重构 - 改善既有代码的设计》 《云原生服务网格 Istio：原理、实践、架构与源码解析》 《凤凰项目：一个 IT 运维的传奇故事》 《一人企业：一个人也能赚钱的商业新模式》 英语语法 《English Grammar In Use》 《英语语法新思维——初级教程》 - 8/366 神秘学（没准啥时候我就想写点小说…） 《性命圭旨》 ","date":"2021-02-01","objectID":"/now/:6:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#五备选书单"},{"categories":null,"content":" 探索互联网上的高质量内容 主要参考自 https://www.owenyoung.com/sources/ 互联网信息泛滥，我从各种信息来源中挑选了一部分我感兴趣的、受推荐次数较多的信息源列在了这里。也要注意的是，天天看最新的新闻与热点，对个人并没有什么好处。在有兴趣或者无聊的时候翻一翻，了解一下世界的变化就好了。 中国与国际上的政史内容： 爱思想网: 国内的一个学术分享站点，国内许多学者在上面针砭时弊。 中国 1850 - 1950 年间的各种报纸 时事新闻（全英文）： 有人精选了一些信息源，并制作了一份中文摘要索引: https://www.buzzing.cc/ 纽约客: 纽约客是一份美国的文学、艺术和时事杂志，以其对政治、文化和时事的评论而闻名。 大西洋: 大西洋是一份美国社论杂志，的特色文章涉及政治、外交，商业与经济，文化与艺术，科技和科学等领域。 英国卫报: 英国卫报是英国的一家全国性报纸。 日本读卖新闻: 日本读卖新闻是日本的一家全国性报纸。 半岛电视台: 卡塔尔的国际媒体，由卡塔尔王室拥有。其特点是中东阿拉伯世界的视角。 一些新兴媒体 新政治家: 英国的一份进步政治与文化杂志。 Rest of the world: 科技报道 NOEMA: 「探索席卷我们世界的变革」，发表有关哲学、治理、地缘政治、经济、技术和文化交叉领域的文章。 Semafor: 将无可争议的事实与记者对这些事实的分析分开，提供不同的和更全面的观点，并分享其他媒体对该主题的有力报道。 ","date":"2021-02-01","objectID":"/now/:7:0","series":null,"tags":null,"title":"此时此刻的我","uri":"/now/#探索互联网上的高质量内容"},{"categories":["tech"],"content":" 注意：这篇文章并不是一篇入门教程，学习 Argo Workflows 请移步官方文档Argo Documentation Argo Workflows 是一个云原生工作流引擎，专注于编排并行任务。它的特点如下： 使用 Kubernetes 自定义资源(CR)定义工作流，其中工作流中的每个步骤都是一个容器。 将多步骤工作流建模为一系列任务，或者使用有向无环图（DAG）描述任务之间的依赖关系。 可以在短时间内轻松运行用于机器学习或数据处理的计算密集型作业。 Argo Workflows 可以看作 Tekton 的加强版，因此显然也可以通过 Argo Workflows 运行 CI/CD 流水线(Pipielines)。 阿里云是 Argo Workflows 的深度使用者和贡献者，另外 Kubeflow 底层的工作流引擎也是 Argo Workflows. ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:0:0","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#"},{"categories":["tech"],"content":" 一、Argo Workflows 对比 Jenkins我们在切换到 Argo Workflows 之前，使用的 CI/CD 工具是 Jenkins，下面对 Argo Workflows 和 Jenkins 做一个比较详细的对比，以了解 Argo Workflows 的优缺点。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:0","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#一argo-workflows-对比-jenkins"},{"categories":["tech"],"content":" 1. Workflow 的定义Workflow 使用 kubernetes CR 进行定义，因此显然是一份 yaml 配置。 一个 Workflow，就是一个运行在 Kubernetes 上的流水线，对应 Jenkins 的一次 Build. 而 WorkflowTemplate 则是一个可重用的 Workflow 模板，对应 Jenkins 的一个 Job. WorkflowTemplate 的 yaml 定义和 Workflow 完全一致，只有 Kind 不同！ WorkflowTemplate 可以被其他 Workflow 引用并触发，也可以手动传参以生成一个 Workflow 工作流。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:1","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#1-workflow-的定义"},{"categories":["tech"],"content":" 2. Workflow 的编排Argo Workflows 相比其他流水线项目(Jenkins/Tekton/Drone/Gitlab-CI)而言，最大的特点，就是它强大的流水线编排能力。 其他流水线项目，对流水线之间的关联性考虑得很少，基本都假设流水线都是互相独立的。 而 Argo Workflows 则假设「任务」之间是有依赖关系的，针对这个依赖关系，它提供了两种协调编排「任务」的方法：Steps 和 DAG 再借助templateRef 或者Workflow of Workflows， 就能实现 Workflows 的编排了。 我们之所以选择 Argo Workflows 而不是 Tekton，主要就是因为 Argo 的流水线编排能力比 Tekton 强大得多。（也许是因为我们的后端中台结构比较特殊，导致我们的 CI 流水线需要具备复杂的编排能力） 一个复杂工作流的示例如下： https://github.com/argoproj/argo/issues/1088#issuecomment-445884543 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:2","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#2-workflow-的编排"},{"categories":["tech"],"content":" 3. Workflow 的声明式配置Argo 使用 Kubernetes 自定义资源(CR)来定义 Workflow，熟悉 Kubernetes Yaml 的同学上手应该都很快。 下面对 Workflow 定义文件和 Jenkinsfile 做个对比： argo 完全使用 yaml 来定义流水线，学习成本比 Jenkinsfile 的 groovy 低。对熟悉 Kubernetes 的同学尤其如此。 将 jenkinsfile 用 argo 重写后，代码量出现了明显的膨胀。一个 20 行的 Jenkinsfile，用 Argo 重写可能就变成了 60 行。 配置出现了膨胀是个问题，但是考虑到它的可读性还算不错，而且 Argo 的 Workflow 编排功能，能替代掉我们目前维护的部分 Python 构建代码，以及一些其他优点，配置膨胀这个问题也就可以接受了。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:3","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#3-workflow-的声明式配置"},{"categories":["tech"],"content":" 4. Web UIArgo Workflows 的 Web UI 感觉还很原始。确实该支持的功能都有，但是它貌似不是面向「用户」的，功能比较底层。 它不像 Jenkins 一样，有很友好的使用界面(虽然说 Jenkins 的 UI 也很显老…) 另外它所有的 Workflow 都是相互独立的，没办法直观地找到一个 WorkflowTemplate 的所有构建记录，只能通过 label/namespace 进行分类，通过任务名称进行搜索。 而 Jenkins 可以很方便地看到同一个 Job 的所有构建历史。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:4","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#4-web-ui"},{"categories":["tech"],"content":" 5. Workflow 的分类 为何需要对 Workflow 做细致的分类常见的微服务项目，往往会拆分成众多 Git 仓库（微服务）进行开发，众多的 Git 仓库会使我们创建众多的 CI/CD 流水线。如果没有任何的分类，这一大堆的流水线如何管理，就成了一个难题。 最显见的需求：前端和后端的流水线最好能区分一下，往下细分，前端的 Web 端和客户端最好也能区分，后端的业务层和中台最好也区分开来。 另外我们还希望将运维、自动化测试相关的任务也集成到这个系统中来（目前我们就是使用 Jenkins 完成运维、自动化测试任务的），如果没有任何分类，这一大堆流水线将混乱无比。 Argo Workflows 的分类能力当 Workflow 越来越多的时候，如果不做分类，一堆 WorkflowTemplate 堆在一起就会显得特别混乱。 （没错，我觉得 Drone 就有这个问题…） Argo 是完全基于 Kubernetes 的，因此目前它也只能通过 namespace/labels 进行分类。 这样的分类结构和 Jenkins 的视图-文件夹体系大相径庭，目前感觉不是很好用（也可能纯粹是 Web UI 的锅）。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:5","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#5-workflow-的分类"},{"categories":["tech"],"content":" 5. Workflow 的分类 为何需要对 Workflow 做细致的分类常见的微服务项目，往往会拆分成众多 Git 仓库（微服务）进行开发，众多的 Git 仓库会使我们创建众多的 CI/CD 流水线。如果没有任何的分类，这一大堆的流水线如何管理，就成了一个难题。 最显见的需求：前端和后端的流水线最好能区分一下，往下细分，前端的 Web 端和客户端最好也能区分，后端的业务层和中台最好也区分开来。 另外我们还希望将运维、自动化测试相关的任务也集成到这个系统中来（目前我们就是使用 Jenkins 完成运维、自动化测试任务的），如果没有任何分类，这一大堆流水线将混乱无比。 Argo Workflows 的分类能力当 Workflow 越来越多的时候，如果不做分类，一堆 WorkflowTemplate 堆在一起就会显得特别混乱。 （没错，我觉得 Drone 就有这个问题…） Argo 是完全基于 Kubernetes 的，因此目前它也只能通过 namespace/labels 进行分类。 这样的分类结构和 Jenkins 的视图-文件夹体系大相径庭，目前感觉不是很好用（也可能纯粹是 Web UI 的锅）。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:5","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#为何需要对-workflow-做细致的分类"},{"categories":["tech"],"content":" 5. Workflow 的分类 为何需要对 Workflow 做细致的分类常见的微服务项目，往往会拆分成众多 Git 仓库（微服务）进行开发，众多的 Git 仓库会使我们创建众多的 CI/CD 流水线。如果没有任何的分类，这一大堆的流水线如何管理，就成了一个难题。 最显见的需求：前端和后端的流水线最好能区分一下，往下细分，前端的 Web 端和客户端最好也能区分，后端的业务层和中台最好也区分开来。 另外我们还希望将运维、自动化测试相关的任务也集成到这个系统中来（目前我们就是使用 Jenkins 完成运维、自动化测试任务的），如果没有任何分类，这一大堆流水线将混乱无比。 Argo Workflows 的分类能力当 Workflow 越来越多的时候，如果不做分类，一堆 WorkflowTemplate 堆在一起就会显得特别混乱。 （没错，我觉得 Drone 就有这个问题…） Argo 是完全基于 Kubernetes 的，因此目前它也只能通过 namespace/labels 进行分类。 这样的分类结构和 Jenkins 的视图-文件夹体系大相径庭，目前感觉不是很好用（也可能纯粹是 Web UI 的锅）。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:5","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#argo-workflows-的分类能力"},{"categories":["tech"],"content":" 6. 触发构建的方式Argo Workflows 的流水线有多种触发方式： 手动触发：手动提交一个 Workflow，就能触发一次构建。可以通过workflowTemplateRef 直接引用一个现成的流水线模板。 定时触发：CronWorkflow 通过 Git 仓库变更触发：借助 argo-events 可以实现此功能，详见其文档。 另外目前也不清楚 WebHook 的可靠程度如何，会不会因为宕机、断网等故障，导致 Git 仓库变更了，而 Workflow 却没触发，而且还没有任何显眼的错误通知？如果这个错误就这样藏起来了，就可能会导致很严重的问题！ ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:6","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#6-触发构建的方式"},{"categories":["tech"],"content":" 7. secrets 管理Argo Workflows 的流水线，可以从 kubernetes secrets/configmap 中获取信息，将信息注入到环境变量中、或者以文件形式挂载到 Pod 中。 Git 私钥、Harbor 仓库凭据、CD 需要的 kubeconfig，都可以直接从 secrets/configmap 中获取到。 另外因为 Vault 很流行，也可以将 secrets 保存在 Vault 中，再通过 vault agent 将配置注入进 Pod。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:7","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#7-secrets-管理"},{"categories":["tech"],"content":" 8. ArtifactsArgo 支持接入对象存储，做全局的 Artifact 仓库，本地可以使用 MinIO. 使用对象存储存储 Artifact，最大的好处就是可以在 Pod 之间随意传数据，Pod 可以完全分布式地运行在 Kubernetes 集群的任何节点上。 另外也可以考虑借助 Artifact 仓库实现跨流水线的缓存复用（未测试），提升构建速度。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:8","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#8-artifacts"},{"categories":["tech"],"content":" 9. 容器镜像的构建借助 Buildkit 等容器镜像构建工具，可以实现容器镜像的分布式构建。 Buildkit 对构建缓存的支持也很好，可以直接将缓存存储在容器镜像仓库中。 不建议使用 Google 的 Kaniko，它对缓存复用的支持不咋地，社区也不活跃。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:9","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#9-容器镜像的构建"},{"categories":["tech"],"content":" 10. 客户端/SDKArgo 有提供一个命令行客户端，也有 HTTP API 可供使用。 如下项目值得试用： argo-client-python: Argo Workflows 的 Python 客户端 说实话，感觉和 kubernetes-client/python 一样难用，毕竟都是 openapi-generator 生成出来的… argo-python-dsl: 使用 Python DSL 编写 Argo Workflows 感觉使用难度比 yaml 高，也不太好用。 couler: 为 Argo/Tekton/Airflow 提供统一的构建与管理接口 理念倒是很好，待研究 感觉 couler 挺不错的，可以直接用 Python 写 WorkflowTemplate，这样就一步到位，所有 CI/CD 代码全部是 Python 了。 此外，因为 Argo Workflows 是 kubernetes 自定义资源 CR，也可以使用 helm/kustomize 来做 workflow 的生成。 目前我们一些步骤非常多，但是重复度也很高的 Argo 流水线配置，就是使用 helm 生成的——关键数据抽取到 values.yaml 中，使用 helm 模板 + range 循环来生成 workflow 配置。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:1:10","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#10-客户端sdk"},{"categories":["tech"],"content":" 二、安装 Argo Workflows 参考官方文档：https://argoproj.github.io/argo-workflows/installation/ 安装一个集群版(cluster wide)的 Argo Workflows，使用 MinIO 做 artifacts 存储： shell kubectl apply -f https://raw.githubusercontent.com/argoproj/argo/stable/manifests/install.yaml 部署 MinIO: shell helm repo add minio https://helm.min.io/ # official minio Helm charts # 查看历史版本 helm search repo minio/minio -l | head # 下载并解压 chart helm pull minio/minio --untar --version 8.0.9 # 编写 custom-values.yaml，然后部署 minio kubectl create namespace minio helm install minio ./minio -n argo -f custom-values.yaml minio 部署好后，它会将默认的 accesskey 和 secretkey 保存在名为 minio 的 secret 中。我们需要修改 argo 的配置，将 minio 作为它的默认 artifact 仓库。 在 configmap workflow-controller-configmap 的 data 中添加如下字段： shell artifactRepository: | # 是否将 main 容器的日志保存为 artifact，这样 pod 被删除后，仍然可以在 artifact 中找到日志 archiveLogs: true s3: bucket: argo-bucket # bucket 名称，这个 bucket 需要先手动创建好！ endpoint: minio:9000 # minio 地址 insecure: true # 从 minio 这个 secret 中获取 key/secret accessKeySecret: name: minio key: accesskey secretKeySecret: name: minio key: secretkey 现在还差最后一步：手动进入 minio 的 Web UI，创建好 argo-bucket 这个 bucket. 直接访问 minio 的 9000 端口（需要使用 nodeport/ingress 等方式暴露此端口）就能进入 Web UI，使用前面提到的 secret minio 中的 key/secret 登录，就能创建 bucket. ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:2:0","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#二安装-argo-workflows"},{"categories":["tech"],"content":" ServiceAccount 配置 https://argoproj.github.io/argo-workflows/service-accounts/ Argo Workflows 依赖于 ServiceAccount 进行验证与授权，而且默认情况下，它使用所在 namespace 的 default ServiceAccount 运行 workflow. 可 default 这个 ServiceAccount 默认根本没有任何权限！所以 Argo 的 artifacts, outputs, access to secrets 等功能全都会因为权限不足而无法使用！ 为此，Argo 的官方文档提供了两个解决方法。 方法一，直接给 default 绑定 cluster-admin ClusterRole，给它集群管理员的权限，只要一行命令（但是显然安全性堪忧）： shell kubectl create rolebinding default-admin --clusterrole=admin --serviceaccount=\u003cnamespace\u003e:default -n \u003cnamespace\u003e 方法二，官方给出了Argo Workflows 需要的最小权限的 Role 定义， 方便起见我将它改成一个 ClusterRole: yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: argo-workflows-role rules: # pod get/watch is used to identify the container IDs of the current pod # pod patch is used to annotate the step's outputs back to controller (e.g. artifact location) - apiGroups: - \"\" resources: - pods verbs: - get - watch - patch # logs get/watch are used to get the pods logs for script outputs, and for log archival - apiGroups: - \"\" resources: - pods/log verbs: - get - watch 创建好上面这个最小的 ClusterRole，然后为每个名字空间，跑一下如下命令，给 default 账号绑定这个 clusterrole: shell kubectl create rolebinding default-argo-workflows --clusterrole=argo-workflows-role --serviceaccount=\u003cnamespace\u003e:default -n \u003cnamespace\u003e 这样就能给 default 账号提供最小的 workflow 运行权限。 或者如果你希望使用别的 ServiceAccount 来运行 workflow，也可以自行创建 ServiceAccount，然后再走上面方法二的流程，但是最后，要记得在 workflow 的 spec.serviceAccountName 中设定好 ServiceAccount 名称。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:2:1","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#serviceaccount-配置"},{"categories":["tech"],"content":" Workflow Executors https://argoproj.github.io/argo-workflows/workflow-executors/ Workflow Executor 是符合特定接口的一个进程(Process)，Argo 可以通过它执行一些动作，如监控 Pod 日志、收集 Artifacts、管理容器生命周期等等… Workflow Executor 有多种实现，可以通过前面提到的 configmap workflow-controller-configmap 来选择。 可选项如下： docker(默认): 目前使用范围最广，但是安全性最差。它要求一定要挂载访问 docker.sock，因此一定要 root 权限！ kubelet: 应用非常少，目前功能也有些欠缺，目前也必须提供 root 权限 Kubernetes API (k8sapi): 直接通过调用 k8sapi 实现日志监控、Artifacts 手机等功能，非常安全，但是性能欠佳。 Process Namespace Sharing (pns): 安全性比 k8sapi 差一点，因为 Process 对其他所有容器都可见了。但是相对的性能好很多。 在 docker 被 kubernetes 抛弃的当下，如果你已经改用 containerd 做为 kubernetes 运行时，那 argo 将会无法工作，因为它默认使用 docker 作为运行时！ 我们建议将 workflow executore 改为 pns，兼顾安全性与性能，workflow-controller-configmap 按照如下方式修改： yaml apiVersion: v1 kind: ConfigMap metadata: name: workflow-controller-configmap data: config: | # ...省略若干配置... # Specifies the container runtime interface to use (default: docker) # must be one of: docker, kubelet, k8sapi, pns containerRuntimeExecutor: pns # ... ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:2:2","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#workflow-executors"},{"categories":["tech"],"content":" 三、使用 Argo Workflows 做 CI 工具官方的 Reference 还算详细，也有提供非常多的 examples 供我们参考，这里提供我们几个常用的 workflow 定义。 使用 buildkit 构建镜像：https://github.com/argoproj/argo-workflows/blob/master/examples/buildkit-template.yaml buildkit 支持缓存，可以在这个 example 的基础上自定义参数 注意使用 PVC 来跨 step 共享存储空间这种手段，速度会比通过 artifacts 高很多。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:3:0","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#三使用-argo-workflows-做-ci-工具"},{"categories":["tech"],"content":" 四、常见问题","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:4:0","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#四常见问题"},{"categories":["tech"],"content":" 1. workflow 默认使用 root 账号？workflow 的流程默认使用 root 账号，如果你的镜像默认使用非 root 账号，而且要修改文件，就很可能遇到 Permission Denined 的问题。 解决方法：通过 Pod Security Context 手动设定容器的 user/group: Workflow Pod Security Context 安全起见，我建议所有的 workflow 都手动设定 securityContext，示例： yaml apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: xxx spec: securityContext: runAsNonRoot: true runAsUser: 1000 或者也可以通过 workflow-controller-configmap 的 workflowDefaults 设定默认的 workflow 配置。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:4:1","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#1-workflow-默认使用-root-账号"},{"categories":["tech"],"content":" 2. 如何从 hashicorp vault 中读取 secrets? 参考Support to get secrets from Vault hashicorp vault 目前可以说是云原生领域最受欢迎的 secrets 管理工具。我们在生产环境用它做为分布式配置中心，同时在本地 CI/CD 中，也使用它存储相关的敏感信息。 现在迁移到 argo，我们当然希望能够有一个好的方法从 vault 中读取配置。 目前最推荐的方法，是使用 vault 的 vault-agent，将 secrets 以文件的形式注入到 pod 中。 通过 valut-policy - vault-role - k8s-serviceaccount 一系列认证授权配置，可以制定非常细粒度的 secrets 权限规则，而且配置信息阅后即焚，安全性很高。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:4:2","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#2-如何从-hashicorp-vault-中读取-secrets"},{"categories":["tech"],"content":" 3. 如何在多个名字空间中使用同一个 secrets?使用 Namespace 对 workflow 进行分类时，遇到的一个常见问题就是，如何在多个名字空间使用private-git-creds/docker-config/minio/vault 等 workflow 必要的 secrets. 常见的方法是把 secrets 在所有名字空间 create 一次。 但是也有更方便的 secrets 同步工具： 比如，使用 kyverno 进行 secrets 同步的配置： yaml apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: sync-secrets spec: background: false rules: # 将 secret vault 从 argo Namespace 同步到其他所有 Namespace - name: sync-vault-secret match: resources: kinds: - Namespace generate: kind: Secret name: regcred namespace: \"{{request.object.metadata.name}}\" synchronize: true clone: namespace: argo name: vault # 可以配置多个 rules，每个 rules 同步一个 secret 上面提供的 kyverno 配置，会实时地监控所有 Namespace 变更，一但有新 Namespace 被创建，它就会立即将 vault secret 同步到该 Namespace. 或者，使用专门的 secrets/configmap 复制工具：kubernetes-replicator ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:4:3","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#3-如何在多个名字空间中使用同一个-secrets"},{"categories":["tech"],"content":" 4. Argo 对 CR 资源的验证不够严谨，写错了 key 都不报错待研究 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:4:4","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#4-argo-对-cr-资源的验证不够严谨写错了-key-都不报错"},{"categories":["tech"],"content":" 5. 如何归档历史数据？Argo 用的时间长了，跑过的 Workflows/Pods 全都保存在 Kubernetes/Argo Server 中，导致 Argo 越用越慢。 为了解决这个问题，Argo 提供了一些配置来限制 Workflows 和 Pods 的数量，详见：Limit The Total Number Of Workflows And Pods 这些限制都是 Workflow 的参数，如果希望设置一个全局默认的限制，可以按照如下示例修改 argo 的workflow-controller-configmap 这个 configmap: yaml apiVersion: v1 kind: ConfigMap metadata: name: workflow-controller-configmap data: config: | # Default values that will apply to all Workflows from this controller, unless overridden on the Workflow-level # See more: docs/default-workflow-specs.md workflowDefaults: spec: # must complete in 8h (28,800 seconds) activeDeadlineSeconds: 28800 # keep workflows for 1d (86,400 seconds) ttlStrategy: secondsAfterCompletion: 86400 # secondsAfterSuccess: 5 # secondsAfterFailure: 500 # delete all pods as soon as they complete podGC: # 可选项：\"OnPodCompletion\", \"OnPodSuccess\", \"OnWorkflowCompletion\", \"OnWorkflowSuccess\" strategy: OnPodCompletion ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:4:5","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#5-如何归档历史数据"},{"categories":["tech"],"content":" 6. Argo 的其他进阶配置Argo Workflows 的配置，都保存在 workflow-controller-configmap 这个 configmap 中，我们前面已经接触到了它的部分内容。 这里给出此配置文件的完整 examples:https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.yaml 其中一些可能需要自定义的参数如下： parallelism: workflow 的最大并行数量 persistence: 将完成的 workflows 保存到 postgresql/mysql 中，这样即使 k8s 中的 workflow 被删除了，还能查看 workflow 记录 也支持配置过期时间 sso: 启用单点登录 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:4:6","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#6-argo-的其他进阶配置"},{"categories":["tech"],"content":" 7. 是否应该尽量使用 CI/CD 工具提供的功能？我从同事以及网络上，了解到部分 DevOps 人员主张尽量自己使用 Python/Go 来实现 CI/CD 流水线，CI/CD 工具提供的功能能不使用就不要使用。 因此有此一问。下面做下详细的分析： 尽量使用 CI/CD 工具提供的插件/功能，好处是不需要自己去实现，可以降低维护成本。但是相对的运维人员就需要深入学习这个 CI/CD 工具的使用，另外还会和 CI/CD 工具绑定，会增加迁移难度。 而尽量自己用 Python 等代码去实现流水线，让 CI/CD 工具只负责调度与运行这些 Python 代码，那 CI/CD 就可以很方便地随便换，运维人员也不需要去深入学习 CI/CD 工具的使用。缺点是可能会增加 CI/CD 代码的复杂性。 我观察到 argo/drone 的一些 examples，发现它们的特征是： 所有 CI/CD 相关的逻辑，全都实现在流水线中，不需要其他构建代码 每一个 step 都使用专用镜像：golang/nodejs/python 比如先使用 golang 镜像进行测试、构建，再使用 kaniko 将打包成容器镜像 那是否应该尽量使用 CI/CD 工具提供的功能呢？ 其实这就是有多种方法实现同一件事，该用哪种方法的问题。这个问题在各个领域都很常见。 以我目前的经验来看，需要具体问题具体分析，以 Argo Workflows 为例： 流水线本身非常简单，那完全可以直接使用 argo 来实现，没必要自己再搞个 python 脚本 简单的流水线，迁移起来往往也非常简单。没必要为了可迁移性，非要用 argo 去调用 python 脚本。 流水线的步骤之间包含很多逻辑判断/数据传递，那很可能是你的流水线设计有问题！ 流水线的步骤之间传递的数据应该尽可能少！复杂的逻辑判断应该尽量封装在其中一个步骤中！ 这种情况下，就应该使用 python 脚本来封装复杂的逻辑，而不应该将这些逻辑暴露到 Argo Workflows 中！ 我需要批量运行很多的流水线，而且它们之间还有复杂的依赖关系：那显然应该利用上 argo workflow 的高级特性。 argo 的 dag/steps 和 workflow of workflows 这两个功能结合，可以简单地实现上述功能。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:4:7","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#7-是否应该尽量使用-cicd-工具提供的功能"},{"categories":["tech"],"content":" 8. 如何提升 Argo Workflows 的创建和销毁速度？我们发现 workflow 的 pod，创建和销毁消耗了大量时间，尤其是销毁。这导致我们单个流水线在 argo 上跑，还没在 jenkins 上跑更快。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:5:0","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#8-如何提升-argo-workflows-的创建和销毁速度"},{"categories":["tech"],"content":" 使用体验目前已经使用 Argo Workflows 一个月多了，总的来说，最难用的就是 Web UI。 其他的都是小问题，只有 Web UI 是真的超难用，感觉根本就没有好好做过设计… 急需一个第三方 Web UI… ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:6:0","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#使用体验"},{"categories":["tech"],"content":" 画外 - 如何处理其他 Kubernetes 资源之间的依赖关系Argo 相比其他 CI 工具，最大的特点，是它假设「任务」之间是有依赖关系的，因此它提供了多种协调编排「任务」的方法。 但是貌似 Argo CD 并没有继承这个理念，Argo CD 部署时，并不能在 kubernetes 资源之间，通过 DAG 等方法定义依赖关系。 微服务之间存在依赖关系，希望能按依赖关系进行部署，而 ArgoCD/FluxCD 部署 kubernetes yaml 时都是不考虑任何依赖关系的。这里就存在一些矛盾。 解决这个矛盾的方法有很多，我查阅了很多资料，也自己做了一些思考，得到的最佳实践来自解决服务依赖 - 阿里云 ACK 容器服务，它给出了两种方案： 应用端服务依赖检查: 即在微服务的入口添加依赖检查逻辑，确保所有依赖的微服务/数据库都可访问了，就续探针才能返回 200. 如果超时就直接 Crash 独立的服务依赖检查逻辑: 部分遗留代码使用方法一改造起来或许会很困难，这时可以考虑使用 pod initContainer 或者容器的启动脚本中，加入依赖检查逻辑。 但是这两个方案也还是存在一些问题，在说明问题前，我先说明一下我们「按序部署」的应用场景。 我们是一个很小的团队，后端做 RPC 接口升级时，通常是直接在开发环境做全量升级+测试。因此运维这边也是，每次都是做全量升级。 因为没有协议协商机制，新的微服务的「RPC 服务端」将兼容 v1 v2 新旧两种协议，而新的「RPC 客户端」将直接使用 v2 协议去请求其他微服务。这就导致我们必须先升级「RPC 服务端」，然后才能升级「RPC 客户端」。 为此，在进行微服务的全量升级时，就需要沿着 RPC 调用链路按序升级，这里就涉及到了 Kubernetes 资源之间的依赖关系。 我目前获知的关键问题在于：我们使用的并不是真正的微服务开发模式，而是在把整个微服务系统当成一个「单体服务」在看待，所以引申出了这样的依赖关键的问题。我进入的新公司完全没有这样的问题，所有的服务之间在 CI/CD 这个阶段都是解耦的，CI/CD 不需要考虑服务之间的依赖关系，也没有自动按照依赖关系进行微服务批量发布的功能，这些都由开发人员自行维护。或许这才是正确的使用姿势，如果动不动就要批量更新一大批服务，那微服务体系的设计、拆分肯定是有问题了，生产环境也不会允许这么轻率的更新。 前面讲了，阿里云提供的「应用端服务依赖检查」和「独立的服务依赖检查逻辑」是最佳实践。它们的优点有： 简化部署逻辑，每次直接做全量部署就 OK。 提升部署速度，具体体现在：GitOps 部署流程只需要走一次（按序部署要很多次）、所有镜像都提前拉取好了、所有 Pod 也都提前启动了。 但是这里有个问题是「灰度发布」或者「滚动更新」，这两种情况下都存在新旧版本共存的问题。 如果出现了 RPC 接口升级，那就必须先完成「RPC 服务端」的「灰度发布」或者「滚动更新」，再去更新「RPC 客户端」。 否则如果直接对所有微服务做灰度更新，只依靠「服务依赖检查」，就会出现这样的问题——「RPC 服务端」处于「薛定谔」状态，你调用到的服务端版本是新还是旧，取决于负载均衡的策略和概率。 因此在做 RPC 接口的全量升级时，只依靠「服务依赖检查」是行不通的。我目前想到的方案，有如下几种： 我们当前的使用方案：直接在 yaml 部署这一步实现按序部署，每次部署后就轮询 kube-apiserver，确认全部灰度完成，再进行下一阶段的 yaml 部署。 让后端加个参数来控制客户端使用的 RPC 协议版本，或者搞一个协议协商。这样就不需要控制微服务发布顺序了。 社区很多有状态应用的部署都涉及到部署顺序等复杂操作，目前流行的解决方案是使用 Operator+CRD 来实现这类应用的部署。Operator 会自行处理好各个组件的部署顺序。 ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:7:0","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#画外---如何处理其他-kubernetes-资源之间的依赖关系"},{"categories":["tech"],"content":" 参考文档 Argo加入CNCF孵化器，一文解析Kubernetes原生工作流 视频: How to Multiply the Power of Argo Projects By Using Them Together - Hong Wang ","date":"2021-01-27","objectID":"/posts/experience-of-argo-workflows/:8:0","series":["云原生相关"],"tags":["云原生","CI","持续集成","流水线","Kubernetes"],"title":"云原生流水线 Argo Workflows 的安装、使用以及个人体验","uri":"/posts/experience-of-argo-workflows/#参考文档"},{"categories":["tech"],"content":"Vault 是 hashicorp 推出的 secrets 管理、加密即服务与权限管理工具。它的功能简介如下： secrets 管理：支持保存各种自定义信息、自动生成各类密钥，vault 自动生成的密钥还能自动轮转(rotate) 认证方式：支持接入各大云厂商的账号体系（比如阿里云RAM子账号体系）或者 LDAP 等进行身份验证，不需要创建额外的账号体系。 权限管理：通过 policy，可以设定非常细致的 ACL 权限。 密钥引擎：也支持接管各大云厂商的账号体系（比如阿里云RAM子账号体系），实现 API Key 的自动轮转。 支持接入 kubernetes rbac 认证体系，通过 serviceaccount+role 为每个 Pod 单独配置认证角色。 支持通过 sidecar/init-container 将 secrets 注入到 pod 中，或者通过 k8s operator 将 vault 数据同步到 k8s secrets 中 在使用 Vault 之前，我们是以携程开源的 Apollo 作为微服务的分布式配置中心。 Apollo 在国内非常流行。它功能强大，支持配置的继承，也有提供 HTTP API 方便自动化。缺点是权限管理和 secrets 管理比较弱，也不支持信息加密，不适合直接存储敏感信息。因此我们现在切换到了 Vault. 目前我们本地的 CI/CD 流水线和云上的微服务体系，都是使用的 Vault 做 secrets 管理. ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:0:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#"},{"categories":["tech"],"content":" 一、Vault 基础概念 「基本概念」这一节，基本都翻译自官方文档:https://www.vaultproject.io/docs/internals/architecture 首先看一下 Vault 的架构图： vault layers 可以看到，几乎所有的 Vault 组件都被统称为「屏障（Barrier）」。 Vault 可以简单地被划分为存储后端（Storage Backend）、屏障（Barrier）和 HTTP/S API 三个部分。 Vault，翻译成中文就是金库。类比银行金库，「屏障」就是用于保护金库的合金大门和钢筋混凝土，存储后端和客户端之间的所有数据流动都需要经过它。 「屏障」确保只有加密数据会被写入存储后端，加密数据在经过「屏障」被读出的过程中被验证与解密。 和银行金库的大门非常类似，「屏障」也必须先解封，才能解密存储后端中的数据。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:1:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#一vault-基础概念"},{"categories":["tech"],"content":" 1. 数据存储及加密解密存储后端（Storage Backend）: Vault 自身不存储数据，因此需要为它配置一个存储后端。存储后端是不受信任的，只用于存储加密数据。 初始化（Initialization）: Vault 在首次启动时需要初始化，这一步生成一个加密密钥（Encryption Key）用于加密数据，加密完成的数据才能被保存到存储后端。 解封（Unseal）: Vault 启动后，因为不知道加密密钥所以无法解密数据，这种状态被形象得称作已封印（Sealed）。在解封前 Vault 无法进行任何操作。 加密密钥被主密钥（Master Key）保护，我们必须提供主密钥才能解密出 Vault 的加密密钥，从而完成解封操作。 默认情况下，Vault 使用沙米尔密钥分割算法 将主密钥分割成五个分割密钥（Key Shares），必须要提供其中任意三个分割密钥才能重建出主密钥，完成解封操作。 vault-shamir-secret-sharing 分割密钥的总数，以及重建主密钥最少需要的分割密钥数量，都是可以调整的。沙米尔密钥分割算法也可以关闭，这样主密钥将被直接提供给管理员，管理员可直接使用它进行解封操作。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:1:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#1-数据存储及加密解密"},{"categories":["tech"],"content":" 2. 认证系统及权限系统在解封完成后，Vault 就可以开始处理请求了。 HTTP 请求进入后的整个处理流程都由 vault core 管理，core 会强制进行 ACL 检查，并确保审计日志(audit logging)完成记录。 客户端首次连接 vault 时，需要先完成身份认证，vault 的 auth methods 模块有很多身份认证方法可选： 用户友好的认证方法，适合管理员使用：username/password、云服务商、ldap 在创建 user 的时候，需要为 user 绑定 policy，给予合适的权限。 应用友好的方法，适合应用程序使用：public/private keys、tokens、kubernetes、jwt 身份验证请求流经 core 并进入 auth methods，auth methods 确定请求是否有效并返回「关联策略(policies)」的列表。 ACL 策略由 policy store 负责管理与存储，由 core 进行 ACL 检查。ACL 的默认行为是拒绝，这意味着除非明确配置 policy 允许某项操作，否则该操作将被拒绝。 在通过 auth methods 完成了身份认证，并且返回的关联策略也没毛病之后，token store 将会生成并管理一个新的凭证（token），这个 token 会被返回给客户端，用于进行后续请求。 类似 web 网站的 cookie，token 也都存在一个租期（lease）或者说有效期，这加强了安全性。 token 关联了相关的策略 policies，这些策略将被用于验证请求的权限。 请求经过验证后，将被路由到 secret engine。如果 secret engine 返回了一个secret（由 vault 自动生成的 secret），core 会将其注册到 expiration manager，并给它附加一个 lease ID。lease ID 被客户端用于更新(renew)或吊销(revoke)它得到的 secret. 如果客户端允许租约(lease)到期，expiration manager 将自动吊销这个 secret. core 还负责处理审核代理 audit broker的请求及响应日志，将请求发送到所有已配置的审核设备 audit devices. 不过默认情况下这个功能貌似是关闭的。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:1:2","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#2-认证系统及权限系统"},{"categories":["tech"],"content":" 3. Secret EngineSecret Engine 是保存、生成或者加密数据的组件，它非常灵活。 有的 Secret Engines 只是单纯地存储与读取数据，比如 kv 就可以看作一个加密的 Redis。而其他的 Secret Engines 则连接到其他的服务并按需生成动态凭证。 还有些 Secret Engines 提供「加密即服务(encryption as a service)」的能力，如 transit、证书管理等。 常用的 engine 举例： AliCloud Secrets Engine: 基于 RAM 策略动态生成 AliCloud Access Token，或基于 RAM 角色动态生成 AliCloud STS 凭据 Access Token 会自动更新(Renew)，而 STS 凭据是临时使用的，过期后就失效了。 kv: 键值存储，可用于存储一些静态的配置。它一定程度上能替代掉携程的 Apollo 配置中心。 Transit Secrets Engine: 提供加密即服务的功能，它只负责加密和解密，不负责存储。主要应用场景是帮 app 加解密数据，但是数据仍旧存储在 MySQL 等数据库中。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:1:3","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#3-secret-engine"},{"categories":["tech"],"content":" 二、部署 Vault官方建议通过 Helm 部署 vault，大概流程： 使用 helm/docker 部署运行 vault. 初始化/解封 vault: vault 安全措施，每次重启必须解封(可设置自动解封). ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:2:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#二部署-vault"},{"categories":["tech"],"content":" 0. 如何选择存储后端？首先，我们肯定需要高可用 HA，至少要保留能升级到 HA 的能力，所以不建议选择不支持 HA 的后端。 而具体的选择，就因团队经验而异了，人们往往倾向于使用自己熟悉的、知根知底的后端，或者选用云服务。 比如我们对 MySQL/PostgreSQL 比较熟悉，而且使用云服务提供的数据库不需要考虑太多的维护问题，MySQL/PostgreSQL 作为一个通用协议也不会被云厂商绑架，那我们就倾向于使用这两者之一。 而如果你们是本地自建，那你可能更倾向于使用 Etcd/Consul/Raft 做后端存储。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:2:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#0-如何选择存储后端"},{"categories":["tech"],"content":" 1. docker-compose 部署（非 HA） 推荐用于本地开发测试环境，或者其他不需要高可用的环境。 docker-compose.yml 示例如下： yaml version: \"3.3\" services: vault: # 文档：https://hub.docker.com/_/vault image: vault:1.6.0 container_name: vault ports: # rootless 容器，内部不能使用标准端口 443 - \"443:8200\" restart: always volumes: # 审计日志存储目录（`file` audit backend） - ./logs:/vault/logs # 当使用 file data storage 插件时，数据被存储在这里。默认不往这写任何数据。 - ./file:/vault/file # vault 配置 - ./config.hcl:/vault/config/config.hcl # TLS 证书 - ./certs:/certs # vault 需要锁定内存以防止敏感值信息被交换(swapped)到磁盘中 # 为此需要添加如下 capability cap_add: - IPC_LOCK # 必须设定 entrypoint，否则 vault 容器默认以 development 模式运行 entrypoint: vault server -config /vault/config/config.hcl config.hcl 内容如下： hcl ui = true // 使用文件做数据存储（单节点） storage \"file\" { path = \"/vault/file\" } listener \"tcp\" { address = \"[::]:8200\" tls_disable = false tls_cert_file = \"/certs/server.crt\" tls_key_file = \"/certs/server.key\" } 将如上两份配置保存在同一文件夹内，同时在 ./certs 中提供 TLS 证书 server.crt 和私钥server.key。 然后 docker-compose up -d 就能启动运行一个 vault 实例。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:2:2","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#1-docker-compose-部署非-ha"},{"categories":["tech"],"content":" 2. 通过 helm 部署高可用的 vault 推荐用于生产环境 通过 helm 部署： shell # 添加 valut 仓库 helm repo add hashicorp https://helm.releases.hashicorp.com # 查看 vault 版本号 helm search repo hashicorp/vault -l | head # 下载某个版本号的 vault helm pull hashicorp/vault --version 0.11.0 --untar 参照下载下来的 ./vault/values.yaml 编写 custom-values.yaml，部署一个以 mysql 为后端存储的 HA vault，配置示例如下: 配置内容虽然多，但是大都是直接拷贝自 ./vault/values.yaml，改动很少。测试 Vault 时可以忽略掉其中大多数的配置项。 yaml global: # enabled is the master enabled switch. Setting this to true or false # will enable or disable all the components within this chart by default. enabled: true # TLS for end-to-end encrypted transport tlsDisable: false injector: # True if you want to enable vault agent injection. enabled: true replicas: 1 # If true, will enable a node exporter metrics endpoint at /metrics. metrics: enabled: false # Mount Path of the Vault Kubernetes Auth Method. authPath: \"auth/kubernetes\" certs: # secretName is the name of the secret that has the TLS certificate and # private key to serve the injector webhook. If this is null, then the # injector will default to its automatic management mode that will assign # a service account to the injector to generate its own certificates. secretName: null # caBundle is a base64-encoded PEM-encoded certificate bundle for the # CA that signed the TLS certificate that the webhook serves. This must # be set if secretName is non-null. caBundle: \"\" # certName and keyName are the names of the files within the secret for # the TLS cert and private key, respectively. These have reasonable # defaults but can be customized if necessary. certName: tls.crt keyName: tls.key server: # Resource requests, limits, etc. for the server cluster placement. This # should map directly to the value of the resources field for a PodSpec. # By default no direct resource request is made. # Enables a headless service to be used by the Vault Statefulset service: enabled: true # Port on which Vault server is listening port: 8200 # Target port to which the service should be mapped to targetPort: 8200 # This configures the Vault Statefulset to create a PVC for audit # logs. Once Vault is deployed, initialized and unseal, Vault must # be configured to use this for audit logs. This will be mounted to # /vault/audit # See https://www.vaultproject.io/docs/audit/index.html to know more auditStorage: enabled: false # Run Vault in \"HA\" mode. There are no storage requirements unless audit log # persistence is required. In HA mode Vault will configure itself to use Consul # for its storage backend. The default configuration provided will work the Consul # Helm project by default. It is possible to manually configure Vault to use a # different HA backend. ha: enabled: true replicas: 3 # Set the api_addr configuration for Vault HA # See https://www.vaultproject.io/docs/configuration#api_addr # If set to null, this will be set to the Pod IP Address apiAddr: null # config is a raw string of default configuration when using a Stateful # deployment. Default is to use a Consul for its HA storage backend. # This should be HCL. # Note: Configuration files are stored in ConfigMaps so sensitive data # such as passwords should be either mounted through extraSecretEnvironmentVars # or through a Kube secret. For more information see: # https://www.vaultproject.io/docs/platform/k8s/helm/run#protecting-sensitive-vault-configurations config: | ui = true listener \"tcp\" { address = \"[::]:8200\" cluster_address = \"[::]:8201\" # 注意，这个值要和 helm 的参数 global.tlsDisable 一致 tls_disable = false tls_cert_file = \"/etc/certs/vault.crt\" tls_key_file = \"/etc/certs/vault.key\" } # storage \"postgresql\" { # connection_url = \"postgres://username:password@\u003chost\u003e:5432/vault?sslmode=disable\" # ha_enabled = true # } service_registration \"kubernetes\" {} # Example configuration for using auto-unseal, using AWS KMS. # the cluster must have a service account that is authorized to access AWS KMS, through an IAM Role. # seal \"awskms\" { # region = \"us-east-1\" # kms_key_id = \"\u003csome-key-i","date":"2021-01-24","objectID":"/posts/experience-of-vault/:2:3","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#install-by-helm"},{"categories":["tech"],"content":" 3. 初始化并解封 vault 官方文档：Initialize and unseal Vault - Vault on Kubernetes Deployment Guide 通过 helm 部署 vault，默认会部署一个三副本的 StatefulSet，但是这三个副本都会处于 NotReady 状态（docker 方式部署的也一样）。接下来还需要手动初始化并解封 vault，才能 Ready: 第一步：从三个副本中随便选择一个，运行 vault 的初始化命令：kubectl exec -ti vault-0 -- vault operator init 初始化操作会返回 5 个 unseal keys，以及一个 Initial Root Token，这些数据非常敏感非常重要，一定要保存到安全的地方！ 第二步：在每个副本上，使用任意三个 unseal keys 进行解封操作。 一共有三个副本，也就是说要解封 3*3 次，才能完成 vault 的完整解封！ shell # 每个实例都需要解封三次！ ## Unseal the first vault server until it reaches the key threshold $ kubectl exec -ti vault-0 -- vault operator unseal # ... Unseal Key 1 $ kubectl exec -ti vault-0 -- vault operator unseal # ... Unseal Key 2 $ kubectl exec -ti vault-0 -- vault operator unseal # ... Unseal Key 3 这样就完成了部署，但是要注意，vault 实例每次重启后，都需要重新解封！也就是重新进行第二步操作！ ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:2:4","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#3-初始化并解封-vault"},{"categories":["tech"],"content":" 4. 初始化并设置自动解封在未设置 auto unseal 的情况下，vault 每次重启都要手动解封所有 vault 实例，实在是很麻烦，在云上自动扩缩容的情况下，vault 实例会被自动调度，这种情况就更麻烦了。 为了简化这个流程，可以考虑配置 auto unseal 让 vault 自动解封。 自动解封目前有两种方法： 使用阿里云/AWS/Azure 等云服务提供的密钥库来管理 encryption key AWS: awskms Seal 如果是 k8s 集群，vault 使用的 ServiceAccount 需要有权限使用 AWS KMS，它可替代掉 config.hcl 中的 access_key/secret_key 两个属性 阿里云：alicloudkms Seal 如果你不想用云服务，那可以考虑autounseal-transit， 这种方法使用另一个 vault 实例提供的 transit 引擎来实现 auto-unseal. 简单粗暴：直接写个 crontab 或者在 CI 平台上加个定时任务去执行解封命令，以实现自动解封。不过这样安全性就不好说了。 以使用 awskms 为例，首先创建 aws IAM 的 policy 内容如下: json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VaultKMSUnseal\", \"Effect\": \"Allow\", \"Action\": [\"kms:Decrypt\", \"kms:Encrypt\", \"kms:DescribeKey\"], \"Resource\": \"*\" } ] } 然后创建 IAM Role 绑定上面的 policy，并为 vault 的 k8s serviceaccount 创建一个 IAM Role， 绑定上这个 policy. 这样 vault 使用的 serviceaccount 自身就拥有了访问 awskms 的权限，也就不需要额外通过 access_key/secret_key 来访问 awskms. 关于 IAM Role 和 k8s serviceaccount 如何绑定，参见官方文档：IAM roles for EKS service accounts 完事后再修改好前面提供的 helm 配置，部署它，最后使用如下命令初始化一下： shell # 初始化命令和普通模式并无不同 kubectl exec -ti vault-0 -- vault operator init # 会打印出一个 root token，以及五个 Recovery Key（而不是 Unseal Key） # Recover Key 不再用于解封，但是重新生成 root token 等操作仍然会需要用到它. 然后就大功告成了，可以尝试下删除 vault 的 pod，新建的 Pod 应该会自动解封。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:2:5","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#4-初始化并设置自动解封"},{"categories":["tech"],"content":" 三、Vault 自身的配置管理Vault 本身是一个复杂的 secrets 工具，它提供了 Web UI 和 CLI 用于手动管理与查看 Vault 的内容。 但是作为一名 DevOps，我们当然更喜欢更自治的方法，这有两种选择: 使用 vault 的 sdk: python-hvac 使用 terraform-provider-vault 或者 pulumi-vault 实现 vault 配置的自动化管理。 Web UI 适合手工操作，而 sdk/terraform-provider-vault 则适合用于自动化管理 vault. 我们的测试环境就是使用 pulumi-vault 完成的自动化配置 vault policy 和 kubernetes role，然后自动化注入所有测试用的 secrets. ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:3:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#三vault-自身的配置管理"},{"categories":["tech"],"content":" 1. 使用 pulumi 自动化配置 vault使用 pulumi 管理 vault 配置的优势是很大的，因为云上资源的敏感信息（数据库账号密码、资源 ID、RAM子账号）都是 pulumi 创建的。 再结合使用 pulumi_valut，就能实现敏感信息自动生成后，立即保存到 vault 中，实现完全自动化。 后续微服务就可以通过 kubernetes 认证，直接从 vault 读取敏感信息。 或者是写入到本地的 vault 中留做备份，在需要的时候，管理员能登入进去查看相关敏感信息。 1.1 Token 的生成pulumi_vault 本身挺简单的，声明式的配置嘛，直接用就是了。 但是它一定要求提供 VAULT_TOKEN 作为身份认证的凭证（实测 userpass/approle 都不能直接使用，会报错 no vault token found），而且 pulumi 还会先生成临时用的 child token，然后用这个 child token 进行后续的操作。 首先安全起见，肯定不应该直接提供 root token！root token 应该封存，除了紧急情况不应该启用。 那么应该如何生成一个权限有限的 token 给 vault 使用呢？我的方法是创建一个 userpass 账号，通过 policy 给予它有限的权限。然后先手动(或者自动)登录获取到 token，再将 token 提供给 pulumi_vault 使用。 这里面有个坑，就是必须给 userpass 账号创建 child token 的权限： hcl path \"local/*\" { capabilities = [\"read\", \"list\"] } // 允许创建 child token path \"auth/token/create\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } 不给这个权限，pulumi_vault 就会一直报错。。 然后还得给它「自动化配置」需要的权限，比如自动创建/更新 policy/secrets/kubernetes 等等，示例如下: hcl # To list policies - Step 3 path \"sys/policy\" { capabilities = [\"read\"] } # Create and manage ACL policies broadly across Vault path \"sys/policy/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } # List, create, update, and delete key/value secrets path \"secret/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } path \"auth/kubernetes/role/*\" { capabilities = [\"create\", \"read\", \"update\", \"list\"] } ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:3:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#1-使用-pulumi-自动化配置-vault"},{"categories":["tech"],"content":" 1. 使用 pulumi 自动化配置 vault使用 pulumi 管理 vault 配置的优势是很大的，因为云上资源的敏感信息（数据库账号密码、资源 ID、RAM子账号）都是 pulumi 创建的。 再结合使用 pulumi_valut，就能实现敏感信息自动生成后，立即保存到 vault 中，实现完全自动化。 后续微服务就可以通过 kubernetes 认证，直接从 vault 读取敏感信息。 或者是写入到本地的 vault 中留做备份，在需要的时候，管理员能登入进去查看相关敏感信息。 1.1 Token 的生成pulumi_vault 本身挺简单的，声明式的配置嘛，直接用就是了。 但是它一定要求提供 VAULT_TOKEN 作为身份认证的凭证（实测 userpass/approle 都不能直接使用，会报错 no vault token found），而且 pulumi 还会先生成临时用的 child token，然后用这个 child token 进行后续的操作。 首先安全起见，肯定不应该直接提供 root token！root token 应该封存，除了紧急情况不应该启用。 那么应该如何生成一个权限有限的 token 给 vault 使用呢？我的方法是创建一个 userpass 账号，通过 policy 给予它有限的权限。然后先手动(或者自动)登录获取到 token，再将 token 提供给 pulumi_vault 使用。 这里面有个坑，就是必须给 userpass 账号创建 child token 的权限： hcl path \"local/*\" { capabilities = [\"read\", \"list\"] } // 允许创建 child token path \"auth/token/create\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"] } 不给这个权限，pulumi_vault 就会一直报错。。 然后还得给它「自动化配置」需要的权限，比如自动创建/更新 policy/secrets/kubernetes 等等，示例如下: hcl # To list policies - Step 3 path \"sys/policy\" { capabilities = [\"read\"] } # Create and manage ACL policies broadly across Vault path \"sys/policy/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } # List, create, update, and delete key/value secrets path \"secret/*\" { capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"] } path \"auth/kubernetes/role/*\" { capabilities = [\"create\", \"read\", \"update\", \"list\"] } ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:3:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#11-token-的生成"},{"categories":["tech"],"content":" 四、在 Kubernetes 中使用 vault 注入 secretsvault-k8s-auth-workflow 前面提到过 vault 支持通过 Kubernetes 的 ServiceAccount 为每个 Pod 单独分配权限。 应用程序有两种方式去读取 vault 中的配置： 借助 Vault Sidecar，将 secrets 以文件的形式自动注入到 Pod 中，比如/vault/secrets/config.json vault sidecar 在常驻模式下每 15 秒更新一次配置，应用程序可以使用 watchdog 实时监控 secrets 文件的变更。 应用程序自己使用 SDK 直接访问 vault api 获取 secrets 上述两种方式，都可以借助 Kubernetes ServiceAccount 进行身份验证和权限分配。 下面以 Sidecar 模式为例，介绍如何将 secrets 以文件形式注入到 Pod 中。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:4:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#四在-kubernetes-中使用-vault-注入-secrets"},{"categories":["tech"],"content":" 1. 部署并配置 vault agent首先启用 Vault 的 Kubernetes 身份验证: shell # 配置身份认证需要在 vault pod 中执行，启动 vault-0 的交互式会话 kubectl exec -n vault -it vault-0 -- /bin/sh export VAULT_TOKEN='\u003cyour-root-token\u003e' export VAULT_ADDR='http://localhost:8200' # 启用 Kubernetes 身份验证 vault auth enable kubernetes # kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证 vault write auth/kubernetes/config \\ token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\ kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt 1.1 使用集群外部的 valut 实例 如果你没这个需求，请跳过这一节。 详见Install the Vault Helm chart configured to address an external Vault kubernetes 也可以和外部的 vault 实例集成，集群中只部署 vault-agent. 这适用于多个 kubernetes 集群以及其他 APP 共用一个 vault 实例的情况，比如我们本地的多个开发测试集群，就都共用着同一个 vault 实例，方便统一管理应用的 secrets. 首先，使用 helm chart 部署 vault-agent，接入外部的 vault 实例。使用的 custom-values.yaml 示例如下： yaml global: # enabled is the master enabled switch. Setting this to true or false # will enable or disable all the components within this chart by default. enabled: true # TLS for end-to-end encrypted transport tlsDisable: false injector: # True if you want to enable vault agent injection. enabled: true replicas: 1 # If multiple replicas are specified, by default a leader-elector side-car # will be created so that only one injector attempts to create TLS certificates. leaderElector: enabled: true image: repository: \"gcr.io/google_containers/leader-elector\" tag: \"0.4\" ttl: 60s # If true, will enable a node exporter metrics endpoint at /metrics. metrics: enabled: false # External vault server address for the injector to use. Setting this will # disable deployment of a vault server along with the injector. # TODO 这里的 https ca.crt 要怎么设置？mTLS 又该如何配置？ externalVaultAddr: \"https://\u003cexternal-vault-url\u003e\" # Mount Path of the Vault Kubernetes Auth Method. authPath: \"auth/kubernetes\" certs: # secretName is the name of the secret that has the TLS certificate and # private key to serve the injector webhook. If this is null, then the # injector will default to its automatic management mode that will assign # a service account to the injector to generate its own certificates. secretName: null # caBundle is a base64-encoded PEM-encoded certificate bundle for the # CA that signed the TLS certificate that the webhook serves. This must # be set if secretName is non-null. caBundle: \"\" # certName and keyName are the names of the files within the secret for # the TLS cert and private key, respectively. These have reasonable # defaults but can be customized if necessary. certName: tls.crt keyName: tls.key 部署命令和 通过 helm 部署 vault 一致，只要更换 custom-values.yaml 就行。 vault-agent 部署完成后，第二步是为 vault 创建 serviceAccount、secret 和 ClusterRoleBinding，以允许 vault 审查 kubernetes 的 token, 完成对 pod 的身份验证. yaml 配置如下： yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: vault-auth namespace: vault --- apiVersion: v1 kind: Secret metadata: name: vault-auth namespace: vault annotations: kubernetes.io/service-account.name: vault-auth type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: role-tokenreview-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: vault-auth namespace: vault 现在在 vault 实例这边，启用 kubernetes 身份验证，在 vault 实例内，执行如下命令： vault 实例内显然没有 kubectl 和 kubeconfig，简便起见，下列的 vault 命令也可以通过 Web UI 完成。 shell export VAULT_TOKEN='\u003cyour-root-token\u003e' export VAULT_ADDR='http://localhost:8200' # 启用 Kubernetes 身份验证 vault auth enable kubernetes # kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证 # TOKEN_REVIEW_JWT: 就是我们前面创建的 secret `vault-auth` TOKEN_REVIEW_JWT=$(kubectl -n vault get secret vault-auth -o go-template='{{ .data.token }}' | base64 --decode) # kube-apiserver 的 ca 证书 KUBE_CA_CERT=$(kubectl -n vault config view --raw --minify --flatten -o jsonpath='{.clusters[].cl","date":"2021-01-24","objectID":"/posts/experience-of-vault/:4:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#1-部署并配置-vault-agent"},{"categories":["tech"],"content":" 1. 部署并配置 vault agent首先启用 Vault 的 Kubernetes 身份验证: shell # 配置身份认证需要在 vault pod 中执行，启动 vault-0 的交互式会话 kubectl exec -n vault -it vault-0 -- /bin/sh export VAULT_TOKEN='' export VAULT_ADDR='http://localhost:8200' # 启用 Kubernetes 身份验证 vault auth enable kubernetes # kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证 vault write auth/kubernetes/config \\ token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\ kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt 1.1 使用集群外部的 valut 实例 如果你没这个需求，请跳过这一节。 详见Install the Vault Helm chart configured to address an external Vault kubernetes 也可以和外部的 vault 实例集成，集群中只部署 vault-agent. 这适用于多个 kubernetes 集群以及其他 APP 共用一个 vault 实例的情况，比如我们本地的多个开发测试集群，就都共用着同一个 vault 实例，方便统一管理应用的 secrets. 首先，使用 helm chart 部署 vault-agent，接入外部的 vault 实例。使用的 custom-values.yaml 示例如下： yaml global: # enabled is the master enabled switch. Setting this to true or false # will enable or disable all the components within this chart by default. enabled: true # TLS for end-to-end encrypted transport tlsDisable: false injector: # True if you want to enable vault agent injection. enabled: true replicas: 1 # If multiple replicas are specified, by default a leader-elector side-car # will be created so that only one injector attempts to create TLS certificates. leaderElector: enabled: true image: repository: \"gcr.io/google_containers/leader-elector\" tag: \"0.4\" ttl: 60s # If true, will enable a node exporter metrics endpoint at /metrics. metrics: enabled: false # External vault server address for the injector to use. Setting this will # disable deployment of a vault server along with the injector. # TODO 这里的 https ca.crt 要怎么设置？mTLS 又该如何配置？ externalVaultAddr: \"https://\" # Mount Path of the Vault Kubernetes Auth Method. authPath: \"auth/kubernetes\" certs: # secretName is the name of the secret that has the TLS certificate and # private key to serve the injector webhook. If this is null, then the # injector will default to its automatic management mode that will assign # a service account to the injector to generate its own certificates. secretName: null # caBundle is a base64-encoded PEM-encoded certificate bundle for the # CA that signed the TLS certificate that the webhook serves. This must # be set if secretName is non-null. caBundle: \"\" # certName and keyName are the names of the files within the secret for # the TLS cert and private key, respectively. These have reasonable # defaults but can be customized if necessary. certName: tls.crt keyName: tls.key 部署命令和 通过 helm 部署 vault 一致，只要更换 custom-values.yaml 就行。 vault-agent 部署完成后，第二步是为 vault 创建 serviceAccount、secret 和 ClusterRoleBinding，以允许 vault 审查 kubernetes 的 token, 完成对 pod 的身份验证. yaml 配置如下： yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: vault-auth namespace: vault --- apiVersion: v1 kind: Secret metadata: name: vault-auth namespace: vault annotations: kubernetes.io/service-account.name: vault-auth type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: role-tokenreview-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: vault-auth namespace: vault 现在在 vault 实例这边，启用 kubernetes 身份验证，在 vault 实例内，执行如下命令： vault 实例内显然没有 kubectl 和 kubeconfig，简便起见，下列的 vault 命令也可以通过 Web UI 完成。 shell export VAULT_TOKEN='' export VAULT_ADDR='http://localhost:8200' # 启用 Kubernetes 身份验证 vault auth enable kubernetes # kube-apiserver API 配置，vault 需要通过 kube-apiserver 完成对 serviceAccount 的身份验证 # TOKEN_REVIEW_JWT: 就是我们前面创建的 secret `vault-auth` TOKEN_REVIEW_JWT=$(kubectl -n vault get secret vault-auth -o go-template='{{ .data.token }}' | base64 --decode) # kube-apiserver 的 ca 证书 KUBE_CA_CERT=$(kubectl -n vault config view --raw --minify --flatten -o jsonpath='{.clusters[].cl","date":"2021-01-24","objectID":"/posts/experience-of-vault/:4:1","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#11-使用集群外部的-valut-实例"},{"categories":["tech"],"content":" 2. 关联 k8s rbac 权限系统和 vault接下来需要做的事： 通过 vault policy 定义好每个 role（微服务）能访问哪些资源。 为每个微服务生成一个 role，这个 role 需要绑定对应的 vault policy 及 kubernetes serviceaccount 这个 role 是 vault 的 kubernetes 插件自身的属性，它和 kubernetes role 没有半毛钱关系。 创建一个 ServiceAccount，并使用这个 使用这个 ServiceAccount 部署微服务 其中第一步和第二步都可以通过 vault api 自动化完成. 第三步可以通过 kubectl 部署时完成。 方便起见，vault policy / role / k8s serviceaccount 这三个配置，都建议和微服务使用相同的名称。 上述配置中，role 起到一个承上启下的作用，它关联了 k8s serviceaccount 和 vault policy 两个配置。 比如创建一个名为 my-app-policy 的 vault policy，内容为: hcl # 允许读取数据 path \"my-app/data/*\" { capabilities = [\"read\", \"list\"] } // 允许列出 myapp 中的所有数据(kv v2) path \"myapp/metadata/*\" { capabilities = [\"read\", \"list\"] } 然后在 vault 的 kubernetes 插件配置中，创建 role my-app-role，配置如下: 关联 k8s default 名字空间中的 serviceaccount my-app-account，并创建好这个 serviceaccount. 关联 vault token policy，这就是前面创建的 my-app-policy 设置 token period（有效期） 这之后，每个微服务就能通过 serviceaccount 从 vault 中读取 my-app 中的所有信息了。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:4:2","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#2-关联-k8s-rbac-权限系统和-vault"},{"categories":["tech"],"content":" 3. 部署 Pod 参考文档：https://www.vaultproject.io/docs/platform/k8s/injector 下一步就是将配置注入到微服务容器中，这需要使用到 Agent Sidecar Injector。vault 通过 sidecar 实现配置的自动注入与动态更新。 具体而言就是在 Pod 上加上一堆 Agent Sidecar Injector 的注解，如果配置比较多，也可以使用 configmap 保存，在注解中引用。 需要注意的是 vault-inject-agent 有两种运行模式： init 模式: 仅在 Pod 启动前初始化一次，跑完就退出（Completed） 常驻模式: 容器不退出，持续监控 vault 的配置更新，维持 Pod 配置和 vualt 配置的同步。 示例： yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: my-app name: my-app namespace: default spec: minReadySeconds: 3 progressDeadlineSeconds: 60 revisionHistoryLimit: 3 selector: matchLabels: app: my-app strategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate template: metadata: annotations: vault.hashicorp.com/agent-init-first: \"true\" # 是否使用 initContainer 提前初始化配置文件 vault.hashicorp.com/agent-inject: \"true\" vault.hashicorp.com/secret-volume-path: vault vault.hashicorp.com/role: \"my-app-role\" # vault kubernetes 插件的 role 名称 vault.hashicorp.com/agent-inject-template-config.json: | # 渲染模板的语法在后面介绍 vault.hashicorp.com/agent-limits-cpu: 250m vault.hashicorp.com/agent-requests-cpu: 100m # 包含 vault 配置的 configmap，可以做更精细的控制 # vault.hashicorp.com/agent-configmap: my-app-vault-config labels: app: my-app spec: containers: - image: registry.svc.local/xx/my-app:latest imagePullPolicy: IfNotPresent # 此处省略若干配置... serviceAccountName: my-app-account 常见错误： vault-agent(sidecar) 报错: namespace not authorized auth/kubernetes/config 中的 role 没有绑定 Pod 的 namespace vault-agent(sidecar) 报错: permission denied 检查 vault 实例的日志，应该有对应的错误日志，很可能是 auth/kubernetes/config 没配对，vault 无法验证 kube-apiserver 的 tls 证书，或者使用的 kubernetes token 没有权限。 vault-agent(sidecar) 报错: service account not authorized auth/kubernetes/config 中的 role 没有绑定 Pod 使用的 serviceAccount ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:4:3","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#3-部署-pod"},{"categories":["tech"],"content":" 4. vault agent 配置vault-agent 的配置，需要注意的有： 如果使用 configmap 提供完整的 config.hcl 配置，注意 agent-init vautl-agent 的 template 说明： 目前来说最流行的配置文件格式应该是 json/yaml，以 json 为例，对每个微服务的 kv 数据，可以考虑将它所有的个性化配置都保存在 \u003cengine-name\u003e/\u003cservice-name\u003e/ 下面，然后使用如下 template 注入配置： consul-template { {{ range secrets \"\u003cengine-name\u003e/metadata/\u003cservice-name\u003e/\" }} \"{{ printf \"%s\" . }}\": {{ with secret (printf \"\u003cengine-name\u003e/\u003cservice-name\u003e/%s\" .) }} {{ .Data.data | toJSONPretty }}, {{ end }} {{ end }} } template 的详细语法参见: https://github.com/hashicorp/consul-template#secret 注意：v2 版本的 kv secrets，它的 list 接口有变更，因此在遍历 v2 kv secrets 时，必须要写成 range secrets \"\u003cengine-name\u003e/metadata/\u003cservice-name\u003e/\"，也就是中间要插入metadata，而且 policy 中必须开放 \u003cengine-name\u003e/metadata/\u003cservice-name\u003e/ 的 read/list 权限！官方文档完全没提到这一点，我通过 wireshark 抓包调试，对照官方的KV Secrets Engine - Version 2 (API) 才搞明白这个。 这样生成出来的内容将是 json 格式，不过有个不兼容的地方：最后一个 secrets 的末尾有逗号 , 渲染出的效果示例： json { \"secret-a\": { \"a\": \"b\", \"c\": \"d\" }, \"secret-b\": { \"v\": \"g\", \"r\": \"c\" } } 因为存在尾部逗号(trailing comma)，直接使用 json 标准库解析它会报错。那该如何去解析它呢？我在万能的 stackoverflow 上找到了解决方案：yaml 完全兼容 json 语法，并且支持尾部逗号！ 以 python 为例，直接 yaml.safe_load() 就能完美解析 vault 生成出的 json 内容。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:4:4","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#4-vault-agent-配置"},{"categories":["tech"],"content":" 5. 拓展：在 kubernetes 中使用 vault 的其他姿势除了使用官方提供的 sidecar 模式进行 secrets 注入，社区也提供了一些别的方案，可以参考： hashicorp/vault-csi-provider: 官方的 Beta 项目，通过 Secrets Store CSI 驱动将 vault secrets 以数据卷的形式挂载到 pod 中 kubernetes-external-secrets: 提供 CRD 定义，根据定义将 secret 从 vault 中同步到 kubernetes secrets 官方的 sidecar/init-container 模式仍然是最推荐使用的。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:4:5","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#5-拓展在-kubernetes-中使用-vault-的其他姿势"},{"categories":["tech"],"content":" 五、使用 vault 实现 AWS IAM Credentials 的自动轮转待续。。。 ","date":"2021-01-24","objectID":"/posts/experience-of-vault/:5:0","series":["写给开发人员的实用密码学","云原生相关"],"tags":["Vault","云原生","Secrets","配置","配置管理"],"title":"secrets 管理工具 Vault 的介绍、安装及使用","uri":"/posts/experience-of-vault/#五使用-vault-实现-aws-iam-credentials-的自动轮转"},{"categories":["tech"],"content":" QEMU/KVM 虚拟化 QEMU/KVM 有一定的使用门槛，本文假设你已经拥有基础的虚拟化相关知识，最好是已经有 virtualbox 或 vmware workstation 的使用经验。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:1:0","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#qemukvm-虚拟化"},{"categories":["tech"],"content":" 前言虚拟机（Virtual Machine）是指通过软件模拟的具有完整硬件系统功能的、运行在一个完全隔离环境中的完整计算机系统。它的主要用途有： 测试、尝鲜新的操作系统。 快速创建完全隔离的沙箱环境，用于运行某些不安全的或者敏感的文件/程序。 云服务商或企业会通过服务器虚拟化，提升服务器的利用率。 虚拟机可以创建快照跟备份，系统环境可以随时还原到旧的快照，也能方便地拷贝给他人。 而 QEMU/KVM 则是目前最流行的企业级虚拟化技术，它基于 Linux 内核提供的 KVM 模块，结构精简， 性能损失小，而且开源免费，因此成了大部分企业的首选虚拟化方案。 目前各大云厂商的虚拟化方案，新的服务器实例基本都是用的 KVM 技术。即使是起步最早，一直重度使用 Xen 的 AWS，从 EC2 C5 开始就改用了基于 KVM 定制的 Nitro 虚拟化技术。 但是 KVM 作为一个企业级的底层虚拟化技术，却没有对桌面使用做深入的优化，因此如果想把它当成桌面虚拟化软件来使用，替代掉VirtualBox/VMware Workstation， 有一定难度。 本文是我个人学习 KVM 的一个总结性文档，其目标是使用 KVM 作为桌面虚拟化软件。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:2:0","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#前言"},{"categories":["tech"],"content":" 一、安装 QEMU/KVMQEMU/KVM 环境需要安装很多的组件，它们各司其职： qemu: 模拟各类输入输出设备（网卡、磁盘、USB端口等） qemu 底层使用 kvm 模拟 CPU 和 RAM，比软件模拟的方式快很多。 libvirt: 提供简单且统一的工具和 API，用于管理虚拟机，屏蔽了底层的复杂结构。（支持 qemu-kvm/virtualbox/vmware） ovmf: 为虚拟机启用 UEFI 支持 virt-manager: 用于管理虚拟机的 GUI 界面（可以管理远程 kvm 主机）。 virt-viewer: 通过 GUI 界面直接与虚拟机交互（可以管理远程 kvm 主机）。 dnsmasq vde2 bridge-utils openbsd-netcat: 网络相关组件，提供了以太网虚拟化、网络桥接、NAT网络等虚拟网络功能。 dnsmasq 提供了 NAT 虚拟网络的 DHCP 及 DNS 解析功能。 vde2: 以太网虚拟化 bridge-utils: 顾名思义，提供网络桥接相关的工具。 openbsd-netcat: TCP/IP 的瑞士军刀，详见socat \u0026 netcat，这里不清楚是哪个网络组件会用到它。 安装命令： shell # archlinux/manjaro sudo pacman -S qemu virt-manager virt-viewer dnsmasq vde2 bridge-utils openbsd-netcat # ubuntu,参考了官方文档，但未测试 sudo apt install qemu-kvm libvirt-daemon-system virt-manager virt-viewer virtinst bridge-utils # centos,参考了官方文档，但未测试 sudo yum groupinstall \"Virtualization Host\" sudo yum install virt-manager virt-viewer virt-install # opensuse # see: https://doc.opensuse.org/documentation/leap/virtualization/html/book-virt/cha-vt-installation.html sudo yast2 virtualization # enter to terminal ui, select kvm + kvm tools, and then install it. 安装完成后，还不能直接使用，需要做些额外的工作。请继续往下走。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:0","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#一安装-qemukvm"},{"categories":["tech"],"content":" 1. libguestfs - 虚拟机磁盘映像处理工具libguestfs 是一个虚拟机磁盘映像处理工具，可用于直接修改/查看/虚拟机映像、转换映像格式等。 它提供的命令列表如下： virt-df centos.img: 查看硬盘使用情况 virt-ls centos.img /: 列出目录文件 virt-copy-out -d domain /etc/passwd /tmp：在虚拟映像中执行文件复制 virt-list-filesystems /file/xx.img：查看文件系统信息 virt-list-partitions /file/xx.img：查看分区信息 guestmount -a /file/xx.qcow2(raw/qcow2都支持) -m /dev/VolGroup/lv_root --rw /mnt：直接将分区挂载到宿主机 guestfish: 交互式 shell，可运行上述所有命令。 virt-v2v: 将其他格式的虚拟机(比如 ova) 转换成 kvm 虚拟机。 virt-p2v: 将一台物理机转换成虚拟机。 学习过程中可能会使用到上述命令，提前安装好总不会有错，安装命令如下： shell # opensuse sudo zypper install libguestfs # archlinux/manjaro，目前缺少 virt-v2v/virt-p2v 组件 sudo pacman -S libguestfs # ubuntu sudo apt install libguestfs-tools # centos sudo yum install libguestfs-tools ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:1","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#1-libguestfs---虚拟机磁盘映像处理工具"},{"categories":["tech"],"content":" 2. 启动 QEMU/KVM通过 systemd 启动 libvirtd 后台服务： shell sudo systemctl enable libvirtd.service sudo systemctl start libvirtd.service ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:2","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#2-启动-qemukvm"},{"categories":["tech"],"content":" 3. 让非 root 用户能正常使用 kvmqumu/kvm 装好后，默认情况下需要 root 权限才能正常使用它。为了方便使用，首先编辑文件/etc/libvirt/libvirtd.conf: unix_sock_group = \"libvirt\"，取消这一行的注释，使 libvirt 用户组能使用 unix 套接字。 unix_sock_rw_perms = \"0770\"，取消这一行的注释，使用户能读写 unix 套接字。 然后新建 libvirt 用户组，将当前用户加入该组： shell newgrp libvirt sudo usermod -aG libvirt $USER 最后重启 libvirtd 服务，应该就能正常使用了： shell sudo systemctl restart libvirtd.service ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:3","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#3-让非-root-用户能正常使用-kvm"},{"categories":["tech"],"content":" 3. 启用嵌套虚拟化如果你需要在虚拟机中运行虚拟机（比如在虚拟机里测试 katacontainers 等安全容器技术），那就需要启用内核模块 kvm_intel 或 kvm_amd 实现嵌套虚拟化。 首先通过如下指令验证下是否已经启用了嵌套虚拟化（一般的发行版默认都不会启用）： shell # intel 用这个命令，输出 Y 则表示启用了嵌套虚拟化 cat /sys/module/kvm_intel/parameters/nested # amd 用如下指令，输出 1 则表示启用了嵌套虚拟化 cat /sys/module/kvm_amd/parameters/nested 如果输出不是 Y/1，说明默认未启用嵌套虚拟化，需要手动启用，步骤如下。 如果是 intel cpu，需要使用如下命令启用嵌套虚拟化功能： shell ## 1. 关闭所有虚拟机，并卸载 kvm_intel 内核模块 sudo modprobe -r kvm_intel ## 2. 启用嵌套虚拟化功能 sudo modprobe kvm_intel nested=1 ## 3. 保存配置，使嵌套虚拟化功能在重启后自动启用 cat \u003c\u003cEOF | sudo tee /etc/modprobe.d/kvm.conf options kvm_intel nested=1 EOF 如果是 amd cpu，则应使用如下命令启用嵌套虚拟化功能： shell ## 1. 关闭所有虚拟机，并卸载 kvm_intel 内核模块 sudo modprobe -r kvm_amd ## 2. 启用嵌套虚拟化功能 sudo modprobe kvm_amd nested=1 ## 3. 保存配置，使嵌套虚拟化功能在重启后自动启用 cat \u003c\u003cEOF | sudo tee /etc/modprobe.d/kvm.conf options kvm_amd nested=1 EOF 改完后再利用前面提到的命令验证下是否启用成功。 至此，KVM 的安装就大功告成啦，现在应该可以在系统中找到 virt-manager 的图标，进去就可以使用了。virt-manager 的使用方法和 virtualbox/vmware workstation 大同小异，这里就不详细介绍了， 自己摸索摸索应该就会了。 如下内容是进阶篇，主要介绍如何通过命令行来管理虚拟机磁盘，以及 KVM。如果你还是 kvm 新手，建议先通过图形界面 virt-manager 熟悉熟悉，再往下继续读。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:3:4","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#3-启用嵌套虚拟化"},{"categories":["tech"],"content":" 二、虚拟机磁盘映像管理这需要用到两个工具： libguestfs: 虚拟机磁盘映像管理工具，前面介绍过了 qemu-img: qemu 的磁盘映像管理工具，用于创建磁盘、扩缩容磁盘、生成磁盘快照、查看磁盘信息、转换磁盘格式等等。 shell # 创建磁盘 qemu-img create -f qcow2 -o cluster_size=128K virt_disk.qcow2 20G # 扩容磁盘 qemu-img resize ubuntu-server-cloudimg-amd64.img 30G # 查看磁盘信息 qemu-img info ubuntu-server-cloudimg-amd64.img # 转换磁盘格式 qemu-img convert -f raw -O qcow2 vm01.img vm01.qcow2 # raw =\u003e qcow2 qemu-img convert -f qcow2 -O raw vm01.qcow2 vm01.img # qcow2 =\u003e raw ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:0","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#二虚拟机磁盘映像管理"},{"categories":["tech"],"content":" 1. 导入 vmware 镜像直接从 vmware ova 文件导入 kvm，这种方式转换得到的镜像应该能直接用（网卡需要重新配置）： shell virt-v2v -i ova centos7-test01.ova -o local -os /vmhost/centos7-01 -of qcow2 也可以先从 ova 中解压出 vmdk 磁盘映像，将 vmware 的 vmdk 文件转换成 qcow2 格式，然后再导入 kvm（网卡需要重新配置）： shell # 转换映像格式 qemu-img convert -p -f vmdk -O qcow2 centos7-test01-disk1.vmdk centos7-test01.qcow2 # 查看转换后的映像信息 qemu-img info centos7-test01.qcow2 直接转换 vmdk 文件得到的 qcow2 镜像，启会报错，比如「磁盘无法挂载」。根据Importing Virtual Machines and disk images - ProxmoxVE Docs 文档所言，需要在网上下载安装 MergeIDE.zip 组件，另外启动虚拟机前，需要将硬盘类型改为 IDE， 才能解决这个问题。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:1","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#1-导入-vmware-镜像"},{"categories":["tech"],"content":" 2. 导入 img 镜像img 镜像文件，就是所谓的 raw 格式镜像，也被称为裸镜像，IO 速度比 qcow2 快，但是体积大，而且不支持快照等高级特性。如果不追求 IO 性能的话，建议将它转换成 qcow2 再使用。 shell qemu-img convert -f raw -O qcow2 vm01.img vm01.qcow2 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:4:2","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#2-导入-img-镜像"},{"categories":["tech"],"content":" 三、虚拟机管理虚拟机管理可以使用命令行工具 virsh/virt-install，也可以使用 GUI 工具 virt-manager. GUI 很傻瓜式，就不介绍了，这里主要介绍命令行工具 virsh/virt-install 先介绍下 libvirt 中的几个概念： Domain: 指代运行在虚拟机器上的操作系统的实例 - 一个虚拟机，或者用于启动虚拟机的配置。 Guest OS: 运行在 domain 中的虚拟操作系统。 大部分情况下，你都可以把下面命令中涉及到的 domain 理解成虚拟机。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:5:0","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#三虚拟机管理"},{"categories":["tech"],"content":" 0. 设置默认 URIvirsh/virt-install/virt-viewer 等一系列 libvirt 命令，sudo virsh net-list –all 默认情况下会使用 qemu:///session 作为 URI 去连接 QEMU/KVM，只有 root 账号才会默认使用qemu:///system. 另一方面 virt-manager 这个 GUI 工具，默认也会使用 qemu:///system 去连接 QEMU/KVM（和 root 账号一致） qemu:///system 是系统全局的 qemu 环境，而 qemu:///session 的环境是按用户隔离的。另外qemu:///session 没有默认的 network，创建虚拟机时会出毛病。。。 因此，你需要将默认的 URI 改为 qemu:///system，否则绝对会被坑: shell echo 'export LIBVIRT_DEFAULT_URI=\"qemu:///system\"' \u003e\u003e ~/.bashrc ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:5:1","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#0-设置默认-uri"},{"categories":["tech"],"content":" 1. 虚拟机网络qemu-kvm 安装完成后，qemu:///system 环境中默认会创建一个 default 网络，而qemu:///session 不提供默认的网络，需要手动创建。 我们通常使用 qemu:///system 环境就好，可以使用如下方法查看并启动 default 网络，这样后面创建虚拟机时才有网络可用。 shell # 列出所有虚拟机网络 $ sudo virsh net-list --all Name State Autostart Persistent ---------------------------------------------- default inactive no yes # 启动默认网络 $ virsh net-start default Network default started # 将 default 网络设为自启动 $ virsh net-autostart --network default Network default marked as autostarted # 再次检查网络状况，已经是 active 了 $ sudo virsh net-list --all Name State Autostart Persistent -------------------------------------------- default active yes yes 也可以创建新的虚拟机网络，这需要手动编写网络的 xml 配置，然后通过virsh net-define --file my-network.xml 创建，这里就不详细介绍了，因为暂时用不到… ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:5:2","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#1-虚拟机网络"},{"categories":["tech"],"content":" 2. 创建虚拟机 - virt-install shell # 使用 iso 镜像创建全新的 proxmox 虚拟机，自动创建一个 60G 的磁盘。 virt-install --virt-type kvm \\ --name pve-1 \\ --vcpus 4 --memory 8096 \\ --disk size=60 \\ --network network=default,model=virtio \\ --os-type linux \\ --os-variant generic \\ --graphics vnc \\ --cdrom proxmox-ve_6.3-1.iso # 使用已存在的 opensuse cloud 磁盘创建虚拟机 virt-install --virt-type kvm \\ --name opensuse15-2 \\ --vcpus 2 --memory 2048 \\ --disk opensuse15.2-openstack.qcow2,device=disk,bus=virtio \\ --disk seed.iso,device=cdrom \\ --os-type linux \\ --os-variant opensuse15.2 \\ --network network=default,model=virtio \\ --graphics vnc \\ --import 其中的 --os-variant 用于设定 OS 相关的优化配置，官方文档强烈推荐设定，其可选参数可以通过 osinfo-query os 查看。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:5:3","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#2-创建虚拟机---virt-install"},{"categories":["tech"],"content":" 3. 虚拟机管理 - virsh虚拟机创建好后，可使用 virsh 管理虚拟机。 首先介绍万能的帮助命令： shell virsh help 除了官方的 help 之外，我也总结了下 virsh 的常用命令，如下。 查看虚拟机列表： text # 查看正在运行的虚拟机 virsh list # 查看所有虚拟机，包括 inactive 的虚拟机 virsh list --all 使用 virt-viewer 以 vnc 协议登入虚拟机终端： shell # 使用虚拟机 ID 连接 virt-viewer 8 # 使用虚拟机名称连接，并且等待虚拟机启动 virt-viewer --wait opensuse15 启动、关闭、暂停(休眠)、重启虚拟机： shell virsh start opensuse15 virsh suspend opensuse15 virsh resume opensuse15 virsh reboot opensuse15 # 优雅关机 virsh shutdown opensuse15 # 强制关机 virsh destroy opensuse15 # 启用自动开机 virsh autostart opensuse15 # 禁用自动开机 virsh autostart --disable opensuse15 虚拟机快照管理： shell # 列出一个虚拟机的所有快照 virsh snapshot-list --domain opensuse15 # 给某个虚拟机生成一个新快照 virsh snapshot-create \u003cdomain\u003e # 使用快照将虚拟机还原 virsh snapshot-restore \u003cdomain\u003e \u003csnapshotname\u003e # 删除快照 virsh snapshot-delete \u003cdomain\u003e \u003csnapshotname\u003e 删除虚拟机： shell virsh undefine opensuse15 迁移虚拟机： shell # 使用默认参数进行离线迁移，将已关机的服务器迁移到另一个 qemu 实例 virsh migrate 37 qemu+ssh://tux@jupiter.example.com/system # 还支持在线实时迁移，待续 cpu/内存修改： shell # 改成 4 核 virsh setvcpus opensuse15 4 # 改成 4G virsh setmem opensuse15 4096 虚拟机监控： shell # 待续 修改磁盘、网络及其他设备： shell # 添加新设备 virsh attach-device virsh attach-disk virsh attach-interface # 删除设备 virsh detach-disk virsh detach-device virsh detach-interface ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:5:4","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#3-虚拟机管理---virsh"},{"categories":["tech"],"content":" 四、使用 cloudinit 自动配置虚拟机在本机的 KVM 环境中，也可以使用 cloud-init 来初始化虚拟机。好处是创建虚拟机的时候，就能设置好虚拟机的 hostname/network/user-pass/disk-size 等一系列参数，不需要每次启动后再手动登录到机器中配置。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:6:0","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#四使用-cloudinit-自动配置虚拟机"},{"categories":["tech"],"content":" 下载 cloud image 注意：下面的几种镜像都分别有自己的坑点，仅 Ubuntu/OpenSUSE 测试通过，其他发行版的 Cloud 镜像都有各种毛病… 首先下载 Cloud 版本的系统镜像： Ubuntu Cloud Images (RELEASED): 提供 img 格式的裸镜像（PVE 也支持此格式） 请下载带有 .img 结尾的镜像，其中 kvm.img 结尾的镜像会更精简一点。 OpenSUSE Cloud Images 请下载带有 NoCloud 或者 OpenStack 字样的镜像。 对于其他镜像，可以考虑手动通过 iso 来制作一个 cloudinit 镜像，参考openstack - create ubuntu cloud images from iso 上述镜像和我们普通虚拟机使用的 ISO 镜像的区别，一是镜像格式不同，二是都自带了cloud-init/qemu-guest-agent/cloud-utils-growpart 等 cloud 相关软件。 其中 NoCloud 表示支持 cloudinit NoCloud 数据源——即使用 seed.iso 提供 user-data/meta-data/network-config 配置，PVE 就是使用的这种模式。而 Openstack 镜像通常也都支持 NoCloud 模式，所以一般也是可以使用的。 cloud image 基本都没有默认密码，并且禁用了 SSH 密码登录，必须通过 cloud-init 设置私钥方式进行 ssh 登录。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:6:1","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#下载-cloud-image"},{"categories":["tech"],"content":" 配置 cloudinit 并创建虚拟机这需要用到一个工具：cloud-utils shell # manjaro sudo pacman -S cloud-utils # ubuntu sudo apt install cloud-utils # opensuse，包仓库里找不到 cloud-utils，只能源码安装 git clone https://github.com/canonical/cloud-utils git checkout 0.32 cd cloud-utils \u0026\u0026 sudo make install # 生成 iso 文件还需要 genisoimage，请使用一键安装：https://software.opensuse.org/package/genisoimage cloud-utils 提供 cloud-init 相关的各种实用工具，其中有一个 cloud-localds 命令，可以通过 cloud 配置生成一个非 cloud 的 bootable 磁盘映像，供本地的虚拟机使用。 首先编写 user-data: yaml #cloud-config hostname: opensuse15-2 fqdn: opensuse15-2.local # 让 cloud-init 自动更新 /etc/hosts 中 localhost 相关的内容 manage_etc_hosts: localhost package_upgrade: true disable_root: false # 设置 root 的 ssh 密钥 user: root # 设置密码，仅用于控制台登录 password: xxxxx # 使用密钥登录 ssh_authorized_keys: - \"\u003cssh-key content\u003e\" chpasswd: # expire 使密码用完即失效，用户每次登录都需要设置并使用密码！ expire: false # ssh 允许密码登录（不建议开启） ssh_pwauth: false 注意 user-data 的第一行的 #cloud-config 绝对不能省略！它标识配置格式为text/cloud-config！ 再编写 network-config(其格式和 ubuntu 的 netplan 基本完全一致，但是我只测通了 v1 版本，v2 版没测通): yaml version: 1 config: - type: physical name: eth0 subnets: - type: static address: 192.168.122.160 netmask: 255.255.255.0 gateway: 192.168.122.1 - type: nameserver interface: eth0 address: - 114.114.114.114 - 8.8.8.8 # search: # search domain # - xxx shell cloud-localds seed.iso user-data --network-config network-config 每次都手动生成 seed.iso 太麻烦了，实际使用，建议用后面介绍的自动化功能 proxmox-libvirt 或者 terraform-libvirt-provider~ 这样就生成出了一个 seed.iso，创建虚拟机时同时需要载入 seed.iso 和 cloud image，cloud-image 自身为启动盘，这样就大功告成了。示例命令如下： shell virt-install --virt-type kvm \\ --name k8s-master-0 \\ --vcpus 2 --memory 3072 \\ --disk k8s-master-0.qcow2,device=disk,bus=virtio \\ --disk ../vm-seeds/160-seed-k8s-master-0.iso,device=cdrom \\ --os-type linux \\ --os-variant opensuse15.3 \\ --network network=default,model=virtio \\ --graphics vnc \\ --import 也可以使用 virt-viewer 的 GUI 界面进行操作。 这样设置完成后，cloud 虚拟机应该就可以启动了，可以检查下 hostname、网络、root 的密码和私钥、ssh 配置是否均正常。 一切正常后，还有个问题需要解决——初始磁盘应该很小。可以直接手动扩容 img 的大小，cloud-init 在虚拟机启动时就会自动扩容分区： shell qemu-img resize ubuntu-server-cloudimg-amd64.img 30G ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:6:2","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#配置-cloudinit-并创建虚拟机"},{"categories":["tech"],"content":" cloud image 的坑 1. ubuntu cloud image 的坑 ubuntu 启动时会报错 no such device: root，但是过一会就会正常启动。 这是 ubuntu cloud image 的 bug: https://bugs.launchpad.net/cloud-images/+bug/1726476 ubuntu 启动后很快就会进入登录界面，但是 root 密码可能还没改好，登录会报密码错误，等待一会再尝试登录就 OK 了 ubuntu 的默认网卡名称是 ens3，不是 eth0，注意修改 network_config 的网卡名称，否则网络配置不会生效 2. opensuse cloud image 的坑 opensuse leap 15 只支持 network_config v1，对 v2 的支持有 bug，gateway4 不会生效 3. debian cloud image 的坑debian 的 cloud 镜像根本没法用，建议避免使用它。 debian 启动时会彻底卡住，或者直接报错 kernel panic 原因是添加了 spice 图形卡，换成 vnc 就正常了 Debian Cloud Images 中的 nocloud 镜像不会在启动时运行 cloudinit，cloudinit 完全不生效 不知道是啥坑，没解决 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:6:3","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#cloud-image-的坑"},{"categories":["tech"],"content":" cloud image 的坑 1. ubuntu cloud image 的坑 ubuntu 启动时会报错 no such device: root，但是过一会就会正常启动。 这是 ubuntu cloud image 的 bug: https://bugs.launchpad.net/cloud-images/+bug/1726476 ubuntu 启动后很快就会进入登录界面，但是 root 密码可能还没改好，登录会报密码错误，等待一会再尝试登录就 OK 了 ubuntu 的默认网卡名称是 ens3，不是 eth0，注意修改 network_config 的网卡名称，否则网络配置不会生效 2. opensuse cloud image 的坑 opensuse leap 15 只支持 network_config v1，对 v2 的支持有 bug，gateway4 不会生效 3. debian cloud image 的坑debian 的 cloud 镜像根本没法用，建议避免使用它。 debian 启动时会彻底卡住，或者直接报错 kernel panic 原因是添加了 spice 图形卡，换成 vnc 就正常了 Debian Cloud Images 中的 nocloud 镜像不会在启动时运行 cloudinit，cloudinit 完全不生效 不知道是啥坑，没解决 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:6:3","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#1-ubuntu-cloud-image-的坑"},{"categories":["tech"],"content":" cloud image 的坑 1. ubuntu cloud image 的坑 ubuntu 启动时会报错 no such device: root，但是过一会就会正常启动。 这是 ubuntu cloud image 的 bug: https://bugs.launchpad.net/cloud-images/+bug/1726476 ubuntu 启动后很快就会进入登录界面，但是 root 密码可能还没改好，登录会报密码错误，等待一会再尝试登录就 OK 了 ubuntu 的默认网卡名称是 ens3，不是 eth0，注意修改 network_config 的网卡名称，否则网络配置不会生效 2. opensuse cloud image 的坑 opensuse leap 15 只支持 network_config v1，对 v2 的支持有 bug，gateway4 不会生效 3. debian cloud image 的坑debian 的 cloud 镜像根本没法用，建议避免使用它。 debian 启动时会彻底卡住，或者直接报错 kernel panic 原因是添加了 spice 图形卡，换成 vnc 就正常了 Debian Cloud Images 中的 nocloud 镜像不会在启动时运行 cloudinit，cloudinit 完全不生效 不知道是啥坑，没解决 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:6:3","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#2-opensuse-cloud-image-的坑"},{"categories":["tech"],"content":" cloud image 的坑 1. ubuntu cloud image 的坑 ubuntu 启动时会报错 no such device: root，但是过一会就会正常启动。 这是 ubuntu cloud image 的 bug: https://bugs.launchpad.net/cloud-images/+bug/1726476 ubuntu 启动后很快就会进入登录界面，但是 root 密码可能还没改好，登录会报密码错误，等待一会再尝试登录就 OK 了 ubuntu 的默认网卡名称是 ens3，不是 eth0，注意修改 network_config 的网卡名称，否则网络配置不会生效 2. opensuse cloud image 的坑 opensuse leap 15 只支持 network_config v1，对 v2 的支持有 bug，gateway4 不会生效 3. debian cloud image 的坑debian 的 cloud 镜像根本没法用，建议避免使用它。 debian 启动时会彻底卡住，或者直接报错 kernel panic 原因是添加了 spice 图形卡，换成 vnc 就正常了 Debian Cloud Images 中的 nocloud 镜像不会在启动时运行 cloudinit，cloudinit 完全不生效 不知道是啥坑，没解决 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:6:3","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#3-debian-cloud-image-的坑"},{"categories":["tech"],"content":" 画外：cloudinit 主机名称cloudinit 有三个参数与 hostname 相关。其中有两个，就是上面提到的 user-data 中的： hostname: 主机名称 fqdn: 主机的完全限定域名，优先级比 hostname 更高 这两个参数的行为均受 preserve_hostname: true/false 这个参数的影响。 另一个是 meta-data 中，可以设置一个 local-hostname，此参数的地位好像和 user-data 中的 hostname 相同，不过可能优先级会高一些吧。没有找到相关文档。 ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:6:4","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#画外cloudinit-主机名称"},{"categories":["tech"],"content":" 自动化可以使用 pulumi/terraform 自动化创建与管理 QEMU/KVM 虚拟机，相当方便： terraform-provider-libvirt pulumi-libvirt#examples ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:6:5","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#自动化"},{"categories":["tech"],"content":" 参考 Virtualization Guide - OpenSUSE Complete Installation of KVM, QEMU and Virt Manager on Arch Linux and Manjaro virtualization-libvirt - ubuntu docs RedHat Docs - KVM 在 QEMU 使用 Ubuntu Cloud Images ","date":"2021-01-17","objectID":"/posts/qemu-kvm-usage/:7:0","series":null,"tags":["虚拟化","Visualization","KVM","QEMU","libvirt"],"title":"QEMU/KVM 虚拟化环境的搭建与使用","uri":"/posts/qemu-kvm-usage/#参考"},{"categories":null,"content":" 本站的文章可能以中文或英文两种语言书写，其中部分文章可能是双语，也有部分文章只有中文或者英文版本，请读者按需阅读。 The articles on this site may be written in Chinese orEnglish, some of them may be bilingual, and some of them are only available in Chinese orEnglish. Please read them as needed. ","date":"2021-01-16","objectID":"/about/:0:0","series":null,"tags":null,"title":"关于","uri":"/about/#"},{"categories":null,"content":" 关于我 昵称：中文昵称「於清樂」「二花」，英文 ID「ryan4yin」「ryan_yin」 性别：他/He/Him 工作与学习经历 2012-6 ~ 2015-6：在（湖南邵阳）武冈一中读高中 2015-6 ~ 2019-6：在安徽建筑大学读声学专业，没错就是初中物理课上敲音叉的那个声学 （不是音乐…）。本人专业知识战五渣，学位证都没拿到 emmm 2019-6 ~ 2021-2：在某不知名创业公司做全能运维（全干工程师），上至搭建阿里云生产环境、搞 Kubernetes 集群、Istio 服务网格、CICD、性能测试，下至搞洋垃圾戴尔服务器与 vSphere、装水管、修门禁、扫地拖地、当苦力搬运货物，反正没人干的就运维干呗 2021-3 ~ 2025-1：在 Mobiuspace 担任 SRE 工程师，目前主要负责 AIGC 基础设施以及线上网关的维护与优化，也曾负责过线上服务 K8s 集群的维护优化、云成本分析管控等工作。 音乐： 喜欢听后摇、蓝草、民谣、器乐 最喜欢的歌手是虚拟歌手洛天依 有在断断续续地学习竹笛跟口琴（复音、蓝调都有在学），另外也有在学习使用Synthesizer V/ACE 跟 Reaper 运动：喜欢轮滑以及游泳，哦还有 VR 游戏《Beat Saber》《Pistol Whip》，但是目前都是半吊子哈哈~ 茶：2021 年在朋友家喝过一次青钱柳后就一直念念不忘，年底就入坑了凤牌滇红、天之红祁门红茶、极白安吉白茶，目前比较喜欢喝红茶 书籍：读得最多的正经书是 IT 技术书籍，另外也喜欢看科幻，以及戒不掉的网文/轻小说 影视：看得最多的是动漫，另外就是欧美科幻片、温情片 中文输入方案：小鹤音形 自然语言 English: Keep learning. Good at reading technical articles, but weak in writing, listening and speaking 中文：母语，高中语文中上水准。希望能学会用中文写小说，就先从短篇开始吧 编程语言 Go/Python: 目前的主力，也是我最熟悉的语言 C: 入门水平，正在使用 C 语言学习 Linux 系统编程 Rust: 学习中，大量函数式的语法糖，贴心的编译器提示，感觉很好用（当然也比较磨人…） 感兴趣的技术： Linux: 本人 Linux 爱好者一名，也靠它吃饭。最喜欢并且投入了最多精力的发行版是 NixOS Neovim: 本人的主力编辑器，也是我最喜欢的编辑器。它能带给我类似形码输入法的丝滑输入体验，让我爱不释手 Window Manager: 窗口管理器，目前我 i3/hyprland 都有在用。与 Neovim 类似，它们的好处是可以让我不用离开键盘就能完成大部分操作，体验很丝滑 Embedded: 即嵌入式系统，感觉硬件是个很好玩的领域，业余折腾了许多，也花了不少钱在这上面 Kubernetes: SRE 的饭碗之一，云原生的基石 Istio: 最成熟的服务网格产品，也是饭碗之一 联系方式 邮件：bXkgZW1haWw6IHJ5YW40eWluQGxpbnV4LmNvbQ== 豆瓣：本人是豆瓣书影音标记的重度使用者，从 2015 年以来一直使用豆瓣标记读过的书看过的电影。 Twitter: 自 2021 年起在 Twitter 上发布一些生活动态与技术内容，同时也很喜欢看 Twitter 上各位画手大触的作品。是我日常消遣与接触碎片化信息的重要渠道之一。 Bilibili: 日常用 Bilibili 消遣，日常喜欢看 MMD 舞蹈、虚拟歌姬、硬核技术相关内容。 Github Issues: 也可以考虑直接在此仓库的 Issue 区联系我。 ","date":"2021-01-16","objectID":"/about/:1:0","series":null,"tags":null,"title":"关于","uri":"/about/#关于我"},{"categories":null,"content":" 关于此博客 “对我来说，博客首先是一种知识管理工具，其次才是传播工具。我的技术文章，主要用来整理我还不懂的知识。我只写那些我还没有完全掌握的东西，那些我精通的东西，往往没有动力写。炫耀从来不是我的动机，好奇才是。\" ──阮一峰 我从 2016 年开始写博客，已经有很多年了，也试用过国内外很多的写作平台，不过最终还是选择了搭建一个自己的站点，因为它完全受我自己管控，自由度最高，文章不会无故被删除或封锁。其次是很多免费的静态站点服务可用（如 GitHub Pages、Vercel 等），基本不花钱。 不论是生活博客还是技术博客，每位写博客的博主，都有着自己的目的，我写博客的目的变化过很多次。最开始我只是想尝试下新鲜事物；后来变成想获得更多的阅读量跟评论，因为这能给我强烈的成就感；再后来我也试过把博客当成技术笔记本跟日记本来用，写了很多琐碎的东西；接着呢就到了现在，现在我写博客的首要目的是通过它梳理与构建我的知识体系，我的博客跟我的个人笔记ryan4yin/knowledge 就是我的知识宝库，我在不断往里面补充新的内容的同时，也越来越多地依赖它完成自己的日常工作甚至业余玩乐；而次要目标则是将自己学到的知识分享给有需要的人，助人为乐嘛；最后一个目的，是记录我自己，记录我的喜怒哀乐，记录我的成长。 我的域名都买了最长的十年，而且隔几年就续满，我希望这个博客能陪伴我很久很久。下一个十年，再回头看看我的博客，看到我这十年走过的路，我想那个时候一定会很有趣！ ","date":"2021-01-16","objectID":"/about/:2:0","series":null,"tags":null,"title":"关于","uri":"/about/#关于此博客"},{"categories":null,"content":" 我曾用过的箴言这些箴言曾经陪伴我与这个博客度过了许多春夏秋冬，它们都曾在某个时期给过我力量，每每看到都令人怀念。在这里也将它们送给各位读者，希望它们也能给你以力量！ 2021-02-06 ~ 2022-01-03 拆破玉笼飞彩凤，顿开金锁走蛟龙。 —— 来自好友 @二牛 的赠语 2021-01-16 - 2022-04-04 双手合十 闭上眼睛 心里什么也不去想嘴角就高高扬起 笑出声来 赞美快乐~ 2022-04-04 ~ 2022-08-19 我错过花，却看见海。 2021-01-18 ~ 2022-08-19 有很多的绝望，但也有美的时刻，只不过在美的时刻，时间是不同于以往的。 ──《刺猬的优雅》 2022-08-19 ~ now 脚下虽有万水千山，但行者必至。 ——《在峡江的转弯处 - 陈行甲人生笔记》 ","date":"2021-01-16","objectID":"/about/:2:1","series":null,"tags":null,"title":"关于","uri":"/about/#我曾用过的箴言"},{"categories":null,"content":" 博客时间线 博客时间线 2016-06-17：（大一下学期）在博客园创建博客 https://www.cnblogs.com/kirito-c/ 第一篇博文是贪吃蛇—C—基于easyx图形库，现在还能回忆起收到第一条评论时的兴奋之情。 2021-01-16：（工作一年多后）申请域名并开设独立博客 https://ryan4yin.space/ 2021-01-28：站点从 Cloudflare + GitHub Pages 迁移到 Vercel，国内访问速度有一定提升。 2021-02-07：（第一份工作结束后赋闲）将博客主域名切换为 https://thiscute.world/，另外新增备用域名 https://writefor.fun 2022-02-07：站点添加「阅读排行」页，展示从 Google Analytics 拉取的站点统计数据。 2022-02-16：站点通过十年之约审核，正式加入十年之约 博客快照-2020-01-21 博客园快照 博客快照-2022-07-31 ThisCute.World 快照 ","date":"2021-01-16","objectID":"/about/:2:2","series":null,"tags":null,"title":"关于","uri":"/about/#博客时间线"},{"categories":null,"content":" 注意事项本站所有技术内容均为个人观点，不保证正确，另外随着时间变化部分技术内容也可能会失效，请读者自行甄别。 另外本站使用的许多配图都来源于网络，如有侵权，请联系我删除。 ","date":"2021-01-16","objectID":"/about/:3:0","series":null,"tags":null,"title":"关于","uri":"/about/#注意事项"},{"categories":null,"content":" 画外互联网浩如烟海，这个小站偏安一隅，如果它有幸被你发现，而且其中文字对你还有些帮助，那可真是太棒了！感谢有你~ 如果你觉得这个小站还不错，欢迎将它分享给你的朋友，或者在文章底下的评论区留下只言片语，让我知道你的想法，这是对我最大的鼓励！ ","date":"2021-01-16","objectID":"/about/:4:0","series":null,"tags":null,"title":"关于","uri":"/about/#画外"},{"categories":["tech"],"content":"Pulumi 是一个基础设施的自动管理工具，使用 Python/TypeScript/Go/Dotnet 编写好声明式的资源配置，就能实现一键创建/修改/销毁各类资源，这里的资源可以是： AWS/阿里云等云上的负载均衡、云服务器、TLS 证书、DNS、CDN、OSS、数据库…几乎所有的云上资源 本地自建的 vSphere/Kubernetes/ProxmoxVE/libvirt 环境中的虚拟机、容器等资源 相比直接调用 AWS/阿里云/Kubernetes 的 API，使用 pulumi 的好处有： 声明式配置：你只需要声明你的资源属性就 OK，所有的状态管理、异常处理都由 pulumi 完成。 统一的配置方式：提供统一的配置方法，来声明式的配置所有 AWS/阿里云/Kubernetes 资源。 声明式配置的可读性更好，更便于维护 试想一下，通过传统的手段去从零搭建一个云上测试环境、或者本地开发环境，需要手工做多少繁琐的工作。 而依靠 Pulumi 这类「基础设施即代码（Infrastructure as Code, IaC）」的工具，只需要一行命令就能搭建好一个可复现的云上测试环境或本地开发环境。 比如我们的阿里云测试环境，包括两个 kubernetes 集群、负载均衡、VPC 网络、数据库、云监控告警 /日志告警、RAM账号权限体系等等，是一个比较复杂的体系。 人工去配置这么多东西，想要复现是很困难的，非常繁琐而且容易出错。 但是使用 pulumi，只需要一行命令，就能创建并配置好这五花八门一大堆的玩意儿。销毁整个测试环境也只需要一行命令。 实际使用体验：我们使用 Pulumi 自动化了阿里云测试环境搭建 95%+ 的操作，这个比例随着阿里云的 pulumi provider 的完善，还可以进一步提高！ ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:0:0","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#"},{"categories":["tech"],"content":" Pulumi vs Terraform vs CloudFormation先介绍下 CloudFormation，它是 AWS 提供的一个 IaC 工具， 它使用 json/yaml 编写声明式配置文件，然后完全在 AWS 云上进行资源的创建、管理、销毁。其所创建的资源跟 CloudFormation Task 同生命周期，因此删除该 CloudFormation Task 就会自动销毁所有相关资源。因此它的好处应该是可以完全在云上运行，本地客户端只是一个提交配置的工具。而缺点则是只能在 AWS 上使用。 而在通用的「基础设施即代码」领域，有一个工具比 Pulumi 更流行，它就是Terraform. 实际上我们一开始使用的也是 Terraform，但是后来使用 Pulumi 完全重写了一遍。 主要原因是，Pulumi 解决了 Terraform 配置的一个痛点：配置语法太过简单，导致配置繁琐。而且还要额外学习一门 DSL - HCL Terraform 虽然应用广泛，但是它默认使用的 HCL 语言太简单，表现力不够强。这就导致在一些场景下使用 Terraform，会出现大量的重复配置。 一个典型的场景是「批量创建资源，动态生成资源参数」。比如批量创建一批名称类似的 ECS 服务器 /VPC交换机。如果使用 terraform，就会出现大量的重复配置。 改用 terraform 提供的 module 能在一定程度上实现配置的复用，但是它还是解决不了问题。要使用 module，你需要付出时间去学习 module 的概念，为了拼接参数，你还需要学习 HCL 的一些高级用法。 但是付出了这么多，最后写出的 module 还是不够灵活——它被 HCL 局限住了。 为了实现如此的参数化动态化，我们不得不引入 Python 等其他编程语言。于是构建流程就变成了： 借助 Python 等其他语言先生成出 HCL 配置 通过 terraform 命令行进行 plan 与 apply 通过 Python 代码解析 terraform.tfstat，获取 apply 结果，再进行进一步操作。 这显然非常繁琐，主要困难就在于 Python 和 Terraform 之间的交互。 进一步思考，既然其他编程语言如 Python/Go 的引入不可避免，那是不是能使用它们彻底替代掉 HCL 呢？能不能直接使用 Python/Go 编写配置？如果 Terraform 原生就支持 Python/Go 来编写配置，那就不存在交互问题了。 相比于使用领域特定语言 HCL，使用通用编程语言编写配置，好处有： Python/Go/TypeScript 等通用的编程语言，也支持 Yaml 这样方便自动化生成的配置语言，能满足你的一切需求。 作为一个开发人员/DevOps，你应该对 Python/Go 等语言相当熟悉，可以直接利用上已有的经验。 更方便测试：可以使用各编程语言中流行的测试框架来测试 pulumi 配置！ 于是 Pulumi 横空出世。 另一个和 Pulumi 功能类似的工具，是刚出炉没多久的terraform-cdk，但是目前它还很不成熟。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:1:0","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#pulumi-vs-terraform-vs-cloudformation"},{"categories":["tech"],"content":" Pulumi 特点介绍 原生支持通过 Python/Go/TypeScript/Dotnet 等语言编写配置，也就完全解决了上述的 terraform 和 python 的交互问题。 pulumi 是目前最流行的 真-IaaS 工具，对各语言的支持都很成熟。 兼容 terraform 的所有 provider，只是需要自行使用pulumi-tf-provider-boilerplate 重新打包，有些麻烦。 pulumi 官方的 provider 几乎全都是封装的 terraform provider，包括 aws/azure/alicloud，目前只发现 kubernetes 是原生的（独苗啊）。 状态管理和 secrets 管理有如下几种选择： 使用 app.pulumi.com（默认）:免费版提供 stack 历史管理，可以看到所有的历史记录。另外还提供一个资源关系的可视化面板。总之很方便，但是多人合作就需要收费。 本地文件存储：pulumi login file:///app/data 云端对象存储， 支持 s3 等对象存储协议，因此可以使用 AWS 或者本地的 MinIO 来做 Backend. pulumi login 's3://\u003cbucket-path\u003e?endpoint=my.minio.local:8080\u0026disableSSL=true\u0026s3ForcePathStyle=true' minio/aws 的 credential 可以通过 AWS_ACCESS_KEY_ID 和 AWS_SECRET_ACCESS_KEY 两个环境变量设置。另外即使是使用 MinIO，AWS_REGION 这个没啥用的环境变量也必须设置！否则会报错。 gitlab 13 支持 Terraform HTTP State 协议， 等这个 pr 合并，pulumi 也能以 gitlab 为 backend 了。 使用 pulumi 企业版（自建服务）：比 app.pulumi.com 提供更多的特性，但是显然是收费的。。 总之，非常香，强烈推荐各位 DevOps 试用。 以下内容是我对 pulumi 的一些思考，以及使用 pulumi 遇到的各种问题+解决方法，适合对 pulumi 有一定了解的同学阅读。 如果你刚接触 Pulumi 而且有兴趣学习，建议先移步pulumi get started 入个门，再接着看下面的内容。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:2:0","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#pulumi-特点介绍"},{"categories":["tech"],"content":" 使用建议 建议查看对应的 terraform provider 文档：pulumi 的 provider 基本都是封装的 terraform 版本，而且文档是自动生成的，比（简）较（直）难（一）看（坨）懂（shi），examples 也少。 stack: pulumi 官方提供了两种 stack 用法：「单体」和「微-stack」 单体: one stack rule them all，通过 stack 参数来控制步骤。stack 用来区分环境 dev/pro 等。 微-stack: 每一个 stack 是一个步骤，所有 stack 组成一个完整的项目。 实际使用中，我发现「微-stack」模式需要使用到 pulumi 的 inter-stack dependencies，报一堆的错，而且不够灵活。因此目前更推荐「单体」模式。 我们最近使用 pulumi 完全重写了以前用 terraform 编写的云上配置，简化了很多繁琐的配置，也降低了我们 Python 运维代码和 terraform 之间的交互难度。另外我们还充分利用上了 Python 的类型检查和语法检查，很多错误 IDE 都能直接给出提示，强化了配置的一致性和可维护性。 不过由于阿里云 provider 暂时还： 不支持管理 ASM 服务网格、DTS 数据传输等资源 OSS 等产品的部分参数也暂时不支持配置（比如 OSS 不支持配置图片样式、ElasticSearch 暂时不支持自动创建 7.x 版本） 不支持创建 ElasticSearch 7.x 这些问题，导致我们仍然有部分配置需要手动处理，另外一些耗时长的资源，需要单独去创建。因此还不能实现完全的「一键」。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:3:0","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#使用建议"},{"categories":["tech"],"content":" 常见问题","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:0","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#常见问题"},{"categories":["tech"],"content":" 1. Output 的用法 pulumi 通过资源之间的属性引用（Output[str]）来确定依赖关系，如果你通过自定义的属性 (str)解耦了资源依赖，会导致资源创建顺序错误而创建失败。 Output[str] 是一个异步属性，类似 Future，不能被用在 pulumi 参数之外的地方！ Output[str] 提供两种方法能直接对 Output[str] 进行一些操作： Output.concat(\"http://\", domain, \"/\", path): 此方法将 str 与 Output[str] 拼接起来，返回一个新的 Output[str] 对象，可用做 pulumi 属性。 domain.apply(lambda it: print(it)): Output[str] 的 apply 方法接收一个函数。在异步获取到数据后，pulumi 会调用这个函数，把具体的数据作为参数传入。 另外 apply 也会将传入函数的返回值包装成 Output 类型返回出来。 可用于：在获取到数据后，将数据打印出来/发送到邮箱/调用某个 API 上传数据等等。 Output.all(output1, output2, ...).apply(lambda it: print(it)) 可用于将多个output 值，拼接成一个 Output 类型，其内部的 raw 值为一个 tuple 对象(str1, str2, ...). 官方举例：connection_string = Output.all(sql_server.name, database.name).apply(lambda args: f\"Server=tcp:{args[0]}.database.windows.net;initial catalog={args[1]}...\") ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:1","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#1-output-的用法"},{"categories":["tech"],"content":" 2. 如何使用多个云账号/多个 k8s 集群？默认情况下 pulumi 使用默认的 provider，但是 pulumi 所有的资源都有一个额外的 opts 参数， 可用于设定其他 provider。 通过这个 opts，我们可以实现在一个 pulumi 项目中，使用多个云账号，或者管理多个 k8s 集群。 示例： python from pulumi import get_stack, ResourceOptions, StackReference from pulumi_alicloud import Provider, oss # 自定义 provider，key/secret 通过参数设定，而不是从默认的环境变量读取。 # 可以自定义很多个 providers provider = pulumi_alicloud.Provider( \"custom-alicloud-provider\", region=\"cn-hangzhou\", access_key=\"xxx\", secret_key=\"jjj\", ) # 通过 opts，让 pulumi 使用自定义的 provider（替换掉默认的） bucket = oss.Bucket(..., opts=ResourceOptions(provider=provider)) ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:2","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#2-如何使用多个云账号多个-k8s-集群"},{"categories":["tech"],"content":" 3. inter-stack 属性传递 这东西还没搞透，待研究。 多个 stack 之间要互相传递参数，需要通过 pulumi.export 导出属性，通过 stack.require_xxx 获取属性。 从另一个 stack 读取属性的示例： python from pulumi import StackReference cfg = pulumi.Config() stack_name = pulumi.get_stack() # stack 名称 project = pulumi.get_project() infra = StackReference(f\"ryan4yin/{project}/{stack_name}\") # 这个属性在上一个 stack 中被 export 出来 vpc_id = infra.require(\"resources.vpc.id\") ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:3","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#3-inter-stack-属性传递"},{"categories":["tech"],"content":" 4. pulumi up 被中断，或者对资源做了手动修改，会发生什么？ 强行中断 pulumi up，会导致资源进入 pending 状态，必须手动修复。 修复方法：pulumi stack export，删除 pending 资源，再 pulumi stack import 手动删除了云上资源，或者修改了一些对资源管理无影响的参数，对 pulumi 没有影响，它能正确检测到这种情况。 可以通过 pulumi refresh 手动从云上拉取最新的资源状态。 手动更改了资源之间的依赖关系（比如绑定 EIP 之类的），很可能导致 pulumi 无法正确管理资源之间的依赖。 这种情况必须先手动还原依赖关系（或者把相关资源全部手动删除掉），然后才能继续使用 pulumi。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:4","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#4-pulumi-up-被中断或者对资源做了手动修改会发生什么"},{"categories":["tech"],"content":" 5. 如何手动声明资源间的依赖关系？有时候因为一些问题（比如 pulumi provider 功能缺失，使用了 restful api 实现部分功能），pulumi 可能无法识别到某些资源之间的依赖关系。 这时可以为资源添加 dependsOn 属性，这个属性能显式地声明依赖关系。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:5","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#5-如何手动声明资源间的依赖关系"},{"categories":["tech"],"content":" 6. 如何导入已经存在的资源？如果你司不是一开始就使用了 pulumi 这类工具，那通常绝大部分云上资源都是手动管理、或者由其他工具自动化管理的，该如何将它们纳入 pulumi 管辖呢？ 官方有提供一篇相关文档Importing Infrastructure. 文档有提到两种资源导入的方法，导入成功后都会自动生成资源的状态，以及对应的 pulumi 代码。第一种是使用 pulumi import 命令，第二种是在代码中使用 import 参数。 除此之外，社区还有几个其他资源导入工具（reverse IaC）值得研究： former2: 为已有的 AWS 资源生成 terraform/pulumi/cloudformation 等配置，但是不支持生成 tfstate 状态 terraformer: 为已有的 AWS/GCP/Azure/Alicloud/DigitalOcean 等多种云资源生成 terraform 配置以及 tfstate 状态 terracognita: 功能跟 terraformer 一样，都支持生成 terraform 配置以及 tfstate 状态，但是它支持 AWS/GCP/Azure 三朵云 pulumi-terraform: 这个 provider 使你可以在 pulumi 项目里使用 tfstate 状态文件 tf2pulumi: 将 terraform 配置转换为 pulumi typescript 配置 6.1 通过 pulumi import 命令导入资源使用 pulumi import 命令导入资源的好处是，不需要为每个资源手写代码，此命令会自动生成资源的 stack state 与配置代码。 使用此命令导入的资源，默认会启用删除保护，你可通过参数 --protect=false 来关闭删除保护。 资源名称可通过命令行参数，或者 Json 文件来指定。 下面我们演示一个导入一个 s3 bucket 的流程： shell # 导入一个名为 test-sre 的 s3 bucket，资源 ID 为 p-test-sre $ pulumi import aws:s3/bucket:Bucket p-test-sre test-sre ...... Do you want to perform this import? yes Importing (dev): Type Name Status + pulumi:pulumi:Stack pulumi-test-dev created = └─ aws:s3:Bucket p-test-sre imported Resources: + 1 created = 1 imported 2 changes Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws p_test_sre = aws.s3.Bucket(\"p-test-sre\", arn=\"arn:aws:s3:::test-sre\", bucket=\"test-sre\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"test-sre\", \"Team\": \"Platform\", }, opts=pulumi.ResourceOptions(protect=True)) 能看到它会自动导入对应资源的 state，并同时打印出对应的 python 代码，要求我们手动将代码复制粘贴到项目中。而且代码会自带 arn/hosted_zone_id/protect 等属性，说明这个资源实际上是无法像普通 pulumi 资源一样，通过 pulumi up/pulumi destroy 自动创建销毁的。要通过 pulumi 删除该资源，需要首先解除删除保护，然后将对应的代码片段删除掉，最后执行 pulumi up。 也可通过 json 来批量导入资源，首先编写一个 json 资源清单： text { \"resources\": [{ \"type\": \"aws:s3/bucket:Bucket\", \"name\": \"s3-bucket_xxx-debug\", \"id\": \"xxx-debug\" }, { \"type\": \"aws:s3/accessPoint:AccessPoint\", \"name\": \"s3-accesspoint_xxx-debug\", \"id\": \"112233445566:xxx-debug\" } ] } 然后执行如下命令批量导入资源： shell $ pulumi import -f test-resources.json ...... Do you want to perform this import? yes Importing (dev): Type Name Status pulumi:pulumi:Stack pulumi-test-dev = ├─ aws:s3:AccessPoint s3-accesspoint_xxx-debug imported = └─ aws:s3:Bucket s3-bucket_xxx-debug imported Resources: = 2 imported 2 unchanged Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws s3_bucket_snappea_dl_debug = aws.s3.Bucket(\"s3-bucket_xxx-debug\", arn=\"arn:aws:s3:::xxx-debug\", bucket=\"xxx-debug\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"xxx-debug\", \"Team\": \"Xxx\", }, opts=pulumi.ResourceOptions(protect=True)) s3_accesspoint_snappea_dl_debug = aws.s3.AccessPoint(\"s3-accesspoint_xxx-debug\", account_id=\"112233445566\", bucket=\"xxx-debug\", name=\"xxx-debug\", public_access_block_configuration=aws.s3.AccessPointPublicAccessBlockConfigurationArgs( block_public_acls=False, block_public_policy=False, ignore_public_acls=False, restrict_public_buckets=False, ), opts=pulumi.ResourceOptions(protect=True)) 能看到同样的生成出了两个资源的 stack 状态，以及对应的代码。 6.2 通过代码导入资源通过代码导入资源，需要你手工为每个资源编写代码，并且确保代码的所有参数与资源本身的状态完全一致。 因此可以看到这种导入方式很不灵活，通常不推荐使用，pulumi import 自动生成代码它不香么 emmmm 大概的流程如下，首先编写一个资源的配置代码，并将其标注为 import: python p_test_sre = aws.s3.Bucket(\"p-test-sre\", bucket=\"test-sre\", tags={ \"Name\": \"test-sre\", \"Team\": \"xxx\", # 这里我故意写错了，pulumi 会检测到这里有问题，提示导入将失败 }, opts","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:6","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#6-如何导入已经存在的资源"},{"categories":["tech"],"content":" 6. 如何导入已经存在的资源？如果你司不是一开始就使用了 pulumi 这类工具，那通常绝大部分云上资源都是手动管理、或者由其他工具自动化管理的，该如何将它们纳入 pulumi 管辖呢？ 官方有提供一篇相关文档Importing Infrastructure. 文档有提到两种资源导入的方法，导入成功后都会自动生成资源的状态，以及对应的 pulumi 代码。第一种是使用 pulumi import 命令，第二种是在代码中使用 import 参数。 除此之外，社区还有几个其他资源导入工具（reverse IaC）值得研究： former2: 为已有的 AWS 资源生成 terraform/pulumi/cloudformation 等配置，但是不支持生成 tfstate 状态 terraformer: 为已有的 AWS/GCP/Azure/Alicloud/DigitalOcean 等多种云资源生成 terraform 配置以及 tfstate 状态 terracognita: 功能跟 terraformer 一样，都支持生成 terraform 配置以及 tfstate 状态，但是它支持 AWS/GCP/Azure 三朵云 pulumi-terraform: 这个 provider 使你可以在 pulumi 项目里使用 tfstate 状态文件 tf2pulumi: 将 terraform 配置转换为 pulumi typescript 配置 6.1 通过 pulumi import 命令导入资源使用 pulumi import 命令导入资源的好处是，不需要为每个资源手写代码，此命令会自动生成资源的 stack state 与配置代码。 使用此命令导入的资源，默认会启用删除保护，你可通过参数 --protect=false 来关闭删除保护。 资源名称可通过命令行参数，或者 Json 文件来指定。 下面我们演示一个导入一个 s3 bucket 的流程： shell # 导入一个名为 test-sre 的 s3 bucket，资源 ID 为 p-test-sre $ pulumi import aws:s3/bucket:Bucket p-test-sre test-sre ...... Do you want to perform this import? yes Importing (dev): Type Name Status + pulumi:pulumi:Stack pulumi-test-dev created = └─ aws:s3:Bucket p-test-sre imported Resources: + 1 created = 1 imported 2 changes Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws p_test_sre = aws.s3.Bucket(\"p-test-sre\", arn=\"arn:aws:s3:::test-sre\", bucket=\"test-sre\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"test-sre\", \"Team\": \"Platform\", }, opts=pulumi.ResourceOptions(protect=True)) 能看到它会自动导入对应资源的 state，并同时打印出对应的 python 代码，要求我们手动将代码复制粘贴到项目中。而且代码会自带 arn/hosted_zone_id/protect 等属性，说明这个资源实际上是无法像普通 pulumi 资源一样，通过 pulumi up/pulumi destroy 自动创建销毁的。要通过 pulumi 删除该资源，需要首先解除删除保护，然后将对应的代码片段删除掉，最后执行 pulumi up。 也可通过 json 来批量导入资源，首先编写一个 json 资源清单： text { \"resources\": [{ \"type\": \"aws:s3/bucket:Bucket\", \"name\": \"s3-bucket_xxx-debug\", \"id\": \"xxx-debug\" }, { \"type\": \"aws:s3/accessPoint:AccessPoint\", \"name\": \"s3-accesspoint_xxx-debug\", \"id\": \"112233445566:xxx-debug\" } ] } 然后执行如下命令批量导入资源： shell $ pulumi import -f test-resources.json ...... Do you want to perform this import? yes Importing (dev): Type Name Status pulumi:pulumi:Stack pulumi-test-dev = ├─ aws:s3:AccessPoint s3-accesspoint_xxx-debug imported = └─ aws:s3:Bucket s3-bucket_xxx-debug imported Resources: = 2 imported 2 unchanged Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws s3_bucket_snappea_dl_debug = aws.s3.Bucket(\"s3-bucket_xxx-debug\", arn=\"arn:aws:s3:::xxx-debug\", bucket=\"xxx-debug\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"xxx-debug\", \"Team\": \"Xxx\", }, opts=pulumi.ResourceOptions(protect=True)) s3_accesspoint_snappea_dl_debug = aws.s3.AccessPoint(\"s3-accesspoint_xxx-debug\", account_id=\"112233445566\", bucket=\"xxx-debug\", name=\"xxx-debug\", public_access_block_configuration=aws.s3.AccessPointPublicAccessBlockConfigurationArgs( block_public_acls=False, block_public_policy=False, ignore_public_acls=False, restrict_public_buckets=False, ), opts=pulumi.ResourceOptions(protect=True)) 能看到同样的生成出了两个资源的 stack 状态，以及对应的代码。 6.2 通过代码导入资源通过代码导入资源，需要你手工为每个资源编写代码，并且确保代码的所有参数与资源本身的状态完全一致。 因此可以看到这种导入方式很不灵活，通常不推荐使用，pulumi import 自动生成代码它不香么 emmmm 大概的流程如下，首先编写一个资源的配置代码，并将其标注为 import: python p_test_sre = aws.s3.Bucket(\"p-test-sre\", bucket=\"test-sre\", tags={ \"Name\": \"test-sre\", \"Team\": \"xxx\", # 这里我故意写错了，pulumi 会检测到这里有问题，提示导入将失败 }, opts","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:6","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#61-通过-pulumi-import-命令导入资源"},{"categories":["tech"],"content":" 6. 如何导入已经存在的资源？如果你司不是一开始就使用了 pulumi 这类工具，那通常绝大部分云上资源都是手动管理、或者由其他工具自动化管理的，该如何将它们纳入 pulumi 管辖呢？ 官方有提供一篇相关文档Importing Infrastructure. 文档有提到两种资源导入的方法，导入成功后都会自动生成资源的状态，以及对应的 pulumi 代码。第一种是使用 pulumi import 命令，第二种是在代码中使用 import 参数。 除此之外，社区还有几个其他资源导入工具（reverse IaC）值得研究： former2: 为已有的 AWS 资源生成 terraform/pulumi/cloudformation 等配置，但是不支持生成 tfstate 状态 terraformer: 为已有的 AWS/GCP/Azure/Alicloud/DigitalOcean 等多种云资源生成 terraform 配置以及 tfstate 状态 terracognita: 功能跟 terraformer 一样，都支持生成 terraform 配置以及 tfstate 状态，但是它支持 AWS/GCP/Azure 三朵云 pulumi-terraform: 这个 provider 使你可以在 pulumi 项目里使用 tfstate 状态文件 tf2pulumi: 将 terraform 配置转换为 pulumi typescript 配置 6.1 通过 pulumi import 命令导入资源使用 pulumi import 命令导入资源的好处是，不需要为每个资源手写代码，此命令会自动生成资源的 stack state 与配置代码。 使用此命令导入的资源，默认会启用删除保护，你可通过参数 --protect=false 来关闭删除保护。 资源名称可通过命令行参数，或者 Json 文件来指定。 下面我们演示一个导入一个 s3 bucket 的流程： shell # 导入一个名为 test-sre 的 s3 bucket，资源 ID 为 p-test-sre $ pulumi import aws:s3/bucket:Bucket p-test-sre test-sre ...... Do you want to perform this import? yes Importing (dev): Type Name Status + pulumi:pulumi:Stack pulumi-test-dev created = └─ aws:s3:Bucket p-test-sre imported Resources: + 1 created = 1 imported 2 changes Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws p_test_sre = aws.s3.Bucket(\"p-test-sre\", arn=\"arn:aws:s3:::test-sre\", bucket=\"test-sre\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"test-sre\", \"Team\": \"Platform\", }, opts=pulumi.ResourceOptions(protect=True)) 能看到它会自动导入对应资源的 state，并同时打印出对应的 python 代码，要求我们手动将代码复制粘贴到项目中。而且代码会自带 arn/hosted_zone_id/protect 等属性，说明这个资源实际上是无法像普通 pulumi 资源一样，通过 pulumi up/pulumi destroy 自动创建销毁的。要通过 pulumi 删除该资源，需要首先解除删除保护，然后将对应的代码片段删除掉，最后执行 pulumi up。 也可通过 json 来批量导入资源，首先编写一个 json 资源清单： text { \"resources\": [{ \"type\": \"aws:s3/bucket:Bucket\", \"name\": \"s3-bucket_xxx-debug\", \"id\": \"xxx-debug\" }, { \"type\": \"aws:s3/accessPoint:AccessPoint\", \"name\": \"s3-accesspoint_xxx-debug\", \"id\": \"112233445566:xxx-debug\" } ] } 然后执行如下命令批量导入资源： shell $ pulumi import -f test-resources.json ...... Do you want to perform this import? yes Importing (dev): Type Name Status pulumi:pulumi:Stack pulumi-test-dev = ├─ aws:s3:AccessPoint s3-accesspoint_xxx-debug imported = └─ aws:s3:Bucket s3-bucket_xxx-debug imported Resources: = 2 imported 2 unchanged Duration: 8s Please copy the following code into your Pulumi application. Not doing so will cause Pulumi to report that an update will happen on the next update command. Please note that the imported resources are marked as protected. To destroy them you will need to remove the `protect` option and run `pulumi update` *before* the destroy will take effect. import pulumi import pulumi_aws as aws s3_bucket_snappea_dl_debug = aws.s3.Bucket(\"s3-bucket_xxx-debug\", arn=\"arn:aws:s3:::xxx-debug\", bucket=\"xxx-debug\", hosted_zone_id=\"ZZBBCC332211KK\", request_payer=\"BucketOwner\", tags={ \"Name\": \"xxx-debug\", \"Team\": \"Xxx\", }, opts=pulumi.ResourceOptions(protect=True)) s3_accesspoint_snappea_dl_debug = aws.s3.AccessPoint(\"s3-accesspoint_xxx-debug\", account_id=\"112233445566\", bucket=\"xxx-debug\", name=\"xxx-debug\", public_access_block_configuration=aws.s3.AccessPointPublicAccessBlockConfigurationArgs( block_public_acls=False, block_public_policy=False, ignore_public_acls=False, restrict_public_buckets=False, ), opts=pulumi.ResourceOptions(protect=True)) 能看到同样的生成出了两个资源的 stack 状态，以及对应的代码。 6.2 通过代码导入资源通过代码导入资源，需要你手工为每个资源编写代码，并且确保代码的所有参数与资源本身的状态完全一致。 因此可以看到这种导入方式很不灵活，通常不推荐使用，pulumi import 自动生成代码它不香么 emmmm 大概的流程如下，首先编写一个资源的配置代码，并将其标注为 import: python p_test_sre = aws.s3.Bucket(\"p-test-sre\", bucket=\"test-sre\", tags={ \"Name\": \"test-sre\", \"Team\": \"xxx\", # 这里我故意写错了，pulumi 会检测到这里有问题，提示导入将失败 }, opts","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:6","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#62-通过代码导入资源"},{"categories":["tech"],"content":" 6.3 如何从 pulumi 中移除被导入的资源格式如下： shell pulumi state delete \u003cresource URN\u003e [flags] 比如要删除先前导入的 arn:aws:s3:::test-sre，首先删除对应的代码，然后执行pulumi preview，就会报错并打印出对应的资源 urn: text $ pulumi preview ... Diagnostics: aws:s3:Bucket (p-test-sre): error: Preview failed: unable to delete resource \"urn:pulumi:dev::pulumi-test::aws:s3/bucket:Bucket::p-test-sre\" as it is currently marked for protection. To unprotect the resource, either remove the `protect` flag from the resource in your Pulumi program and run `pulumi up` or use the command: `pulumi state unprotect 'urn:pulumi:dev::pulumi-test::aws:s3/bucket:Bucket::p-test-sre'` 接下来使用如下命令强制从 state 文件中移除此资源（仅修改配置，对实际资源无任何影响）： shell pulumi state delete urn:pulumi:dev::pulumi-test::aws:s3/bucket:Bucket::p-test-sre --force ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:7","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#63-如何从-pulumi-中移除被导入的资源"},{"categories":["tech"],"content":" 5. pulumi-kubernetes？pulumi-kubernetes 是一条龙服务： 在 yaml 配置生成这一步，它能结合/替代掉 helm/kustomize，或者你高度自定义的 Python 脚本。 在 yaml 部署这一步，它能替代掉 argo-cd 这类 gitops 工具。 强大的状态管理，argo-cd 也有状态管理，可以对比看看。 也可以仅通过 kubernetes_pulumi 生成 yaml，再通过 argo-cd 部署，这样 pulumi_kubernetes 就仅用来简化 yaml 的编写，仍然通过 gitops 工具/kubectl 来部署。 使用 pulumi-kubernetes 写配置，要警惕逻辑和数据的混合程度。因为 kubernetes 的配置复杂度比较高，如果动态配置比较多，很容易就会写出难以维护的 python 代码来。 渲染 yaml 的示例： python from pulumi import get_stack, ResourceOptions, StackReference from pulumi_kubernetes import Provider from pulumi_kubernetes.apps.v1 import Deployment, DeploymentSpecArgs from pulumi_kubernetes.core.v1 import ( ContainerArgs, ContainerPortArgs, EnvVarArgs, PodSpecArgs, PodTemplateSpecArgs, ResourceRequirementsArgs, Service, ServicePortArgs, ServiceSpecArgs, ) from pulumi_kubernetes.meta.v1 import LabelSelectorArgs, ObjectMetaArgs provider = Provider( \"render-yaml\", render_yaml_to_directory=\"rendered\", ) deployment = Deployment( \"redis\", spec=DeploymentSpecArgs(...), opts=ResourceOptions(provider=provider), ) 如示例所示，pulumi-kubernetes 的配置是完全结构化的，比 yaml/helm/kustomize 要灵活非常多。 总之它非常灵活，既可以和 helm/kustomize 结合使用，替代掉 argocd/kubectl。也可以和 argocd/kubectl 使用，替代掉 helm/kustomize。 具体怎么使用好？我也还在研究。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:8","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#5-pulumi-kubernetes"},{"categories":["tech"],"content":" 6. 阿里云资源 replace 报错？阿里云有部分资源，只能创建删除，不允许修改，比如「资源组」。对这类资源做变更时，pulumi 会直接报错：「Resources aleardy exists」，这类资源，通常都有一个「force」参数，指示是否强制修改——即先删除再重建。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:9","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#6-阿里云资源-replace-报错"},{"categories":["tech"],"content":" 7. 有些资源属性无法使用 pulumi 配置？这得看各云服务提供商的支持情况。 比如阿里云很多资源的属性，pulumi 都无法完全配置，因为 alicloud provider 的功能还不够全面。 目前我们生产环境，大概 95%+ 的东西，都可以使用 pulumi 实现自动化配置。而其他 OSS 的高级参数、新出的 ASM 服务网格、kubernetes 的授权管理、ElasticSearch7 等资源，还是需要手动配置。 这个没办法，只能等阿里云提供支持。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:10","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#7-有些资源属性无法使用-pulumi-配置"},{"categories":["tech"],"content":" 8. CI/CD 中如何使 pulumi 将状态保存到文件？CI/CD 中我们可能会希望 pulumi 将状态保存到本地，避免连接 pulumi 中心服务器。这一方面能加快速度，另一方面一些临时状态我们可能根本不想存储，可以直接丢弃。 方法： shell # 指定状态文件路径 pulumi login file://\u003cfile-path\u003e # 保存到默认位置: ~/.pulumi/credentials.json pulumi login --local # 保存到远程 S3 存储（minio/ceph 或者各类云对象存储服务，都兼容 aws 的 s3 协议） pulumi login s3://\u003cbucket-path\u003e 登录完成后，再进行 pulumi up 操作，数据就会直接保存到你设定的路径下。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:11","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#8-cicd-中如何使-pulumi-将状态保存到文件"},{"categories":["tech"],"content":" 9. 如何估算资源变更导致的成本变化？目前 pulumi 貌似没有类似的工具，但是 terraform 有一个infracost 可以干这个活，值得关注。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:4:12","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#9-如何估算资源变更导致的成本变化"},{"categories":["tech"],"content":" 缺点","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:5:0","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#缺点"},{"categories":["tech"],"content":" 1. 报错信息不直观pulumi 和 terraform 都有一个缺点，就是封装层次太高了。 封装的层次很高，优点是方便了我们使用，可以使用很统一很简洁的声明式语法编写配置。而缺点，则是出了 bug，报错信息往往不够直观，导致问题不好排查。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:5:1","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#1-报错信息不直观"},{"categories":["tech"],"content":" 2. 资源状态被破坏时，修复起来非常麻烦在很多情况下，都可能发生资源状态被破坏的问题： 在创建资源 A，因为参数是已知的，你直接使用了常量而不是 Output。这会导致 pulumi 无法识别到依赖关系！从而创建失败，或者删除时资源状态被破坏！ 有一个 pulumi stack 一次在三台物理机上创建资源。你白天创建资源晚上删除资源，但是某一台物理机晚上会关机。这将导致 pulumi 无法查询到这台物理机上的资源状态，这个 pulumi stack 在晚上就无法使用，它会一直报错！ ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:5:2","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#2-资源状态被破坏时修复起来非常麻烦"},{"categories":["tech"],"content":" 常用 Provider pulumi-alicloud: 管理阿里云资源 pulumi-vault: 我这边用它来快速初始化 vault，创建与管理 vault 的所有配置。 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:6:0","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#常用-provider"},{"categories":["tech"],"content":" 我创建维护的 Provider由于 Pulumi 生态还比较小，有些 provider 只有 terraform 才有。 我为了造(方)福(便)大(自)众(己)，创建并维护了两个本地虚拟机相关的 Providers: ryan4yin/pulumi-proxmox: 目前只用来自动创建 PVE 虚拟机 可以考虑结合 kubespray/kubeadm 快速创建 k8s 集群 ryan4yin/pulumi-libvirt: 快速创建 kvm 虚拟机 可以考虑结合 kubespray/kubeadm 快速创建 k8s 集群 ","date":"2021-01-08","objectID":"/posts/experience-of-pulumi/:7:0","series":["云原生相关"],"tags":["基础设施即代码","云原生","Pulumi","Terraform"],"title":"Pulumi 使用体验 - 基础设施代码化","uri":"/posts/experience-of-pulumi/#我创建维护的-provider"},{"categories":["tech"],"content":"openSUSE 是一个基于 RPM 的发行版，这和 RHEL/CentOS 一致。但是它的官方包管理器是专有的 zypper，挺好用的，软件也很新。 我最近从 Manjaro 切换到了 openSUSE，发现 KDE 桌面确实比 Manjaro 更丝滑，而且社区源 OBS 体验下来比 AUR 更舒服。 尤其是容器/Kubernetes 方面，源里面的东西比 AUR 更丰富，而且是官方维护的。本文算是对迁移流程做的一个总结。 本文以 openSUSE Tumbleweed 为基础编写，这是一个和 Manjaro/Arch 一样的滚动发行版，软件源都很新。openSUSE 社区的大部分用户都是使用的 Tumbleweed. 它的硬件兼容性也要比 openSUSE Leap（稳定版）好——实测小米游戏本安装 Leap，休眠后 Touchpad 会失灵。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:0:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#"},{"categories":["tech"],"content":" 一、zypper 的基础命令zypper 的源在国内比较慢，但实际上下载的时候，zypper 会智能选择最快的镜像源下载软件包，比如国内的清华源等。 但是我发现官方的源索引更新太慢，甚至经常失败。因此没办法，还是得手动设置镜像源： shell # 禁用原有的官方软件源 sudo zypper mr --disable repo-oss repo-non-oss repo-update repo-update-non-oss repo-debug # 添加北外镜像源 sudo zypper ar -fcg https://mirrors.bfsu.edu.cn/opensuse/tumbleweed/repo/oss/ bfsu-oss sudo zypper ar -fcg https://mirrors.bfsu.edu.cn/opensuse/tumbleweed/repo/non-oss/ bfsu-non-oss 然后就是 zypper 的常用命令： shell sudo zypper refresh # refresh all repos sudo zypper update # update all software sudo zypper search --installed-only \u003cpackage-name\u003e # 查找本地安装的程序 sudo zypper search \u003cpackage-name\u003e # 查找本地和软件源中的程序 sudo zypper install \u003cpackage-name\u003e # 安装程序 sudo zypper remove --clean-deps \u003cpackage-name\u003e # 卸载程序，注意添加 --clean-deps 或者 -u，否则不会卸载依赖项！ sudo zypper clean # 清理本地的包缓存 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:1:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#一zypper-的基础命令"},{"categories":["tech"],"content":" Install Software 这里需要用到OBS(Open Build Service, 类似 arch 的 AUR，但是是预编译的包)， 因为 OBS 东西太多了，因此不存在完整的国内镜像，平均速度大概 300kb/s。建议有条件可以在路由器上加智能代理提速。 安装需要用到的各类软件: shell # 启用 Packman 仓库，使用阿里云镜像源 sudo zypper ar \"http://mirrors.aliyun.com/packman/suse/openSUSE_Tumbleweed/\" Packman # install video player and web browser sudo zypper install mpv ffmpeg-4 chromium firefox # install screenshot and other utils # 安装好后可以配个截图快捷键 alt+a =\u003e `flameshot gui` sudo zypper install flameshot peek nomacs # install git clang/make/cmake sudo zypper install git gcc clang make cmake # install wireshark sudo zypper install wireshark sudo gpasswd --add $USER wireshark # 将你添加到 wireshark 用户组中 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#install-software"},{"categories":["tech"],"content":" IDE + 编程语言 shell # install vscode: https://en.openSUSE.org/Visual_Studio_Code sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo zypper addrepo https://packages.microsoft.com/yumrepos/vscode vscode sudo zypper refresh sudo zypper install code # 安装 dotnet 5: https://docs.microsoft.com/en-us/dotnet/core/install/linux-openSUSE#openSUSE-15- sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo zypper addrepo https://packages.microsoft.com/openSUSE/15/prod/ microsoft-prod sudo zypper refresh sudo zypper install dotnet-sdk-5.0 # 安装新版本的 go（源中的版本比较低，更建议从 go 官网下载安装） sudo zypper install go 通过 tarball/script 安装： shell # rustup，rust 环境管理器 curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh # jetbrains toolbox app，用于安装和管理 pycharm/idea/goland/android studio 等 IDE # 参见：https://www.jetbrains.com/toolbox-app/ # 不使用系统 python，改用 miniconda 装 python3.8 # 参考：https://github.com/ContinuumIO/docker-images/blob/master/miniconda3/debian/Dockerfile wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh sudo /bin/bash /tmp/miniconda.sh -b -p /opt/conda rm /tmp/miniconda.sh sudo /opt/conda/bin/conda clean -tipsy sudo ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh echo \". /opt/conda/etc/profile.d/conda.sh\" \u003e\u003e ~/.bashrc echo \"conda activate base\" \u003e\u003e ~/.bashrc # miniconda 的 entrypoint 默认安装在如下目录，添加到 PATH 中 echo \"export PATH=\\$PATH:\\$HOME/.local/bin\" \u003e\u003e ~/.bashrc 接下来安装 VSCode 插件，下列是我的插件列表： 语言： python/go/c#/julia/flutter c/c++ extension pack rust-analyzer shellchecker redhat xml \u0026 yaml even-better-toml edit-csv vscode-proto3 ansible/terraform markdown all in one + Markdown Preview Enhanced 美化： community material theme vscode icons glasslt-vsc docker/kubernetes IntelliJ IDEA Keybindings gitlens prettier utils comment translate path intellisense svg visual studio intellicode remote ssh + remote containers rest client vscode databases ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:1","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#ide--编程语言"},{"categories":["tech"],"content":" 容器 + Kubernetes shell # 时髦的新容器套装: https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-podman-overview.html sudo zypper in podman kompose skopeo buildah katacontainers # 安装 kubernetes 相关工具，tumbleweed 官方仓库的包都非常新！很舒服 sudo zypper in helm k9s kubernetes-client # 本地测试目前还是 docker-compose 最方便，docker 仍有必要安装 sudo zypper in docker sudo gpasswd --add $USER docker sudo systemctl enable docker sudo systemctl start docker # 简单起见，直接用 pip 安装 docker-compose 和 podman-compose sudo pip install docker-compose podman-compose ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:2","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#容器--kubernetes"},{"categories":["tech"],"content":" 办公、音乐、聊天 shell # 添加 openSUSE_zh 源：https://build.opensuse.org/project/show/home:opensuse_zh sudo zypper addrepo 'https://download.opensuse.org/repositories/home:/opensuse_zh/openSUSE_Tumbleweed' openSUSE_zh sudo zypper refresh sudo zypper install wps-office netease-cloud-music # linux qq: https://im.qq.com/linuxqq/download.html # 虽然简陋但也够用，发送文件比 KDE Connect 要方便一些。 sudo rpm -ivh linux_qq.rpm ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:3","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#办公音乐聊天"},{"categories":["tech"],"content":" 安装输入法我用的输入法是小鹤音形，首先安装 fcitx-rime: shell # 添加 m17n obs 源：https://build.openSUSE.org/repositories/M17N sudo zypper addrepo 'https://download.opensuse.org/repositories/M17N/openSUSE_Tumbleweed' m17n sudo zypper refresh sudo zypper install fcitx5 fcitx5-configtool fcitx5-qt5 fcitx5-rime 然后，从 http://flypy.ys168.com/ 下载最新的鼠须管（MacOS）配置文件，将解压得到的 rime 文件夹拷贝到 ~/.local/share/fcitx5/ 下： shell mv rime ~/.local/share/fcitx5/ 现在重启系统，在 fcitx5 配置里面添加 rime「中州韵」，就可以正常使用小鹤音形了。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:4","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#安装输入法"},{"categories":["tech"],"content":" QEMU/KVM不得不说，openSUSE 安装 KVM 真的超方便，纯 GUI 操作： shell # see: https://doc.openSUSE.org/documentation/leap/virtualization/html/book-virt/cha-vt-installation.html sudo yast2 virtualization # enter to terminal ui, select kvm + kvm tools, and then install it. KVM 的详细文档参见 KVM/README.md ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:5","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#qemukvm"},{"categories":["tech"],"content":" VPN 连接与防火墙防火墙默认会禁用 pptp 等 vpn 协议的端口，需要手动打开. 允许使用 PPTP 协议： shell # 允许 gre 数据包流入网络 sudo firewall-cmd --permanent --zone=public --direct --add-rule ipv4 filter INPUT 0 -p gre -j ACCEPT sudo firewall-cmd --permanent --zone=public --direct --add-rule ipv6 filter INPUT 0 -p gre -j ACCEPT # masquerade: 自动使用 interface 地址伪装所有流量（将主机当作路由器使用，vpn 是虚拟网络，需要这个功能） sudo firewall-cmd --permanent --zone=public --add-masquerade # pptp 客户端使用固定端口 1723/tcp 通信 firewall-cmd --add-port=1723/tcp --permanent sudo firewall-cmd --reload 允许使用 wireguard 协议，此协议只使用 tcp 协议，而且可以端口号可以自定义。不过 wireguard 自身的配置文件 /etc/wireguard/xxx.conf 就能配置 iptables 参数放行相关端口，这里就不赘述了。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:2:6","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#vpn-连接与防火墙"},{"categories":["tech"],"content":" 触摸板手势参考 libinput-gestures ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:3:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#触摸板手势"},{"categories":["tech"],"content":" 安装 Nvidia 闭源驱动 完全参考官方文档 https://en.opensuse.org/SDB:NVIDIA_drivers shell # 添加 Nvidia 官方镜像源 zypper addrepo --refresh https://download.nvidia.com/opensuse/tumbleweed NVIDIA # 搜索驱动以及对应的版本号 # 在如下 Nvidia 官方站点根据提示检索出合适的最新驱动，然后使用其版本号在如下 zypper 命令输出中找到对应的驱动名称 # https://www.nvidia.com/Download/index.aspx # 比如我的显卡是 RTX3070，通过版本号能找到其对应的驱动为 x11-video-nvidiaG06 zypper se -s x11-video-nvidiaG0* # 安装驱动 zypper in x11-video-nvidiaG06 x11-video-nvidiaG06 如果你还需要安装 CUDA 来本地炼丹，CUDA 的安装有两种方法： 直接将 CUDA 安装在本机，可参考NVIDIA CUDA Installation Guide for Linux 使用 Docker 容器炼丹，可参考 nvidia-docker ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:4:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#安装-nvidia-闭源驱动"},{"categories":["tech"],"content":" 设置代理工具为了加速网络或者访问一些国内不存在的网站，网络代理是必不可少的工具。 我习惯使用 clash，极简安装方法如下： 首先 sudo zypper in clash 安装好 clash clash 环境配置（记不清是否得手动配置这个了…） 下载好 Country.mmdb 放到~/.config/clash 中 好像还需要下载下 clash-dashboard 到 ~/.config/clash/ui 找到你自己的 clash 配置订阅地址（各种机场都会提供的），写个小脚本内容为curl \"\u003c订阅地址\u003e\" \u003e ~/.config/clash/config.yaml 如果这个文件夹还没有就先创建一下 如果代理是自己搭建的，那就自己写这个配置咯 把脚本放在 PATH 中的某个目录中，方便随时调用更新 使用 tmux 后台启动 clash，代理就设置完成了 浏览器可以通过 swichomega 来设置细致的代理规则 命令行可以直接使用export HTTP_PROXY=http://127.0.0.1; export HTTPS_PROXY=http://127.0.0.1 来使用代理，大部分命令行程序都会使用这两个环境变量的配置。 一般机场给的 clash 配置都会直接开启 clash Web 配置页，可以通过http://localhost:9090/ui/#/proxies 直接访问 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:5:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#设置代理工具"},{"categories":["tech"],"content":" 设置 zypper 使用 proxy 下载更新zypper 默认不会读取 HTTP_PROXY 跟 HTTPS_PROXY 等环境变量，对于一些无国内镜像的源而言， 可以通过如下方式配置走代理提升下载速度（这需要你已经有本地代理才行，比如说 clash）： 进入 YaST GUI 在搜索框直接搜索 proxy 即可找到对应的配置项 配置好 http 以及 https 协议的代理地址，如果是本地的 clash，可以直接填http://127.0.0.1:7890 在「No Proxy Domains」中添加国内镜像源地址，使它们不要走代理 如果是跟我的教程走的，应该需要将这个值改成localhost,127.0.0.1,mirrors.bfsu.edu.cn,mirrors.aliyun.com ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:6:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#设置-zypper-使用-proxy-下载更新"},{"categories":["tech"],"content":" KDE ConnectKDE Connect 是一个 PC 手机协同工具，可以在电脑和手机之间共享剪切版、远程输入、发送文件、共享文件夹、通知同步等等。总而言之非常好用，只要手机和 PC 处于同一个局域网就行，不需要什么数据线。 如果安装系统时选择了打开防火墙，KDE Connect 是连不上的，需要手动开放端口号： shell # see: https://userbase.kde.org/KDEConnect#firewalld # 还可以使用 --add-source=xx.xx.xx.xx/xx 设置 ip 白名单 sudo firewall-cmd --zone=public --permanent --add-port=1714-1764/tcp sudo firewall-cmd --zone=public --permanent --add-port=1714-1764/udp sudo firewall-cmd --reload # 确认下已经开放这些端口 sudo firewall-cmd --list-all 然后手机（Android）安装好 KDE Connect，就能开始享受了。 目前存在的 Bug: Android 10 禁止了后台应用读取剪切版，这导致 KDE Connect 只能从 PC 同步到手机，而无法反向同步。 如果你有 ROOT 权限，可以参考Fix clipboard permission on Android 10 的方法，安装 ClipboardWhitelist 来打开权限。 否则，貌似就只能使用手机端的「远程输入」模块来手动传输文本了。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:6:1","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#kde-connect"},{"categories":["tech"],"content":" VPN 连接与其他防火墙相关配置防火墙默认会禁用所有对外端口，需要手动打开。 允许使用 PPTP 协议： shell # 允许 gre 数据包流入网络 sudo firewall-cmd --permanent --zone=public --direct --add-rule ipv4 filter INPUT 0 -p gre -j ACCEPT sudo firewall-cmd --permanent --zone=public --direct --add-rule ipv6 filter INPUT 0 -p gre -j ACCEPT # masquerade: 自动使用 interface 地址伪装所有流量（将主机当作路由器使用，vpn 是虚拟网络，需要这个功能） sudo firewall-cmd --permanent --zone=public --add-masquerade # pptp 客户端使用固定端口 1723/tcp 通信 firewall-cmd --add-port=1723/tcp --permanent sudo firewall-cmd --reload # 确认下已经开放这些端口 sudo firewall-cmd --list-all 允许使用 wireguard 协议： 此协议只使用 tcp 协议，而且可以端口号可以自定义。不过 wireguard 自身的配置文件/etc/wireguard/xxx.conf 就能配置 iptables 参数放行相关端口，这里就不赘述了。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:6:2","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#vpn-连接与其他防火墙相关配置"},{"categories":["tech"],"content":" OpenSSH 服务为了局域网数据传输方便，以及远程使用 PC，我一般都会给自己的 Linux 机器打开 OpenSSH 服务。 在 OpenSUSE 上启用 OpenSSH 服务的流程： shell sudo systemctl enable sshd sudo systemctl start sshd sudo systemctl status sshd 2. 设置使用密钥登录显然密钥登录才足够安全，这里介绍下我如何设置密钥登录。 先生成密钥对（如果你常用 github，本地应该已经有密钥对了，可以考虑直接使用同一个密钥对，这样就能跳过这一步）： shell # 或者直接命令行指定密钥算法类型(-t)、名称与路径(-f)、注释(-C)、密钥的保护密码(-P)。 ## 当密钥较多时，注释可用于区分密钥的用途。 ## 算法推荐使用 ed25519/ecdsa，默认的 rsa 算法目前已不推荐使用（需要很长的密钥和签名才能保证安全）。 ssh-keygen -t ed25519 -f id_rsa_for_xxx -C \"ssh key for xxx\" -P '' 接下来需要把公钥追加到主机的 $HOME/.ssh/authorized_keys 文件的末尾（$HOME 是 user 的家目录，不是 root 的家目录，请看清楚了）： shell # 方法一，手动将公钥添加到 ~/.ssh/authorized_keys 中 # 然后手动将 ~/.ssh/authorized_keys 的权限设为 600 chmod 600 ~/.ssh/authorized_keys # 方法二：如果你的密钥对在其他主机上，可以直接在该主机上执行如下命令将公钥添加到 openSUSE 机器上，会有提示输入密码 # -i 设定公钥位置，默认使用 ~/.ssh/id_rsa.pub ssh-copy-id -i path/to/key_name.pub user@host 现在就完事了，可以使用密钥钥登录试试： shell # -i 设定私钥位置，默认使用 ~/.ssh/id_rsa ssh \u003cusername\u003e@\u003cserver-ip\u003e -i \u003crsa_private_key\u003e # 举例 ssh ubuntu@111.222.333.444 -i ~/.ssh/id_rsa_for_server 2. 调整安全设置openSUSE 的 OpenSSH 服务默认是允许密码登录的，虽然也有登录速率限制，还是会有点危险。 既然我们前面已经设置好了密钥登录，现在就可以把密码登录功能完全禁用掉，提升安全性。 请取消注释并修改 /usr/etc/ssh/sshd_config 中如下参数的值： 注意 OpenSSH 的主配置文件是 /usr/etc/ssh/sshd_config，而不是大部分 Linux 发行版使用的/etc/ssh/sshd_config。 conf # 安全相关配置 LoginGraceTime 2m StrictModes yes MaxAuthTries 6 MaxSessions 10 # 禁止使用 root 用户登录 PermitRootLogin no # 允许使用公钥认证 PubkeyAuthentication yes # 禁用明文密码登录 PasswordAuthentication no # 禁用掉基于 password 的交互式认证 KbdInteractiveAuthentication no # 禁用 PAM 模块 UsePAM no 改完后再重启下 sshd 服务并用 ssh 命令登录测试确认功能已生效： shell # 尝试在使用「公钥验证」之外的其他方法登录 # 如果 sshd 服务的设置正确，这行命令应该登录失败并报错「Permission Denied(publickey)」 ssh -o PubkeyAuthentication=no user@host ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:6:3","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#openssh-服务"},{"categories":["tech"],"content":" OpenSSH 服务为了局域网数据传输方便，以及远程使用 PC，我一般都会给自己的 Linux 机器打开 OpenSSH 服务。 在 OpenSUSE 上启用 OpenSSH 服务的流程： shell sudo systemctl enable sshd sudo systemctl start sshd sudo systemctl status sshd 2. 设置使用密钥登录显然密钥登录才足够安全，这里介绍下我如何设置密钥登录。 先生成密钥对（如果你常用 github，本地应该已经有密钥对了，可以考虑直接使用同一个密钥对，这样就能跳过这一步）： shell # 或者直接命令行指定密钥算法类型(-t)、名称与路径(-f)、注释(-C)、密钥的保护密码(-P)。 ## 当密钥较多时，注释可用于区分密钥的用途。 ## 算法推荐使用 ed25519/ecdsa，默认的 rsa 算法目前已不推荐使用（需要很长的密钥和签名才能保证安全）。 ssh-keygen -t ed25519 -f id_rsa_for_xxx -C \"ssh key for xxx\" -P '' 接下来需要把公钥追加到主机的 $HOME/.ssh/authorized_keys 文件的末尾（$HOME 是 user 的家目录，不是 root 的家目录，请看清楚了）： shell # 方法一，手动将公钥添加到 ~/.ssh/authorized_keys 中 # 然后手动将 ~/.ssh/authorized_keys 的权限设为 600 chmod 600 ~/.ssh/authorized_keys # 方法二：如果你的密钥对在其他主机上，可以直接在该主机上执行如下命令将公钥添加到 openSUSE 机器上，会有提示输入密码 # -i 设定公钥位置，默认使用 ~/.ssh/id_rsa.pub ssh-copy-id -i path/to/key_name.pub user@host 现在就完事了，可以使用密钥钥登录试试： shell # -i 设定私钥位置，默认使用 ~/.ssh/id_rsa ssh @ -i # 举例 ssh ubuntu@111.222.333.444 -i ~/.ssh/id_rsa_for_server 2. 调整安全设置openSUSE 的 OpenSSH 服务默认是允许密码登录的，虽然也有登录速率限制，还是会有点危险。 既然我们前面已经设置好了密钥登录，现在就可以把密码登录功能完全禁用掉，提升安全性。 请取消注释并修改 /usr/etc/ssh/sshd_config 中如下参数的值： 注意 OpenSSH 的主配置文件是 /usr/etc/ssh/sshd_config，而不是大部分 Linux 发行版使用的/etc/ssh/sshd_config。 conf # 安全相关配置 LoginGraceTime 2m StrictModes yes MaxAuthTries 6 MaxSessions 10 # 禁止使用 root 用户登录 PermitRootLogin no # 允许使用公钥认证 PubkeyAuthentication yes # 禁用明文密码登录 PasswordAuthentication no # 禁用掉基于 password 的交互式认证 KbdInteractiveAuthentication no # 禁用 PAM 模块 UsePAM no 改完后再重启下 sshd 服务并用 ssh 命令登录测试确认功能已生效： shell # 尝试在使用「公钥验证」之外的其他方法登录 # 如果 sshd 服务的设置正确，这行命令应该登录失败并报错「Permission Denied(publickey)」 ssh -o PubkeyAuthentication=no user@host ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:6:3","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#2-设置使用密钥登录"},{"categories":["tech"],"content":" OpenSSH 服务为了局域网数据传输方便，以及远程使用 PC，我一般都会给自己的 Linux 机器打开 OpenSSH 服务。 在 OpenSUSE 上启用 OpenSSH 服务的流程： shell sudo systemctl enable sshd sudo systemctl start sshd sudo systemctl status sshd 2. 设置使用密钥登录显然密钥登录才足够安全，这里介绍下我如何设置密钥登录。 先生成密钥对（如果你常用 github，本地应该已经有密钥对了，可以考虑直接使用同一个密钥对，这样就能跳过这一步）： shell # 或者直接命令行指定密钥算法类型(-t)、名称与路径(-f)、注释(-C)、密钥的保护密码(-P)。 ## 当密钥较多时，注释可用于区分密钥的用途。 ## 算法推荐使用 ed25519/ecdsa，默认的 rsa 算法目前已不推荐使用（需要很长的密钥和签名才能保证安全）。 ssh-keygen -t ed25519 -f id_rsa_for_xxx -C \"ssh key for xxx\" -P '' 接下来需要把公钥追加到主机的 $HOME/.ssh/authorized_keys 文件的末尾（$HOME 是 user 的家目录，不是 root 的家目录，请看清楚了）： shell # 方法一，手动将公钥添加到 ~/.ssh/authorized_keys 中 # 然后手动将 ~/.ssh/authorized_keys 的权限设为 600 chmod 600 ~/.ssh/authorized_keys # 方法二：如果你的密钥对在其他主机上，可以直接在该主机上执行如下命令将公钥添加到 openSUSE 机器上，会有提示输入密码 # -i 设定公钥位置，默认使用 ~/.ssh/id_rsa.pub ssh-copy-id -i path/to/key_name.pub user@host 现在就完事了，可以使用密钥钥登录试试： shell # -i 设定私钥位置，默认使用 ~/.ssh/id_rsa ssh @ -i # 举例 ssh ubuntu@111.222.333.444 -i ~/.ssh/id_rsa_for_server 2. 调整安全设置openSUSE 的 OpenSSH 服务默认是允许密码登录的，虽然也有登录速率限制，还是会有点危险。 既然我们前面已经设置好了密钥登录，现在就可以把密码登录功能完全禁用掉，提升安全性。 请取消注释并修改 /usr/etc/ssh/sshd_config 中如下参数的值： 注意 OpenSSH 的主配置文件是 /usr/etc/ssh/sshd_config，而不是大部分 Linux 发行版使用的/etc/ssh/sshd_config。 conf # 安全相关配置 LoginGraceTime 2m StrictModes yes MaxAuthTries 6 MaxSessions 10 # 禁止使用 root 用户登录 PermitRootLogin no # 允许使用公钥认证 PubkeyAuthentication yes # 禁用明文密码登录 PasswordAuthentication no # 禁用掉基于 password 的交互式认证 KbdInteractiveAuthentication no # 禁用 PAM 模块 UsePAM no 改完后再重启下 sshd 服务并用 ssh 命令登录测试确认功能已生效： shell # 尝试在使用「公钥验证」之外的其他方法登录 # 如果 sshd 服务的设置正确，这行命令应该登录失败并报错「Permission Denied(publickey)」 ssh -o PubkeyAuthentication=no user@host ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:6:3","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#2-调整安全设置"},{"categories":["tech"],"content":" firewall 防火墙介绍firewall 是 SUSE/RedHat 等 RPM 发行版使用的防火墙程序，它底层使用的是 iptables/nftables， 常用命令如下： bash systemctl enable firewalld # 启用 firewalld 服务，即打开「开机自启」功能 systemctl disable firewalld # 关闭 firewalld 服务，即关闭「开机自启」功能 systemctl status firewalld # 查看 firewalld 的状态 systemctl start firewalld # 启动 systemctl stop firewalld # 停止 # 打开 SSH 端口 sudo firewall-cmd --zone=public --add-port=22/tcp --permanent # 关闭 SSH 端口 firewall-cmd --zone=public --remove-port=22/tcp --permanent # 修改防火墙规则后需要重载配置 sudo firewall-cmd --reload # 查看 firewall 的状态 sudo firewall-cmd --list-all ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:6:4","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#firewall-防火墙介绍"},{"categories":["tech"],"content":" 其他设置从 Windows 带过来的习惯是单击选中文件，双击才打开，这个可以在「系统设置」-「工作空间行为」-「常规行为」-「点击行为」中修改。 ","date":"2021-01-04","objectID":"/posts/opensuse-instruction/:7:0","series":null,"tags":["openSUSE","Linux"],"title":"openSUSE 使用指南","uri":"/posts/opensuse-instruction/#其他设置"},{"categories":["life","tech"],"content":" 闲言碎语一晃一年又过去了，今年可真是魔幻的一年，口罩带了一年没能摘下来，美国疫情感染人数 1500 万。 上面这段话要是让去年的我看到了，没准都以为今年真的生化危机了hhh… 言归正传，从去年 6 月底入职，到现在有一年半了，这一年半学到的东西真的非常多，完全重塑了我的技术栈。现在我的整个技术栈，基本都是围绕着云原生这一块发展了。 ","date":"2020-12-12","objectID":"/posts/2020-summary/:1:0","series":["年终总结"],"tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/#闲言碎语"},{"categories":["life","tech"],"content":" 活动今年也参加了几个技术沙龙，有些收获，但是没去年那么新奇了，主要是很多东西自己已经懂了hhh。大概有这么几个活动： 2019 年腾讯蓝鲸第5届运维技术沙龙：在深圳腾讯大厦参加的，点心和咖啡很棒，讲的东西里，腾讯自己分享的「研发运维一体化平台」比较有收获，我收藏了那一份 PPT Rancher - 企业云原生的探索与落地：去年参加 Rancher 的沙龙觉得很高大上，因为自己很多东西都不懂。但是今年来听，明显就感觉他们讲的很基础，对我没什么价值了。也侧面说明我确实进步了非常多哈哈。 2020 PyconChina 深圳场：额，也觉得没什么干货，好几个都是在推销自家的产品（Azure AI 平台和一个 Django 写的低代码平台），有个讲 Nix 包管理的大佬但是没讲好，后面我们就直接溜了… 另外就是，今年心血来潮买了四张 Live House 的演出票，体验下来觉得钱花得很值，给我充值了不少正能量。 景德镇文艺复兴《小歌行》：这是我超级喜欢的一个乐队，演出效果超棒！听到了完整的故事，而且见到了九三姑娘本人，太高兴了！ 徐海俏 - 游离片刻：这位歌手我之前其实没接触过，但是听了下她的《空》发现很不错很帅气，就买了。但是整场下来感觉俏俏状态不佳，有点唱不动的感觉。中场问歌迷们有没有带野格酒，末了又问深圳现在能游泳么哈哈，是个很随性的歌手。后面可能还真游泳去了。 夏小虎 - 逝年：这是个民谣歌手，以前上大学的时候听过，只有吉他和人声，其实是有些伤感的歌。因为我最近状态很好，我去之前还担心氛围不适合我。然后夏小虎说开心最重要，带了个乐队来伴奏，架子鼓就是灵魂，整个演出都因鼓点而欢快了起来。效果也非常棒！ 时光胶囊乐队：这也是一个国风后摇乐队，在一个深圳福田一个小酒吧「红糖罐上步店」演出的， 比较简陋，人也不多，就四五十人的样子（出乎意料）。但是演出效果很棒，《旅途》《忆长安》《磐石》都非常好听。尤其是在这样的场合唱《我不知道你的名字》，挺有感触的。 ","date":"2020-12-12","objectID":"/posts/2020-summary/:2:0","series":["年终总结"],"tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/#活动"},{"categories":["life","tech"],"content":" 技术能力总结今年我的工作重点有这么几个： 重构及维护 CI/CD 代码，让它能在多个产品线上复用 云上环境管理：今年熟悉了阿里云这一套东西，并且用上了自动化工具对云上环境进行管理。 一开始是使用 terraform，但是 terraform 的 hcl 语法不够灵活，最近切换到了 pulumi+python，不得不说真香。目前云上的资源及配置 95% 都完全用 pulumi 管理了（还剩大概 5% 因为各种原因，需要手动配置）。 kubernetes云原生: 今年我在这个领域的进步最大，熟悉了 k8s/istio/flagger/vault/prometheus/helm/traefik 等等。不过目前这里面大部分工具还停留在「会用」的状态。 服务器虚拟化系统从 vSphere 切换到 PVE VMware 的 vCenter 吃的资源太多，而且还不能自动扩缩容硬盘，Python SDK 也超难用。因此我在公司尝试使用 PVE 替换 vSphere 这一套，效果很不错。 PVE+pulumi/terraform+cloudinit 能实现自动化部署虚拟机，自动配置网络、账号及SSH密钥、自动扩缩容硬盘，非常方便！ 而且 PVE 不收费，去中心化，一套用下来舒服太多了。只是 pve+cloud-init 门槛稍微高一点， 需要一定时间去熟悉。 CI/CD 系统：基于 Jenkins 的 CI/CD 在我司各种水土不服，小毛病不断。Jenkins 本身就存在单点故障，不适合云原生，加上 Jenkinsfile 有学习成本，而且不方便复用，我就想把 Jenkins 换掉。我在这一年里调研了大量的开源 CI/CD 工具，都各有不足。主要还是因为我们当下的 Jenkins 承载了太多的功能，已经是一个CI/CD、自动化测试、自动化运维平台了，另一方面公司后端的流水线还存在依赖关系，需要进行复杂的编排。 目前我就找到 Argo Workflows 的功能很符合我们的需求，目前正在尝试迁移一部分功能到 Argo Workflows 试用。 因为 argo 的 UI 和 jenkins 差别过大，暂定仍以 jenkins 为前端，通过 python 将任务分派给 argo 运行。这样 argo 对使用者而言是隐形的，用户体验基本上没区别。 杂事：修水电、修服务器、组装办公电脑、搬机房… ","date":"2020-12-12","objectID":"/posts/2020-summary/:3:0","series":["年终总结"],"tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/#技术能力总结"},{"categories":["life","tech"],"content":" 今年在技术方面的感受 Podman/Skopeo/Buildah/Kaniko 等技术进一步发展，正在逐渐蚕食 Docker 的地盘. kubernetes 已经弃用 docker-shim，直接对接 containerd，下一步应该是彻底切换到 CRI-O。 Istio 1.5 合并为单体架构效果很明显，各微信公众号三天两头就讲服务网格，服务网格是毋庸置疑的未来 阿里云的 OAM 进一步发展，目前阿里基于 OAM 研发的 Kubevela 致力于封装 Kubernetes 的功能，让小白也能用上 Kubernetes。而这同时还能保留 k8s 完整的能力，值得期待。 云上安全越来越引起重视了，目前 CNCF 社区上容器安全相关的项目在快速发展。包括镜像安全/安全容器(kata containers)等。 使用 Kubernetes 来管理数据库已经是大趋势，毕竟成本优势太明显了。 很多公司已经在使用 docker 运行数据库，毕竟性能没啥损失，就能方便很多。但是仍然手动搭建集群，也不使用分布式存储。 目前好像只有大厂如阿里京东才有这个实力，使用 kubernetes 和分布式存储来跑数据库。容器化的分布式存储系统维护(如 ceph)，其中的难点我目前还不是很清楚，不过无外乎性能、稳定性、故障恢复这些。 ","date":"2020-12-12","objectID":"/posts/2020-summary/:4:0","series":["年终总结"],"tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/#今年在技术方面的感受"},{"categories":["life","tech"],"content":" 明年的展望 Go 语言必须学起来，今年入门了两遍语法，但是没写过啥东西，又忘差不多了。 要进一步熟悉 k8s/istio/flagger/vault/prometheus/helm/traefik/caddy 这些工具，会用还不够，要深入底层。 深入学习计算机网络 + Linux 网络 + Kubernetes 网络！这非常重要。 学习 Podman/Docker 的底层原理，学习 katacontainers 等安全容器技术。 为 kubevela/dapr/knative 等云原生项目做一些贡献，要参与到开源中去。 掌握 Argo Workflows/tekton，将 CI/CD 搬到 k8s 上去。 学习设计模式 有机会的话，熟悉下分布式存储、分布式数据库。这方面我目前还相当欠缺。 学习 KVM 虚拟化 如果学（xian）有（de）余（dan）力（teng）的话，也可以考虑搞搞下面这几个： rust 语言：rust 通过 owner+lifetime 实现内存的智能管理，性能很高，而且编译器提示非常友好，值得一学。 机器学习/深度学习: 这个领域可是当下的大热门，可以用 python/julia 玩一玩，顺便补一补微积分线代概率论。。 《编程语言实现模式》：嗯，希望能自己造轮子，写些简单的 parser。 elixir: ruby 语法+ erlang 并发模型(actor), 如果有时间的话，也可以玩玩，了解下原生分布式的函数式语言的特点。 回看了下去年的总结，发现我 go/c# 都没学多少，设计模式也没动，python 还在原地踏步hhh… 去年的展望很多都没实现。 不过云原生这一块倒是进步很快，总体很满意今年的成果哈哈~ 明年加油！ ","date":"2020-12-12","objectID":"/posts/2020-summary/:5:0","series":["年终总结"],"tags":["总结"],"title":"2020 年年终总结","uri":"/posts/2020-summary/#明年的展望"},{"categories":["音乐","life"],"content":" 前言2020 年 11 月 28 日，我第一次参加 Live House，演出乐队是「景德镇文艺复兴」。 「景德镇文艺复兴」是我很喜欢的一支后摇乐队，我喜欢上这支乐队，还得从我的昵称「於清樂」说起。 17 年的时候，听了许多后摇，网易云就给我推荐「景德镇文艺复兴」的歌，如此结缘。 其中有一首歌是「满世」，后摇嘛，歌词只有四句： 月下灵鸟吟 花香无处寻 再看破土人 一满又一世 听这首歌的时候，我想起看过韩寒的《长安乱》，里面女主名叫「喜乐」，这名字里寄托着家人对她的期许——平安喜乐。我心里也升起一个不可能实现的愿望： 希冀能于这尘世之中，享得半世清闲，一生喜乐 这愿望已然不可能实现，过往的岁月里，我有过太多苦恼，做过了太多错事；可遇见的未来，也还没到清闲享乐的时候。那至少把昵称改成「於清樂」，提醒着自己，你有过这样一个愿望。 ","date":"2020-11-28","objectID":"/posts/jingdezhen-renaissance-band-2020-shenzhen/:1:0","series":null,"tags":["景德镇文艺复兴","后摇"],"title":"「小歌行」-景德镇文艺复兴-2020巡演-深圳","uri":"/posts/jingdezhen-renaissance-band-2020-shenzhen/#前言"},{"categories":["音乐","life"],"content":" 演出演出的内容是《小歌行》这张专辑，乐队通过一个自创的神话故事，将整张专辑串成了一个类似音乐剧的形式，进行演出，效果很棒！ 我用手机录下了几乎全程，因为第一次参加 Live House，又是自己这么喜欢的乐队，想要录下来，留做纪念。 视频已经上传到了 Bilibili: 录到最后手机没电了，为了留点电量刷公交车卡和门禁卡，最后一首《水码头》没有录完。（到家时真的差点刷不了门禁hhh） 好了，下面是演出的照片集锦： Live House 入口的宣传海报 老村长1 老村长2 老村长3 拉小提琴的小哥哥 九三舞蹈 小提琴伴奏 九三是朝廷命官 阿弥陀佛 唱 唱 唱 唱 唱 九三最漂亮的一瞬间 九三超帅气 结束： Live House 后台 结束鞠躬 大合照 签售： 签售 ","date":"2020-11-28","objectID":"/posts/jingdezhen-renaissance-band-2020-shenzhen/:2:0","series":null,"tags":["景德镇文艺复兴","后摇"],"title":"「小歌行」-景德镇文艺复兴-2020巡演-深圳","uri":"/posts/jingdezhen-renaissance-band-2020-shenzhen/#演出"},{"categories":["tech"],"content":" 个人笔记，不保证正确 ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:0:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#"},{"categories":["tech"],"content":" 问题我以前只知道 Base64 这个编码算法很常用，自己也经常在 JWT 等场景下使用，但是从来没了解过它的原理，一直先入为主地认为它的编码应该是唯一的。 但是今天测试 JWT 时，发现修改 JWT 的最后一个字符（其实不是我发现的。。），居然有可能不影响 JWT 的正确性。比如下这个使用 HS256 算法的 JWT: text eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c 把它的最后一个字符改成 d e或者 f，都能成功通过 http://jwt.io 的验证。 这让我觉得很奇怪（难道我发现了一个 Bug？），在 QQ 群里一问，就有大佬找到根本原因：这是 Base64 编码的特性。并且通过 python 进行了实际演示： python In [1]: import base64 # 使用 jwt 的 signature 进行验证 In [2]: base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c==\") Out[2]: b'I\\xf9J\\xc7\\x04IH\\xc7\\x8a(]\\x90O\\x87\\xf0\\xa4\\xc7\\x89\\x7f~\\x8f:N\\xb2%V\\x9dB\\xcb0\\xe5' In [3]: base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5d==\") Out[3]: b'I\\xf9J\\xc7\\x04IH\\xc7\\x8a(]\\x90O\\x87\\xf0\\xa4\\xc7\\x89\\x7f~\\x8f:N\\xb2%V\\x9dB\\xcb0\\xe5' In [4]: base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5e==\") Out[4]: b'I\\xf9J\\xc7\\x04IH\\xc7\\x8a(]\\x90O\\x87\\xf0\\xa4\\xc7\\x89\\x7f~\\x8f:N\\xb2%V\\x9dB\\xcb0\\xe5' In [5]: base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5f==\") Out[5]: b'I\\xf9J\\xc7\\x04IH\\xc7\\x8a(]\\x90O\\x87\\xf0\\xa4\\xc7\\x89\\x7f~\\x8f:N\\xb2%V\\x9dB\\xcb0\\xe5' # 两个等于号之后的任何内容，都会被直接丢弃。这个是实现相关的，有的 base64 处理库对这种情况会报错。 In [6]: base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5f==fdf=df==dfd=fderwe=r\") Out[6]: b'I\\xf9J\\xc7\\x04IH\\xc7\\x8a(]\\x90O\\x87\\xf0\\xa4\\xc7\\x89\\x7f~\\x8f:N\\xb2%V\\x9dB\\xcb0\\xe5' 可以看到有两个现象： 将同一个 base64 串的最后一个字母分别改成 d e f，解码出来的内容没有任何变化。 在 base64 串末尾 == 后面添加了一堆随机字符，对解码出的内容也没有任何影响。 ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:0:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#问题"},{"categories":["tech"],"content":" 原因分析base64 编码将二进制内容(bytes)从左往右每 6 bits 分为一组，每一组编码为一个可打印字符。 bas64 从 ASCII 字符集中选出了 64 个字符（=号除外）进行编码。因为 $2^6=64$，使用 64 个字符才能保证上述编码的唯一性。 但是被编码的二进制内容(bytes)的 bits 数不一定是 6 的倍数，无法被编码为 6 bits 一组。为了解决这个问题，就需要在这些二进制内容的末尾填充上 2 或 4 个 bit 位，这样才能使用 base64 进行编码。 关于这些被填充的 bits，在 RFC4648 中定义了规范行为：全部补 0. 但是这并不是一个强制的行为， 因此实际上你可以随便补，在进行 base64 解析时，被填补的 bits 会被直接忽略掉。 这就导致了上面描述的行为：修改 JWT 的最后一个字符(6 bits，其中可能包含 2 或 4 个填充比特位)可能并不影响被编码的实际内容！ RFC4648 中对这个 bits 填充的描述如下： text 3.5. Canonical Encoding The padding step in base 64 and base 32 encoding can, if improperly implemented, lead to non-significant alterations of the encoded data. For example, if the input is only one octet for a base 64 encoding, then all six bits of the first symbol are used, but only the first two bits of the next symbol are used. These pad bits MUST be set to zero by conforming encoders, which is described in the descriptions on padding below. If this property do not hold, there is no canonical representation of base-encoded data, and multiple base- encoded strings can be decoded to the same binary data. If this property (and others discussed in this document) holds, a canonical encoding is guaranteed. In some environments, the alteration is critical and therefore decoders MAY chose to reject an encoding if the pad bits have not been set to zero. The specification referring to this may mandate a specific behaviour. 它讲到在某些环境下，base64 解析器可能会严格检查被填充的这几个 bits，要求它们全部为 0. 但是我测试发现，Python 标准库和 https://jwt.io 都没有做这样的限制。因此我认为绝大部分环境下，被填充的 bits 都是会被忽略的。 ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:1:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#原因分析"},{"categories":["tech"],"content":" 问题一：为什么只需要填充 2 或 4 个 bit 位？这是看到「填充上 2 或 4 个 bit 位」时的第一想法——如果要补足到 6 的倍数，不应该是要填充 1-5 个 bit 位么？ 要解答这个问题，我们得看 base64 的定义。在 RFC4648 的 base64 定义中，有如下这样一段话： The Base 64 encoding is designed to represent arbitrary sequences of octets in a form that allows the use of both upper- and lowercase letters but that need not be human readable. 注意重点：octets—— 和 bytes 同义，表示 8 bits 一组的位序列。这表示 base64 只支持编码 bits 数为 8 的倍数的二进制内容，而 $8x \\bmod 6$ 的结果只可能是 0/2/4 三种情况。 因此只需要填充 2 或 4 个 bit 位。 这样的假设也并没有什么问题，因为现代计算机都是统一使用 8 bits(byte) 为最小的可读单位的。即使是 c 语言的「位域」也是如此。因为 Byte(8 bits) 现代 CPU 数据读写操作的基本单位，学过汇编的对这个应该都有些印象。 你仔细想想，所有文件的最小计量单位，是不是都是 byte？ ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:2:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#问题一为什么只需要填充-2-或-4-个-bit-位"},{"categories":["tech"],"content":" 问题二：为什么用 python 测试时可能需要在 JWT signature 的末尾添加多个 =，而 JWT 中不需要？前面已经讲过，base64 的编码步骤是是将字节(byte, 8 bits)序列，从左往右每 6 个 bits 转换成一个可打印字符。 查阅 RFC4648 第 4 小节中 base64 的定义，能看到它实际上是每次处理 24 bits，因为这是 6 和 8 的最小公倍数，可以刚好用 4 个字符表示。 在被处理的字节序列的比特(bits)数不是 24 的整数时，就需要在序列末尾填充 0 使末尾的 bits 数是 6 的倍数(6-bit groups)。有可能会出现三种情况： 被处理的字节序列 S 的比特数刚好是 24 的倍数：不需要补比特位，末尾也就不需要加 = S 的比特数是 $24x+8$: 末尾需要补 4 个 bits，这样末尾剩余的 bits 才是 6-bit groups，才能编码成 base64。然后添加两个 == 使编码后的字符数为 4 的倍数。 S 的比特数为 $24x+16$：末尾需要添加 2 个 bits 才能编码成 base64。然后添加一个 = 使编码后的字符数为 4 的倍数。 其实可以看到，添加 = 的目的只是为了使编码后的字符数为 4 的倍数而已，= 这个 padding 其实是冗余信息，完全可以去掉。 在解码完成后，应用程序会自动去除掉末尾这不足 1 byte 的 2 或 4 个填充位。 因此 JWT 就去掉了它以减少传输的数据量。 可以用前面讲到的 JWT signature 进行验证： python In [1]: import base64 In [2]: s = base64.b64decode(\"SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c==\") # len(s) * 8 得到 bits 数 In [3]: len(s) * 8 % 24 Out[3]: 8 可以看到这里的被编码内容比特数为 $24x+8$，所以末尾需要添加两个 == 号才符合 RFC4648 的定义。 ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:3:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#问题二为什么用-python-测试时可能需要在-jwt-signature-的末尾添加多个-而-jwt-中不需要"},{"categories":["tech"],"content":" 参考 Remove trailing “=” when base64 encoding RFC4648 - base64 定义 Difference between RFC 3548 and RFC 4648 Base64 隐写原理和提取脚本 ","date":"2020-05-31","objectID":"/posts/base64-encoding-is-not-unique/:4:0","series":null,"tags":["Base64","编码"],"title":"Base64 编码并不唯一","uri":"/posts/base64-encoding-is-not-unique/#参考"},{"categories":["tech"],"content":" 抓包分析抓包分析工具主要有两种： http/https 网络代理工具：mitmproxy/fiddler 都属于这一类，用于分析 http 非常方便。但是只支持 http/https，有局限性。 tcp/udp/icmp 等网络嗅探工具：tcpdump/tshark 都属于这一类，网络故障分析等场景常用。 这里主要介绍如何使用 tcpdump + wireshark 进行远程实时抓包分析。而 mitmproxy 抓包和 wireshark 本地抓包都相当简单，就不介绍了。 P.S. tshark 是 wireshark 的命令行版本，用法 tcpdump 非常相似。 ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:0:0","series":["计算机网络相关"],"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#抓包分析"},{"categories":["tech"],"content":" 一、wireshark 的基本用法WireShark 的 UI 界面如何使用，网上能搜得到各种类型的 wireshark 演示，多看几篇博客就会了。搜索 [xxx 协议 wireshark 抓包分析] 就能找到各种各样的演示，比如 「gRPC 协议 wireshark 抓包分析」 「WebSocket 协议 wireshark 抓包分析」 「TCP 协议 wireshark 抓包分析」 等等 主要需要介绍的，应该是 wireshark 的数据包过滤器。wireshark 中有两种包过滤器： 捕获过滤器：在抓包的时候使用它对数据包进行过滤。 显示过滤器：对抓到的所有数据包进行过滤。 显示过滤器是最有用的，下面简要介绍下显示过滤器的语法。 可以直接通过「协议名称」进行过滤： text # 只看 tcp 流量 tcp # 只看 http 流量 http # 使用感叹号（或 not）进行反向过滤 !arp # 过滤掉所有 arp 数据包 也可以通过「协议名称.协议属性」和「比较操作符（比如 ==）」进行更精确的过滤： text # 通过 ip 的源地址 src 或 dst 进行过滤 ip.src==192.168.1.33 # 通过 IP 地址（ip.addr）进行过滤（匹配 ip.src 或 ip.dst） ip.addr==192.168.0.5 # 上一条过滤表达式等价于： ip.src==192.168.0.5 or ip.dst==192.168.0.5 # 通过 tcp 端口号进行过滤 tcp.port==80 tcp.port\u003e4000 # 通过 http 的 host 属性进行过滤 http.host != \"xxx.baidu.com\" # 通过 http.referer 属性进行过滤 http.referer == \"xxx.baidu.com\" # 多个过滤器之间用 and、or 进行组合 http.host != \"xxx.baidu.com\" and http.referer == \"xxx.baidu.com\" ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:1:0","series":["计算机网络相关"],"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#一wireshark-的基本用法"},{"categories":["tech"],"content":" 二、tcpdump + ssh + wireshark 远程实时抓包在进行远程网络抓包分析时，我们通常的做法是，首先使用如下命令在远程主机上抓包，保存为 pcap 格式的文件： shell tcpdump -i eth0 -w temp.pcap # 当然你也可以在抓包阶段就加一些过滤条件，降低抓到的数据量与工作负载 # 具体的命令行过滤条件在后面的第三节有讲，这里仅简单举例 # 嗅探 eth0 接口， src/dst ip 地址为 x.x.x.x/a.a.a.a 的所有 tcp 数据包 tcpdump -i eth0 -w temp.pcap 'tcp and (host x.x.x.x or host a.a.a.a)' 然后再将 pcap 文件拷贝到本地，使用 wireshark 对其进行分析。 但是这样做没有时效性，而且数据拷贝来去也比较麻烦。 考虑使用流的方式，在远程主机上使用 tcpdump 抓包，本地使用 wireshark 进行实时分析。 使用 ssh 协议进行流式传输的示例如下： shell # eth0 更换成你的机器 interface 名称，虚拟机可能是 ens33 ssh root@some.host \"tcpdump -i eth0 -l -w -\" | wireshark -k -i - # 添加一些 tcpdump 过滤条件进行精确过滤，这也能避免数据量过大 ssh root@some.host \"tcpdump -i eth0 -l -w - 'tcp and (host x.x.x.x or host a.a.a.a)'\" | wireshark -k -i - 在不方便使用 ssh 协议的情况下（比如容器抓包、Android 抓包），可以考虑使用 nc(netcat) 进行数据流的转发： shell # 1. 远程主机抓包：将数据流通过 11111 端口暴露出去 tcpdump -i wlan0 -s0 -w - | nc -l -p 11111 # 2. 本地主机从远程主机的 11111 端口读取数据，提供给 wireshark nc \u003cremote-host\u003e 11111 | wireshark -k -S -i - 如果是抓取 Android 手机的数据，方便起见，可以通过 adb 多进行一次数据转发： shell # 方案一：root 手机后，将 arm 版的 tcpdump 拷贝到手机内进行抓包 # 1. 在 adb shell 里使用 tcpdump 抓 WiFi 的数据包，转发到 11111 端口 ## 需要先获取到 root 权限，将 tcpdump 拷贝到 /system/bin/ 目录下 tcpdump -i wlan0 -s0 -w - | nc -l -p 11111 # 2. 在本机使用 adb forward 将手机的 11111 端口绑定到本机(PC)的 11111 端口 adb forward tcp:11111 tcp:11111 # 3. 直接从本机(PC)的 11111 端口读取数据，提供给 wireshark nc localhost 11111 | wireshark -k -S -i - ## 通过数据转发，本机 11111 端口的数据，就是安卓手机内 tcmpdump 的 stdout 内容。 # 方案二： # 如果手机不方便 root，推荐 PC 开启 WiFi 热点，手机连上这个热点访问网络。 # 这样手机的数据就一定会走 PC，直接在 PC 上通过 wireshark 抓包就行。 # 如果你只需要简单地抓 http/https 包，请使用 fiddler/mitmproxy 如果需要对 Kubernetes 集群中的容器进行抓包，推荐直接使用ksniff! ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:2:0","series":["计算机网络相关"],"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#二tcpdump--ssh--wireshark-远程实时抓包"},{"categories":["tech"],"content":" Windows 系统另外如果你本机是 Windows 系统，要分 shell 讨论： cmd: 可以直接使用上述命令。 powershell: PowerShell 管道对 native commands 的支持不是很好，管道两边的命令貌似是串行执行的，这会导致 wireshark 无法启动！目前没有找到好的解决方法。。 另外如果你使用 wsl，那么可以通过如下命令使 wsl 调用 windows 中的 wireshark 进行抓包分析： shell # 添加软链接 sudo ln -s \"$(which wireshark.exe)\" /usr/local/bin/wireshark 添加了上述软链接后，就可以正常地在 wsl 中使用前面介绍的所有抓包指令了（包括ksniff）。它能正常调用 windows 中的 wireshark，数据流也能正常地通过 shell 管道传输。 ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:2:1","series":["计算机网络相关"],"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#windows-系统"},{"categories":["tech"],"content":" 三、直接在命令行抓包检查","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:3:0","series":["计算机网络相关"],"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#三直接在命令行抓包检查"},{"categories":["tech"],"content":" termshark可以直接使用命令行 UI 工具 termshark 进行实时抓包分析 有的时候，远程实时抓包因为某些原因无法实现，而把 pcap 数据拷贝到本地分析又比较麻烦。这时你可以考虑直接使用命令行版本的 wireshark UI:termshark，直接在命令行进行实时的抓包分析。 可以直接使用 termshark 抓包查看： shell termshark -i eth0 icmp 也可以用 termshark 分析 pcap 文件： shell termshark -r test.pcap kubectl-debug 默认的调试镜像中，就自带termshark. ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:3:1","series":["计算机网络相关"],"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#termshark"},{"categories":["tech"],"content":" tcpdump也可以直接使用 tcpdump 将抓到的数据打印到 stdout 查看，这种方法比较适合在命令行中临时抓些明文数据看看。 先简单介绍下 tcpdump 的命令行参数（可通过 man tcpdump 查看详细文档）： text -A 以 ASCII 字符方式打印所有抓到的数据内容，在抓取纯文本数据时很好用 -i interface 仅抓取某个特定网络接口上的数据包，特殊参数 `all` 表示所有接口 -s snaplen 设置每个包的最大长度，默认为 262144 bytes。在较老版本的 tcpdump 中 -s 0 即表示使用默认的包长度 --snapshot-length=snaplen 常用命令如下： 注意 tcpdump 在直接打印数据时，有可能会发现较长的数据尾部会被截断，丢失信息。这是 tcpdump 本身的问题，在这种模式下 tcpdump 是针对每个 tcp 数据包应用 http 过滤参数，而过长的 body 后半部分在另一个 tcp 包里，就过滤不出来了。如果有必要抓这类 body 很大的数据的话，建议结合 wireshark 分析。 shell # 1. 嗅探所有接口，80 端口上所有 HTTP 协议请求与响应的 headers 以及 body tcpdump -A -s 0 'tcp port 80 and (((ip[2:2] - ((ip[0]\u00260xf)\u003c\u003c2)) - ((tcp[12]\u00260xf0)\u003e\u003e2)) != 0)' ## 改抓 8080 端口 tcpdump -A -s 0 'tcp port 8080 and (((ip[2:2] - ((ip[0]\u00260xf)\u003c\u003c2)) - ((tcp[12]\u00260xf0)\u003e\u003e2)) != 0)' # 2. 嗅探 eth0 接口，80 端口上所有 HTTP GET 请求（'GET ' =\u003e 0x47455420） tcpdump -A -i eth0 -s 0 'tcp port 80 and tcp[((tcp[12:1] \u0026 0xf0) \u003e\u003e 2):4] = 0x47455420' ## 改抓 8080 端口 tcpdump -A -i eth0 -s 0 'tcp port 8080 and tcp[((tcp[12:1] \u0026 0xf0) \u003e\u003e 2):4] = 0x47455420' # 3. 嗅探 eth0 接口，80 端口上所有 HTTP POST 请求（'POST' =\u003e 0x504F5354） tcpdump -A -i eth0 -s 0 'tcp port 80 and tcp[((tcp[12:1] \u0026 0xf0) \u003e\u003e 2):4] = 0x504F5354' ## 改抓 8080 端口 tcpdump -A -i eth0 -s 0 'tcp port 8080 and tcp[((tcp[12:1] \u0026 0xf0) \u003e\u003e 2):4] = 0x504F5354' # 4. 也可以使用 not 进行参数排除，比如排除掉 9091 跟 2379 端口 tcpdump -A -s 0 'tcp and port not 9091 and port not 2379 and (((ip[2:2] - ((ip[0]\u00260xf)\u003c\u003c2)) - ((tcp[12]\u00260xf0)\u003e\u003e2)) != 0)' # 5. 嗅探 eth0 接口， src/dst ip 地址为 x.x.x.x/a.a.a.a 的所有 tcp 数据包 tcpdump -A -i eth0 -s 0 'tcp and (host x.x.x.x or host a.a.a.a)' # 6. 通过 ip + port 组合过滤 tcp 数据包 tcpdump -A -i eth0 -s 0 'tcp and !(dst host 192.168.1.100 and dst port 1111) and !(dst host 192.168.1.101 and dst port 3333)' # 7. 根据 CIDR 网段过滤 tcp 数据包 tcpdump -A -i eth0 -s 0 'tcp and net 172.16.0.0/16' ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:3:2","series":["计算机网络相关"],"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#tcpdump"},{"categories":["tech"],"content":" 参考 WireShark使用教程 Tracing network traffic using tcpdump and tshark Android remote sniffing using Tcpdump, nc and Wireshark 聊聊tcpdump与Wireshark抓包分析 ","date":"2020-05-28","objectID":"/posts/tcpdump-and-wireshark/:4:0","series":["计算机网络相关"],"tags":["网络","Wireshark","tcpdump","抓包分析"],"title":"使用 tcpdump 和 Wireshark 进行远程实时抓包分析","uri":"/posts/tcpdump-and-wireshark/#参考"},{"categories":["tech"],"content":" 本文基于 Istio1.5 编写测试 Istio 支持使用 JWT 对终端用户进行身份验证（Istio End User Authentication），支持多种 JWT 签名算法。 目前主流的 JWT 算法是 RS256/ES256。（请忽略 HS256，该算法不适合分布式 JWT 验证） 这里以 RSA256 算法为例进行介绍，ES256 的配置方式也是一样的。 注意：Istio 仅负责身份验证，用户需要首先通过业务层的登录 API 获得 JWT 信息。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:0","series":["云原生相关"],"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#"},{"categories":["tech"],"content":" 1. 介绍 JWK 与 JWKSIstio 要求提供 JWKS 格式的信息，用于 JWT 签名验证。因此这里得先介绍一下 JWK 和 JWKS. JWKS ，也就是 JWK Set，json 结构如下： text { \"keys\": [ \u003cjwk-1\u003e, \u003cjwk-2\u003e, ... ]} JWKS 描述一组 JWK 密钥。它能同时描述多个可用的公钥，应用场景之一是密钥的 Rotate. 而 JWK，全称是 Json Web Key，它描述了一个加密密钥（公钥或私钥）的各项属性，包括密钥的值。 Istio 使用 JWK 描述验证 JWT 签名所需要的信息。在使用 RSA 签名算法时，JWK 描述的应该是用于验证的 RSA 公钥。 一个 RSA 公钥的 JWK 描述如下： text { \"alg\": \"RS256\", # 算法「可选参数」 \"kty\": \"RSA\", # 密钥类型 \"use\": \"sig\", # 被用于签名「可选参数」 \"kid\": \"NjVBRjY5MDlCMUIwNzU4RTA2QzZFMDQ4QzQ2MDAyQjVDNjk1RTM2Qg\", # key 的唯一 id \"n\": \"yeNlzlub94YgerT030codqEztjfU_S6X4DbDA_iVKkjAWtYfPHDzz_sPCT1Axz6isZdf3lHpq_gYX4Sz-cbe4rjmigxUxr-FgKHQy3HeCdK6hNq9ASQvMK9LBOpXDNn7mei6RZWom4wo3CMvvsY1w8tjtfLb-yQwJPltHxShZq5-ihC9irpLI9xEBTgG12q5lGIFPhTl_7inA1PFK97LuSLnTJzW0bj096v_TMDg7pOWm_zHtF53qbVsI0e3v5nmdKXdFf9BjIARRfVrbxVxiZHjU6zL6jY5QJdh1QCmENoejj_ytspMmGW7yMRxzUqgxcAqOBpVm0b-_mW3HoBdjQ\", \"e\": \"AQAB\" } RSA 是基于大数分解的加密/签名算法，上述参数中，e 是公钥的模数(modulus)，n 是公钥的指数 (exponent)，两个参数都是 base64 字符串。 JWK 中 RSA 公钥的具体定义参见RSA Keys - JSON Web Algorithms (JWA) ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:1","series":["云原生相关"],"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#1-介绍-jwk-与-jwks"},{"categories":["tech"],"content":" 2. JWK 的生成要生成 JWK 公钥，需要先生成私钥，生成方法参见JWT 签名算法 HS256、RS256 及 ES256 及密钥生成。 公钥不需要用上述方法生成，因为我们需要的是 JWK 格式的公钥。后面会通过 python 生成出 JWK 公钥。 上面的命令会将生成出的 RSA 私钥写入 key.pem 中，查看一下私钥内容。 text ryan@RYAN-MI-DESKTOP:~/istio$ cat key.pem -----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEAt1cKkQqPh8iOv5BhKh7Rx6A2+1ldpO/jczML/0GBKu4X+lHr Y8YbJrt29jyAXlWM8vHC7tXsqgUG+WziRD0D8nhnh10XC14SeH+3mVuBqph+TqhX TWsh9gtAIbeUHJjEI4I79QK4/wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAy Y35kJZgyJSbrxLsEExL2zujUD/OY+/In2bq/3rFtDGNlgHyC7Gu2zXSXvfOA4O5m 9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4+9q7sc3Dnkc5 EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxwIDAQABAoIBABIKhaaqJF+XM7zU B0uuxrPfJynqrFVbqcUfQ9H1bzF7Rm7CeuhRiUBxeA5Y+8TMpFcPxT/dWzGL1xja RxWx715/zKg8V9Uth6HF55o2r/bKlLtGw3iBz1C34LKwrul1eu+HlEDS6MNoGKco BynE0qvFOedsCu/Pgv7xhQPLow60Ty1uM0AhbcPgi6yJ5ksRB1XjtEnW0t+c8yQS nU3mU8k230SdMhf4Ifud/5TPLjmXdFpyPi9uYiVdJ5oWsmMWEvekXoBnHWDDF/eT VkVMiTBorT4qn+Ax1VjHL2VOMO5ZbXEcpbIc3Uer7eZAaDQ0NPZK37IkIn9TiZ21 cqzgbCkCgYEA5enHZbD5JgfwSNWCaiNrcBhYjpCtvfbT82yGW+J4/Qe/H+bY/hmJ RRTKf0kVPdRwZzq7GphVMWIuezbOk0aFGhk/SzIveW8QpLY0FV/5xFnGNjV9AuNc xrmgVshUsyQvr1TFkbdkC6yuvNgQfXfnbEoaPsXYEMCii2zqdF5lWGUCgYEAzCR2 6g8vEQx0hdRS5d0zD2/9IRYNzfP5oK0+F3KHH2OuwlmQVIo7IhCiUgqserXNBDef hj+GNcU8O/yXLomAXG7VG/cLWRrpY8d9bcRMrwb0/SkNr0yNrkqHiWQ/PvR+2MLk viWFZTTp8YizPA+8pSC/oFd1jkZF0UhKVAREM7sCgYB5+mfxyczFopyW58ADM7uC g0goixXCnTuiAEfgY+0wwXVjJYSme0HaxscQdOOyJA1ml0BBQeShCKgEcvVyKY3g ZNixunR5hrVbzdcgKAVJaR/CDuq+J4ZHYKByqmJVkLND4EPZpWSM1Rb31eIZzw2W 5FG8UBbr/GfAdQ6GorY+CQKBgQCzWQHkBmz6VG/2t6AQ9LIMSP4hWEfOfh78q9dW MDdIO4JomtkzfLIQ7n49B8WalShGITwUbLDTgrG1neeQahsMmg6+X99nbD5JfBTV H9WjG8CWvb+ZF++NhUroSNtLyu+6LhdaeopkbQVvPwMArG62wDu6ebv8v/5MrG8o uwrUSwKBgQCxV43ZqTRnEuDlF7jMN+2JZWhpbrucTG5INoMPOC0ZVatePszZjYm8 LrmqQZHer2nqtFpyslwgKMWgmVLJTH7sVf0hS9po0+iSYY/r8e/c85UdUreb0xyT x8whrOnMMODCAqu4W/Rx1Lgf2vXIx0pZmlt8Df9i2AVg/ePR6jO3Nw== -----END RSA PRIVATE KEY----- 接下来通过 Python 编程生成 RSA Public Key 和 JWK（jwk 其实就是公钥的另一个表述形式而已）: python # 需要先安装依赖: pip install jwcrypto from jwcrypto.jwk import JWK from pathlib import Path private_key = Path(\"key.pem\").read_bytes() jwk = JWK.from_pem(private_key) # 导出公钥 RSA Public Key public_key = jwk.public().export_to_pem() print(public_key) print(\"=\"*30) # 导出 JWK jwk_bytes = jwk.public().export() print(jwk_bytes) Istio 需要 JWK 进行 JWT 验证，而我们手动验证 JWT 时一般需要用到 Public Key. 方便起见，上述代码把这两个都打印了出来。内容如下： text # Public Key 内容，不包含这行注释 -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAt1cKkQqPh8iOv5BhKh7R x6A2+1ldpO/jczML/0GBKu4X+lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG+WziRD0D 8nhnh10XC14SeH+3mVuBqph+TqhXTWsh9gtAIbeUHJjEI4I79QK4/wquPHHIGZBQ DQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD/OY+/In2bq/ 3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4 KLb6oyvIzoeiprt4+9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ew xwIDAQAB -----END PUBLIC KEY----- text # jwk 内容 { 'e': 'AQAB', 'kid': 'oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo', 'kty': 'RSA', 'n': 't1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw' } ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:2","series":["云原生相关"],"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#2-jwk-的生成"},{"categories":["tech"],"content":" 4. 测试密钥可用性接下来在 jwt.io 中填入测试用的公钥私钥，还有 Header/Payload。一是测试公私钥的可用性，二是生成出 JWT 供后续测试 Istio JWT 验证功能的可用性。 可以看到左下角显示「Signature Verified」，成功地生成出了 JWT。后续可以使用这个 JWT 访问 Istio 网关，测试 Istio JWT 验证功能。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:3","series":["云原生相关"],"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#4-测试密钥可用性"},{"categories":["tech"],"content":" 5. 启用 Istio 的身份验证编写 istio 配置： yaml apiVersion: \"security.istio.io/v1beta1\" kind: \"RequestAuthentication\" metadata: name: \"jwt-example\" namespace: istio-system # istio-system 名字空间中的配置，默认情况下会应用到所有名字空间 spec: selector: matchLabels: istio: ingressgateway # 在带有这些 labels 的 ingressgateway/sidecar 上生效 jwtRules: # issuer 即签发者，需要和 JWT payload 中的 iss 属性完全一致。 - issuer: \"testing@secure.istio.io\" jwks: | { \"keys\": [ { \"e\": \"AQAB\", \"kid\": \"oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo\", # kid 需要与 jwt header 中的 kid 完全一致。 \"kty\": \"RSA\", \"n\": \"t1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw\" } ] } # jwks 或 jwksUri 二选其一 # jwksUri: \"http://nginx.test.local/istio/jwks.json\" 现在 kubectl apply 一下，JWT 验证就添加到全局了。 可以看到 jwtRules 是一个列表，因此可以为每个 issuers 配置不同的 jwtRule. 对同一个 issuers（jwt 签发者），可以通过 jwks 设置多个公钥，以实现JWT签名密钥的轮转。JWT 的验证规则是： JWT 的 payload 中有 issuer 属性，首先通过 issuer 匹配到对应的 istio 中配置的 jwks。 JWT 的 header 中有 kid 属性，第二步在 jwks 的公钥列表中，中找到 kid 相同的公钥。 使用找到的公钥进行 JWT 签名验证。 配置中的 spec.selector 可以省略，这样会直接在整个 namespace 中生效，而如果是在istio-system 名字空间，该配置将在全集群的所有 sidecar/ingressgateway 上生效！ ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:4","series":["云原生相关"],"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#5-启用-istio-的身份验证"},{"categories":["tech"],"content":" 6. 启用 Payload 转发/Authorization 转发默认情况下，Istio 在完成了身份验证之后，会去掉 Authorization 请求头再进行转发。这将导致我们的后端服务获取不到对应的 Payload，无法判断 End User 的身份。因此我们需要启用 Istio 的 Authorization 请求头的转发功能，在前述的 RequestAuthentication yaml 配置中添加一个参数就行： yaml apiVersion: \"security.istio.io/v1beta1\" kind: \"RequestAuthentication\" metadata: name: \"jwt-example\" namespace: istio-system spec: selector: matchLabels: istio: ingressgateway jwtRules: - issuer: \"testing@secure.istio.io\" jwks: | { \"keys\": [ { \"e\": \"AQAB\", \"kid\": \"oyYwZSLCLVVPHdVp0jXIcLNpGn6dMCumlY-6wSenmFo\", \"kty\": \"RSA\", \"n\": \"t1cKkQqPh8iOv5BhKh7Rx6A2-1ldpO_jczML_0GBKu4X-lHrY8YbJrt29jyAXlWM8vHC7tXsqgUG-WziRD0D8nhnh10XC14SeH-3mVuBqph-TqhXTWsh9gtAIbeUHJjEI4I79QK4_wquPHHIGZBQDQQnuMh6vAS3VaUYJdEIoKvUBnAyY35kJZgyJSbrxLsEExL2zujUD_OY-_In2bq_3rFtDGNlgHyC7Gu2zXSXvfOA4O5m9BBXOc7eEqj7PoOKNaTxLN3YcuRtgR6NIXL4KLb6oyvIzoeiprt4-9q7sc3Dnkc5EV9kwWlEW2DHzhP6VYca0WXIIXc53U1AM3ewxw\" } ] } # ===================== 添加如下参数=========================== forwardOriginalToken: true # 转发 Authorization 请求头 加了转发后，流程图如下（需要 mermaid 渲染）： sequenceDiagram; # autonumber participant User as 用户 participant Auth as 授权服务 participant IG as IngressGateway participant SVC as 某服务 User-\u003e\u003e+Auth: Login Auth-\u003e\u003eAuth: 用私钥生成 JWT 签名 Auth--\u003e\u003e-User: 返回 JWT User-\u003e\u003e+IG: 请求信息（带 JWT）IG-\u003e\u003eIG: 用公钥验证 JWT 签名 IG-\u003e\u003e-SVC: 请求信息（转发 JWT）SVC--\u003e\u003eIG: 返回信息 IG--\u003e\u003eUser: 返回信息 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:5","series":["云原生相关"],"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#6-启用-payload-转发authorization-转发"},{"categories":["tech"],"content":" 7. 设定强制认证规则Istio 的 JWT 验证规则，默认情况下会直接忽略不带 Authorization 请求头（即 JWT）的流量，因此这类流量能直接进入网格内部。通常这是没问题的，因为没有 Authorization 的流量即使进入到内部，也会因为无法通过 payload 判别身份而被拒绝操作。如果需要禁止不带 JWT 的流量，就需要额外配置 AuthorizationPolicy 策略。 比如拒绝任何 JWT 无效的请求（包括 Authorization 的情况）： yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: \"deny-requests-with-out-authorization\" namespace: istio-system spec: selector: matchLabels: istio: ingressgateway action: DENY # 拒绝 rules: - from: - source: notRequestPrincipals: [\"*\"] # 不存在任何请求身份（Principal）的 requests 如果仅希望强制要求对部分 path 的请求必须带有 Authorization Header，可以这样设置： yaml apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: \"deny-requests-with-out-authorization\" namespace: istio-system spec: selector: matchLabels: istio: ingressgateway action: DENY # 拒绝 rules: - from: - source: notRequestPrincipals: [\"*\"] # 不存在任何请求身份（Principal）的 requests # 仅强制要求如下 host/path 相关的请求，必须带上 JWT token to: - operation: hosts: [\"another-host.com\"] paths: [\"/headers\"] 注意这两个 Istio CR 返回的错误码是不同的： RequestsAuthentication 验证失败的请求，Istio 会返回 401 状态码。 AuthorizationPolicy 验证失败的请求，Istio 会返回 403 状态码。 这会导致在使用 AuthorizationPolicy 禁止了不带 Authorization 头的流量后，这类请求会直接被返回 403，在使用 RESTful API 时，这种情况可能会造成问题。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:6","series":["云原生相关"],"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#7-设定强制认证规则"},{"categories":["tech"],"content":" 8. Response HeadersRequestsAuthentication 不支持自定义响应头信息，这导致对于前后端分离的 Web API 而言，一旦 JWT 失效，Istio 会直接将 401 返回给前端 Web 页面。因为响应头中不包含Access-Crontrol-Allow-Origin，响应将被浏览器拦截！ 这可能需要通过 EnvoyFilter 自定义响应头，添加跨域信息。 ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:0:7","series":["云原生相关"],"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#8-response-headers"},{"categories":["tech"],"content":" 参考 JSON Web Key Set Properties - Auth0 JWK - RFC7517 Sample JWT and JWKS data for demo - Istio Security End User Authentication - Istio JWTRule - Istio jwt.io - 动态生成 jwt ","date":"2020-04-06","objectID":"/posts/use-istio-for-jwt-auth/:1:0","series":["云原生相关"],"tags":["Kubernetes","Istio","服务网格"],"title":"使用 Istio 进行 JWT 身份验证（充当 API 网关）","uri":"/posts/use-istio-for-jwt-auth/#参考"},{"categories":["tech"],"content":" 个人笔记，观点不一定正确. 适合对 Kubernetes 有一定了解的同学。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:0:0","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#"},{"categories":["tech"],"content":" 前言最近一直在学习 Kubernetes，但是手头没有个自有域名，要测试 ingress 就比较麻烦，每次都是手动改 hosts 文件。。 今天突然想到——K8s 内部就是用 DNS 做的服务发现，我为啥不自己弄一个 DNS 服务器呢？然后所有节点的 DNS 都配成它，这样有需要时直接改这个 DNS 服务器的配置就行， 一劳永逸。 我首先想到的是 群晖/Windows Server 自带的那种自带图形化界面的 DNS 服务器，但是这俩都是平台特定的。 网上搜一圈没找到类似带 UI 的 DNS 工具，搜到的 powerdns/bind 相比 coredns 也没看出啥优势来，所以决定就用 CoreDNS，刚好熟悉一下它的具体使用。 不过讲 CoreDNS 前，我们还是先来熟悉一下 DNS 的基础概念吧。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:1:0","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#前言"},{"categories":["tech"],"content":" 一、DNS 是个啥？ 没有写得很清楚，不适合初学。建议先通过别的资料熟悉下 DNS 基础。 DNS，即域名系统（Domain Name System），是一项负责将一个 human readable 的所谓域名，转换成一个 ip 地址的协议。 而域名的好处，有如下几项： 域名对人类更友好，可读的字符串比一串 ip 数字可好记多了。 一个域名可以对应多个 ip，可实现所谓的负载均衡。 多个域名可以对应同一个 ip，以不同的域名访问该 ip，能访问不同的应用。（通过 nginx 做代理实现） DNS 协议是一个基于 UDP 的应用层协议，它默认使用 53 端口进行通信。应用程序通常将 DNS 解析委派给操作系统的 DNS Resolver 来执行，程序员对它几乎无感知。 DNS 虽然说一般只用来查个 ip 地址，但是它提供的记录类型还蛮多的，主要有如下几种： A 记录：它记录域名与 IPv4 地址的对应关系。目前用的最多的 DNS 记录就是这个。 AAAA 记录：它对应的是 IPv6，可以理解成新一代的 A 记录。以后会用的越来越多的。 NS 记录：记录 DNS 域对应的权威服务器域名，权威服务器域名必须要有对应的 A 记录。 通过这个记录，可以将子域名的解析分配给别的 DNS 服务器。 CNAME 记录: 记录域名与另一个域名的对应关系，用于给域名起别名。这个用得也挺多的。 MX 记录：记录域名对应的邮件服务器域名，邮件服务器的域名必须要有对应的 A 记录。 SRV 记录：SRV 记录用于提供服务发现，看名字也能知道它和 SERVICE 有关。 SRV 记录的内容有固定格式：优先级 权重 端口 目标地址，例如0 5 5060 sipserver.example.com 主要用于企业域控(AD)、微服务发现（Kubernetes） 上述的所有 DNS 记录，都是属于将域名解析为 IP 地址，或者另一个域名，这被称做 DNS 正向解析。除了这个正向解析外，还有个非常冷门的反向解析，基本上只在设置邮件服务器时才会用到。（Kubernetes 可能也有用到） 反向解析主要的记录类型是：PTR 记录，它提供将 IP 地址反向解析为域名的功能。而且因为域名是从右往左读的（最右侧是根, www.baidu.com.），而 IP 的网段（如 192.168.0.0/16）刚好相反，是左边优先。因此 PTR 记录的“域名”必须将 IP 地址反着写，末尾再加上 .in-addr.arpa. 表示这是一个反向解析的域名。（ipv6 使用 ip6.arpa.）拿 baidu.com 的邮件服务器测试一下： PTR 记录查询 其他还有些 TXT、CAA 等奇奇怪怪的记录，就用到的时候自己再查了。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:2:0","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#一dns-是个啥"},{"categories":["tech"],"content":" 二、域名的分层结构国际域名系统被分成四层： 根域（Root Zone）：所有域名的根。 根域名服务器负责解析顶级域名，给出顶级域名的 DNS 服务器地址。 全世界仅有十三组根域名服务器，这些服务器的 ip 地址基本不会变动。 它的域名是 “\"，空字符串。而它的**全限定域名（FQDN）**是 .，因为 FQDN 总是以 . 结尾。（FQDN 在后面解释，可暂时忽略） 顶级域（Top Level Domains, TLD）：.com .cn 等国际、国家级的域名 顶级域名服务器负责解析次级域名，给出次级域名的 DNS 服务器地址。 每个顶级域名都对应各自的服务器，它们之间是完全独立的。.cn 的域名解析仅由 .cn 顶级域名服务器提供。 目前国际 DNS 系统中已有上千个 TLD，包括中文「.我爱你」甚至藏文域名，详细列表参见IANA TLD 数据库 除了国际可用的 TLD，还有一类类似「内网 IP 地址」的“私有 TLD”，最常见的比如 xxx.local xxx.lan，被广泛用在集群通信中。后面详细介绍 次级域（Second Level Domains）：这个才是个人/企业能够买到的域名，比如 baidu.com 每个次级域名都有一到多个权威 DNS 服务器，这些 DNS 服务器会以 NS 记录的形式保存在对应的顶级域名（TLD）服务器中。 权威域名服务器则负责给出最终的解析结果：ip 地址(A 记录 )，另一个域名（CNAME 记录）、另一个 DNS 服务器（NS 记录）等。 子域（Sub Domains）：*.baidu.com 统统都是 baidu.com 的子域。 每一个子域都可以有自己独立的权威 DNS 服务器，这通过在子域中添加 NS 记录实现。 普通用户通常是通过域名提供商如阿里云购买的次级域名，接下来我们以 rea.ink 为例介绍域名的购买到可用的整个流程。 域名的购买与使用流程： 你在某域名提供商处购买了一个域名 rea.ink 域名提供商向 .ink 对应的顶级域名服务器中插入一条以上的 NS 记录，指向它自己的次级 DNS 服务器，如 dns25.hichina.com. 阿里云会向 TLD 中插入几条 NS 记录，指向阿里云的次级 DNS 服务器（如vip1.alidns.com）。 你在该域名提供商的 DNS 管理界面中添加 A 记录，值为你的服务器 IP。 OK 现在 ping 一下 rea.ink，就会发现它已经解析到你自己的服务器了。 上述流程中忽略了我大天朝的特殊国情——备案，勿介意。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:3:0","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#二域名的分层结构"},{"categories":["tech"],"content":" 三、DNS 递归解析器：在浏览器中输入域名后发生了什么？下面的图片拷贝自 Amazon Aws 文档，它展示了在不考虑任何 DNS 缓存的情况下，一次 Web 请求的经过，详细描绘了 DNS 解析的部分。 DNS 解析流程 其中的第 3 4 5 步按顺序向前面讲过的根域名服务器、顶级域名服务器、权威域名服务器发起请求， 以获得下一个 DNS 服务器的信息。这很清晰。 图中当前还没介绍的部分，是紫色的 DNS Resolver(域名解析器)，也叫Recursive DNS resolver（DNS 递归解析器）。它本身只负责递归地请求 3 4 5 步中的上游服务器，然后把获取的最终结果返回给客户端，同时将记录缓存到本地以加快解析速度。 这个 DNS 解析器，其实就是所谓的公共 DNS 服务器：Google 的 8.8.8.8，国内著名的114.114.114.114。 这些公共 DNS 用户量大，缓存了大量的 DNS 记录，有效地降低了上游 DNS 服务器的压力，也加快了网络上的 DNS 查询速度。 接下来使用 dig +trace baidu.com 复现一下上述的查询流程（这种情况下 dig 自己就是一个 DNS 递归解析器）： dig +trace baidu.com 另外前面有讲过 DNS 的反向解析，也是同样的层级结构，是从根服务器开始往下查询的，下面拿 baidu 的一个邮件服务器进行测试： 反向解析 dig 工具未来可能会被 drill 取代。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:4:0","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#三dns-递归解析器在浏览器中输入域名后发生了什么"},{"categories":["tech"],"content":" DNS 泛解析通配符 *DNS 记录允许使用通配符 *，但仅支持同级匹配，也就是说 * 不能匹配域名中的 . 这个字符。 举例来说，*.aliyun.com 能用于 xx.aliyun.com 和 abc.aliyun.com，但不能用于aliyun.com, a.b.aliyun.com 或 bb.xx.aliyun.com. ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:4:1","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#dns-泛解析通配符-"},{"categories":["tech"],"content":" TTL（Time To Live）上面讲了公共 DNS 服务器通过缓存技术，降低了上游 DNS 服务器的压力，也加快了网络上的 DNS 查询速度。 可缓存总得有个过期时间吧！为了精确地控制 DNS 记录的过期时间，每条 DNS 记录都要求设置一个时间属性——TTL，单位为秒。这个时间可以自定义。 任何一条 DNS 缓存，在超过过期时间后都必须丢弃！另外在没超时的时候，DNS 缓存也可以被主动或者被动地刷新。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:4:2","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#ttltime-to-live"},{"categories":["tech"],"content":" 四、本地 DNS 服务器与私有 DNS 域这类服务器只在当前局域网内有效，是一个私有的 DNS 服务器，企业常用。一般通过 DHCP 或者手动配置的方式，使内网的服务器都默认使用局域网 DNS 服务器进行解析。该服务器可以只解析自己的私有 DNS 域，而将其他 DNS 域的解析 forward 到公网 DNS 解析器去。 这个私有 DNS 域，会覆盖掉公网的同名域(如果公网上有这个域的话)。私有 dns 域也可使用公网不存在的 TLD，比如 xxx.local xxx.lan 等。vmware vcenter 就默认使用 vsphere.local 作为它的 sso (单点登录)系统的域名。kubernetes 默认使用 svc.cluster.local 作为集群内部域名。 私有 DNS 域的选择，参见DNS 私有域的选择：internal.xxx.com xxx.local 还是 xxx.zone？ 局域网 DNS 服务器的规模与层级，视局域网的大小而定。一般小公司一个就行，要容灾设三个副本也够了。 以 CoreDNS 为例，局域网 DNS 服务器也可以被设置成一个 DNS Resolver，可以设置只转发特定域名的 DNS 解析。这叫将某个域设为「转发区域」。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:5:0","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#四本地-dns-服务器与私有-dns-域"},{"categories":["tech"],"content":" 五、操作系统的 DNS 解析器应用程序实际上都是调用的操作系统的 DNS Resolver 进行域名解析的。在 Linux 中 DNS Resolver 由 glibc/musl 提供，配置文件为 /etc/resolv.conf。 比如 Python 的 DNS 解析，就来自于标准库的 socket 包，这个包只是对底层 c 语言库的一个简单封装。 基本上只有专门用于网络诊断的 DNS 工具包，才会自己实现 DNS 协议。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:0","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#五操作系统的-dns-解析器"},{"categories":["tech"],"content":" 1. hosts 文件操作系统中还有一个特殊文件：Linux 中的 /etc/hosts 和 Windows 中的C:\\Windows\\System32\\drivers\\etc\\hosts 系统中的 DNS resolver 会首先查看这个 hosts 文件中有没有该域名的记录，如果有就直接返回了。没找到才会去查找本地 DNS 缓存、别的 DNS 服务器。 只有部分专门用于网络诊断的应用程序（e.g. dig）不会依赖 OS 的 DNS 解析器，因此这个 hosts 会失效。hosts 对于绝大部分程序都有效。 移动设备上 hosts 可能会失效，部分 app 会绕过系统，使用新兴的 HTTPDNS 协议进行 DNS 解析。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:1","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#1-hosts-文件"},{"categories":["tech"],"content":" 2. HTTPDNS传统的 DNS 协议因为使用了明文的 UDP 协议，很容易被劫持。顺应移动互联网的兴起，目前一种新型的 DNS 协议——HTTPDNS 应用越来越广泛，国内的阿里云腾讯云都提供了这项功能。 HTTPDNS 通过 HTTP 协议直接向权威 DNS 服务器发起请求，绕过了一堆中间的 DNS 递归解析器。好处有二： 权威 DNS 服务器能直接获取到客户端的真实 IP（而不是某个中间 DNS 递归解析器的 IP），能实现就近调度。 因为是直接与权威 DNS 服务器连接，避免了 DNS 缓存污染的问题。 HTTPDNS 协议需要程序自己引入 SDK，或者直接请求 HTTP API。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:2","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#2-httpdns"},{"categories":["tech"],"content":" 3. 默认 DNS 服务器操作系统的 DNS 解析器通常会允许我们配置多个上游 Name Servers，比如 Linux 就是通过/etc/resolv.conf 配置 DNS 服务器的。 conf $ cat /etc/resolv.conf nameserver 8.8.8.8 nameserver 8.8.4.4 search lan 不过现在这个文件基本不会手动修改了，各 Linux 发行版都推出了自己的网络配置工具，由这些工具自动生成 Linux 的各种网络配置，更方便。比如 Ubuntu 就推荐使用 netplan 工具进行网络设置。 Kubernetes 就是通过使用容器卷映射的功能，修改 /etc/resolv.conf，使集群的所有容器都使用集群 DNS 服务器（CoreDNS）进行 DNS 解析。 通过重复使用 nameserver 字段，可以指定多个 DNS 服务器（Linux 最多三个）。DNS 查询会按配置中的顺序选用 DNS 服务器。 仅在靠前的 DNS 服务器没有响应（timeout）时，才会使用后续的 DNS 服务器！所以指定的服务器中的 DNS 记录最好完全一致！！！不要把第一个配内网 DNS，第二个配外网！！！ ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:3","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#3-默认-dns-服务器"},{"categories":["tech"],"content":" 4. DNS 搜索域上一小节给出的 /etc/resolv.conf 文件内容的末尾，有这样一行: search lan，它指定的，是所谓的 DNS 搜索域。 讲到 DNS 搜索域，就不得不提到一个名词：全限定域名（Full Qulified Domain Name, FQDN），即一个域名的完整名称，www.baidu.com。 一个普通的域名，有下列四种可能： www.baidu.com.: 末尾的 . 表示根域，说明 www.baidu.com 是一个 FQDN，因此不会使用搜索域！ www.baidu.com: 末尾没 .，但是域名包含不止一个 .。首先当作 FQDN 进行查询，没查找再按顺序在各搜索域中查询。 /etc/resolv.conf 的 options 参数中，可以指定域名中包含 . 的临界个数，默认是 1. local: 不包含 .，被当作 host 名称，非 FQDN。首先在 /etc/hosts 中查找，没找到的话，再按顺序在各搜索域中查找。 上述搜索顺序可以通过 host -v \u003cdomain-name\u003e 进行测试，该命令会输出它尝试过的所有 FQDN。修改 /etc/resolv.conf 中的 search 属性并测试，然后查看输出。 就如上面说例举的，在没有 DNS 搜索域 这个东西的条件下，我们访问任何域名，都必须输入一个全限定域名 FQDN。有了搜索域我们就可以稍微偷点懒，省略掉域名的一部分后缀，让 DNS Resolver 自己去在各搜索域中搜索。 在 Kubernetes 中就使用到了搜索域，k8s 中默认的域名 FQDN 是service.namespace.svc.cluster.local，但是对于 default namespace 中的 service，我们可以直接通过 service 名称查询到它的 IP。对于其他名字空间中的 service，也可以通过service.namespace 查询到它们的 IP，不需要给出 FQDN。 Kubernetes 中 /etc/resolv.conf 的示例如下： conf nameserver 10.43.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 可以看到 k8s 设置了一系列的搜索域，并且将 . 的临界值设为了 5。也就是少于 5 个 dots 的域名，都首先当作非 FQDN 看待，优先在搜索域里面查找。 该配置文件的详细描述参见manpage - resolv.conf，或者在 Linux 中使用 man resolv.conf 命令查看。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:6:4","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#4-dns-搜索域"},{"categories":["tech"],"content":" 六、DNS 诊断的命令行工具 shell dig +trace baidu.com # 诊断 dns 的主要工具，非常强大 host -a baidu.com # host 基本就是 dig 的弱化版，不过 host 有个有点就是能打印出它测试过的所有 FQDN nslookup baidu.com # 和 host 没啥大差别，多个交互式查询不过一般用不到 whois baidu.com # 查询域名注册信息，内网诊断用不到 详细的使用请 man dig ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:7:0","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#六dns-诊断的命令行工具"},{"categories":["tech"],"content":" 七、CoreDNS 的使用主流的本地 DNS 服务器中，提供 UI 界面的有 Windows DNS Server 和群晖 DNS Server，很方便，不过这两个都是操作系统绑定的。 开源的 DNS 服务器里边儿，BIND 好像是最有名的，各大 Linux 发行版自带的dig/host/nslookup，最初都是 Bind 提供的命令行工具。不过为了一举两得（DNS+K8s），咱还是直接学习 CoreDNS 的使用。 CoreDNS 最大的特点是灵活，可以很方便地给它编写插件以提供新功能。功能非常强大，相比传统 DNS 服务器，它非常“现代化”。在 K8s 中它被用于提供服务发现功能。 接下来以 CoreDNS 为例，讲述如何配置一个 DNS 服务器，添加私有的 DNS 记录，并设置转发规则以解析公网域名。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:0","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#七coredns-的使用"},{"categories":["tech"],"content":" 1. 配置文件：CorefileCoreDNS 因为是 Go 语言写的，编译结果是单个可执行文件，它默认以当前文件夹下的 Corefile 为配置文件。以 kubernetes 中的 Corefile 为例： corefile .:53 { errors # 启用错误日志 health # 启用健康检查 api ready # 启用 readiness 就绪 api # 启用 kubernetes 集群支持，详见 https://coredns.io/plugins/kubernetes/ # 此插件只处理 cluster.local 域，以及 PTR 解析 kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream # fallthrough in-addr.arpa ip6.arpa # 向下传递 DNS 反向查询 ttl 30 # 过期时间 } prometheus :9153 # 启用 prometheus metrics 支持 forward . 114.114.114.114 19.29.29.29 # 将非集群域名的 DNS 请求，转发给公网 DNS 服务器。 cache 30 # 启用前端缓存，缓存的 TTL 设为 30 loop # 检测并停止死循环解析 reload # 支持动态更新 Corefile # 随机化 A/AAAA/MX 记录的顺序以实现负载均衡。 # 因为 DNS resolver 通常使用第一条记录，而第一条记录是随机的。这样客户端的请求就能被随机分配到多个后端。 loadbalance } Corefile 首先定义 DNS 域，域后的代码块内定义需要使用的各种插件。注意这里的插件顺序是没有任何意义的！插件的调用链是在 CoreDNS 编译时就定义好的，不能在运行时更改。 通过上述配置启动的 CoreDNS 是无状态的，它以 Kubernetes ApiServer 为数据源，CoreDNS 本身只相当于一个查询器/缓存，因此它可以很方便地扩缩容。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:1","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#1-配置文件corefile"},{"categories":["tech"],"content":" 2. 将 CoreDNS 设置成一个私有 DNS 服务器现在清楚了 Corefile 的结构，让我们来设计一个通过文件配置 DNS 条目的 Corefile 配置： corefile # 定义可复用 Block (common) { log errors cache loop # 检测并停止死循环解析 } # 本地开发环境的 DNS 解析 dev-env.local:53 { import common # 导入 Block file dev-env.local { # 从文件 `dev-env.local` 中读取 DNS 数据 reload 30s # 每 30s 检查一次配置的 Serial，若该值有变更则重载整个 Zone 的配置。 } } # 本地测试环境 test-env.local:53 { import common file test-env.local { reload 30s } } # 其他 .:53 { forward . 114.114.114.114 # 解析公网域名 log errors cache } 上面的 Corefile 定义了两个本地域名 dev-env.local 和 test-env.local，它们的 DNS 数据分别保存在 file 指定的文件中。 这个 file 指定的文件和 bind9 一样，都是使用在rfc1035 中定义的 Master File 格式，dig 命令输出的就是这种格式的内容。示例如下： text ;; 與整個領域相關性較高的設定包括 NS, A, MX, SOA 等標誌的設定處！ $TTL 30 @ IN SOA dev-env.local. devops.dev-env.local. ( 20200202 ; SERIAL，每次修改此文件，都应该同步修改这个“版本号”，可将它设为修改时间。 7200 ; REFRESH 600 ; RETRY 3600000 ; EXPIRE 60) ; MINIMUM @ IN NS dns1.dev-env.local. ; DNS 伺服器名稱 dns1.dev-env.local. IN A 192.168.23.2 ; DNS 伺服器 IP redis.dev-env.local. IN A 192.168.23.21 mysql.dev-env.local. IN A 192.168.23.22 elasticsearch.dev-env.local. IN A 192.168.23.23 ftp IN A 192.168.23.25 ; 這是簡化的寫法！ 详细的格式说明参见鳥哥的 Linux 私房菜 - DNS 正解資料庫檔案的設定 test-env.local 也是一样的格式，根据上面的模板修改就行。这两个配置文件和 Corefile 放在同一个目录下： text root@test-ubuntu:~/dns-server# tree . ├── coredns # coredns binary ├── Corefile ├── dev-env.local └── test-env.local 然后通过 ./coredns 启动 coredns。通过 dig 检验： DNS 测试 可以看到 ftp.dev-env.local 已经被成功解析了。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:2","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#2-将-coredns-设置成一个私有-dns-服务器"},{"categories":["tech"],"content":" 3. 可选插件（External Plugins）CoreDNS 提供的预编译版本，不包含 External Plugins 中列出的部分，如果你需要，可以自行修改 plugin.cfg，然后手动编译。 不得不说 Go 语言的编译，比 C 语言是方便太多了。自动拉取依赖，一行命令编译！只要配好GOPROXY，启用可选插件其实相当简单。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:3","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#3-可选插件external-plugins"},{"categories":["tech"],"content":" 4. 设置 DNS 集群单台 DNS 服务器的性能是有限的，而且存在单点故障问题。因此在要求高可用或者高性能的情况下， 就需要设置 DNS 集群。 虽然说 CoreDNS 本身也支持各种 DNS Zone 传输，主从 DNS 服务器等功能，不过我想最简单的，可能还是直接用 K8s。 直接用 ConfigMap 存配置，通过 Deployment 扩容就行，多方便。 要修改起来更方便，还可以启用可选插件：redis，直接把配置以 json 的形式存在 redis 里，通过 redis-desktop-manager 进行查看与修改。 ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:8:4","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#4-设置-dns-集群"},{"categories":["tech"],"content":" 参考 DNS 原理入门 What Is DNS? | How DNS Works - Cloudflare What is DNS? - Amazon AWS 鸟哥的 Linux 私房菜——主機名稱控制者： DNS 伺服器 CoreDNS - Manual Kubernetes - DNS for Services and Pods Kubernetes - Customizing DNS Service ","date":"2020-03-29","objectID":"/posts/about-dns-protocol/:9:0","series":["计算机网络相关"],"tags":["DNS","CoreDNS","网络"],"title":"Linux网络学习笔记（二）：域名解析(DNS)——以 CoreDNS 为例","uri":"/posts/about-dns-protocol/#参考"},{"categories":["tech"],"content":" 签名算法介绍具体的 JWT 签名算法前，先解释一下签名、摘要/指纹、加密这几个名词的含义： 数字签名(Digital Signature): 就和我们日常办理各种手续时需要在文件上签上你自己的名字一样，数字签名的主要用途也是用于身份认证。 更准确的讲，数字签名可保证数据的三个特性：真实性（未被伪造）、完整性（不存在缺失）、不可否认性（确实是由你本人认可并签名） 数字摘要(digest)/数字指纹(fingerprint): 指的是数据的 Hash 值。 加密算法：这个应该不需要解释，就是对数据进行加密。。 数字签名的具体实现，通常是先对数据进行一次 Hash 摘要(SHA1/SHA256/SHA512 等)，然后再使用非对称加密算法(RSA/ECDSA 等)的私钥对这个摘要进行加密，这样得到的结果就是原始数据的一个签名。 用户在验证数据时，只需要使用公钥解密出 Hash 摘要，然后自己再对数据进行一次同样的摘要，对比两个摘要是否相同即可。 注意：签名算法是使用私钥加密，确保得到的签名无法被伪造，同时所有人都可以使用公钥解密来验证签名。这和正常的数据加密算法是相反的。（非对称加密算法支持使用密钥对的任何一个加密数据，再用另一个密钥解密） 因为数字签名多了非对称加密这一步，就能保证只有拥有私钥的人才能生成出正确的数字签名，达到了防止伪造签名的目的。而数字摘要（Hash）则谁都可以计算出来，通常由可信方公布数据的 Hash 值，用户下载数据后，可通过 Hash 值对比来判断数据是否损坏，或者被人调包。 重点在于，Hash 摘要必须由可信方公布出来，否则不能保证安全性。而数字签名可以随数据一起提供，不需要担心被伪造。 JWT 是签名和数据一起提供的，因此必须使用签名才能保证安全性。 P.S. 在 Android/IOS 开发中，经常会遇到各类 API 或者 APP 商店要求提供 APP 的签名，还指明需要的是 MD5/SHA1 值。这个地方需要填的 MD5/SHA1 值，实际上只是你「签名证书(=公钥+证书拥有者信息)」的「数字指纹/摘要」，和 JWT 的签名不是一回事。 ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:0:0","series":["写给开发人员的实用密码学"],"tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/#签名算法"},{"categories":["tech"],"content":" 前言JWT 规范的详细说明请见「参考」部分的链接。这里主要说明一下 JWT 最常见的几种签名算法 (JWA)：HS256(HMAC-SHA256) 、RS256(RSA-SHA256) 还有 ES256(ECDSA-SHA256)。 这三种算法都是一种消息签名算法，得到的都只是一段无法还原的签名。区别在于消息签名与签名验证需要的 「key」不同。 HS256 使用同一个「secret_key」进行签名与验证（对称加密）。一旦 secret_key 泄漏，就毫无安全性可言了。 因此 HS256 只适合集中式认证，签名和验证都必须由可信方进行。 传统的单体应用广泛使用这种算法，但是请不要在任何分布式的架构中使用它！ RS256 是使用 RSA 私钥进行签名，使用 RSA 公钥进行验证。公钥即使泄漏也毫无影响，只要确保私钥安全就行。 RS256 可以将验证委托给其他应用，只要将公钥给他们就行。 ES256 和 RS256 一样，都使用私钥签名，公钥验证。算法速度上差距也不大，但是它的签名长度相对短很多（省流量），并且算法强度和 RS256 差不多。 对于单体应用而言，HS256 和 RS256 的安全性没有多大差别。而对于需要进行多方验证的微服务架构而言，显然只有 RS256/ES256 才能提供足够的安全性。在使用 RS256 时，只有「身份认证的微服务 (auth)」需要用 RSA 私钥生成 JWT，其他微服务使用公开的公钥即可进行签名验证，私钥得到了更好的保护。 更进一步，「JWT 生成」和「JWT 公钥分发」都可以直接委托给第三方的通用工具，比如hydra。甚至「JWT 验证」也可以委托给「API 网关」来处理，应用自身可以把认证鉴权完全委托给外部的平台，而应用自身只需要专注于业务。这也是目前的发展趋势。 RFC 7518 - JSON Web Algorithms (JWA) 中给出的 JWT 算法列表如下： +--------------+-------------------------------+--------------------+ | \"alg\" Param | Digital Signature or MAC | Implementation | | Value | Algorithm | Requirements | +--------------+-------------------------------+--------------------+ | HS256 | HMAC using SHA-256 | Required | | HS384 | HMAC using SHA-384 | Optional | | HS512 | HMAC using SHA-512 | Optional | | RS256 | RSASSA-PKCS1-v1_5 using | Recommended | | | SHA-256 | | | RS384 | RSASSA-PKCS1-v1_5 using | Optional | | | SHA-384 | | | RS512 | RSASSA-PKCS1-v1_5 using | Optional | | | SHA-512 | | | ES256 | ECDSA using P-256 and SHA-256 | Recommended+ | | ES384 | ECDSA using P-384 and SHA-384 | Optional | | ES512 | ECDSA using P-521 and SHA-512 | Optional | | PS256 | RSASSA-PSS using SHA-256 and | Optional | | | MGF1 with SHA-256 | | | PS384 | RSASSA-PSS using SHA-384 and | Optional | | | MGF1 with SHA-384 | | | PS512 | RSASSA-PSS using SHA-512 and | Optional | | | MGF1 with SHA-512 | | | none | No digital signature or MAC | Optional | | | performed | | +--------------+-------------------------------+--------------------+ The use of \"+\" in the Implementation Requirements column indicates that the requirement strength is likely to be increased in a future version of the specification. 目前应该所有 jwt 相关的库都支持 HS256/RS256/ES256 这三种算法。 ES256 使用 ECDSA 进行签名，它的安全性和运算速度目前和 RS256 差距不大，但是拥有更短的签名长度。对于需要频繁发送的 JWT 而言，更短的长度长期下来可以节约大量流量。 因此更推荐使用 ES256 算法。 ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:0:0","series":["写给开发人员的实用密码学"],"tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/#前言"},{"categories":["tech"],"content":" 使用 OpenSSL 生成 RSA/ECC 公私钥RS256 使用 RSA 算法进行签名，可通过如下命令生成 RSA 密钥： shell # 1. 生成 2048 位（不是 256 位）的 RSA 密钥 openssl genrsa -out rsa-private-key.pem 2048 # 2. 通过密钥生成公钥 openssl rsa -in rsa-private-key.pem -pubout -out rsa-public-key.pem ES256 使用 ECDSA 算法进行签名，该算法使用 ECC 密钥，生成命令如下： shell # 1. 生成 ec 算法的私钥，使用 prime256v1 曲线（NIST P-256 标准），密钥长度 256 位。（强度大于 2048 位的 RSA 密钥） openssl ecparam -genkey -name prime256v1 -out ecc-private-key.pem # 2. 通过密钥生成公钥 openssl ec -in ecc-private-key.pem -pubout -out ecc-public-key.pem 密钥的使用应该就不需要介绍了，各类语言都有对应 JWT 库处理这些，请自行查看文档。 如果是调试/学习 JWT，需要手动签名与验证的话，推荐使用jwt 工具网站 - jwt.io ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:1:0","series":["写给开发人员的实用密码学"],"tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/#使用-openssl-生成-rsaecc-公私钥"},{"categories":["tech"],"content":" 参考 RFC 7518 - JSON Web Algorithms (JWA) 什么是 JWT – JSON WEB TOKEN jwt 工具网站 - jwt.io JWT 算法比较 ","date":"2020-03-03","objectID":"/posts/jwt-algorithm-key-generation/:2:0","series":["写给开发人员的实用密码学"],"tags":["JWT","算法","OpenSSL"],"title":"JWT 签名算法 HS256、RS256 及 ES256 及密钥生成","uri":"/posts/jwt-algorithm-key-generation/#参考"},{"categories":["life","tech"],"content":" 迟到的年终总结 ","date":"2020-01-31","objectID":"/posts/2019-summary/:0:0","series":["年终总结"],"tags":["总结"],"title":"2019 年年终总结","uri":"/posts/2019-summary/#"},{"categories":["life","tech"],"content":" 闲言碎语我是今年六月底到的深圳，运气很好，第一面就面上了现在所在的公司，以下就叫它 W 公司吧。公司的技术栈也很适合我，在入职到现在的这半年里，我学到了不少知识。 但是运气也差，只有这么一家公司约了我面试，投的其他简历都石沉大海… 总之，今年尝试参加过两次技术分享，Rancher 的技术沙龙，前几天又去听了 OSChina 的源创会。 ","date":"2020-01-31","objectID":"/posts/2019-summary/:1:0","series":["年终总结"],"tags":["总结"],"title":"2019 年年终总结","uri":"/posts/2019-summary/#闲言碎语"},{"categories":["life","tech"],"content":" 技术能力总结我入职后做的是运维开发，主要负责通过 Jenkins Pipeline + Python 进行自动化的测试、构建和部署： 测试：指 UI 测试、API 测试、压力测试。单元测试算在构建流程中。 构建：更新依赖-\u003e单元测试-\u003e构建 Library 或镜像 公司的内部代码使用分层结构，底层封装了各种第三方包，并实现了一些通用的功能，形成了所谓的中台。目前是通过批量任务逐级自下向上构建。 部署：扫描镜像仓库中各镜像，生成最新的 k8s 部署文件，然后进行部署。 所以这半年中，我差不多熟悉了自动化运维的工作。主要包括 Jenkins Pipeline 的编写，我们基本都是使用 Jenkins 调用 Python 代码来进行具体的构建。 公司的构建有很多自己特殊的需求，Jenkins 自带的插件无法满足。 熟悉了 Python 的 subprocess 库，为了远程调用，又熟悉了 fabric（当作 library 用）。 做压测时，熟悉了 locust 因为基本都是通过命令行进行测试、构建，我现在比前后端组还熟悉 csharp/flutter/golang 的 cli… 学会了 Dockerfile 语法。我们的后端全部都是以容器方式部署的，这个是基本技能。 熟悉并且用上了 Kubernetes. 这东西基本上就是未来了，也将是我的主攻方向。 但是，也存在一些问题： 对 Linux/网络/vSphere 不够了解，导致每次处理这类问题只会排除法。 对监控/日志/告警不够了解，监控面板一堆参数却看不出问题，日志不知道怎么用 kibana 进行搜索，告警还没配过。。 解决问题的能力还有待提升，考虑总是不够全面，老是出问题。（不能让人放心） 总是想得太多，拖慢了解决问题的速度。（这倒也不能完全算是缺点。） ","date":"2020-01-31","objectID":"/posts/2019-summary/:2:0","series":["年终总结"],"tags":["总结"],"title":"2019 年年终总结","uri":"/posts/2019-summary/#技术能力总结"},{"categories":["life","tech"],"content":" 今年在技术上的感受 Kubernetes 和云原生正在席卷整个互联网/物联网行业。 Kubernetes 目前主要用于 Stateless 应用，那后端的 数据库/缓存/消息服务 要如何做分布式呢？这也是大家关注的重点。 分布式、微服务模式下的监控(prometheus)、日志分析(elk)、安全、链路追踪(jaeger)，是运维关注的重点。 服务网格正在走向成熟，Istio 很值得学习和试用。 开源的分布式数据库/云数据库成为越来越多企业的选择，开源的 TiDB（HTAP）和阿里云的 PolarDB（计算存储分离）都应该了解了解。 Transaction Processing: 面向交易，数据的变动(增删改)多，涉及的数据量和计算量(查) 少，实时性要求高。 Analytical Processing：面向分析，数据的变动少，但涉及的数据量和计算量很多！ HTAP（Hybrid transaction/analytical processing）：混合型数据库，可同时被用于上述两种场景。 Knative/Jenkins-X 这类 Serverless 的 CI/CD 也正在快速发展，需要深入调研。 ","date":"2020-01-31","objectID":"/posts/2019-summary/:3:0","series":["年终总结"],"tags":["总结"],"title":"2019 年年终总结","uri":"/posts/2019-summary/#今年在技术上的感受"},{"categories":["life","tech"],"content":" 明年的展望作为一名萌新运维开发，明年显然还要继续在这条路上继续向前。 我明年的任务，第一件，就是优化掉部分自己目前存在的问题（前面有提到），第二呢，就是紧跟技术潮流。重点有下面几项： 充实自己网络部分欠缺的知识，尤其是 DNS 解析(CoreDNS)和 NAT(iptables)这俩玩意儿。 学习数据库组件的使用和性能调优：MySQL/Redis/ElasticSearch/MongoDB，另外熟悉 PostgreSQL 和分布式数据库 TiDB/Vitess Kubernetes/Istio Kubernetes 上的 CI/CD：Knative,Istio-GitOps 监控告警：Prometheus/Grafana 总结一套故障排除的方法论：网络故障、CPU/RAM/Disk 性能异常等、应用故障等。 最重要的任务，是维护公司这一套微服务在阿里云上的正常运行，积累经验。 关注 CNCF 蓝图 上的各项新技术。 另外呢，就是开发方面的任务： 设计模式应该要学学了！ Python 不能止步于此，要制定源码学习计划。 学习 C# 语言，阅读公司的源码，熟悉企业级的业务代码。 学习 go 语言，用于 DevOps。（其实还想学 rust，不过明年可能没时间） 要把 xhup 那个项目完成，也不知道能不能抽出时间。。 ","date":"2020-01-31","objectID":"/posts/2019-summary/:4:0","series":["年终总结"],"tags":["总结"],"title":"2019 年年终总结","uri":"/posts/2019-summary/#明年的展望"},{"categories":["tech"],"content":" Pod 常见错误 OOMKilled: Pod 的内存使用超出了 resources.limits 中的限制，被强制杀死。 SandboxChanged: Pod sandbox changed, it will be killed and re-created: 很可能是由于内存限制导致容器被 OOMKilled，或者其他资源不足 如果是 OOM，容器通常会被重启，kubectl describe 能看到容器上次被重启的原因State.Last State.Reason = OOMKilled, Exit Code=137. Pod 不断被重启，kubectl describe 显示重启原因State.Last State.Reason = Error, Exit Code=137，137 对应 SIGKILL(kill -9) 信号，说明容器被强制重启。可能的原因： 最有可能的原因是，存活探针（livenessProbe）检查失败 节点资源不足，内核强制关闭了进程以释放资源，这种情况可以通过 journalctl -k 查看详细的系统日志。 CrashLoopBackoff: Pod 进入 崩溃-重启循环，重启间隔时间从 10 20 40 80 一直翻倍到上限 300 秒，然后以 300 秒为间隔无限重启。 Pod 一直 Pending: 这说明没有任何节点能满足 Pod 的要求，容器无法被调度。比如端口被别的容器用 hostPort 占用，节点有污点等。 FailedCreateSandBox: Failed create pod sandbox: rpc error: code = DeadlineExceeded desc = context deadline exceeded： 很可能是 CNI 网络插件的问题（比如 ip 地址溢出）， FailedSync: error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded: 常和前两个错误先后出现，很可能是 CNI 网络插件的问题。 开发集群，一次性部署所有服务时，各 Pod 互相争抢资源，导致 Pod 生存探针失败，不断重启， 重启进一步加重资源使用。恶性循环。 需要给每个 Pod 加上 resources.requests，这样资源不足时，后续 Pod 会停止调度，直到资源恢复正常。 Pod 出现大量的 Failed 记录，Deployment 一直重复建立 Pod: 通过kubectl describe/edit pod \u003cpod-name\u003e 查看 pod Events 和 Status，一般会看到失败信息，如节点异常导致 Pod 被驱逐。 Kubernetes 问题排查：Pod 状态一直 Terminating 创建了 Deployment 后，却没有自动创建 Pod: 缺少某些创建 Pod 必要的东西，比如设定的 ServiceAccount 不存在。 Pod 运行失败，状态为 MatchNodeSelector: 对主节点进行关机、迁移等操作，导致主调度器下线时，会在一段时间内导致 Pod 调度失败，调度失败会报这个错。 Pod 仍然存在，但是 Service 的 Endpoints 却为空，找不到对应的 Pod IPs: 遇到过一次，是因为时间跳变（从未来的时间改回了当前时间）导致的问题。 Pod 无法调度，报错 x node(s) had volume node affinity conflict: 说明该 pod 所绑定的 PV 有 nodeAffinity 无法满足，可以 check 对应的 PV yaml. 通常原因是 PV 所在的可用区，没有可用的节点，导致 Pod 无法调度。 最简单的解决方法是，在对应的可用区补充节点 如果数据可以丢，也可以考虑直接删除重建 PV/PVC ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:1:0","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#pod-常见错误"},{"categories":["tech"],"content":" 控制面故障可能会导致各类奇怪的异常现象对于生产环境的集群，因为有高可用，通常我们比较少遇到控制面故障问题。但是一旦控制面发生故障，就可能会导致各类奇怪的异常现象。如果能在排查问题时，把控制面异常考虑进来，在这种情况下，就能节约大量的排查时间，快速定位到问题。 其中比较隐晦的就是 controller-manager 故障导致的异常： 节点的服务器已经被终止，但是 Kubernetes 里还显示 node 为 Ready 状态，不会更新为 NotReady. 被删除的 Pods 可能会卡在 Terminating 状态，只有强制删除才能删除掉它们。并且确认 Pod 没有 metadata.finalizers 属性 HPA 的动态伸缩功能失效 … 如果这些现象同时发生，就要怀疑是否是 kube-controller-manager 出问题了. 最简单的排查命令： bash $ kubectl get componentstatuses Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy ok # 类似这种错误 $ kubectl events --types=Warning -n your-namespace LAST SEEN TYPE REASON OBJECT MESSAGE 56m Warning UpdateLoadBalancerFailed Service/xxx-xxx Error updating load balancer with new hosts [xxx-4-3dsiuh \u003c1 more\u003e], error: failed to update load-balancer with ID xxx: xxx error 如果三个组件任一个显示 Unhealthy，就能确定是控制面出现了问题。 这个 API 已被废弃，有人建议使用 kubectl get --raw='/readyz?verbose' 来替代。但我实测发现即使这个命令返回全都 OK，但 controller-manager/scheduler 仍旧可能出问题。 其他控制面异常的详细分析，参见kubernetes 控制面故障现象及分析 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:1:1","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#控制面故障可能会导致各类奇怪的异常现象"},{"categories":["tech"],"content":" Pod 无法删除可能是某些资源无法被GC，这会导致容器已经 Exited 了，但是 Pod 一直处于 Terminating 状态。 这个问题在网上能搜到很多案例,但大都只是提供了如下的强制清理命令，未分析具体原因： shell kubectl delete pods \u003cpod\u003e --grace-period=0 --force 最近找到几篇详细的原因分析文章，值得一看： 腾讯云原生 -【Pod Terminating原因追踪系列】之 containerd 中被漏掉的 runc 错误信息 腾讯云原生 -【Pod Terminating原因追踪系列之二】exec连接未关闭导致的事件阻塞 腾讯云原生 -【Pod Terminating原因追踪系列之三】让docker事件处理罢工的cancel状态码 Pod terminating - 问题排查 - KaKu Li 大致总结一下，主要原因来自 docker 18.06 以及 kubernetes 的 docker-shim 运行时的底层逻辑， 已经在新版本被修复了。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:1:2","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#pod-无法删除"},{"categories":["tech"],"content":" initContainers 不断 restart，但是 Containers 却都显示已 readyKubernetes 应该确保所有 initContainers 都 Completed，然后才能启动 Containers. 但是我们发现有一个节点上，所有包含 initContainers 的 Pod，状态全都是Init:CrashLoopBackOff 或者 Init:Error. 而且进一步 kubectl describe po 查看细节，发现 initContainer 的状态为: text ... State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 2 Started: Tue, 03 Aug 2021 06:02:42 +0000 Finished: Tue, 03 Aug 2021 06:02:42 +0000 Ready: False Restart Count: 67 ... 而 Containers 的状态居然是 ready: text ... Host Port: 0/TCP State: Running Started: Tue, 03 Aug 2021 00:35:30 +0000 Ready: True Restart Count: 0 ... initContainers 还未运行成功，而 Containers 却 Ready 了，非常疑惑。 仔细想了下，早上因为磁盘余量告警，有手动运行过 docker system prune 命令，那么问题可能就是这条命令清理掉了已经 exited 的 initContainers 容器，导致 k8s 故障，不断尝试重启该容器。 网上一搜确实有相关的信息： https://stackoverflow.com/questions/62333064/cant-delete-exited-init-container https://github.com/kubernetes/kubernetes/issues/62362 结论：使用外部的垃圾清理命令可能导致 k8s 行为异常。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:1:3","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#initcontainers-不断-restart但是-containers-却都显示已-ready"},{"categories":["tech"],"content":" 节点常见错误 DiskPressure： 节点的可用空间不足。（通过df -h 查看，保证可用空间不小于 15%） The node was low on resource: ephemeral-storage: 同上，节点的存储空间不够了。 节点存储告警可能的原因： kubelet 的资源 GC 设置有问题，遗留的镜像等资源未及时 GC 导致告警 存在运行的 pod 使用了大量存储空间，在节点上通过 docker ps -a --size | grep G 可以查看到 如果使用的是 EKS，并且磁盘告警的挂载点为/var/lib/kubelet/plugins/kubernetes.io/aws-ebs/mounts/aws/us-east-1b/vol-xxxxx 显然是 EBS 存储卷快满了导致的 可通过 kubectl get pv -A -o yaml | grep -C 30 vol-xxxxx 来定位到具体的存储卷 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:2:0","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#节点常见错误"},{"categories":["tech"],"content":" 网络常见错误","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:3:0","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#网络常见错误"},{"categories":["tech"],"content":" 1. Ingress/Istio Gateway 返回值 404：不存在该 Service/Istio Gateway，或者是服务自身返回 404 500：大概率是服务自身的错误导致 500，小概率是代理（Sidecar/Ingress 等）的错误 503：服务不可用，有如下几种可能的原因： Service 对应的 Pods 不存在，endpoints 为空 Service 对应的 Pods 全部都 NotReady，导致 endpoints 为空 也有可能是服务自身出错返回的 503 如果你使用了 envoy sidecar， 503 可能的原因就多了。基本上 sidecar 与主容器通信过程中的任何问题都会使 envoy 返回 503，使客户端重试。 详见 Istio：503、UC 和 TCP 502：Bad Gateway，通常是由于上游未返回正确的响应导致的，可能的根本原因： 应用程序未正确处理 SIGTERM 信号，在请求未处理完毕时直接终止了进程。详见优雅停止（Gracful Shutdown）与 502/504 报错 - K8s 最佳实践 网络插件 bug 504：网关请求 upstream 超时，主要有两种可能 考虑是不是 Ingress Controller 的 IP 列表未更新，将请求代理到了不存在的 ip，导致得不到响应 Service Endpoints 移除不够及时，在 Pod 已经被终止后，仍然有个别请求被路由到了该 Pod，得不到响应导致 504。详见优雅停止（Gracful Shutdown）与 502/504 报错 - K8s 最佳实践 Pod 响应太慢，代码问题 再总结一下常见的几种错误： 未设置优雅停止，导致 Pod 被重新终止时，有概率出现 502/504 服务的所有 Pods 的状态在「就绪」和「未就绪」之间摆动，导致间歇性地出现大量 503 错误 服务返回 5xx 错误导致客户端不断重试，请求流量被放大，导致服务一直起不来 解决办法：限流、熔断（网关层直接返回固定的相应内容） Ingress 相关网络问题的排查流程： Which ingress controller? Timeout between client and ingress controller, or between ingress controller and backend service/pod? HTTP/504 generated by the ingress controller, proven by logs from the ingress controller? If you port-forward to skip the internet between client and ingress controller, does the timeout still happen? ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:3:1","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#1-ingressistio-gateway-返回值"},{"categories":["tech"],"content":" 2. 上了 istio sidecar 后，应用程序偶尔（间隔几天半个月）会 redis 连接相关的错误考虑是否和 tcp 长时间使用有关，比如连接长时间空闲的话，可能会被 istio sidecar 断开。如果程序自身的重连机制有问题，就会导致这种现象。 确认方法： 检查 istio 的 idleTimeout 时长（默认 1h） 创建三五个没流量的 Pod 放置 1h（与 istio idleTimeout 时长一致），看看是否会准时开始报 redis 的错。 对照组：创建三五个同样没流量的 Pod，但是不注入 istio sidecar，应该一直很正常 这样就能确认问题，后续处理： 抓包观察程序在出错后的 tcp 层行为 查阅 redis sdk 的相关 issue、代码，通过升级 SDK 应该能解决问题。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:3:2","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#2-上了-istio-sidecar-后应用程序偶尔间隔几天半个月会-redis-连接相关的错误"},{"categories":["tech"],"content":" 名字空间常见错误","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:4:0","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#名字空间常见错误"},{"categories":["tech"],"content":" 名字空间无法删除这通常是某些资源如 CR(custom resources)/存储等资源无法释放导致的。比如常见的 monitoring 名字空间无法删除，应该就是 CR 无法 GC 导致的。 可手动删除 namespace 配置中的析构器（spec.finalizer，在名字空间生命周期结束前会生成的配置项），这样名字空间就会直接跳过 GC 步骤： shell # 编辑名字空间的配置 kubectl edit namespace \u003cns-name\u003e # 将 spec.finalizers 改成空列表 [] 如果上述方法也无法删除名字空间，也找不到具体的问题，就只能直接从 etcd 中删除掉它了(有风险，谨慎操作！)。方法如下： shell # 登录到 etcd 容器中，执行如下命令： export ETCDCTL_API=3 cd /etc/kubernetes/pki/etcd/ # 列出所有名字空间 etcdctl --cacert ca.crt --cert peer.crt --key peer.key get /registry/namespaces --prefix --keys-only # （谨慎操作！！！）强制删除名字空间 `monitoring`。这可能导致相关资源无法被 GC！ etcdctl --cacert ca.crt --cert peer.crt --key peer.key del /registry/namespaces/monitoring ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:4:1","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#名字空间无法删除"},{"categories":["tech"],"content":" kubectl/istioctl 等客户端工具异常 socat not found: kubectl 使用 socat 进行端口转发，集群的所有节点，以及本机都必须安装有 socat 工具。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:5:0","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#kubectlistioctl-等客户端工具异常"},{"categories":["tech"],"content":" 批量清理 Evicted 记录有时候 Pod 因为节点选择器的问题，被不断调度到有问题的 Node 上，就会不断被 Evicted，导致出现大量的 Evicted Pods。排查完问题后，需要手动清理掉这些 Evicted Pods. 批量删除 Evicted 记录: shell kubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:6:0","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#批量清理-evicted-记录"},{"categories":["tech"],"content":" 容器镜像GC、Pod驱逐以及节点压力节点压力 DiskPressure 会导致 Pod 被驱逐，也会触发容器镜像的 GC。 根据官方文档配置资源不足时的处理方式，Kubelet 提供如下用于配置容器 GC 及 Evicetion 的阈值： --eviction-hard 和 eviction-soft: 对应旧参数 --image-gc-high-threshold，这两个参数配置镜像 GC 及驱逐的触发阈值。磁盘使用率的阈值默认为 85% 区别在于 eviction-hard 是立即驱逐，而 eviction-soft 在超过eviction-soft-grace-period 之后才驱逐。 --eviction-minimum-reclaim: 对应旧参数 --image-gc-low-threshold。这是进行资源回收 （镜像GC、Pod驱逐等）后期望达到的磁盘使用率百分比。磁盘使用率的阈值默认值为 80%。 问：能否为 ImageGC 设置一个比 DiskPressure 更低的阈值？因为我们希望能自动进行镜像 GC，但是不想立即触发 Pod 驱逐。 答：这应该可以通过设置 eviction-soft 和长一点的 eviction-soft-grace-period 来实现。另外 --eviction-minimum-reclaim 也可以设小一点，清理得更干净。示例如下： shell --eviction-soft=memory.available\u003c1Gi,nodefs.available\u003c2Gi,imagefs.available\u003c200Gi --eviction-soft-grace-period=3m --eviction-minimum-reclaim=memory.available=0Mi,nodefs.available=1Gi,imagefs.available=2Gi ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:7:0","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#容器镜像gcpod驱逐以及节点压力"},{"categories":["tech"],"content":" 监控/HPA 常见错误","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:8:0","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#监控hpa-常见错误"},{"categories":["tech"],"content":" 服务设置了 HPA 阈值为 50% CPU，所有业务容器在启动后不久就会 OOM，CPU 暴涨然后挂掉。但是无法触发 CPU 扩缩容，Prometheus 监控指标也不对劲。根据metrics-sever - how-often-metrics-are-scraped 描述，metrics-sever 默认情况下每 60s 采集一次指标，而 Prometheus 的采集间隔通常会配置为 15s/30s。 这说明如果业务容器每次重启后，都坚持不过 60s 就会挂掉，就很可能导致 metrics-sever 采集不到足够的指标，HPA 查询到的平均 CPU 将会是 0%，无法触发扩容操作。 Prometheus 也是一样的逻辑，如果容器每次启动都坚持不过 30s，那就会导致 prometheus 经常抓不到指标，监控图表或者告警就会出问题。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:8:1","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#服务设置了-hpa-阈值为-50-cpu所有业务容器在启动后不久就会-oomcpu-暴涨然后挂掉但是无法触发-cpu-扩缩容prometheus-监控指标也不对劲"},{"categories":["tech"],"content":" 其他问题","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:9:0","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#其他问题"},{"categories":["tech"],"content":" 隔天 Istio 等工具的 sidecar 自动注入莫名其妙失效了如果服务器晚上会关机，可能导致第二天网络插件出问题，导致 sidecar 注入器无法观察到 pod 的创建，也就无法完成 sidecar 注入。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:9:1","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#隔天-istio-等工具的-sidecar-自动注入莫名其妙失效了"},{"categories":["tech"],"content":" 如何重新运行一个 Job？我们有一个 Job 因为外部原因运行失败了，修复好后就需要重新运行它。 方法是：删除旧的 Job，再使用同一份配置重建 Job. 如果你使用的是 fluxcd 这类 GitOps 工具，就只需要手工删除旧 Pod，fluxcd 会定时自动 apply 所有配置，这就完成了 Job 的重建。 ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:9:2","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#如何重新运行一个-job"},{"categories":["tech"],"content":" 参考 Kubernetes管理经验 504 Gateway Timeout when accessing workload via ingress Kubernetes Failure Stories Istio：503、UC 和 TCP istio 实践指南 - imroc.cc Kubernetes 实践指南 - imroc.cc ","date":"2019-11-24","objectID":"/posts/kubernetes-common-errors-and-solutions/:10:0","series":["云原生相关"],"tags":["Kubernetes"],"title":"Kubernetes 常见错误、原因及处理方法","uri":"/posts/kubernetes-common-errors-and-solutions/#参考"},{"categories":["tech"],"content":"Manjaro 是一个基于 Arch Linux 的 Linux 滚动发行版，用着挺舒服的。最大的特点，是包仓库很丰富，而且都很新。代价是偶尔会出些小毛病。 2021-09-22 更新：今天被群友科普，可能我下面列举的几个滚挂事件，可能都和我使用了 archlinuxcn 这个源有关，这确实有可能。 我一年多的使用中，遇到过 qv2-ray 动态链接库炸掉的问题，没专门去找修复方法，好像是等了一两个月，升级了两个大版本才恢复。另一个就是VSCode - Incorrect locale ’en-US’ used everywhere 还遇到过 libguestfs 的一个问题：vrit-v2v/virt-p2v 两个工具被拆分出去，导致 manjaro 只能通过源码安装这俩货。这貌似目前仍旧没有解决。 总的来说体验很不错，能很及时地用上各种新版本的软件。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:0:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#"},{"categories":["tech"],"content":" 一、pacman/yay 的基础命令Manjaro 装好后，需要运行的第一条命令： sh sudo pacman -Syy ## 强制更新 package 目录 sudo pacman-mirrors --interactive --country China # 列出所有国内的镜像源，并提供交互式的界面手动选择镜像源 sudo pacman -Syyu # 强制更新 package 目录，并尝试更新已安装的所有 packages. sudo pacman -S yay # 安装 yay pacman 是 arch/manjaro 的官方包管理器，而刚刚安装的 yay，则是一个能查询 arch linux 的 aur 仓库的第三方包管理器，非常流行。 pacman 的常用命令语法： sh pacman -S package_name # 安装软件 pacman -S extra/package_name # 安装不同仓库中的版本 pacman -Syu # 升级整个系统，y是更新数据库，yy是强制更新，u是升级软件 pacman -Ss string # 在包数据库中查询软件 pacman -Si package_name # 显示软件的详细信息 pacman -Sc # 清除软件缓存，即/var/cache/pacman/pkg目录下的文件 pacman -R package_name # 删除单个软件 pacman -Rs package_name # 删除指定软件及其没有被其他已安装软件使用的依赖关系 pacman -Qs string # 查询已安装的软件包 pacman -Qi package_name # 查询本地安装包的详细信息 pacman -Ql package_name # 获取已安装软件所包含的文件的列表 pacman -U package.tar.zx # 从本地文件安装 pactree package_name # 显示软件的依赖树 yay 的用法和 pacman 完全类似，上述所有 pacman xxx 命令，均可替换成 yay xxx 执行。 此外，还有一条 yay 命令值得记一下： sh yay -c # 卸载所有无用的依赖。类比 apt-get autoremove ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:1:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#一pacmanyay-的基础命令"},{"categories":["tech"],"content":" 常用软件与配置","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#常用软件与配置"},{"categories":["tech"],"content":" 1. 添加 archlinux 中文社区仓库Arch Linux 中文社区仓库 是由 Arch Linux 中文社区驱动的非官方用户仓库，包含一些额外的软件包以及已有软件的 git 版本等变种。部分软件包的打包脚本来源于 AUR。 一些国内软件，如果直接从 aur 安装，那就会有一个编译过程，有点慢。而 archlinuxcn 有已经编译好的包，可以直接安装。更新速度也很快，推荐使用。 配置方法见Arch Linux Chinese Community Repository。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:1","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#1-添加-archlinux-中文社区仓库"},{"categories":["tech"],"content":" 2. 安装常用软件 sh sudo pacman -S google-chrome firefox # 浏览器 sudo pacman -S netease-cloud-music # 网易云音乐 sudo pacman -S noto-fonts-cjk wqy-bitmapfont wqy-microhei wqy-zenhei # 中文字体：思源系列、文泉系列 sudo pacman -S wps-office ttf-wps-fonts sudo pacman -S vim # 命令行编辑器 sudo pacman -S git # 版本管理工具 sudo pacman -S clang make cmake gdb # 编译调试环境 sudo pacman -S visual-studio-code-bin # 代码编辑器 sudo pacman -S wireshark-qt mitmproxy # 抓包工具 sudo pacman -S docker # docker 容器 其中 docker 和 wireshark 需要额外配置，否则会要求管理员权限： sh sudo groupadd wireshark sudo gpasswd --add $USER wireshark # 将你添加到 wireshark 用户组中 sudo groupadd docker sudo gpasswd --add $USER docker # 同上 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:2","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#2-安装常用软件"},{"categories":["tech"],"content":" 3. 中文输入法有两个选择：中州韵（rime）和搜狗拼音（sogoupinyin）。 简单省事用搜狗，要用特殊的输入方案（五笔、音形、二笔等等）就只有 rime 可选了。 3.1 fcitx5-rime 配置小鹤音形首先安装 fcitx5-rime, 注意这些组件一个都不能省略： shell sudo pacman -S fcitx5 fcitx5-chinese-addons fcitx5-gtk fcitx5-qt kcm-fcitx5 fcitx5-rime 第二步是修改环境变量，将 fcitx5-rime 设为默认输入法并自动启动。 添加 ~/.pam_environment 文件，内容如下： conf INPUT_METHOD DEFAULT=fcitx5 GTK_IM_MODULE DEFAULT=fcitx5 QT_IM_MODULE DEFAULT=fcitx5 XMODIFIERS DEFAULT=@im=fcitx5 pam-env 模块会在所有登录会话中读取上面的配置文件，包括 X11 会话和 Wayland 会话。 添加自动启动： shell # ~/.xprofile 是 x11 GUI 的环境变量配置文件 echo 'fcitx5 \u0026' \u003e\u003e ~/.xprofile 然后，从 http://flypy.ys168.com/ 下载最新的鼠须管（MacOS）配置文件，将解压得到的 rime 文件夹拷贝到 ~/.local/share/fcitx5/ 下： shell mv rime ~/.local/share/fcitx5/ 现在重启系统，在 fcitx5 配置里面添加 rime，就可以正常使用小鹤音形了。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:3","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#3-中文输入法"},{"categories":["tech"],"content":" 3. 中文输入法有两个选择：中州韵（rime）和搜狗拼音（sogoupinyin）。 简单省事用搜狗，要用特殊的输入方案（五笔、音形、二笔等等）就只有 rime 可选了。 3.1 fcitx5-rime 配置小鹤音形首先安装 fcitx5-rime, 注意这些组件一个都不能省略： shell sudo pacman -S fcitx5 fcitx5-chinese-addons fcitx5-gtk fcitx5-qt kcm-fcitx5 fcitx5-rime 第二步是修改环境变量，将 fcitx5-rime 设为默认输入法并自动启动。 添加 ~/.pam_environment 文件，内容如下： conf INPUT_METHOD DEFAULT=fcitx5 GTK_IM_MODULE DEFAULT=fcitx5 QT_IM_MODULE DEFAULT=fcitx5 XMODIFIERS DEFAULT=@im=fcitx5 pam-env 模块会在所有登录会话中读取上面的配置文件，包括 X11 会话和 Wayland 会话。 添加自动启动： shell # ~/.xprofile 是 x11 GUI 的环境变量配置文件 echo 'fcitx5 \u0026' \u003e\u003e ~/.xprofile 然后，从 http://flypy.ys168.com/ 下载最新的鼠须管（MacOS）配置文件，将解压得到的 rime 文件夹拷贝到 ~/.local/share/fcitx5/ 下： shell mv rime ~/.local/share/fcitx5/ 现在重启系统，在 fcitx5 配置里面添加 rime，就可以正常使用小鹤音形了。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:2:3","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#31-fcitx5-rime-配置小鹤音形"},{"categories":["tech"],"content":" 坑使用过程中，我也遇到了一些坑： 安装软件包时，无法在线安装旧版本！除非你本地有旧版本的安装包没清除，才可以通过缓存安装旧版本。 这种问题没遇到时好说，但有时候新版本有问题，旧安装包也清理掉了无法回退，就非常麻烦。 而且就算你回退了版本，一升级它就又更新了。。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:3:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#坑"},{"categories":["tech"],"content":" 彻底删除 Manjaro 及其引导项最近(2021-01)切换到了 OpenSUSE，体验很好，于是决定删除掉 Manjaro。 一番操作，总结出的删除流程如下（以下命令均需要 root 权限）： shell # 1. 删除 EFI 引导项 ## 查看 efi 的所有启动项，找到 Manjaro 的编号 efibootmgr ## 删除掉 Manjaro 启动项 sudo efibootmgr --delete-bootnum -b 2 # 2. 删除 manjaro 的 bootloader ## 我使用了 manjaro 默认的安装策略，bootloader 被安装在了和 windows 相同的 EFI 分区下 ## 首先通过 opnsuse 的分区工具，找到 EFI 分区的设备号，然后挂载它 mkdir efi mount /dev/nvme0n1p1 efi # 删除 Manjaro bootloader rm -r EFI/Manjaro # 3. 重建 grub2 引导项 grub2-mkconfig \u003e /boot/grub2/grub.cfg # 4. 最后，通过分区工具删除 Manjaro 的所有分区，我是 SSD，只有一个分区 # 5. 重启系统，所有东西就全删除干净了。 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:4:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#彻底删除-manjaro-及其引导项"},{"categories":["tech"],"content":" 参考 Arch Linux Wiki - 中文 AUR 仓库 Arch Linux 中文社区仓库 yay - Yet another Yogurt - An AUR Helper written in Go 安装Manjaro之后的配置 Arch Linux Wiki - Fcitx5 ","date":"2019-07-13","objectID":"/posts/manjaro-instruction/:5:0","series":null,"tags":["Manjaro","Linux","Arch Linux"],"title":"Manjaro 使用指南","uri":"/posts/manjaro-instruction/#参考"},{"categories":["life"],"content":"四年大学惨淡收场，坐在火车硬座上，心里有些忐忑。 对面坐着一个 16 岁的女孩子，从合肥去深圳当安检，隐约听到“700 块一个月”，“再怎么着十天也有一休吧”，说实话深感惭愧。 小时候我妈对我有点保护过度，从小到大几乎没下田干过活，在家帮忙干过的活屈指可数。因为读书尚可，稀里糊涂读了十多年书，现在惨淡收场。而对面女孩子 16 岁，已经要去离家这么远的地方实习了 （当安检），而且还听到对面说“想和他分了，每次都是我给他打电话，他从来没主动过，现在还抱怨我电话打少了……” 感觉我 22 年，有点白活了。。 可即使这样，还是提不起多大动力去复习面试用的知识点。心里慌的不行，可游戏照打不误。还瞒着家里，花白条分期买了个 6000 的小米游戏本，一月要还 500，万一工作找得不理想，我不知道这个钱窟窿到时候该怎么填上。。。 我的四年大学好像也有过些高光时刻，也有过许多值得铭记的欢乐时光，临到头来却是这么个惨淡的结尾。也有想过努力努力，可一懒散起来，就有了借口——“当初差点高考都没参加，就直接退学了，现在起码还读了四年的三流一本，见了世面，赚到了。” 不知道十年后我再回首，会不会觉得现在内心的忐忑，不算什么。 总之呢，因为一句「搞计算机的话，深圳工作应该很多吧，不如过来找工作？」，我上了这趟火车。 学业啥的都随它去吧，是不是单车变摩托，就看这一把了… ","date":"2019-06-20","objectID":"/posts/escape-my-university/:0:0","series":null,"tags":[],"title":"逃离我的大学","uri":"/posts/escape-my-university/#"},{"categories":["tech"],"content":" 个人笔记，不保证正确。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:0:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#"},{"categories":["tech"],"content":" 一、关系构建：ForeignKey 与 relationship关系构建的重点，在于搞清楚这两个函数的用法。ForeignKey 的用法已经在SQL表达式语言 - 表定义中的约束 讲过了。主要是 ondelete 和 onupdate 两个参数的用法。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:1:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#一关系构建foreignkey-与-relationship"},{"categories":["tech"],"content":" 二、relationshiprelationship 函数在 ORM 中用于构建表之间的关联关系。与 ForeignKey 不同的是，它定义的关系不属于表定义，而是动态计算的。用它定义出来的属性，相当于 SQL 中的视图。 这个函数有点难用，一是因为它的有几个参数不太好理解，二是因为它的参数非常丰富，让人望而却步。下面通过一对多、多对一、多对多几个场景下 relationship 的使用，来一步步熟悉它的用法。 首先初始化： python3 from sqlalchemy import Table, Column, Integer, ForeignKey from sqlalchemy.orm import relationship from sqlalchemy.ext.declarative import declarative_base Base = declarative_base() ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#二relationship"},{"categories":["tech"],"content":" 1. 一对多 python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) # 因为 Child 中有 Parent 的 ForeignKey，这边的声明不需要再额外指定什么。 children = relationship(\"Child\") # children 的集合，相当于一个视图。 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) 一个 Parent 可以有多个 Children，通过 relationship，我们就能直接通过parent.children 得到结果，免去繁琐的 query 语句。 1.1 反向引用 1.1.1 backref 与 back_populates那如果我们需要得知 child 的 parent 对象呢？能不能直接访问 child.parent？ 为了实现这个功能，SQLAlchemy 提供了 backref 和 back_populates 两个参数。 两个参数的效果完全一致，区别在于，backref 只需要在 Parent 类中声明children，Child.parent 会被动态创建。 而 back_populates 必须在两个类中显式地使用 back_populates，更显繁琐。（但是也更清晰？） 先看 backref 版： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=\"parent\") # backref 表示，在 Child 类中动态创建 parent 属性，指向当前类。 # Child 类不需要修改 再看 back_populates 版： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\") # back_populates class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) # 这边也必须声明，不能省略！ parent = relationship(\"Parent\", back_populates=\"children\") # parent 不是集合，是属性！ NOTE：声明的两个 relationship 不需要多余的说明，SQLAlchemy 能自动识别到parent.children 是 collection，child.parent 是 attribute. 1.1.2. 反向引用的参数：sqlalchemy.orm.backref(name, **kwargs)使用 back_populates 时，我们可以很方便地在两个 relationship 函数中指定各种参数： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\", lazy='dynamic') # 指定 lazy 的值 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) parent = relationship(\"Parent\", back_populates=\"children\", lazy='dynamic') # 指定 lazy 的值 但是如果使用 backref，因为我们只有一个 relationship 函数，Child.parent 是被隐式创建的，我们该如何指定这个属性的参数呢？ 答案就是 backref() 函数，使用它替代 backref 参数的值： python3 from sqlalchemy.orm import backref class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=backref(\"parent\", lazy='dynamic')) # 使用 backref() 函数，指定 Child.parent 属性的参数 # Child 类不需要修改 backref() 的参数会被传递给 relationship()，因此它俩的参数也完全一致。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#1-一对多"},{"categories":["tech"],"content":" 1. 一对多 python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) # 因为 Child 中有 Parent 的 ForeignKey，这边的声明不需要再额外指定什么。 children = relationship(\"Child\") # children 的集合，相当于一个视图。 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) 一个 Parent 可以有多个 Children，通过 relationship，我们就能直接通过parent.children 得到结果，免去繁琐的 query 语句。 1.1 反向引用 1.1.1 backref 与 back_populates那如果我们需要得知 child 的 parent 对象呢？能不能直接访问 child.parent？ 为了实现这个功能，SQLAlchemy 提供了 backref 和 back_populates 两个参数。 两个参数的效果完全一致，区别在于，backref 只需要在 Parent 类中声明children，Child.parent 会被动态创建。 而 back_populates 必须在两个类中显式地使用 back_populates，更显繁琐。（但是也更清晰？） 先看 backref 版： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=\"parent\") # backref 表示，在 Child 类中动态创建 parent 属性，指向当前类。 # Child 类不需要修改 再看 back_populates 版： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\") # back_populates class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) # 这边也必须声明，不能省略！ parent = relationship(\"Parent\", back_populates=\"children\") # parent 不是集合，是属性！ NOTE：声明的两个 relationship 不需要多余的说明，SQLAlchemy 能自动识别到parent.children 是 collection，child.parent 是 attribute. 1.1.2. 反向引用的参数：sqlalchemy.orm.backref(name, **kwargs)使用 back_populates 时，我们可以很方便地在两个 relationship 函数中指定各种参数： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\", lazy='dynamic') # 指定 lazy 的值 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) parent = relationship(\"Parent\", back_populates=\"children\", lazy='dynamic') # 指定 lazy 的值 但是如果使用 backref，因为我们只有一个 relationship 函数，Child.parent 是被隐式创建的，我们该如何指定这个属性的参数呢？ 答案就是 backref() 函数，使用它替代 backref 参数的值： python3 from sqlalchemy.orm import backref class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=backref(\"parent\", lazy='dynamic')) # 使用 backref() 函数，指定 Child.parent 属性的参数 # Child 类不需要修改 backref() 的参数会被传递给 relationship()，因此它俩的参数也完全一致。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#11-反向引用"},{"categories":["tech"],"content":" 1. 一对多 python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) # 因为 Child 中有 Parent 的 ForeignKey，这边的声明不需要再额外指定什么。 children = relationship(\"Child\") # children 的集合，相当于一个视图。 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) 一个 Parent 可以有多个 Children，通过 relationship，我们就能直接通过parent.children 得到结果，免去繁琐的 query 语句。 1.1 反向引用 1.1.1 backref 与 back_populates那如果我们需要得知 child 的 parent 对象呢？能不能直接访问 child.parent？ 为了实现这个功能，SQLAlchemy 提供了 backref 和 back_populates 两个参数。 两个参数的效果完全一致，区别在于，backref 只需要在 Parent 类中声明children，Child.parent 会被动态创建。 而 back_populates 必须在两个类中显式地使用 back_populates，更显繁琐。（但是也更清晰？） 先看 backref 版： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=\"parent\") # backref 表示，在 Child 类中动态创建 parent 属性，指向当前类。 # Child 类不需要修改 再看 back_populates 版： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\") # back_populates class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) # 这边也必须声明，不能省略！ parent = relationship(\"Parent\", back_populates=\"children\") # parent 不是集合，是属性！ NOTE：声明的两个 relationship 不需要多余的说明，SQLAlchemy 能自动识别到parent.children 是 collection，child.parent 是 attribute. 1.1.2. 反向引用的参数：sqlalchemy.orm.backref(name, **kwargs)使用 back_populates 时，我们可以很方便地在两个 relationship 函数中指定各种参数： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\", lazy='dynamic') # 指定 lazy 的值 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) parent = relationship(\"Parent\", back_populates=\"children\", lazy='dynamic') # 指定 lazy 的值 但是如果使用 backref，因为我们只有一个 relationship 函数，Child.parent 是被隐式创建的，我们该如何指定这个属性的参数呢？ 答案就是 backref() 函数，使用它替代 backref 参数的值： python3 from sqlalchemy.orm import backref class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=backref(\"parent\", lazy='dynamic')) # 使用 backref() 函数，指定 Child.parent 属性的参数 # Child 类不需要修改 backref() 的参数会被传递给 relationship()，因此它俩的参数也完全一致。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#111-backref-与-back_populates"},{"categories":["tech"],"content":" 1. 一对多 python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) # 因为 Child 中有 Parent 的 ForeignKey，这边的声明不需要再额外指定什么。 children = relationship(\"Child\") # children 的集合，相当于一个视图。 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) 一个 Parent 可以有多个 Children，通过 relationship，我们就能直接通过parent.children 得到结果，免去繁琐的 query 语句。 1.1 反向引用 1.1.1 backref 与 back_populates那如果我们需要得知 child 的 parent 对象呢？能不能直接访问 child.parent？ 为了实现这个功能，SQLAlchemy 提供了 backref 和 back_populates 两个参数。 两个参数的效果完全一致，区别在于，backref 只需要在 Parent 类中声明children，Child.parent 会被动态创建。 而 back_populates 必须在两个类中显式地使用 back_populates，更显繁琐。（但是也更清晰？） 先看 backref 版： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=\"parent\") # backref 表示，在 Child 类中动态创建 parent 属性，指向当前类。 # Child 类不需要修改 再看 back_populates 版： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\") # back_populates class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) # 这边也必须声明，不能省略！ parent = relationship(\"Parent\", back_populates=\"children\") # parent 不是集合，是属性！ NOTE：声明的两个 relationship 不需要多余的说明，SQLAlchemy 能自动识别到parent.children 是 collection，child.parent 是 attribute. 1.1.2. 反向引用的参数：sqlalchemy.orm.backref(name, **kwargs)使用 back_populates 时，我们可以很方便地在两个 relationship 函数中指定各种参数： python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", back_populates=\"parent\", lazy='dynamic') # 指定 lazy 的值 class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) parent = relationship(\"Parent\", back_populates=\"children\", lazy='dynamic') # 指定 lazy 的值 但是如果使用 backref，因为我们只有一个 relationship 函数，Child.parent 是被隐式创建的，我们该如何指定这个属性的参数呢？ 答案就是 backref() 函数，使用它替代 backref 参数的值： python3 from sqlalchemy.orm import backref class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) children = relationship(\"Child\", backref=backref(\"parent\", lazy='dynamic')) # 使用 backref() 函数，指定 Child.parent 属性的参数 # Child 类不需要修改 backref() 的参数会被传递给 relationship()，因此它俩的参数也完全一致。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#112-反向引用的参数sqlalchemyormbackrefname-kwargs"},{"categories":["tech"],"content":" 2. 多对一A many-to-one is similar to a one-to-many relationship. The difference is that this relationship is looked at from the “many” side. ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:2","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#2-多对一"},{"categories":["tech"],"content":" 3. 一对一 python3 class Parent(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) child = relationship(\"Child\", uselist=False, # 不使用 collection！这是关键 back_populates=\"parent\") class Child(Base): __tablename__ = 'child' id = Column(Integer, primary_key=True) parent_id = Column(Integer, ForeignKey('parent.id')) # 包含 ForeignKey 的类，此属性默认为 attribute，因此不需要 uselist=False parent = relationship(\"Parent\", back_populates=\"child\") ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:3","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#3-一对一"},{"categories":["tech"],"content":" 4. 多对多 python # 多对多，必须要使用一个关联表！ association_table = Table('association', Base.metadata, Column('left_id', Integer, ForeignKey('left.id')), # 约定俗成的规矩，左边是 parent Column('right_id', Integer, ForeignKey('right.id')) # 右边是 child ) class Parent(Base): __tablename__ = 'left' id = Column(Integer, primary_key=True) children = relationship(\"Child\", secondary=association_table) # 专用参数 secondary，用于指定使用的关联表 class Child(Base): __tablename__ = 'right' id = Column(Integer, primary_key=True) 要添加反向引用时，同样可以使用 backref 或 back_populates. 4.1 user2user如果多对多关系中的两边都是 user，即都是同一个表时，该怎么声明？ 例如用户的「关注」与「粉丝」，你是 user，你的粉丝是 user，你关注的账号也是 user。 这个时候，关联表 association_table 的两个键都是 user，SQLAlchemy 无法区分主次，需要手动指定，为此需要使用 primaryjoin 和 secondaryjoin 两个参数。 python # 关联表，左侧的 user 正在关注右侧的 user followers = db.Table('followers', db.Column('follower_id', db.Integer, db.ForeignKey('user.id')), # 左侧 db.Column('followed_id', db.Integer, db.ForeignKey('user.id')) # 右侧，被关注的 user ) class User(UserMixin, db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(64), index=True, unique=True, nullable=False) email = db.Column(db.String(120), index=True, unique=True, nullable=False) password_hash = db.Column(db.String(128), nullable=False) # 我关注的 users followed = db.relationship( 'User', secondary=followers, # 指定多对多关联表 primaryjoin=(followers.c.follower_id == id), # 左侧，用于获取「我关注的 users」的 join 条件 secondaryjoin=(followers.c.followed_id == id), # 右侧，用于获取「我的粉丝」的 join 条件 lazy='dynamic', # 延迟求值，这样才能用 filter_by 等过滤函数 backref=db.backref('followers', lazy='dynamic')) # followers 也要延迟求值 这里比较绕的，就是容易搞混 primaryjoin 和 secondaryjoin 两个参数。 primaryjoin：（多对多中）用于从子对象查询其父对象的 condition（child.parents），默认只考虑外键。 secondaryjoin：（多对多中）用于从父对象查询其所有子对象的condition（parent.children）， 同样的，默认情况下只考虑外键。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:4","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#4-多对多"},{"categories":["tech"],"content":" 4. 多对多 python # 多对多，必须要使用一个关联表！ association_table = Table('association', Base.metadata, Column('left_id', Integer, ForeignKey('left.id')), # 约定俗成的规矩，左边是 parent Column('right_id', Integer, ForeignKey('right.id')) # 右边是 child ) class Parent(Base): __tablename__ = 'left' id = Column(Integer, primary_key=True) children = relationship(\"Child\", secondary=association_table) # 专用参数 secondary，用于指定使用的关联表 class Child(Base): __tablename__ = 'right' id = Column(Integer, primary_key=True) 要添加反向引用时，同样可以使用 backref 或 back_populates. 4.1 user2user如果多对多关系中的两边都是 user，即都是同一个表时，该怎么声明？ 例如用户的「关注」与「粉丝」，你是 user，你的粉丝是 user，你关注的账号也是 user。 这个时候，关联表 association_table 的两个键都是 user，SQLAlchemy 无法区分主次，需要手动指定，为此需要使用 primaryjoin 和 secondaryjoin 两个参数。 python # 关联表，左侧的 user 正在关注右侧的 user followers = db.Table('followers', db.Column('follower_id', db.Integer, db.ForeignKey('user.id')), # 左侧 db.Column('followed_id', db.Integer, db.ForeignKey('user.id')) # 右侧，被关注的 user ) class User(UserMixin, db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(64), index=True, unique=True, nullable=False) email = db.Column(db.String(120), index=True, unique=True, nullable=False) password_hash = db.Column(db.String(128), nullable=False) # 我关注的 users followed = db.relationship( 'User', secondary=followers, # 指定多对多关联表 primaryjoin=(followers.c.follower_id == id), # 左侧，用于获取「我关注的 users」的 join 条件 secondaryjoin=(followers.c.followed_id == id), # 右侧，用于获取「我的粉丝」的 join 条件 lazy='dynamic', # 延迟求值，这样才能用 filter_by 等过滤函数 backref=db.backref('followers', lazy='dynamic')) # followers 也要延迟求值 这里比较绕的，就是容易搞混 primaryjoin 和 secondaryjoin 两个参数。 primaryjoin：（多对多中）用于从子对象查询其父对象的 condition（child.parents），默认只考虑外键。 secondaryjoin：（多对多中）用于从父对象查询其所有子对象的condition（parent.children）， 同样的，默认情况下只考虑外键。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:2:4","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#41-user2user"},{"categories":["tech"],"content":" 三、ORM 层 的 “delete” cascade vs. FOREIGN KEY 层的 “ON DELETE” cascade之前有讲过 Table 定义中的级联操作：ON DELETE 和 ON UPDATE，可以通过 ForeignKey 的参数指定为 CASCADE. 可 SQLAlchemy 还有一个 relationship 生成 SQL 语句时的配置参数 cascade，另外passive_deletes 也可以指定为 cascade。 有这么多的 cascade，我真的是很懵。这三个 cascade 到底有何差别呢？ 外键约束中的 ON DELETE 和 ON UPDATE，与 ORM 层的 CASCADE 在功能上，确实有很多重叠的地方。但是也有很多不同： 数据库层面的 ON DELETE 级联能高效地处理 many-to-one 的关联；我们在 many 方定义外键，也在这里添加 ON DELETE 约束。而在 ORM 层，就刚好相反。SQLAlchemy 在 one 方处理 many 方的删除操作，这意味着它更适合处理 one-to-many 的关联。 数据库层面上，不带 ON DELETE 的外键常用于防止父数据被删除，而导致子数据成为无法被索引到的垃圾数据。如果要在一个 one-to-many 映射上实现这个行为，SQLAlchemy 将外键设置为 NULL 的默认行为可以通过以下两种方式之一捕获： 最简单也最常用的方法，当然是将外键定义为 NOT NULL. 尝试将该列设为 NULL 会触发 NOT NULL constraint exception. 另一种更特殊的方法，是将 passive_deletes 标志设置为字 all. 这会完全禁用 SQLAlchemy 将外键列设置为 NULL 的行为，并且 DELETE 父数据而不会对子数据产生任何影响。这样才能触发数据库层面的 ON DELETE 约束，或者其他的触发器。 数据库层面的 ON DELETE 级联 比 ORM 层面的级联更高效。数据库可以同时在多个 relationship 中链接一系列级联操作。 SQLAlchemy 不需要这么复杂，因为我们通过将 passive_deletes 选项与正确配置的外键约束结合使用，提供与数据库的 ON DELETE 功能的平滑集成。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:3:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#三orm-层-的-delete-cascade-vs-foreign-key-层的-on-delete-cascade"},{"categories":["tech"],"content":" 方法一：ORM 层的 cascade 实现relationship 的 cascade 参数决定了修改父表时，什么时候子表要进行级联操作。它的可选项有 （str，选项之间用逗号分隔）： save-update：默认选项之一。在 add（对应 SQL 的 insert 或 update）一个对象的时候，会 add 所有它相关联的对象。 merge：默认选项之一。在 merge（相当字典的update操作，有就替换掉，没有就合并）一个对象的时候，会 merge 所有和它相关联的对象。 expunge ：移除操作的时候，会将相关联的对象也进行移除。这个操作只是从session中移除，并不会真正的从数据库中删除。 delete：删除父表数据时，同时删除与它关联的数据。 delete-orphan：当子对象与父对象解除关系时，删除掉此子对象（孤儿）。（其实还是没懂。。） refresh-expire：不常用。 all：表示选中除 delete-orphan 之外的所有选项。（因此 all, delete-orphan 很常用， 它才是真正的 all） 默认属性是 “save-update, merge”. 这只是简略的说明，上述几个参数的详细文档见SQLAlchemy - Cascades ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:3:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#方法一orm-层的-cascade-实现"},{"categories":["tech"],"content":" 方法二：数据库层的 cascade 实现 将 ForeignKey 的 ondelete 和 onupdate 参数指定为 CASCADE，实现数据库层面的级联。 为 relationship 添加关键字参数 passive_deletes=\"all\"，这样就完全禁用 SQLAlchemy 将外键列设置为 NULL 的行为，并且 DELETE 父数据不会对子数据产生任何影响。 这样 DELETE 操作时，就会触发数据库的 ON DELETE 约束，从而级联删除子数据。 ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:3:2","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#方法二数据库层的-cascade-实现"},{"categories":["tech"],"content":" 参考 SQLAlchemy - Relationship Configuration SQLAlchemy - Cascades SQLAlchemy 中的 backref 和 back_populates The Flask Mega-Tutorial Part VIII: Followers hackersandslackers/sqlalchemy-tutorial ","date":"2019-05-21","objectID":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/:4:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（三）：ORM 中的关系构建","uri":"/posts/sqlalchemy-notes-3-relationship-and-foreignkey/#参考"},{"categories":["life"],"content":"这网络小说一起头，就停不下来。开始还只是追更新，每天看看三四本书的更新（一更或二更）就行了，说不上心满意足，但也没多么欲求不满。 到了三月下旬，突然就想看本新书。念头一起就一发不可收拾，只挑二三十万字的连载书看——首要原因是怕收不住手，书太长的话，废寝忘食起来自己遭罪；其次是前二十多万字都可以免费看，后面才是收费章节。 感觉自己陆陆续续可能看了二三十本书，五六百万字就这么过了，二三十个故事就如过眼云烟，转眼就记不得几个书名了。就像从小到大的同学老师一样，看小说看到讲高中室友，就尝试回想我的高中宿舍，一时竟连自己的床铺都搞错了，仔细回想下，也有好几个室友的名字长相都忘记了。这还算好了， 毕竟高中三年，也过了这么长时间才淡忘。这网文的故事有的光怪陆离，有的异想天开，大部分都是专注一个「爽」字，无不是金手指打怪升级的套路。千篇一律下，能记住的没几个。 倒也有特立独行的，因此也就印象尤其深刻，《月明见君来/云胡不喜》是这样一本文笔优美的古言， 为着“云胡不喜”一句，那时突然就喜欢上了《诗经》。还有位作者的星际言情文：《星球上的完美家园》和《未来世界之我心安处》，写得极为细腻。尤其是前者，只是他写的研究生生活，有点刺激我这个曾经信誓旦旦要考研，现在却大学文凭都拿不到的学渣。。就看不下去。 有段时间对日系轻小说青睐有加，觉得里头的优秀作品就是比国内网文高个档次。现在一想，其实都一样，良莠不齐，都喜欢穿越重生异世界的套路。只是看多了动漫，才对轻小说多点好感而已。网文里写的好的书也不少。 这两天突然就有些厌倦了，字看多了开始头疼，厌烦。就像手冲后进入了贤者时间一样。。。这样一想，这书瘾到底还是没毒品那么可怕，再怎么凶猛，疯狂上一个月，也后继乏力了，不会越陷越深。 这一个月深居简出，连QQ都直接掐掉了好几天。我看起小说来就是这样，别的事完全没兴致理，疯狂起来就更是变本加厉。 忽地又想起了自己内心敏感、性格别扭、胆子小、受不得训斥。再加上这一个月的所作所为，真是凭实力啃老+单身。 颓废完了，自卑，就安慰自己全国大学生占比不过3%，又想想战争年代人们的悲惨，就觉得自己已经比很多人都活得好了。生逢盛世，国泰民安，又能找到自己喜欢的东西，而且主动地追求了。虽然没有尽全力，但是人生苦短嘛，要及时享乐。酸甜苦辣混一起，我总归是知足的。 要真能穿越我肯定不会像过去那样干，但是不能穿越也不是太可惜。我经历过的糟心事和很多人有过的绝望比起来，根本不值一提，我还能有什么不满的呢？ 这个世界总归是不会停止转动，也不会随我心意。我讨厌的繁文缛节在人际交往中是必不可少的，我的自卑是藏不住的，我知识上的短板也是实实在在的。 我还能怎么做呢？总之向前走吧，不论是被人流裹挟着，还是被自己的欲望拉扯着，都只能向前——人生是没有退路的。 ","date":"2019-04-14","objectID":"/posts/webnovel-addiction-recovery/:0:0","series":null,"tags":["Webnovel","网络小说"],"title":"瘾的退却","uri":"/posts/webnovel-addiction-recovery/#"},{"categories":["tech"],"content":" 个人笔记，如有疏漏，还请指正。 使用多线程（threading）和多进程（multiprocessing）完成常规的并发需求，在启动的时候 start、join 等步骤不能省，复杂的需要还要用 1-2 个队列。随着需求越来越复杂，如果没有良好的设计和抽象这部分的功能层次，代码量越多调试的难度就越大。 对于需要并发执行、但是对实时性要求不高的任务，我们可以使用 concurrent.futures 包中的 PoolExecutor 类来实现。 这个包提供了两个执行器：线程池执行器 ThreadPoolExecutor 和进程池执行器 ProcessPoolExecutor，两个执行器提供同样的 API。 池的概念主要目的是为了重用：让线程或进程在生命周期内可以多次使用。它减少了创建创建线程和进程的开销，提高了程序性能。重用不是必须的规则，但它是程序员在应用中使用池的主要原因。 池，只有固定个数的线程/进程，通过 max_workers 指定。 任务通过 executor.submit 提交到 executor 的任务队列，返回一个 future 对象。 Future 是常见的一种并发设计模式。一个Future对象代表了一些尚未就绪（完成）的结果，在「将来」的某个时间就绪了之后就可以获取到这个结果。 任务被调度到各个 workers 中执行。但是要注意，一个任务一旦被执行，在执行完毕前，会一直占用该 worker！ 如果 workers 不够用，其他的任务会一直等待！因此 PoolExecutor 不适合实时任务。 python import concurrent.futures import time from itertools import count number_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] def evaluate_item(x): for i in count(x): # count 是无限迭代器，会一直递增。 print(f\"{x} - {i}\") time.sleep(0.01) if __name__ == \"__main__\": # 进程池 start_time_2 = time.time() # 使用 with 在离开此代码块时，自动调用 executor.shutdown(wait=true) 释放 executor 资源 with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: # 将 10 个任务提交给 executor，并收集 futures futures = [executor.submit(evaluate_item, item) for item in number_list] # as_completed 方法等待 futures 中的 future 完成 # 一旦某个 future 完成，as_completed 就立即返回该 future # 这个方法，使每次返回的 future，总是最先完成的 future # 而不是先等待任务 1，再等待任务 2... for future in concurrent.futures.as_completed(futures): # result 返回被调用函数的返回值 # 如果调用抛出了异常，result 会抛出同样的异常 # 如果调用被取消，result 抛出 CancelledError 异常 print(future.result()) print (\"Thread pool execution in \" + str(time.time() - start_time_2), \"seconds\") 上面的代码中，item 为 1 2 3 4 5 的五个任务会一直占用所有的 workers，而 6 7 8 9 10 这五个任务会永远等待！！！ ","date":"2019-03-15","objectID":"/posts/python-concurrency-pool-executor/:0:0","series":null,"tags":["Python","Concurrency","并发"],"title":"Python 并发编程：PoolExecutor 篇","uri":"/posts/python-concurrency-pool-executor/#"},{"categories":["tech"],"content":" API 详细说明concurrent.futures 包含三个部分的 API： PoolExecutor：也就是两个执行器的 API 构造器：主要的参数是 max_workers，用于指定线程池大小（或者说 workers 个数） submit(fn, *args, **kwargs)：将任务函数 fn 提交到执行器，args 和 kwargs 就是 fn 需要的参数。 返回一个 future，用于获取结果 map(func, *iterables, timeout=None, chunksize=1)：当任务是同一个，只有参数不同时， 可以用这个方法代替 submit。iterables 的每个元素对应 func 的一组参数。 返回一个 futures 的迭代器 shutdown(wait=True)：关闭执行器，一般都使用 with 管理器自动关闭。 Future：任务被提交给执行器后，会返回一个 future future.result(timeout=None)：最常用的方法，返回任务的结果。如果任务尚未结束，这个方法会一直等待！ timeout 指定超时时间，为 None 时没有超时限制。超时会抛出 concurrent.futures.TimeoutError 异常。 如果调用抛出了异常，result 会抛出同样的异常 如果调用被取消，result 抛出 CancelledError 异常 exception(timeout=None)：返回任务抛出的异常。和 result() 一样，也会等待任务结束。 timeout 参数跟 result 一致，超时会抛出 concurrent.futures.TimeoutError 异常。 如果调用抛出了异常，exception 会返回同样的异常，否则返回 None 如果调用被取消，result 抛出 CancelledError 异常 cancel()：取消此任务 add_done_callback(fn)：future 完成后，会执行 fn(future)。 running()：是否正在运行 done()：future 是否已经结束了，boolean …详见官方文档 模块带有的实用函数 concurrent.futures.as_completed(fs, timeout=None)：等待 fs （futures iterable）中的 future 完成 一旦 fs 中的某 future 完成了，这个函数就立即返回该 future。 这个方法，使每次返回的 future，总是最先完成的 future。而不是先等待任务 1，再等待任务 2… 常通过 for future in as_completed(fs): 使用此函数。 concurrent.futures.wait(fs, timeout=None, return_when=ALL_COMPLETED)：一直等待，直到 return_when 所指定的事发生，或者 timeout return_when 有三个选项：ALL_COMPLETED（fs 中的 futures 全部完成），FIRST__COMPLETED（fs 中任意一个 future 完成）还有 FIRST_EXCEPTION（某任务抛出异常） ","date":"2019-03-15","objectID":"/posts/python-concurrency-pool-executor/:1:0","series":null,"tags":["Python","Concurrency","并发"],"title":"Python 并发编程：PoolExecutor 篇","uri":"/posts/python-concurrency-pool-executor/#api-详细说明"},{"categories":["tech"],"content":" Future 设计模式这里的 PoolExecutor 的特点，在于它使用了 Future 设计模式，使任务的执行，与结果的获取，变成一个异步的流程。 我们先通过 submit/map 将任务放入任务队列，这时任务就已经开始执行了！ 然后我们在需要的时候，通过 future 获取结果，或者直接 add_done_callback(fn)。 这里任务的执行是在新的 workers 中的，主进程/线程不会阻塞，因此主线程可以干其他的事。这种方式被称作异步编程。 ","date":"2019-03-15","objectID":"/posts/python-concurrency-pool-executor/:2:0","series":null,"tags":["Python","Concurrency","并发"],"title":"Python 并发编程：PoolExecutor 篇","uri":"/posts/python-concurrency-pool-executor/#future-设计模式"},{"categories":["tech"],"content":" 画外concurrent.futures 基于 multiprocessing.pool 实现，因此实际上它比直接使用 线程/进程 的 Pool 要慢一点。但是它提供了更方便简洁的 API。 ","date":"2019-03-15","objectID":"/posts/python-concurrency-pool-executor/:3:0","series":null,"tags":["Python","Concurrency","并发"],"title":"Python 并发编程：PoolExecutor 篇","uri":"/posts/python-concurrency-pool-executor/#画外"},{"categories":["tech"],"content":" 参考 使用Python进行并发编程-PoolExecutor篇 Python Parallel Programming Cookbook concurrent.futures — Launching parallel tasks 进程线程协程与并发并行 并行设计模式（一）– Future模式 ","date":"2019-03-15","objectID":"/posts/python-concurrency-pool-executor/:4:0","series":null,"tags":["Python","Concurrency","并发"],"title":"Python 并发编程：PoolExecutor 篇","uri":"/posts/python-concurrency-pool-executor/#参考"},{"categories":["tech"],"content":" 个人笔记，不保证正确。 虽然说看到很多人不看好 asyncio，但是这个东西还是必须学的。。基于协程的异步，在很多语言中都有，学会了 Python 的，就一通百通。 ","date":"2019-02-14","objectID":"/posts/python-asyncio/:0:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#"},{"categories":["tech"],"content":" 一、生成器 generatorPython 的 asyncio 是通过 generator 实现的，要学习 async，先得复习下 generator. ","date":"2019-02-14","objectID":"/posts/python-asyncio/:1:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#一生成器-generator"},{"categories":["tech"],"content":" 1. yield众所周知，yield 是用于定义 generator 函数的关键字，调用该函数，会返回一个 generator python \u003e\u003e\u003e def f(): ... yield 1 ... yield 2 ... \u003e\u003e\u003e f() # 返回的是 generator \u003cgenerator object f at 0x7f672c460570\u003e \u003e\u003e\u003e g = f() \u003e\u003e\u003e next(g) # 通过 next 方法从 generator 获取值 1 \u003e\u003e\u003e g.__next__() # next 方法实际是调用了 generator 的 __next__ 方法 2 \u003e\u003e\u003e next(g) # 生成器运行结束，产生一个 StopIteration 的 exception Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e StopIteration 每次调用 next，generator 都只会运行到下一个 yield 关键字所在行，返回 yield 右侧的对象，然后暂停在该处，等待下一次 next 调用。 从上面的例子看，yield 就是延迟求值而已。但是 yield 还有一个特性，就是它是一个 expression，有返回值！看例子： Python \u003e\u003e\u003e def func(): ... r = yield 1 ... yield r ... \u003e\u003e\u003e g = func() \u003e\u003e\u003e next(g) 1 \u003e\u003e\u003e next(g) # 通过 next 调用，yield 的返回值为 None \u003e\u003e\u003e g2 = func() \u003e\u003e\u003e next(g2) # 首先需要通过 next 调用，运行到 yield 语句处 1 \u003e\u003e\u003e g2.send(419) # 现在用 send 方法，这会将当前所在的 yield 语句的值设置为你 send 的值，也就是 419 419 # 然后 generator 运行到下一个 yield，返回右边的值并暂停 generator 有四个实例函数：next、send 是刚刚已经介绍了的，此外还有 throw 用于从 yield 所在处抛出 Exception，和 close 用于关闭 Generator。详见Generator-iterator methods ","date":"2019-02-14","objectID":"/posts/python-asyncio/:1:1","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#1-yield"},{"categories":["tech"],"content":" 2. yield from 可以理解成是 yield \u003cvalue\u003e from \u003citerable\u003e，每次调用时它都会从 \u003citerable\u003e 中取值，直到遇到 StopIteration。才会从下一个 yield 取值。 python \u003e\u003e\u003e def f(): ... yield from [1, 2, 3, 4] # iterable ... yield 5 ... yield from range(4, 0, -1) # iterable ... \u003e\u003e\u003e list(f()) [1, 2, 3, 4, 5, 4, 3, 2, 1] 当然，yield from \u003citerable\u003e 也是一个 expression，也有值。它的值就是 StopIteration 异常的第一个参数，内置类型的这个值都是 None. python \u003e\u003e\u003e def f(): ... r = yield from [1, 2] ... yield f\"value of yield from is {r}\" ... \u003e\u003e\u003e list(f()) [1, 2, 'value of yield from is None'] 当 \u003citerable\u003e 是 generator 时，yield from 会直接将函数调用委托给这个子 generator，这里的调用包括了前面说过的 next、send、throw、close 四个函数。并直接将 sub generator yield 的值 yield 给 caller. ","date":"2019-02-14","objectID":"/posts/python-asyncio/:1:2","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#2-yield-from-iterable"},{"categories":["tech"],"content":" 3. yield 和 return 混用会发生什么？generator 中的 return value，语义上等同于 raise StopIteration(value)： shell \u003e\u003e\u003e def f(): ... yield 1 ... return 2 ... yield 3 # 永远不会被执行 ... \u003e\u003e\u003e g = f() \u003e\u003e\u003e next(g) 1 \u003e\u003e\u003e next(g) # return 引发 StopIteration Traceback (most recent call last): File \"\u003cinput\u003e\", line 1, in \u003cmodule\u003e StopIteration: 2 \u003e\u003e\u003e next(g) # 再次调用，StopIteration 变成无参了。 Traceback (most recent call last): File \"\u003cinput\u003e\", line 1, in \u003cmodule\u003e StopIteration 可以看到 return 引发了 StopIteration 异常，而 return 的值则成了该异常的第一个参数。 之前说过 yield from \u003csub generator\u003e 表达式的值，就是该 \u003csub generator\u003e 的 StopIteration 异常的第一个参数，因此： shell \u003e\u003e\u003e def f2(): ... a = yield from f() ... yield a # a 是 f() 中 return 的值 ... \u003e\u003e\u003e list(f2()) [1, 2] PEP 479 – Change StopIteration handling inside generators 修改了StopIteration 的行为，该 PEP 使人为 raise 的 StopIteration 引发一个 RuntimeError。该 PEP 在 Python 3.5 版本添加到 future 中，并在 Python 3.7 成为默认行为。因此除非你确实想要引发异常，否则应该使用 return 来结束一个 generator 并返回值。 ","date":"2019-02-14","objectID":"/posts/python-asyncio/:1:3","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#3-yield-和-return-混用会发生什么"},{"categories":["tech"],"content":" 二、异步IO、协程与非阻塞 IO先了解一下 进程线程协程与并发并行 和各种 IO 模型 ","date":"2019-02-14","objectID":"/posts/python-asyncio/:2:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#二异步io协程与非阻塞-io"},{"categories":["tech"],"content":" 三、asyncio 的简单使用asyncio 引入了两个新关键字：async 和 await，其中 async 能放在三个地方： async def：用于定义异步函数和异步生成器 不含有 yield 的是 async def 定义的是协程函数（coroutine function），调用该函数返回协程对象（coroutine object），协程对象需要通过 EventLoop 运行。 内部含有 yield 的 async def 定义的是异步生成器函数（asynchronous generator function），调用该函数返回异步生成器（async_generator） 异步生成器只能用在 Coroutine 中 async def 中不允许使用 yield from async for：表示 for 迭代的是一个异步生成器，该 for 循环的每一次迭代，都是异步的。 只能用在 async def 的内部 async with：表示 with 管理的是一个异步上下文管理器（asynchronous context manager） 该 context manager 的 enter 和 exit 两个步骤是异步的 只能用在 async def 的内部 注意异步 generator、context manager，它的 protocol 都和同步的不同，不能混为一谈。具体而言，对同步 protocol xxx 函数，它的异步版本为 axxx，就是加个 a。 而 await，就相当于 yield from，差别在于 await 是异步的。还有我们关心的是 await 表达式的值，而 yield from 中我们更关心它向上层 yield 的值。 在 yield from 中，当前生成器调用另一个生成器，当前生成器会挂起，直到另一个生成器返回。 但是在 await 中，当前 Coroutine 挂起时， eventloop 会寻找其他 task 来跑，这就利用上了 IO 漫长的等待时间。 async for 是每次迭代都会 await 一次，如果迭代对象是 IO 操作，这个 IO 等待时间就会被利用上。 async with 也是同样，如果 context 的 enter 和 exit 是 IO 操作，这个 IO 时间就会被 eventloop 用于运行其他 task. 使用 asyncio 时，我们要用 async def 将所有的 IO 操作都定义成异步操作。然后在调用时，都使用 await/async for/async with 来调用。 ","date":"2019-02-14","objectID":"/posts/python-asyncio/:3:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#三asyncio-的简单使用"},{"categories":["tech"],"content":" 四、Coroutine、Task 和 Future首先，每个协程对象，都是一个独立的协程单元，协程对象之间可以异步运行。 协程需要放到 EventLoop 内运行，要运行一个协程 a，有三种方法： 通过 asyncio.run(coro) 运行一个协程。 该方法会新建一个 EventLoop 在另一个协程 b 中通过 await 调用 a。当 b 运行时， a 也会被 task 运行。 通过 asyncio.create_task(coro)，将需要运行的协程包装成 task，然后通过 task 相关的方法来异步运行它们。 asyncio.gather(*awaitable_objects): 并发执行所有的 task，阻塞到所有 task 结束。返回一个 result 列表。result 的列表顺序和 future 的顺序一致 asyncio.as_completed(aws, *, loop=None, timeout=None)，和 gather 的区别在于，它返回一个异步迭代器，每次迭代都返回最先完成的一个 future. concurrent.futures 是进程线程的异步执行，而 asyncio 是基于协程的单线程异步执行 ","date":"2019-02-14","objectID":"/posts/python-asyncio/:4:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#四coroutinetask-和-future"},{"categories":["tech"],"content":" 五、参考 从0到1，Python异步编程的演进之路 怎么掌握 asyncio Python Async/Await入门指南 谈谈Python协程技术的演进 Python Doc - Coroutines Python Doc - asyncio ","date":"2019-02-14","objectID":"/posts/python-asyncio/:5:0","series":null,"tags":["Python","asyncio"],"title":"Python 异步编程笔记：asyncio","uri":"/posts/python-asyncio/#五参考"},{"categories":["tech"],"content":"照例先看层次图 SQLAlchemy 层次结构 ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#"},{"categories":["tech"],"content":" 一、声明映射关系使用 ORM 时，我们首先需要定义要操作的表（通过 Table），然后再定义该表对应的 Python class，并声明两者之间的映射关系（通过 Mapper）。 方便起见，SQLAlchemy 提供了 Declarative 系统来一次完成上述三个步骤，Declarative 系统提供 base class，这个 base class 会为继承了它的 Python class（可称作 model）创建 Table，并维护两者的映射关系。 python from sqlalchemy.ext.declarative import declarative_base from SQLAlchemy import Column, Integer, String Base = declarative_base() # 拿到 Base 类 class User(Base): id = Column(Integer, primary_key=True) username = Column(String(32), nullable=False, index=True) # 添加 index 提升搜索效率 fullname = Column(String(64)) password = Column(String(32)) # 真实情况下一般只存 hash def __repr__(self): return f\"\u003cUser {self.username}\u003e\" 这样就声明好了一个对象-关系映射，上一篇文章说过所有的 Table 都在某个 MetaData 中，可以通过Base.metadata 获取它。 python Base.metadata.create_all(engine) # 通过 metadata 创建表（或者说生成模式 schema） engine 的创建请见上篇文档SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言 约束条件 可参考 SQL 基础笔记（三）：约束 与SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言 - 表定义中的约束 使用 ORM 来定义约束条件，与直接使用 SQL 表达式语言定义很类似，也有两种方法： 直接将约束条件作为 Column、ForeignKey 的参数传入。这种方式最简洁，也最常用。 使用 UniqueConstraint、CheckConstraint 等类构造约束，然后放入 __table_args__ 属性中。举例： python3 class User(Base): id = Column(Integer, primary_key=True) username = Column(String(32), nullable=False, index=True) # 添加 index 提升搜索效率 fullname = Column(String(64)) password = Column(String(32)) # 真实情况下一般只存 hash # 顾名思义，这是 `Table` 类的参数的序列。里面的约束条件会被用于构建 __table__ __table_args__ = (UniqueConstraint('username', name='c_user'),) # username 的唯一性约束 def __repr__(self): return f\"\u003cUser {self.username}\u003e\" ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#一声明映射关系"},{"categories":["tech"],"content":" 一、声明映射关系使用 ORM 时，我们首先需要定义要操作的表（通过 Table），然后再定义该表对应的 Python class，并声明两者之间的映射关系（通过 Mapper）。 方便起见，SQLAlchemy 提供了 Declarative 系统来一次完成上述三个步骤，Declarative 系统提供 base class，这个 base class 会为继承了它的 Python class（可称作 model）创建 Table，并维护两者的映射关系。 python from sqlalchemy.ext.declarative import declarative_base from SQLAlchemy import Column, Integer, String Base = declarative_base() # 拿到 Base 类 class User(Base): id = Column(Integer, primary_key=True) username = Column(String(32), nullable=False, index=True) # 添加 index 提升搜索效率 fullname = Column(String(64)) password = Column(String(32)) # 真实情况下一般只存 hash def __repr__(self): return f\"\" 这样就声明好了一个对象-关系映射，上一篇文章说过所有的 Table 都在某个 MetaData 中，可以通过Base.metadata 获取它。 python Base.metadata.create_all(engine) # 通过 metadata 创建表（或者说生成模式 schema） engine 的创建请见上篇文档SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言 约束条件 可参考 SQL 基础笔记（三）：约束 与SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言 - 表定义中的约束 使用 ORM 来定义约束条件，与直接使用 SQL 表达式语言定义很类似，也有两种方法： 直接将约束条件作为 Column、ForeignKey 的参数传入。这种方式最简洁，也最常用。 使用 UniqueConstraint、CheckConstraint 等类构造约束，然后放入 __table_args__ 属性中。举例： python3 class User(Base): id = Column(Integer, primary_key=True) username = Column(String(32), nullable=False, index=True) # 添加 index 提升搜索效率 fullname = Column(String(64)) password = Column(String(32)) # 真实情况下一般只存 hash # 顾名思义，这是 `Table` 类的参数的序列。里面的约束条件会被用于构建 __table__ __table_args__ = (UniqueConstraint('username', name='c_user'),) # username 的唯一性约束 def __repr__(self): return f\"\" ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#约束条件"},{"categories":["tech"],"content":" 二、获取 session上一节讲 engine 时，我们是通过 connection 来与数据库交互，而在 ORM 中我们使用 Session 访问数据库。 python from sqlalchemy.orm import sessionmaker Session = sessionmaker(bind=engine) # 获取 session ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:2","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#二获取-session"},{"categories":["tech"],"content":" 三、增删改查直接使用 SQL 表达式语言时，我们使用 insert()、select()、update()、delete() 四个函数构造 SQL，使用 where() 添加条件，使用 model.join(another_model) 进行 join 操作。而使用 ORM 时， 数据库操作不再与 SQL 直接对应。我们现在是通过操作 Python 对象来操作数据库了。 现在，我们通过 db.session.add()、db.session.delete() 进行添加与删除，使用 db.session.query(Model) 进行查询，通过 filter 和 filter_by 添加过滤条件。而修改，则是先查询出对应的 row 对象，直接修改这个对象，然后 commit 就行。 增添： python ed_user = User(name='ed', fullname='Ed Jones', password='edspassword') # 用构造器构造对象 session.add(ed_user) # 添加，此外还有批量添加 add_all([user1, user2...]) session.commit() # 必须手动 commit 修改： python ed_user = session.query(User).filter_by(name='ed').first() # 先获取到 User 对象 ed_user.password = 'f8s7ccs' # 改了密码 session.commit() # 提交 # 批量修改 session.query(User).filter(User.home=='shanghai') \\ .update({User.login_num:0}) # 将所有上海的用户的 login_num 设为 0 session.commit() 删除： python ed_user = session.query(User).filter_by(name='ed').first() # 先获取到 User 对象 session.delete(ed_user) # 直接删除（session 知道 ed_user 属于哪个表） session.commit() # 提交 # 批量删除 session.query(User).filter(User.home=='shanghai') \\ .delete() # 删除所有上海的用户 session.commit() 同样的，也可以在外面检查异常，然后调用 session.rollback() 实现失败回滚。 ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:3","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#三增删改查"},{"categories":["tech"],"content":" 四、进阶查询 filter_by：使用关键字参数进行过滤，前面的演示中已经用过多次了。 filter：它对应 SQL 表达式语言中的 where，支持各种复杂的 SQL 语法。 group_by: 通过指定 column 分组 distinct(): 去重 join(): 关联 python query.filter(User.name == 'ed') # 这个等同于 filter_by，但是更繁琐 query.filter(User.name != 'ed') # 不等于，这个就是 filter_by 无法做到的了 query.filter(User.name.like('%ed%')) # SQL like 的 like 语法 query.filter(User.name.in_(['ed', 'wendy', 'jack'])) # 包含 # 查询还可以嵌套 query.filter(User.name.in_( session.query(User.name).filter(User.name.like('%ed%')) )) query.filter(~User.name.in_(['ed', 'wendy', 'jack'])) # 不包含 query.filter(User.name == None) # NULL 对应 Python 的 None from sqlalchemy import or_, and_, in_ query.filter(or_(User.name == 'ed', User.name == 'wendy')) # OR 语法 query.group_by(User.name) # 分组 query.distinct() # 去重 from sqlalchemy import func # SQL 函数包 session.query(func.count(User.name)).filter_by(xxx=xxx) # 使用 count 函数 # join 关联 # 默认使用内联（inner），即只取两表的交集 session.query(User, Address).filter(User.id==Address.user_id) # 方法一 session.query(User).join(Address).\\ # 方法二 filter(Address.email_address=='jack@google.com') # 外联 outer join，将另一表的列联结到主表，没有的行为 NULL session.query(User).outerjoin(User.addresses) \\ .filter(Address.email_address=='jack@google.com') 执行查询，获取数据查询返回 query 对象，但 SQL 还没有被执行，直到你调用下列几个方法： python # 构造 query 对象 query = session.query(User).filter(User.name.like('%ed')).order_by(User.id) # 1. all 返回所有结果的列表 res_list = query.all() # 2. first 先在 SQL 中加入限制 `limit 1`，然后执行。 res = query.first() # 3. one 执行 sql 并获取所有结果 # 如果结果不止一行，抛出 MultipleResultsFound Error！！！ # 如果结果为空，抛出 NoResultFound Error ！！！ res = query.one() 4. one_or_none 差别在于结果为空，它不抛出异常，而是返回 None res = query.one_or_none() ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:4","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#四进阶查询"},{"categories":["tech"],"content":" 四、进阶查询 filter_by：使用关键字参数进行过滤，前面的演示中已经用过多次了。 filter：它对应 SQL 表达式语言中的 where，支持各种复杂的 SQL 语法。 group_by: 通过指定 column 分组 distinct(): 去重 join(): 关联 python query.filter(User.name == 'ed') # 这个等同于 filter_by，但是更繁琐 query.filter(User.name != 'ed') # 不等于，这个就是 filter_by 无法做到的了 query.filter(User.name.like('%ed%')) # SQL like 的 like 语法 query.filter(User.name.in_(['ed', 'wendy', 'jack'])) # 包含 # 查询还可以嵌套 query.filter(User.name.in_( session.query(User.name).filter(User.name.like('%ed%')) )) query.filter(~User.name.in_(['ed', 'wendy', 'jack'])) # 不包含 query.filter(User.name == None) # NULL 对应 Python 的 None from sqlalchemy import or_, and_, in_ query.filter(or_(User.name == 'ed', User.name == 'wendy')) # OR 语法 query.group_by(User.name) # 分组 query.distinct() # 去重 from sqlalchemy import func # SQL 函数包 session.query(func.count(User.name)).filter_by(xxx=xxx) # 使用 count 函数 # join 关联 # 默认使用内联（inner），即只取两表的交集 session.query(User, Address).filter(User.id==Address.user_id) # 方法一 session.query(User).join(Address).\\ # 方法二 filter(Address.email_address=='jack@google.com') # 外联 outer join，将另一表的列联结到主表，没有的行为 NULL session.query(User).outerjoin(User.addresses) \\ .filter(Address.email_address=='jack@google.com') 执行查询，获取数据查询返回 query 对象，但 SQL 还没有被执行，直到你调用下列几个方法： python # 构造 query 对象 query = session.query(User).filter(User.name.like('%ed')).order_by(User.id) # 1. all 返回所有结果的列表 res_list = query.all() # 2. first 先在 SQL 中加入限制 `limit 1`，然后执行。 res = query.first() # 3. one 执行 sql 并获取所有结果 # 如果结果不止一行，抛出 MultipleResultsFound Error！！！ # 如果结果为空，抛出 NoResultFound Error ！！！ res = query.one() 4. one_or_none 差别在于结果为空，它不抛出异常，而是返回 None res = query.one_or_none() ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:4","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#执行查询获取数据"},{"categories":["tech"],"content":" 参考 SQLAlchemy 对象关系入门 SQLAlchemy ORM 的关联映射定义：一对多、多对多 hackersandslackers/sqlalchemy-tutorial ","date":"2019-02-11","objectID":"/posts/sqlalchemy-notes-2-orm-basics/:0:5","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（二）：ORM 基础","uri":"/posts/sqlalchemy-notes-2-orm-basics/#参考"},{"categories":["tech"],"content":" 一、WebSocketWebSocket 是一个双向通信协议，它在握手阶段采用 HTTP/1.1 协议（暂时不支持 HTTP/2）。 握手过程如下： 首先客户端向服务端发起一个特殊的 HTTP 请求，其消息头如下： headers GET /chat HTTP/1.1 // 请求行 Host: server.example.com Upgrade: websocket // required Connection: Upgrade // required Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ== // required，一个 16bits 编码得到的 base64 串 Origin: http://example.com // 用于防止未认证的跨域脚本使用浏览器 websocket api 与服务端进行通信 Sec-WebSocket-Protocol: chat, superchat // optional, 子协议协商字段 Sec-WebSocket-Version: 13 如果服务端支持该版本的 WebSocket，会返回 101 响应，响应标头如下： headers HTTP/1.1 101 Switching Protocols // 状态行 Upgrade: websocket // required Connection: Upgrade // required Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo= // required，加密后的 Sec-WebSocket-Key Sec-WebSocket-Protocol: chat // 表明选择的子协议 握手完成后，接下来的 TCP 数据包就都是 WebSocket 协议的帧了。 可以看到，这里的握手不是 TCP 的握手，而是在 TCP 连接内部，从 HTTP/1.1 upgrade 到 WebSocket 的握手。 WebSocket 提供两种协议：不加密的 ws:// 和 加密的 wss://. 因为是用 HTTP 握手，它和 HTTP 使用同样的端口：ws 是 80（HTTP），wss 是 443（HTTPS） 在 Python 编程中，可使用 websockets 实现的异步 WebSocket 客户端与服务端。此外 aiohttp 也提供了 WebSocket 支持。 Note：如果你搜索 Flask 的 WebSocket 插件，得到的第一个结果很可能是Flask-SocketIO。但是Flask-ScoektIO 使用的是它独有的 SocketIO 协议，并不是标准的 WebSocket。只是它刚好提供与 WebSocket 相同的功能而已。 SocketIO 的优势在于只要 Web 端使用了 SocketIO.js，就能支持该协议。而纯 WS 协议，只有较新的浏览器才支持。对于客户端非 Web 的情况，更好的选择可能是使用 Flask-Sockets。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:1:0","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#一websocket"},{"categories":["tech"],"content":" JS API javascript // WebSocket API var socket = new WebSocket(\"ws://websocket.example.com\") // Show a connected message when the WebSocket is opened. socket.onopen = function (event) { console.log(\"WebSocket is connected.\") } // Handle messages sent by the server. socket.onmessage = function (event) { var message = event.data console.log(message) } // Handle any error that occurs. socket.onerror = function (error) { console.log(\"WebSocket Error: \" + error) } ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:1:1","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#js-api"},{"categories":["tech"],"content":" 二、HTTP/2HTTP/2 于 2015 年标准化，主要目的是优化性能。其特性如下： 二进制协议：HTTP/2 的消息头使用二进制格式，而非文本格式。并且使用专门设计的 HPack 算法压缩。 多路复用（Multiplexing）：就是说 HTTP/2 可以重复使用同一个 TCP 连接，并且连接是多路的， 多个请求或响应可以同时传输。 对比之下，HTTP/1.1 的长连接也能复用 TCP 连接，但是只能串行，不能“多路”。 服务器推送：服务端能够直接把资源推送给客户端，当客户端需要这些文件的时候，它已经在客户端了。（该推送对 Web App 是隐藏的，由浏览器处理） HTTP/2 允许取消某个正在传输的数据流（通过发送 RST_STREAM 帧），而不关闭 TCP 连接。 这正是二进制协议的好处之一，可以定义多种功能的数据帧。 它允许服务端将资源推送到客户端缓存，我们访问淘宝等网站时，经常会发现很多请求的请求头部分会提示“provisional headers are shown”，这通常就是直接从缓存加载了资源，因此请求根本没有被发送。观察 Chrome Network 的 Size 列，这种请求的该字段一般都是 from disk cache 或者from memory cache. Chrome 可以通过如下方式查看请求使用的协议： 2019-02-10: 使用 Chrome 查看，目前主流网站基本都已经部分使用了 HTTP/2，知乎、bilibili、GIthub 使用了 wss 协议，也有很多网站使用了 SSE（格式如data:image/png;base64,\u003cbase64 string\u003e）而且很多网站都有使用 HTTP/2 + QUIC，该协议的新名称是 HTTP/3，它是基于 UDP 的 HTTP 协议。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:0","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#二http2"},{"categories":["tech"],"content":" SSE服务端推送事件，是通过 HTTP 长连接进行信息推送的一个功能。它首先由浏览器向服务端建立一个 HTTP 长连接，然后服务端不断地通过这个长连接将消息推送给浏览器。JS API 如下： javascript // create SSE connection var source = new EventSource(\"/dates\") // 连接建立时，这些 API 和 WebSocket 的很相似 source.onopen = function (event) { // handle open event } // 收到消息时（它只捕获未命名 event） source.onmessage = function (event) { var data = event.data // 发送过来的实际数据（string） var origin = event.origin // 服务器端URL的域名部分，即协议、域名和端口。 var lastEventId = event.lastEventId // 数据的编号，由服务器端发送。如果没有编号，这个属性为空。 // handle message } source.onerror = function (event) { // handle error event } ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:1","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#sse"},{"categories":["tech"],"content":" 具体的实现在收到客户端的 SSE 请求（HTTP 协议）时，服务端返回的响应首部如下： headers Content-Type: text/event-stream Cache-Control: no-cache Connection: keep-alive 而 body 部分，SSE 定义了四种信息： data：数据栏 event：自定义数据类型 id ：数据 id retry：最大间隔时间，超时则重新连接 body 举例说明： text : 这种格式的消息是注释，会被忽略\\n\\n : 通常服务器每隔一段时间就会发送一个注释，防止超时 retry\\n\\n : 下面这个是一个单行数据\\n\\n data: some text\\n\\n : 下面这个是多行数据，在客户端会重组成一个 data\\n\\n data: {\\n data: \"foo\": \"bar\",\\n data: \"baz\", 555\\n data: }\\n\\n : 这是一个命名 event，只会被事件名与之相同的 listener 捕获\\n\\n event: foo\\n data: a foo event\\n\\n : 未命名事件，会被 onmessage 捕获\\n\\n data: an unnamed event\\n\\n event: bar\\n data: a bar event\\n\\n : 这个 id 对应 event.lastEventId\\n\\n id: msg1\\n data: message\\n\\n ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:2","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#具体的实现"},{"categories":["tech"],"content":" WebSocket、HTTP/2 与 SSE 的比较 加密与否： WebSocket 支持明文通信 ws:// 和加密 wss://， 而 HTTP/2 协议虽然没有规定必须加密，但是主流浏览器都只支持 HTTP/2 over TLS. SSE 是使用的 HTTP 协议通信，支持 http/https 消息推送： WebSocket是全双工通道，可以双向通信。而且消息是直接推送给 Web App. SSE 只能单向串行地从服务端将数据推送给 Web App. HTTP/2 虽然也支持 Server Push，但是服务器只能主动将资源推送到客户端缓存！并不允许将数据推送到客户端里跑的 Web App 本身。服务器推送只能由浏览器处理，不会在应用程序代码中弹出服务器数据，这意味着应用程序没有 API 来获取这些事件的通知。 为了接近实时地将数据推送给 Web App， HTTP/2 可以结合 SSE（Server-Sent Event）使用。 WebSocket 在需要接近实时双向通信的领域，很有用武之地。而 HTTP/2 + SSE 适合用于展示实时数据。 另外在客户端非浏览器的情况下，使用不加密的 HTTP/2 也是可能的。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:3","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#websockethttp2-与-sse-的比较"},{"categories":["tech"],"content":" requests 查看 HTTP 协议版本号可以通过 resp.raw.version 得到响应的 HTTP 版本号： shell \u003e\u003e\u003e import requests \u003e\u003e\u003e resp = requests.get(\"https://zhihu.com\") \u003e\u003e\u003e resp.raw.version 11 但是 requests 默认使用 HTTP/1.1，并且不支持 HTTP/2.（不过这也不是什么大问题，HTTP/2 只是做了性能优化，用 HTTP/1.1 也就是慢一点而已。） ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:2:4","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#requests-查看-http-协议版本号"},{"categories":["tech"],"content":" 三、gRPC 协议gRPC 是一个远程过程调用框架，默认使用 protobuf3 进行数据的高效序列化与 service 定义，使用 HTTP/2 进行数据传输。这里讨论的是gRPC over HTTP/2 协议。 目前 gRPC 主要被用在微服务通信中，但是因为其优越的性能，它也很契合游戏、loT 等需要高性能低延迟的场景。 其实光从协议先进程度上讲，gRPC 基本全面超越 REST: 使用二进制进行数据序列化，比 json 更节约流量、序列化与反序列化也更快。 protobuf3 要求 api 被完全清晰的定义好，而 REST api 只能靠程序员自觉定义。 gRPC 官方就支持从 api 定义生成代码，而 REST api 需要借助 openapi-codegen 等第三方工具。 支持 4 种通信模式：一对一(unary)、客户端流、服务端流、双端流。更灵活 只是目前 gRPC 对 browser 的支持仍然不是很好，如果你需要通过浏览器访问 api，那 gRPC 可能不是你的菜。如果你的产品只打算面向 App 等可控的客户端，可以考虑上 gRPC。 对同时需要为浏览器和 APP 提供服务应用而言，也可以考虑 APP 使用 gRPC 协议，而浏览器使用 API 网关提供的 HTTP 接口，在 API 网关上进行 HTTP - gRPC 协议转换。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:3:0","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#三grpc-协议"},{"categories":["tech"],"content":" gRPC over HTTP/2 定义详细的定义参见官方文档gRPC over HTTP/2. 这里是简要说明几点： gRPC 完全隐藏了 HTTP/2 本身的 method、headers、path 等语义，这些信息对用户而言完全不可见了。 请求统一使用 POST，响应状态统一为 200。只要响应是标准的 gRPC 格式，响应中的 HTTP 状态码将被完全忽略。 gRPC 定义了自己的 status 状态码、格式固定的 path、还有它自己的 headers。 ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:3:1","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#grpc-over-http2-定义"},{"categories":["tech"],"content":" 参考 深入探索 WebSockets 和 HTTP/2 HTTP/2 特性与抓包分析 SSE：服务器发送事件,使用长链接进行通讯 Using server-sent events - MDN Can I Use HTTP/2 on Browsers Python 3.x how to get http version (using requests library) WebSocket 是什么原理？ 原生模块打造一个简单的 WebSocket 服务器 Google Cloud - API design: Understanding gRPC, OpenAPI and REST and when to use them ","date":"2019-02-11","objectID":"/posts/websocket-http2-and-grpc/:4:0","series":null,"tags":["WebSocket","gRPC","HTTP/2"],"title":"WebSocket、HTTP/2 与 gRPC","uri":"/posts/websocket-http2-and-grpc/#参考"},{"categories":["tech"],"content":"不管是做爬虫还是写 Web App，Chrome 和 Firefox 的 DevTools 都是超常用的，但是经常发现别人的截图有什么字段我找不到，别人的什么功能我的 Chrome 没有，仔细一搜索才知道，原来是我不会用。。 Ctrl + Shift + I：打开 DevTools Ctrl + Shift + J：打开控制台 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:0","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#"},{"categories":["tech"],"content":" 搜索 Ctrl + F：在当前位置搜索关键字 在网页界面用这个快捷键，就是页内搜索 在 Network 的 Response 页面，就是在 Response 中搜索 Ctrl + Shift + F：全文搜索，在当前 Web App 的所有文件中搜索。 爬虫必备！！！要寻找一些特殊字符串的来源，用它搜索屡试不爽。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:1","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#搜索"},{"categories":["tech"],"content":" Command在 DevTools 里按快捷键 Ctrl + Shift + P 就能打开 Command 输入框，它和 vscode/sublime 的 Command 一样，用好了特别方便。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:2","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#command"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content， 导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebSocket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#network"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content， 导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebSocket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#1-属性列"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content， 导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebSocket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#2-copy"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content， 导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebSocket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#3-response-的-pretty-print"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content， 导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebSocket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#4-导出-har"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content， 导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebSocket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#5-raw-headers原始消息头"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content， 导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebSocket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#6-审查-websocketchrome-only"},{"categories":["tech"],"content":" Network 1. 属性列Chrome 可以右键属性列名来增减属性列，Firefox-Dev 也是一样： 2. copy在 Chrome 中右键某个请求，选中 copy，会给出几个 options： 而 Firefox-Dev 的更强一点，可以复制消息头（请求头和响应头）： 3. response 的 pretty printChrome 的 Response 页面左下角，有{}按钮，可以 beautify 响应。 而 Firefox-Dev 只在 Debugger 页面提供该按钮，Response 中不支持。 Firefox 响应的 preview 和 payload 是放在 Response 页面下。而 Chrome 是分成了两个标签页 4. 导出 HAR右键任意一个请求，选择 save all as HAR，就可以导出 HAR 文件。（Chrome 有 with content， 导出的 HAR 文件会含有所有请求与响应的 body） 该文件可用于存储当前监听到的所有浏览器请求信息。在以后需要分析这些请求时，将 HAR 文件拖到 Network 页面即可恢复所有请求。 5. Raw Headers（原始消息头）Chrome 只支持查看 HTTP/1.x 的 Raw Headers，对这种请求，会给出 view source 选项。 Chrome 不能查看 HTTP/2 的 Raw Headers。 而 Firefox 则支持查看 HTTP/2 的 Raw Headers。（是恢复后的，HTTP/2 的原始消息头是二进制压缩形式） 它还提供 Edit and Resend 请求的功能，这点要给个赞～ 6. 审查 WebSocket（Chrome only）在 NetWork 中点击对应的 WebSocket 请求，在右侧选择 Frames 标签，就可以看到所有的消息了 7. 跨页面加载时，保留网络请求记录当页面重载或者页面跳转时，默认情况下，Network面板下的网络请求记录表也是刷新的。如果想保留之前页面的网络请求数据，可以勾选Preserve log. 常用的一个应用场景：登录/注册时会调用登录/注册API，开发者想查看这个接口返回的情况，但是登录/注册成功后一般会跳转到新的页面，导致了Network面板的请求记录被刷新从而看不到登录/注册接口返回的情况。此时勾选上Preserve log，无论跳转到那个页面，都能在Network网络请求记录表中查看到之前接口返回的情况。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:3","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#7-跨页面加载时保留网络请求记录"},{"categories":["tech"],"content":" JavaScript 控制台 可以通过 $x(\u003cxpath\u003e, \u003cDOM-element\u003e)，用 xpath 查询 DOM 元素。 通过控制台左上方的选单，可以切换 JS 的环境，它默认是当前页面（top）。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:4","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#javascript-控制台"},{"categories":["tech"],"content":" Elements 页面（Firefox-Dev 是 Inspector 页面） 1. DOM 元素断点（Chrome only）在 Elements 页面，右键一个元素，有一个 Break on 选项，可以控制在特定事件发生时 Break. - subtree modification: 子节点被修改 - attribute modification：当前节点的属性被修改。（inline style 被修改也会触发此事件）- node removal：节点被移除 2. 检索元素上注册的事件（Chrome only）在 Elements 页面选中一个元素（或者直接右键检查该元素），然后在右侧窗口，选择 Event Listeners 标签，就可以看到该元素上注册的所有事件。 3. 颜色选择器选中任一元素，在右侧选择 Styles 选单，会显示当前元素的 css 属性。 其中所有的颜色小方块都是可以点击的，点击颜色方块后 可以将颜色属性转换成多个格式（Chrome only） 默认格式：#207981 RGB格式：rgb(32, 121, 129) HSL格式：hsl(185, 60%, 32%) 提供 color picker，可用于在网页任意位置取色。（Firefox-Dev 也有） 提供复制按键，直接将该颜色当前格式的表达复制到剪切板 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:5","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#elements-页面firefox-dev-是-inspector-页面"},{"categories":["tech"],"content":" Elements 页面（Firefox-Dev 是 Inspector 页面） 1. DOM 元素断点（Chrome only）在 Elements 页面，右键一个元素，有一个 Break on 选项，可以控制在特定事件发生时 Break. - subtree modification: 子节点被修改 - attribute modification：当前节点的属性被修改。（inline style 被修改也会触发此事件）- node removal：节点被移除 2. 检索元素上注册的事件（Chrome only）在 Elements 页面选中一个元素（或者直接右键检查该元素），然后在右侧窗口，选择 Event Listeners 标签，就可以看到该元素上注册的所有事件。 3. 颜色选择器选中任一元素，在右侧选择 Styles 选单，会显示当前元素的 css 属性。 其中所有的颜色小方块都是可以点击的，点击颜色方块后 可以将颜色属性转换成多个格式（Chrome only） 默认格式：#207981 RGB格式：rgb(32, 121, 129) HSL格式：hsl(185, 60%, 32%) 提供 color picker，可用于在网页任意位置取色。（Firefox-Dev 也有） 提供复制按键，直接将该颜色当前格式的表达复制到剪切板 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:5","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#1-dom-元素断点chrome-only"},{"categories":["tech"],"content":" Elements 页面（Firefox-Dev 是 Inspector 页面） 1. DOM 元素断点（Chrome only）在 Elements 页面，右键一个元素，有一个 Break on 选项，可以控制在特定事件发生时 Break. - subtree modification: 子节点被修改 - attribute modification：当前节点的属性被修改。（inline style 被修改也会触发此事件）- node removal：节点被移除 2. 检索元素上注册的事件（Chrome only）在 Elements 页面选中一个元素（或者直接右键检查该元素），然后在右侧窗口，选择 Event Listeners 标签，就可以看到该元素上注册的所有事件。 3. 颜色选择器选中任一元素，在右侧选择 Styles 选单，会显示当前元素的 css 属性。 其中所有的颜色小方块都是可以点击的，点击颜色方块后 可以将颜色属性转换成多个格式（Chrome only） 默认格式：#207981 RGB格式：rgb(32, 121, 129) HSL格式：hsl(185, 60%, 32%) 提供 color picker，可用于在网页任意位置取色。（Firefox-Dev 也有） 提供复制按键，直接将该颜色当前格式的表达复制到剪切板 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:5","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#2-检索元素上注册的事件chrome-only"},{"categories":["tech"],"content":" Elements 页面（Firefox-Dev 是 Inspector 页面） 1. DOM 元素断点（Chrome only）在 Elements 页面，右键一个元素，有一个 Break on 选项，可以控制在特定事件发生时 Break. - subtree modification: 子节点被修改 - attribute modification：当前节点的属性被修改。（inline style 被修改也会触发此事件）- node removal：节点被移除 2. 检索元素上注册的事件（Chrome only）在 Elements 页面选中一个元素（或者直接右键检查该元素），然后在右侧窗口，选择 Event Listeners 标签，就可以看到该元素上注册的所有事件。 3. 颜色选择器选中任一元素，在右侧选择 Styles 选单，会显示当前元素的 css 属性。 其中所有的颜色小方块都是可以点击的，点击颜色方块后 可以将颜色属性转换成多个格式（Chrome only） 默认格式：#207981 RGB格式：rgb(32, 121, 129) HSL格式：hsl(185, 60%, 32%) 提供 color picker，可用于在网页任意位置取色。（Firefox-Dev 也有） 提供复制按键，直接将该颜色当前格式的表达复制到剪切板 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:5","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#3-颜色选择器"},{"categories":["tech"],"content":" 元素审查Ctrl + Shift + C 或者点击 DevTools 最左上角的小箭头按钮，就能进入元素审查模式。 该模式下会自动审查鼠标所指的元素，Elements 页面也会自动定位到该元素。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:6","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#元素审查"},{"categories":["tech"],"content":" Sources 页面（Firefox-Dev 是 Debuuger 页面）Sources 右侧的 Debugger 支持各种断点调试。 条件断点 Sources 中，在任意 JS 代码的行号上单击鼠标左键，就能在该行设置一个普通断点（在 Response 中可不行）。在行号上右键，能直接设置条件断点。 XHR 断点：在右侧 Debugger 中，可以添加 XHR 断点。 如果条件留空，一旦有 XHR 发起，就会无条件进入调试。 条件是 “Break When URL Contaions ” Watch Expressions：表达式审查 双击选中 JS 代码中的任意变量，然后右键它，可以将该变量添加到 Watch 中，这样就可以随时观察它的值。 也可以在右侧 Watch 中手动输入 JS 表达式。 DOM 元素断点（Chrome only）：在 Elements 部分讲过了。 Chrome 的断点功能比 Firefox-Dev 的更丰富。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:7","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#sources-页面firefox-dev-是-debuuger-页面"},{"categories":["tech"],"content":" Screenshot 1. For Chrome方法一：在 DevTools 界面，按快捷键 Ctrl + Shift + P 打开 Command 窗口，然后输入screenshot，在下拉栏里选择你需要的截图命令就行。 方法二：先进 dev tools，点击 左上角的设备图标（toggle device toolbar），然后页面顶部就会出现一个导航栏，在这里好选择设备或者自定义图像尺寸，然后点击该导航栏右侧（不是 dev tools 右侧）的 options 图标，会有两个选项：“截图（capture screenshot）”和“截网页全图（capture full size screenshot）”，如下： 2. For Firefox 只截显示出来的部分：和 Chrome 一样点击设备图标，然后在页面上面的 toolbar 就有截图按钮 截网页全图：在 DevTools 右边的 options 中进入 Settings，勾选take a screenshot of the entire page，DevTools 右上角就会出现截图按钮了。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:8","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#screenshot"},{"categories":["tech"],"content":" Screenshot 1. For Chrome方法一：在 DevTools 界面，按快捷键 Ctrl + Shift + P 打开 Command 窗口，然后输入screenshot，在下拉栏里选择你需要的截图命令就行。 方法二：先进 dev tools，点击 左上角的设备图标（toggle device toolbar），然后页面顶部就会出现一个导航栏，在这里好选择设备或者自定义图像尺寸，然后点击该导航栏右侧（不是 dev tools 右侧）的 options 图标，会有两个选项：“截图（capture screenshot）”和“截网页全图（capture full size screenshot）”，如下： 2. For Firefox 只截显示出来的部分：和 Chrome 一样点击设备图标，然后在页面上面的 toolbar 就有截图按钮 截网页全图：在 DevTools 右边的 options 中进入 Settings，勾选take a screenshot of the entire page，DevTools 右上角就会出现截图按钮了。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:8","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#1-for-chrome"},{"categories":["tech"],"content":" Screenshot 1. For Chrome方法一：在 DevTools 界面，按快捷键 Ctrl + Shift + P 打开 Command 窗口，然后输入screenshot，在下拉栏里选择你需要的截图命令就行。 方法二：先进 dev tools，点击 左上角的设备图标（toggle device toolbar），然后页面顶部就会出现一个导航栏，在这里好选择设备或者自定义图像尺寸，然后点击该导航栏右侧（不是 dev tools 右侧）的 options 图标，会有两个选项：“截图（capture screenshot）”和“截网页全图（capture full size screenshot）”，如下： 2. For Firefox 只截显示出来的部分：和 Chrome 一样点击设备图标，然后在页面上面的 toolbar 就有截图按钮 截网页全图：在 DevTools 右边的 options 中进入 Settings，勾选take a screenshot of the entire page，DevTools 右上角就会出现截图按钮了。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:8","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#2-for-firefox"},{"categories":["tech"],"content":" 其他 1. Fake Geolocation（Chrome only）在 Chrome 中进入 DevTools，点击右上角的 options 按钮，选择 More tools -\u003e Sensors，在 Geolocation 处选择 Custom location，就可以修改地理位置了。 2. 自定义请求头 For Chrome和 上一小节一样，先进 More tools，选择 Network conditions，取消勾选 Select atuomatically， 就可以修改请求头了。 上面的演示中，使用 python-requests/2.21.0 做 user agent，知乎返回 404. For Firefox打开设备模拟，在 toolbar 的右上角勾选 show user agent，然后就可以在 toolbar 修改 user agent 了： ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:9","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#其他"},{"categories":["tech"],"content":" 其他 1. Fake Geolocation（Chrome only）在 Chrome 中进入 DevTools，点击右上角的 options 按钮，选择 More tools -\u003e Sensors，在 Geolocation 处选择 Custom location，就可以修改地理位置了。 2. 自定义请求头 For Chrome和 上一小节一样，先进 More tools，选择 Network conditions，取消勾选 Select atuomatically， 就可以修改请求头了。 上面的演示中，使用 python-requests/2.21.0 做 user agent，知乎返回 404. For Firefox打开设备模拟，在 toolbar 的右上角勾选 show user agent，然后就可以在 toolbar 修改 user agent 了： ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:9","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#1-fake-geolocationchrome-only"},{"categories":["tech"],"content":" 其他 1. Fake Geolocation（Chrome only）在 Chrome 中进入 DevTools，点击右上角的 options 按钮，选择 More tools -\u003e Sensors，在 Geolocation 处选择 Custom location，就可以修改地理位置了。 2. 自定义请求头 For Chrome和 上一小节一样，先进 More tools，选择 Network conditions，取消勾选 Select atuomatically， 就可以修改请求头了。 上面的演示中，使用 python-requests/2.21.0 做 user agent，知乎返回 404. For Firefox打开设备模拟，在 toolbar 的右上角勾选 show user agent，然后就可以在 toolbar 修改 user agent 了： ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:9","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#2-自定义请求头"},{"categories":["tech"],"content":" 其他 1. Fake Geolocation（Chrome only）在 Chrome 中进入 DevTools，点击右上角的 options 按钮，选择 More tools -\u003e Sensors，在 Geolocation 处选择 Custom location，就可以修改地理位置了。 2. 自定义请求头 For Chrome和 上一小节一样，先进 More tools，选择 Network conditions，取消勾选 Select atuomatically， 就可以修改请求头了。 上面的演示中，使用 python-requests/2.21.0 做 user agent，知乎返回 404. For Firefox打开设备模拟，在 toolbar 的右上角勾选 show user agent，然后就可以在 toolbar 修改 user agent 了： ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:9","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#for-chrome"},{"categories":["tech"],"content":" 其他 1. Fake Geolocation（Chrome only）在 Chrome 中进入 DevTools，点击右上角的 options 按钮，选择 More tools -\u003e Sensors，在 Geolocation 处选择 Custom location，就可以修改地理位置了。 2. 自定义请求头 For Chrome和 上一小节一样，先进 More tools，选择 Network conditions，取消勾选 Select atuomatically， 就可以修改请求头了。 上面的演示中，使用 python-requests/2.21.0 做 user agent，知乎返回 404. For Firefox打开设备模拟，在 toolbar 的右上角勾选 show user agent，然后就可以在 toolbar 修改 user agent 了： ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:9","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#for-firefox"},{"categories":["tech"],"content":" 3. Request Blocking（Chrome only）在 Network 的任意请求上右键，菜单中就有 Block request URL（阻塞该 URL）和 Block request domain（阻塞请求所在的整个域） 然后就可以在 More tools -\u003e Request blocking 中看到你设置的阻塞条件。 ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:10","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#3-request-blockingchrome-only"},{"categories":["tech"],"content":" 参考 你不知道的 Chrome 调试技巧 Chrome Dev Tools 中文手册 Firefox Quantum：开发者版本 How to Capture Screenshots in Google Chrome without using Extensions Chrome DevTools - Network ","date":"2019-02-11","objectID":"/posts/web-browser-dev-tools/:0:11","series":null,"tags":["Chrome","Firefox","DevTools","Browser"],"title":"Chrome 与 Firefox-Dev 的 DevTools","uri":"/posts/web-browser-dev-tools/#参考"},{"categories":["tech"],"content":" 前言如果要在Linux 上设置一个开机自启，出现问题自动重启，并且有良好日志的程序，比较流行的方法有supervisord、systemd，除此之外，还有 upstart、runit 等类似的工具。但是自从 systemd 被 ubuntu、centos 等主流 Linux 发行版应用以来，systemd 渐渐成为主流方案。 如果你需要跨平台(Linux/MacOSX/FreeBSD)的方案，那么建议使用 supervisord，如果只需要支持 Linux 则建议选用 systemd. ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:0","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#前言"},{"categories":["tech"],"content":" 配置说明要自定义一个服务，需要在 /usr/lib/systemd/system/ 下添加一个配置文件：\u003csoftware-name\u003e.service 如果 /usr/lib/systemd/system/ 不存在，可考虑使用 /lib/systemd/system/ 或/etc/systemd/system/ ExecXXX 中的命令，均可以正常使用转义字符以及环境变量插值语法，比如用 \\ 结尾表示换行，用 $Xxx 获取环境变量。 配置文件的内容说明： toml [Unit]: 服务的启动顺序与依赖关系 Description: 当前服务的简单描述 After: 当前服务（\u003csoftware-name\u003e.service）需要在这些服务启动后，才启动 Before: 和 After 相反，当前服务需要在这些服务启动前，先启动 Wants：表示当前服务\"弱依赖\"于这些服务。即当前服务依赖于它们，但是没有它们，当前服务也能正常运行。 Requires: 表示\"强依赖\"关系，即如果该服务启动失败或异常退出，那么当前服务也必须退出。 [Service] 服务运行参数的设置 Type=forking 后台运行的形式 PIDFile=/software-name/pid pid文件路径 EnvironmentFile=/xxx/prod.env 通过文件设定环境变量，注意这东西不支持环境变量的插值语法 ${xxx} WorkingDirectory=/xxx/xxx 工作目录 ExecStartPre 为启动做准备的命令 ExecStart 服务的具体运行命令（对非 workingdirectory 的文件，必须用绝对路径！ ExecReload 重载命令，如果程序支持 HUP 信号的话，通常将此项设为 `/bin/kill -HUP $MAINPID` ExecStop 停止命令 ExecStartPre：启动服务之前执行的命令 ExecStartPost：启动服务之后执行的命令 ExecStopPost：停止服务之后执行的命令 RuntimeDirectory=xxxx RuntimeDirectoryMode=0775 PrivateTmp=True 表示给服务分配独立的临时空间 RestartSec 自动重启当前服务间隔的秒数 Restart 定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure 等 # 程序的 user 和 group User=ryan Group=ryan 注意：启动、重载、停止命令全部要求使用绝对路径 [Install] 定义如何安装这个配置文件，即怎样做到开机启动。 # Target的含义是服务组，表示一组服务。 WantedBy=multi-user.target 注意，service 文件不支持行内注释！！！注释必须单独一行 ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:1","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#配置说明"},{"categories":["tech"],"content":" Type 说明Type 感觉是整个配置文件里面最不好理解的一个配置项，它的实际作用就是：告诉 systemd 你的 Service 是如何启动的 Type=simple（默认值）：ExecStart 命令会立即启动你的服务，并且持续运行，不会退出。 Type=forking：ExecStart 命令会 fork 出你的服务主进程，然后正常退出。使用此 Type 时应同时指定 PIDFile=，systemd 使用它跟踪服务的主进程。 Type=oneshot：ExecStart 命令。可能需要同时设置 RemainAfterExit=yes 使得 systemd 在服务进程退出之后仍然认为服务处于激活状态 Type=notify：与 Type=simple 相同，但约定服务会在就绪后向 systemd 发送一个信号，以表明自己已经启动成功。 细节：systemd 会创建一个 unix socket，并将地址通过 $NOTIFY_SOCKET 环境变量提供给服务，同时监听该 socket 上的信号。服务可以使用 systemd 提供的 C 函数 sd_notify() 或者命令行工具 systemd-notify 发送信号给 systemd. 因为多了个 notify 信号，所以这一 Type 要比 simple 更精确一点。但是需要服务的配合， Type=dbus：若以此方式启动，当指定的 BusName 出现在 DBus 系统总线上时，systemd 认为服务就绪。 Type=idle：没搞明白，不过通常也用不到。 更详细的见Systemd 入门教程：命令篇 - 阮一峰。 ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:2","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#type-说明"},{"categories":["tech"],"content":" 配置举例比如 shadsocks Server Service，的配置文件 ss-server.service 的内容为： toml [Unit] Description=shadsocks server After=network.target auditd.service [Service] Type=forking ExecStart=/usr/local/bin/ssserver -c /etc/shadsocks.json --user shadsocks --pid-file /var/run/shadsocks.pid -d start ExecStop=/usr/local/bin/ssserver -c /etc/shadsocks.json --user shadsocks --pid-file /var/run/shadsocks.pid -d stop PIDFile=/var/run/shadsocks.pid Restart=always RestartSec=4 [Install] WantedBy=multi-user.target 而 enginx 的配置文件 nginx.service 的内容是： toml [Unit] Description=The NGINX HTTP and reverse proxy server After=syslog.target network-online.target remote-fs.target nss-lookup.target Wants=network-online.target [Service] Type=forking PIDFile=/run/nginx.pid ExecStartPre=/usr/sbin/nginx -t ExecStart=/usr/sbin/nginx ExecReload=/usr/sbin/nginx -s reload ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target 为了使用环境变量插值，而使用 sh 启动的 etcd 服务，它的 etcd.service 配置如下: toml [Unit] Description=etcd key-value store Documentation=https://github.com/etcd-io/etcd After=network.target [Service] Type=simple # EnvironmentFile 不支持使用 ${xxx} 变量插值，这里不适合使用 # EnvironmentFile=/data/etcd.env # -a 表示传递环境变量 ExecStart=/bin/bash -ac '. /data/etcd.env; /data/bin/etcd' Restart=always RestartSec=5s LimitNOFILE=40000 [Install] WantedBy=multi-user.target 如果你不需要在 /data/etcd.env 中使用环境变量的插值语法，那可以这样写: toml [Unit] Description=etcd key-value store Documentation=https://github.com/etcd-io/etcd After=network.target [Service] Type=notify EnvironmentFile=/data/etcd.env # ExecXXX 的命令中是可以使用 ${Xxx} 插值语法的 ExecStart=/data/bin/etcd \\ --initial-advertise-peer-urls http://${THIS_IP}:2380 \\ --listen-peer-urls http://${THIS_IP}:2380 \\ --advertise-client-urls http://${THIS_IP}:2379 \\ --listen-client-urls http://${THIS_IP}:2379 \\ --initial-cluster \"${NAME_1}=http://${HOST_1}:2380,${NAME_2}=http://${HOST_2}:2380,${NAME_3}=http://${HOST_3}:2380\" Restart=always RestartSec=5s LimitNOFILE=40000 [Install] WantedBy=multi-user.target ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:3","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#配置举例"},{"categories":["tech"],"content":" 服务的启动、关闭 text systemctl enable ss-server.service # 启用服务，即开机自动启动 systemctl disable ss-server.service # 取消服务，取消开机启动 systemctl start ss-server.service # 启动服务 systemctl stop ss-server.service # 停止服务 systemctl restart ss-server.service # 重启服务(stop + start) systemctl reload ss-server.service # 服务不 stop，直接加载配置更新等（对应 ExecReload） # 检查状态 systemctl status ss-server.service -l systemctl list-units --type=service # 查看所有服务 ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:4","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#服务的启动关闭"},{"categories":["tech"],"content":" 参考 Systemd 入门教程：命令篇 - 阮一峰 systemd.exec 中文手册 ","date":"2019-01-28","objectID":"/posts/systemd-basics/:1:5","series":null,"tags":["Linux","Systemd","Init System"],"title":"通过 systemctl 设置自定义 Service","uri":"/posts/systemd-basics/#参考"},{"categories":["tech"],"content":" 个人笔记，如有错误烦请指正。 SQLAlchemy 是一个用 Python 实现的 ORM （Object Relational Mapping）框架，它由多个组件构成，这些组件可以单独使用，也能独立使用。它的组件层次结构如下： SQLAlchemy 层次结构 其中最常用的组件，应该是 ORM 和 SQL 表达式语言，这两者既可以独立使用，也能结合使用。 ORM 的好处在于它 自动处理了数据库和 Python 对象之间的映射关系，屏蔽了两套系统之间的差异。程序员不需要再编写复杂的 SQL 语句，直接操作 Python 对象就行。 屏蔽了各数据库之间的差异，更换底层数据库不需要修改 SQL 语句，改下配置就行。 使数据库结构文档化，models 定义很清晰地描述了数据库的结构。 避免了不规范、冗余、风格不统一的 SQL 语句，可以避免很多人为 Bug，也方便维护。 但是 ORM 需要消耗额外的性能来处理对象关系映射，此外用 ORM 做多表关联查询或复杂 SQL 查询时，效率低下。因此它适用于场景不太复杂，性能要求不太苛刻的场景。 都说 ORM 学习成本高，我自己也更倾向于直接使用 SQL 语句（毕竟更熟悉），因此这一篇笔记不涉及 ORM 部分，只记录 SQLAlchemy 的 Engine 与 SQL 表达式语言。 ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:0:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#"},{"categories":["tech"],"content":" 一、直接使用 Engine 和 Connections第一步是创建数据库引擎实例： python from sqlalchemy import create_engine engine = create_engine('sqlite:///:memory:', echo=True, # echo=True 表示打印出自动生成的 SQL 语句（通过 logging） pool_size=5, # 连接池容量，默认为 5，生产环境下太小，需要修改。 # 下面是 connection 回收的时间限制，默认 -1 不回收 pool_recycle=7200) # 超过 2 小时就重新连接（MySQL 默认的连接最大闲置时间为 8 小时） create_engine 接受的第一个参数是数据库 URI，格式为dialect+driver://username:password@host:port/database，dialect 是具体的数据库名称，driver 是驱动名称。key-value 是可选的参数。举例： shell # PostgreSQL postgresql+psycopg2://scott:tiger@localhost/dbtest # MySQL + PyMySQL（或者用更快的 mysqlclient） mysql+pymysql://scott:tiger@localhost/dbtest # sqlite 内存数据库 # 注意 sqlite 要用三个斜杠，表示不存在 hostname，sqlite://\u003cnohostname\u003e/\u003cpath\u003e sqlite:///:memory: # sqlite 文件数据库 # 四个斜杠是因为文件的绝对路径以 / 开头：/home/ryan/Codes/Python/dbtest.db sqlite:////home/ryan/Codes/Python/dbtest.db # SQL Server + pyodbc # 首选基于 dsn 的连接，dsn 的配置请搜索hhh mssql+pyodbc://scott:tiger@some_dsn 如果你的密码中含有 ‘@’ 等特殊字符，就不能直接放入 URI 中，必须使用urllib.parse.quote_plus 编码，然后再插入 URI. 引擎创建后，我们就可以直接获取 connection，然后执行 SQL 语句了。这种用法相当于把 SQLAlchemy 当成带 log 的数据库连接池使用： python with engine.connect() as conn: res = conn.execute(\"select username from users\") # 无参直接使用 # 使用问号作占位符，前提是下层的 DBAPI 支持。更好的方式是使用 text()，这个后面说 conn.execute(\"INSERT INTO table (id, value) VALUES (?, ?)\", 1, \"v1\") # 参数不需要包装成元组 # 查询返回的是 ResultProxy 对象，有和 dbapi 相同的 fetchone()、fetchall()、first() 等方法，还有一些拓展方法 for row in result: print(\"username:\", row['username']) 但是要注意的是，connection 的 execute 是自动提交的（autocommit），这就像在 shell 里打开一个数据库客户端一样，分号结尾的 SQL 会被自动提交。只有在 BEGIN TRANSACTION 内部，AUTOCOMMIT 会被临时设置为 FALSE，可以通过如下方法开始一个内部事务： python def transaction_a(connection): trans = connection.begin() # 开启一个 transaction try: # do sthings trans.commit() # 这里需要手动提交 except: trans.rollback() # 出现异常则 rollback raise # do other things with engine.connect() as conn: transaction_a(conn) ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:1:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#一直接使用-enginehttpsdocssqlalchemyorgenlatestcoreengineshtmlsqlalchemycreate_engine-和-connections"},{"categories":["tech"],"content":" 1. 使用 text() 构建 SQL相比直接使用 string，text() 的优势在于它： 提供了统一的参数绑定语法，与具体的 DBAPI 无关。 python # 1. 参数绑定语法 from sqlalchemy import text result = connection.execute( # 使用 :key 做占位符 text('select * from table where id \u003c :id and typeName=:type'), {'id': 2,'type':'USER_TABLE'}) # 用 dict 传参数，更易读 # 2. 参数类型指定 from sqlalchemy import DateTime date_param=datetime.today()+timedelta(days=-1*10) sql=\"delete from caw_job_alarm_log where alarm_time \u003c :alarm_time_param\" # bindparams 是 bindparam 的列表，bindparam 则提供参数的一些额外信息（类型、值、限制等） t=text(sql, bindparams=[bindparam('alarm_time_param', type_=DateTime, required=True)]) connection.execute(t, {\"alarm_time_param\": date_param}) 可以很方便地转换 Result 中列的类型 python stmt = text(\"SELECT * FROM table\", # 使用 typemap 指定将 id 列映射为 Integer 类型，name 映射为 String 类型 typemap={'id': Integer, 'name': String}, ) result = connection.execute(stmt) # 对多个查询结果，可以用 for obj in result 遍历 # 也可用 fetchone() 只获取一个 ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:1:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#1-使用-texthttpsdocssqlalchemyorgenlatestcoresqlelementhtmlsqlalchemysqlexpressiontext-构建-sql"},{"categories":["tech"],"content":" 二、SQL 表达式语言 复杂的 SQL 查询可以直接用 raw sql 写，而增删改一般都是单表操作，用 SQL 表达式语言最方便。 SQLAlchemy 表达式语言是一个使用 Python 结构表示关系数据库结构和表达式的系统。 ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:2:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#二sql-表达式语言"},{"categories":["tech"],"content":" 1. 定义并创建表SQL 表达式语言使用 Table 来定义表，而表的列则用 Column 定义。Column 总是关联到一个 Table 对象上。 一组 Table 对象以及它们的子对象的集合就被称作「数据库元数据（database metadata）」。metadata 就像你的网页分类收藏夹，相关的 Table 放在一个 metadata 中。 下面是创建元数据（一组相关联的表）的例子，： python from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey metadata = MetaData() # 先创建元数据（收藏夹） users = Table('user', metadata, # 创建 user 表，并放到 metadata 中 Column('id', Integer, primary_key=True), Column('name', String), Column('fullname', String) ) addresses = Table('address', metadata, Column('id', Integer, primary_key=True), Column('user_id', None, ForeignKey('user.id')), # 外键约束，引用 user 表的 id 列 Column('email_address', String, nullable=False) ) metadata.create_all(engine) # 使用 engine 创建 metadata 内的所有 Tables（会检测表是否已经存在，所以可以重复调用） 表定义中的约束 应该给所有的约束命名，即为 name 参数指定一个不冲突的列名。详见The Importance of Naming Constraints 表还有一个属性：约束条件。下面一一进行说明。 外键约束：用于在删除或更新某个值或行时，对主键/外键关系中一组数据列强制进行的操作限制。 用法一：Column('user_id', None, ForeignKey('user.id'))，直接在 Column 中指定。这也是最常用的方法 用法二：通过 ForeignKeyConstraint(columns, refcolumns) 构建约束，作为参数传给Table. python item = Table('item', metadata, # 商品 table Column('id', Integer, primary_key=True), Column('name', String(60), nullable=False), Column('invoice_id', Integer, nullable=False), # 发票 id，是外键 Column('ref_num', Integer, nullable=False), ForeignKeyConstraint(['invoice_id', 'ref_num'], # 当前表中的外键名称 ['invoice.id', 'invoice.ref_num']) # 被引用的外键名称的序列（被引用的表） ) on delete 与 on update：外键约束的两个约束条件，通过 ForeignKey() 或ForeignKeyConstraint() 的关键字参数 ondelete/onupdate 传入。可选值有：1. 默认行为NO ACTION：什么都不做，直接报错。1. CASCADE：删除/更新 父表数据时，从表数据会同时被 删除/更新。（无报错）1. RESTRICT：不允许直接 删除/更新 父表数据，直接报错。（和默认行为基本一致）1. SET NULL or SET DEFAULT：删除/更新 父表数据时，将对应的从表数据重置为 NULL 或者默认值。 唯一性约束：UniqueConstraint('col2', 'col3', name='uix_1')，作为参数传给 Table. CHECK 约束：CheckConstraint('col2 \u003e col3 + 5', name='check1')， 作为参数传给Table. 主键约束：不解释 方法一：通过 Column('id', Integer, primary_key=True) 指定主键。（参数primary_key 可在多个 Column 上使用） 方法二：使用 PrimaryKeyConstraint python from sqlalchemy import PrimaryKeyConstraint my_table = Table('mytable', metadata, Column('id', Integer), Column('version_id', Integer), Column('data', String(50)), PrimaryKeyConstraint('id', 'version_id', name='mytable_pk') ) ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#1-定义并创建表"},{"categories":["tech"],"content":" 1. 定义并创建表SQL 表达式语言使用 Table 来定义表，而表的列则用 Column 定义。Column 总是关联到一个 Table 对象上。 一组 Table 对象以及它们的子对象的集合就被称作「数据库元数据（database metadata）」。metadata 就像你的网页分类收藏夹，相关的 Table 放在一个 metadata 中。 下面是创建元数据（一组相关联的表）的例子，： python from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey metadata = MetaData() # 先创建元数据（收藏夹） users = Table('user', metadata, # 创建 user 表，并放到 metadata 中 Column('id', Integer, primary_key=True), Column('name', String), Column('fullname', String) ) addresses = Table('address', metadata, Column('id', Integer, primary_key=True), Column('user_id', None, ForeignKey('user.id')), # 外键约束，引用 user 表的 id 列 Column('email_address', String, nullable=False) ) metadata.create_all(engine) # 使用 engine 创建 metadata 内的所有 Tables（会检测表是否已经存在，所以可以重复调用） 表定义中的约束 应该给所有的约束命名，即为 name 参数指定一个不冲突的列名。详见The Importance of Naming Constraints 表还有一个属性：约束条件。下面一一进行说明。 外键约束：用于在删除或更新某个值或行时，对主键/外键关系中一组数据列强制进行的操作限制。 用法一：Column('user_id', None, ForeignKey('user.id'))，直接在 Column 中指定。这也是最常用的方法 用法二：通过 ForeignKeyConstraint(columns, refcolumns) 构建约束，作为参数传给Table. python item = Table('item', metadata, # 商品 table Column('id', Integer, primary_key=True), Column('name', String(60), nullable=False), Column('invoice_id', Integer, nullable=False), # 发票 id，是外键 Column('ref_num', Integer, nullable=False), ForeignKeyConstraint(['invoice_id', 'ref_num'], # 当前表中的外键名称 ['invoice.id', 'invoice.ref_num']) # 被引用的外键名称的序列（被引用的表） ) on delete 与 on update：外键约束的两个约束条件，通过 ForeignKey() 或ForeignKeyConstraint() 的关键字参数 ondelete/onupdate 传入。可选值有：1. 默认行为NO ACTION：什么都不做，直接报错。1. CASCADE：删除/更新 父表数据时，从表数据会同时被 删除/更新。（无报错）1. RESTRICT：不允许直接 删除/更新 父表数据，直接报错。（和默认行为基本一致）1. SET NULL or SET DEFAULT：删除/更新 父表数据时，将对应的从表数据重置为 NULL 或者默认值。 唯一性约束：UniqueConstraint('col2', 'col3', name='uix_1')，作为参数传给 Table. CHECK 约束：CheckConstraint('col2 \u003e col3 + 5', name='check1')， 作为参数传给Table. 主键约束：不解释 方法一：通过 Column('id', Integer, primary_key=True) 指定主键。（参数primary_key 可在多个 Column 上使用） 方法二：使用 PrimaryKeyConstraint python from sqlalchemy import PrimaryKeyConstraint my_table = Table('mytable', metadata, Column('id', Integer), Column('version_id', Integer), Column('data', String(50)), PrimaryKeyConstraint('id', 'version_id', name='mytable_pk') ) ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:2:1","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#表定义中的约束"},{"categories":["tech"],"content":" 2. 增删改查语句 增: python # 方法一，使用 values 传参 ins = users.insert().values(name=\"Jack\", fullname=\"Jack Jones\") # 可以通过 str(ins) 查看自动生成的 sql connection.execute(ins) # 方法二，参数传递给 execute() conn.execute(users.insert(), id=2, name='wendy', fullname='Wendy Williams') # 方法三，批量 INSERT，相当于 executemany conn.execute(addresses.insert(), [ # 插入到 addresses 表 {'user_id': 1, 'email_address': 'jack@yahoo.com'}, # 传入 dict 列表 {'user_id': 1, 'email_address': 'jack@msn.com'}, {'user_id': 2, 'email_address': 'www@www.org'}, {'user_id': 2, 'email_address': 'wendy@aol.com'} ]) # 此外，通过使用 bindparam，INSERT 还可以执行更复杂的操作 stmt = users.insert() \\ .values(name=bindparam('_name') + \" .. name\") # string 拼接 conn.execute(stmt, [ {'id':4, '_name':'name1'}, {'id':5, '_name':'name2'}, {'id':6, '_name':'name3'}, ]) 删： python _table.delete() \\ .where(_table.c.f1==value1) \\ .where(_table.c.f2==value2) # where 指定条件 改： python # 举例 stmt = users.update() \\ .where(users.c.name == 'jack') \\ .values(name='tom') conn.execute(stmt) # 批量更新 stmt = users.update() \\ .where(users.c.name == bindparam('oldname')) \\ .values(name=bindparam('newname')) conn.execute(stmt, [ {'oldname':'jack', 'newname':'ed'}, {'oldname':'wendy', 'newname':'mary'}, {'oldname':'jim', 'newname':'jake'}, ]) 可以看到，所有的条件都是通过 where 指定的，它和后面 ORM 的 filter 接受的参数是一样的。 （详细的会在第二篇文章里讲） 查 python from sqlalchemy.sql import select # 1. 字段选择 s1 = select([users]) # 相当于 select * from users s2 = select([users.c.name, users.c.fullname]) # 这个就是只 select 一部分 # 2. 添加过滤条件 s3 = select([users]) \\ .where(users.c.id == addresses.c.user_id) res = conn.execute(s1) # 可用 for row in res 遍历结果集，也可用 fetchone() 只获取一行 查询返回的是 ResultProxy 对象，这是 SQLAlchemy 对 Python DB-API 的 cursor 的一个封装类，要从中获取结果行，主要有下列几个方法： python row1 = result.fetchone() # 对应 cursor.fetchone() row2 = result.fetchall() # 对应 cursor.fetchall() row3 = result.fetchmany(size=3) # 对应 cursor.fetchmany(size=3) row4 = result.first() # 获取一行，然后立即调用 result 的 close() 方法 col = row[mytable.c.mycol] # 获取 mycol 这一列 result.rowcount # 结果集的行数 同时，result 也实现了 next protocol，因此可以直接用 for 循环遍历 ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:2:2","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#2-增删改查语句"},{"categories":["tech"],"content":" where 进阶通过使用 or*、and*、in_ model.join 等方法，where 可以构建更复杂的 SQL 语句。 python from sqlalchemy.sql import and_, or_, not_ s = select([(users.c.fullname + \", \" + addresses.c.email_address). label('title')]).\\ where(users.c.id == addresses.c.user_id).\\ where(users.c.name.between('m', 'z')).\\ where( or_( addresses.c.email_address.like('%@aol.com'), addresses.c.email_address.like('%@msn.com') ) ) ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:2:3","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#where-进阶"},{"categories":["tech"],"content":" 链接 使用 Engines 和 Connections SQL 表达式语言入门 SQLAlchemy - 定义约束 SQLAlchemy个人学习笔记完整汇总 hackersandslackers/sqlalchemy-tutorial ","date":"2019-01-21","objectID":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/:3:0","series":null,"tags":["SQLAlchemy","Python","ORM","后端","数据库","Database"],"title":"SQLAlchemy 学习笔记（一）：Engine 与 SQL 表达式语言","uri":"/posts/sqlalchemy-notes-1-engine-and-sql-expression-language/#链接"},{"categories":["tech"],"content":" 个人笔记不保证正确。 数据类型是限制我们可以在表里存储什么数据的一种方法。不过，对于许多应用来说， 这种限制实在是太粗糙了。比如，一个包含产品价格的字段应该只接受正数。 但是没有哪种标准数据类型只接受正数。 另外一个问题是你可能需要根据其它字段或者其它行的数据来约束字段数据。比如， 在一个包含产品信息的表中，每个产品编号都应该只有一行。 对于这些问题，SQL 允许你在字段和表上定义约束。约束允许你对数据施加任意控制。 如果用户企图在字段里存储违反约束的数据，那么就会抛出一个错误。 这种情况同时也适用于数值来自默认值的情况。 ","date":"2019-01-20","objectID":"/posts/sql-basics-3-restrict/:0:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（三）约束","uri":"/posts/sql-basics-3-restrict/#"},{"categories":["tech"],"content":" 1. 外键 FOREIGN KEY外键约束声明一个字段(或者一组字段)的数值必须匹配另外一个表中出现的数值。创建外键约束的前提是，该外键所在的表已经存在，并且外键必须是 UNIQUE 的。（主键默认 UNIQUE 且 NOT NULL） sql CREATE TABLE \u003c表名\u003e ( \u003c字段名\u003e \u003c类型\u003e PRIMARY KEY, \u003c字段名\u003e \u003c类型\u003e REFERENCES \u003c外键所在的表名\u003e (\u003c字段名\u003e), -- 这创建了一个外键 ... ); 还有另一种语法，它支持以多个字段为外键（字段约束也可以写成表约束，也就是放在一个独立的行中。而反过来很可能不行）： text CREATE TABLE \u003c表名\u003e ( \u003c字段名1\u003e \u003c类型\u003e PRIMARY KEY, \u003c字段名2\u003e \u003c类型\u003e \u003c字段名3\u003e \u003c类型\u003e ... FOREIGN KEY (\u003c字段名2\u003e, \u003c字段名3\u003e) REFERENCES \u003c外键所在的表名\u003e (\u003c字段名4\u003e, \u003c字段名5\u003e) ); 一个表也可以包含多个外键约束。这个特性用于实现表之间的多对多关系。 比如你有关于产品和订单的表，但现在你想允许一个订单可以包含多种产品 (上面那个结构是不允许这么做的)，你可以使用这样的结构： sql CREATE TABLE products ( product_no integer PRIMARY KEY, name text, price numeric ); CREATE TABLE orders ( order_id integer PRIMARY KEY, shipping_address text, ... ); CREATE TABLE order_items ( product_no integer REFERENCES products, order_id integer REFERENCES orders, quantity integer, PRIMARY KEY (product_no, order_id) ); 外键能通过 ALTER 语句添加或删除 ","date":"2019-01-20","objectID":"/posts/sql-basics-3-restrict/:0:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（三）约束","uri":"/posts/sql-basics-3-restrict/#1-外键-foreign-key"},{"categories":["tech"],"content":" 2. 级联操作 ON DELETE 与 ON UPDATE上面说过：外键约束声明一个字段(或者一组字段)的数值必须匹配另外一个表中出现的数值。 但是以 1. 中最后一个 sql 为例，如果一个订单（order）在创建之后，该订单包含的某个产品 （product）被删除了，会发生什么？ 这个例子中，订单包含的产品通过外键被记录在 order_items 表中。现在如果你要删除 product 中某个被 order_items 引用了的行，默认情况为 NO ACTION，就是直接报错。 这个行为也可以手动指定： sql CREATE TABLE products ( product_no integer PRIMARY KEY, name text, price numeric ); CREATE TABLE orders ( order_id integer PRIMARY KEY, shipping_address text, ... ); CREATE TABLE order_items ( product_no integer REFERENCES products ON DELETE RESTRICT, -- 限制，也就是禁止删除被它引用的行 order_id integer REFERENCES orders ON DELETE CASCADE, -- 级联，在删除被它引用的行的时候，这一行本身也会被自动删除掉 quantity integer, PRIMARY KEY (product_no, order_id) ); 除了 RESTRICT 和 CASCADE 外，在外键上的动作还有两个选项：SET NULL 和 SET DEFAULT， 顾名思义，就是在被引用的行删除后将外键设置为 NULL 或默认值。 ON UPDATE 与 ON DELETE 的动作是一样的，只是 CASCADE 表示同步更新。 ","date":"2019-01-20","objectID":"/posts/sql-basics-3-restrict/:0:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（三）约束","uri":"/posts/sql-basics-3-restrict/#2-级联操作-on-delete-与-on-update"},{"categories":["tech"],"content":" 3. CHECK 约束 sql CREATE TABLE products ( product_no integer, name text, price numeric CHECK (price \u003e 0) ); 你还可以给这个约束取一个独立的名字。这样就可以令错误消息更清晰， 并且在你需要修改它的时候引用这个名字。语法是： sql CREATE TABLE products ( product_no integer, name text, price numeric CONSTRAINT positive_price CHECK (price \u003e 0) ); 稍微复杂一点的例子： sql CREATE TABLE products ( product_no integer, name text, price numeric CHECK (price \u003e 0), discounted_price numeric, CHECK (discounted_price \u003e 0 AND price \u003e discounted_price) ); 同样的，可以为 CHECK 命名，令错误信息更清晰： sql CREATE TABLE products ( product_no integer, name text, price numeric, CHECK (price \u003e 0), discounted_price numeric, CHECK (discounted_price \u003e 0), CONSTRAINT valid_discount CHECK (price \u003e discounted_price) ); 要注意的是，当约束表达式计算结果为真或 NULL 的时候，检查约束会被认为是满足条件的。 因为大多数表达式在含有 NULL 操作数的时候结果都是 NULL ，所以这些约束不能阻止字段值为 NULL 。要排除掉 NULL，只能使用 NOT NULL 约束。（所以就说 NULL 是万恶之源hhh） ","date":"2019-01-20","objectID":"/posts/sql-basics-3-restrict/:0:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（三）约束","uri":"/posts/sql-basics-3-restrict/#3-check-约束"},{"categories":["tech"],"content":" 参考 约束 ","date":"2019-01-20","objectID":"/posts/sql-basics-3-restrict/:0:4","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（三）约束","uri":"/posts/sql-basics-3-restrict/#参考"},{"categories":["tech"],"content":" 个人向，只会记录一些需要注意的点。 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:0:0","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#"},{"categories":["tech"],"content":" 前言学习 Julia 已经有一段时间了，但是进步缓慢。这一方面是最近代码写得少，一方面是 Julia 学习资料少、中文资料更少，但也有我没做笔记的缘故导致学习效率不佳。 最近发现一份很不错的入门教程：Introducing_Julia，但是它的中文版本仍然有很多不足，就打算给它添加翻译和润色（zxj5470 完成了绝大部分翻译工作），顺便总结一份自己的笔记。 NOTE：Julia 的主要语言特征在于类型系统和多重派发，而主要的科学计算特征则是矩阵和整个标准库及生态圈。 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:1:0","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#前言"},{"categories":["tech"],"content":" 一、数组在 Julia 中，数组被用作列表（lists）、向量（vectors）、表（tables）和矩阵（matrices）。 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:0","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#一数组"},{"categories":["tech"],"content":" 1. 数组的创建这里尤其需要注意的是数组构造的几种方法，以及它们的区别。 1.1 一维数组（vector/list） julia julia\u003e v = [1, 2, 3, 4] # 逗号分隔的语法用于创建一维数组 4-element Array{Int64,1}: 1 2 3 4 向量，指列向量，Julia 使用的是 Fortran Order，各种操作都是列优先于行的。（和 numpy 相反，numpy 是 C Order 的，行优先于列） 1.2. 二维数组（table/matrix） julia julia\u003e mat = [1 2 3 4] # 空格分隔的语法，用于创建二维数组（或称行向量） 1×4 Array{Int64,2}: 1 2 3 4 julia\u003e [1 2; 3 4] # 分号和换行符(\\n)，用于分隔数组中不同的行 2×2 Array{Int64,2}: 1 2 3 4 空格对应函数 hcat，表示横向拼接各个矩阵/元素。分号和换行对应函数 vcat，表示垂直拼接各个矩阵/元素。 下面的例子演示了拼接（空格）和单纯分隔各个元素（逗号）的区别： julia julia\u003e [1 2 [3 4] 5] # 用空格做横向拼接（或称水平拼接） 1×5 Array{Int64,2}: 1 2 3 4 5 julia\u003e [1, 2, [3, 4], 5] # 用逗号分隔 4-element Array{Any,1}: 1 2 [3, 4] 5 能看到在拼接操作中，[3 4] 被“解开”了，而用逗号时，它的行为和 Python 的 list 一样（区别只是 Julia 的 list 列优先）。 使用拼接需要注意的情况举例： julia julia\u003e [1 2 [3, 4] 5] # 横向拼接要求 items 的行数相同！ ERROR: DimensionMismatch(\"mismatch in dimension 1 (expected 1 got 2)\") 因为 [3, 4] 有两行，而 数组中的其他项是数值，显然行数不同，所以抛出了 Error. 可以想见，垂直拼接则要求 items 的列数相同。 另外当垂直拼接用于基本元素时，效果等同于逗号。（结果都是单列数组） julia julia\u003e v = [1, 2, 3, 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e h = [1; 2; 3; 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e [[1; 2]; [3, 4]] # 等价于 [[1, 2]; [3, 4]] 4-element Array{Int64,1}: 1 2 3 4 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:1","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#1-数组的创建"},{"categories":["tech"],"content":" 1. 数组的创建这里尤其需要注意的是数组构造的几种方法，以及它们的区别。 1.1 一维数组（vector/list） julia julia\u003e v = [1, 2, 3, 4] # 逗号分隔的语法用于创建一维数组 4-element Array{Int64,1}: 1 2 3 4 向量，指列向量，Julia 使用的是 Fortran Order，各种操作都是列优先于行的。（和 numpy 相反，numpy 是 C Order 的，行优先于列） 1.2. 二维数组（table/matrix） julia julia\u003e mat = [1 2 3 4] # 空格分隔的语法，用于创建二维数组（或称行向量） 1×4 Array{Int64,2}: 1 2 3 4 julia\u003e [1 2; 3 4] # 分号和换行符(\\n)，用于分隔数组中不同的行 2×2 Array{Int64,2}: 1 2 3 4 空格对应函数 hcat，表示横向拼接各个矩阵/元素。分号和换行对应函数 vcat，表示垂直拼接各个矩阵/元素。 下面的例子演示了拼接（空格）和单纯分隔各个元素（逗号）的区别： julia julia\u003e [1 2 [3 4] 5] # 用空格做横向拼接（或称水平拼接） 1×5 Array{Int64,2}: 1 2 3 4 5 julia\u003e [1, 2, [3, 4], 5] # 用逗号分隔 4-element Array{Any,1}: 1 2 [3, 4] 5 能看到在拼接操作中，[3 4] 被“解开”了，而用逗号时，它的行为和 Python 的 list 一样（区别只是 Julia 的 list 列优先）。 使用拼接需要注意的情况举例： julia julia\u003e [1 2 [3, 4] 5] # 横向拼接要求 items 的行数相同！ ERROR: DimensionMismatch(\"mismatch in dimension 1 (expected 1 got 2)\") 因为 [3, 4] 有两行，而 数组中的其他项是数值，显然行数不同，所以抛出了 Error. 可以想见，垂直拼接则要求 items 的列数相同。 另外当垂直拼接用于基本元素时，效果等同于逗号。（结果都是单列数组） julia julia\u003e v = [1, 2, 3, 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e h = [1; 2; 3; 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e [[1; 2]; [3, 4]] # 等价于 [[1, 2]; [3, 4]] 4-element Array{Int64,1}: 1 2 3 4 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:1","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#11-一维数组vectorlist"},{"categories":["tech"],"content":" 1. 数组的创建这里尤其需要注意的是数组构造的几种方法，以及它们的区别。 1.1 一维数组（vector/list） julia julia\u003e v = [1, 2, 3, 4] # 逗号分隔的语法用于创建一维数组 4-element Array{Int64,1}: 1 2 3 4 向量，指列向量，Julia 使用的是 Fortran Order，各种操作都是列优先于行的。（和 numpy 相反，numpy 是 C Order 的，行优先于列） 1.2. 二维数组（table/matrix） julia julia\u003e mat = [1 2 3 4] # 空格分隔的语法，用于创建二维数组（或称行向量） 1×4 Array{Int64,2}: 1 2 3 4 julia\u003e [1 2; 3 4] # 分号和换行符(\\n)，用于分隔数组中不同的行 2×2 Array{Int64,2}: 1 2 3 4 空格对应函数 hcat，表示横向拼接各个矩阵/元素。分号和换行对应函数 vcat，表示垂直拼接各个矩阵/元素。 下面的例子演示了拼接（空格）和单纯分隔各个元素（逗号）的区别： julia julia\u003e [1 2 [3 4] 5] # 用空格做横向拼接（或称水平拼接） 1×5 Array{Int64,2}: 1 2 3 4 5 julia\u003e [1, 2, [3, 4], 5] # 用逗号分隔 4-element Array{Any,1}: 1 2 [3, 4] 5 能看到在拼接操作中，[3 4] 被“解开”了，而用逗号时，它的行为和 Python 的 list 一样（区别只是 Julia 的 list 列优先）。 使用拼接需要注意的情况举例： julia julia\u003e [1 2 [3, 4] 5] # 横向拼接要求 items 的行数相同！ ERROR: DimensionMismatch(\"mismatch in dimension 1 (expected 1 got 2)\") 因为 [3, 4] 有两行，而 数组中的其他项是数值，显然行数不同，所以抛出了 Error. 可以想见，垂直拼接则要求 items 的列数相同。 另外当垂直拼接用于基本元素时，效果等同于逗号。（结果都是单列数组） julia julia\u003e v = [1, 2, 3, 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e h = [1; 2; 3; 4] 4-element Array{Int64,1}: 1 2 3 4 julia\u003e [[1; 2]; [3, 4]] # 等价于 [[1, 2]; [3, 4]] 4-element Array{Int64,1}: 1 2 3 4 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:1","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#12-二维数组tablematrix"},{"categories":["tech"],"content":" 2. 数组的索引数组的索引方式和 numpy 很类似。有很多高级索引方式。 这里我想说的是类似“齐次坐标”的索引特性。 首先，单个元素可以看作是零维的向量，数学上零维也可以看作是任意维，因此可以这样玩： julia julia\u003e 2[1] 2 julia\u003e 2[1, 1] # 被当成二维 2 julia\u003e 2[1][1] # 2[1] 仍然是整数 2 2 julia\u003e 2[1, 1, 1] # 三维 2 julia\u003e 3.14[1] 3.14 julia\u003e π[1, 1] π = 3.1415926535897... julia\u003e '1'[1] '1': ASCII/Unicode U+0031 (category And: Number, decimal digit) julia\u003e '1'[1, 1] '1': ASCII/Unicode U+0031 (category And: Number, decimal digit) 多维数组也能使用类似“齐次坐标”的索引方式： julia julia\u003e m = [1 2; 3 4] 2×2 Array{Int64,2}: 1 2 3 4 julia\u003e m[1][1] # m[1] 是整数 1，这相当于 1[1] 1 julia\u003e m[1, 1, 1] 1 julia\u003e m[1, 1, 1, 1] 1 多维矩阵，在更高的维度上，也能被当成“零维”来看待，前面说过了“零维”也相当于“无限维”，所以多维数组也能用这么索引。 但是拓展的维度索引只能是 1！既然被看作“零维”，就只相当于一个点，自然不可能有更高的索引： julia julia\u003e 1[1, 2] ERROR: BoundsError julia\u003e m[1, 1, 2] ERROR: BoundsError: attempt to access 2×2 Array{Int64,2} at index [1, 1, 2] ... julia\u003e m[1, 1, 1, 2] ERROR: BoundsError: attempt to access 2×2 Array{Int64,2} at index [1, 1, 1, 2] ... ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:2","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#2-数组的索引"},{"categories":["tech"],"content":" 3. 推导式（comprehension）与生成器表达式（generator expression）和 Python 的列表推导式与生成器表达式很像，但是更强大——Julia 是面向矩阵的。 julia julia\u003e [i+j for i in 1:3 for j in 1:3] # 这个语法和 Python 一致 9-element Array{Int64,1}: 2 3 4 3 4 5 4 5 6 julia\u003e [i+j for i in 1:3, j in 1:3] # 这个是多维的语法 3×3 Array{Int64,2}: 2 3 4 3 4 5 4 5 6 # 在后面加 guard 的情况下，结果坍缩成一维（这时两种语法结果没有差别） julia\u003e [i+j for i in 1:3, j in 1:3 if iseven(i+j)] 5-element Array{Int64,1}: 2 4 4 4 6 # 在前面做判断，因为没有过滤元素，所以仍然保持了原有结构。 julia\u003e [(iseven(i+j) ? 1 : 2) for i in 1:3, j in 1:3] 3×3 Array{Int64,2}: 1 2 1 2 1 2 1 2 1 ","date":"2019-01-14","objectID":"/posts/julia-notes-1-array/:2:3","series":null,"tags":["JuliaLang"],"title":"Julia 学习笔记（一）：数组","uri":"/posts/julia-notes-1-array/#3-推导式comprehensionhttpsdocsjulialangorgenv1manualarrayscomprehensions-1与生成器表达式generator-expressionhttpsdocsjulialangorgenv1manualarraysgenerator-expressions-1"},{"categories":["tech"],"content":" 可先浏览加粗部分 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:0:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#"},{"categories":["tech"],"content":" 一、常见压缩档*.zip | zip 程序压缩打包的档案； （很常见，但是因为不包含文档名编码信息，跨平台可能会乱码） *.rar | rar 程序压缩打包的档案；（在windows上很常见，但是是商业软件。） *.gz | gzip 程序压缩的档案； （linux目前使用最广泛的压缩格式） *.bz2 | bzip2 程序压缩的档案； *.xz | xz 程序压缩的档案； *.tar | tar 程序打包的资料，并没有压缩过； *.tar.gz | tar 程序打包的档案，其中并且经过 gzip 的压缩 （最常见） *.tar.bz2 | tar 程序打包的档案，其中并且经过 bzip2 的压缩 *.tar.xz | tar 程序打包的档案，其中并且经过 xz 的压缩 （新一代压缩选择） *.7z | 7zip 程序压缩打包的档案。 目前最常见的是 tar.gz tar.xz tar.7z 这三种格式。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:1:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#一常见压缩档"},{"categories":["tech"],"content":" 二、以能否压缩多文档分类 gzip bzip2 xz 这三个压缩格式都只能压缩单个文档。（换而言之，该进程的输入输出都是流，不包含文档树信息。）因此如果要用它们压缩多个文档或目录，需要使用另一个软件来先将要压缩的文档打包成一个文档（包含文档树信息），这个命令就是 tar. 先使用 tar 归档要压缩的多文档，再对生成的 *.tar 使用 上述压缩指令（或者直接使用管道重定向），Linux 下是这样实现多文档压缩的。 而 7z 和 zip，以及 rar 格式，都同时具有了 归档(tar) 和 压缩 两个功能，（也就是该格式包含了文档树信息咯）也就是说它们可以直接压缩多个文档。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:2:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#二以能否压缩多文档分类"},{"categories":["tech"],"content":" 三、各格式使用的算法差别 gzip 成熟的格式，使用的算法基于 DEFLATE。（压缩比适中） 7z 新一代格式，使用的压缩算法可替换，默认是使用的 lzma/lzma2 算法，使用 AES-256 作为加密算法。 xz 同样使用的 lzma/lzma2 算法，不过只能压缩一个文档。（压缩比很高，相对的用时也更多） zip 同样是支持多种算法的压缩格式，默认应该是使用的 DEFLATE 算法。诞生较早，有很多缺陷。（跨平台乱码、容易被破解等） rar 使用 类DEFLATE 的专有算法，使用 AES 加密。(rar5.0 以后使用 AES-256CBC) 不过 zip 被广泛应用在安卓的 apk 格式、java 的 jar、电子书的 epub，还有 github、云硬盘的多文档下载中，原因嘛大概是 zip 很流行，所以不用担心目标平台没解压软件吧。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:3:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#三各格式使用的算法差别"},{"categories":["tech"],"content":" 四、如何选用压缩方案 tar.gz 在 linux 上最常见，在压缩率和压缩时间上拥有良好的平衡。如果有任何疑惑，就选用它吧，不会错。 tar.xz 是新一代的压缩格式，虽然拥有更好的压缩率，压缩/解压速度相对要慢很多倍。一般在电脑性能足够好的时候，可选用它。 7z 和 xz 同为新一代压缩格式，它更复杂，支持多文档压缩。而且更适合跨平台，推荐使用。 zip 因为跨平台容易导致文档名乱码，不建议使用。（虽然有这样的缺陷，但是却意外的用得很广泛，在前一节有说过） rar 性能不差，但是是商业格式，不开源，不建议使用。（做得比较好的是它的 recovery records，在网络环境不好，容易导致包损坏时，这个功能就特别棒） tar.bz2 算是 linux 压缩历史上，过渡时期的产物，性能也介于 gz 和 xz 之间，一般来说不需要考虑它。 总的来说，就是 Windows 上推荐使用 7z，而 Linux 上 推荐使用 tar.gz tar.xz 7z 之一。此外 rar 的损坏很容易修复，zip 受众多（要注意乱码问题），也可以考虑。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:4:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#四如何选用压缩方案"},{"categories":["tech"],"content":" 五、Linux 上的压缩相关指令","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:0","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#五linux-上的压缩相关指令"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ bash [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意；–exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#1-tar-指令"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ bash [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意；–exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#选项与参数"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ bash [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意；–exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#其实最简单的使用-tar-就只要记忆底下的方式即可"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ bash [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意；–exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#仅解压指定的文档"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ bash [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意；–exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#打包某目录但不含该目录下的某些档案之作法"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ bash [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意；–exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#只打包目录中比指定时刻更新的文档"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ bash [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意；–exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#tarfile-tarball"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ bash [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意；–exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#2-zip格式linux-一般也会自带详细的请man"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ bash [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意；–exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#3-7z格式需要p7zipdeepin自带更多的请man"},{"categories":["tech"],"content":" 1. tar 指令通过之前的介绍，可以看出常用的就是 tar gzip xz 等，如果要压缩多个文档，需要先使用tar，再用管道重定向到 gzip 或 xz，比较麻烦，而这几个指令又很常用。于是后来对tar做了增强。tar 最初只是一个归档进程，而压缩则由其他的压缩软件来完成（一个进程只干一件事）。后来为了方便，丧心病狂地集成了各种压缩指令。因此这里就只介绍这一个命令了（它囊括了所有）。tar 的选项与参数非常的多！我们只讲几个常用的选项，更多选项您可以自行 man tar 查询啰！ bash [dmtsai@study ~]$ tar [-z|-j|-J] [cv] [-f 待创建的新档名] filename... \u003c==打包与压缩 [dmtsai@study ~]$ tar [-z|-j|-J] [tv] [-f 既有的 tar档名] \u003c==察看档名 [dmtsai@study ~]$ tar [-z|-j|-J] [xv] [-f 既有的 tar档名] [-C 目录] \u003c==解压缩 选项与参数-c ：创建打包档案，可搭配 -v 来察看过程中被打包的档名(filename) -t ：察看打包档案的内容含有哪些档名，重点在察看『档名』就是了； -x ：解打包或解压缩的功能，可以搭配 -C (大写) 在特定目录解开 特别留意的是， -c, -t, -x 不可同时出现在一串指令列中。 -z ：透过 gzip 的支持进行压缩/解压缩：此时档名最好为 *.tar.gz -j ：透过 bzip2 的支持进行压缩/解压缩：此时档名最好为 *.tar.bz2 -J ：透过 xz 的支持进行压缩/解压缩：此时档名最好为 *.tar.xz 特别留意， -z, -j, -J 不可以同时出现在一串指令列中 -v ：在压缩/解压缩的过程中，将正在处理的档名显示出来！ -f filename：-f 后面要立刻接要被处理的档名！建议 -f 单独写一个选项啰！(比较不会忘记) -C 目录 ：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。 其他后续练习会使用到的选项介绍： -p(小写) ：保留备份资料的原本权限与属性，常用于备份(-c)重要的设定档 -P(大写) ：保留绝对路径，亦即允许备份资料中含有根目录存在之意；–exclude=FILE：在压缩的过程中，不要将 FILE 打包！ 其实最简单的使用 tar 就只要记忆底下的方式即可 压缩：tar -zcv -f filename.tar.gz \u003c要被压缩的档案或目录名称\u003e 查看文档树：tar -ztv -f filename.tar.gz 解压缩：tar -zxv -f filename.tar.gz -C \u003c欲解压缩的目录\u003e` 上面的命令需要根据压缩格式的不同，选用 -z -j -J 选项，而实际上文档的后缀就已经表明了它的压缩格式，不免让人觉得多余。因此就有这幺一条通用的压缩/解压 option -a, --auto-compress Use archive suffix to determine the compression program. 使用这个，便有了通用的解压指令： tar -axv -f file.tar.* （它适用于上述三种压缩格式） 仅解压指定的文档 先查看文档树找到需要解压的文档的文档名 tar -zxv -f 打包档.tar.gz 待解开档名 打包某目录，但不含该目录下的某些档案之作法使用 –exclude=FILE 选项（支持文档名的模式匹配，而且可重复） tar -zcv -f filename.tar.gz directory --exclude=FILE1 --exclude=func* 只打包目录中比指定时刻更新的文档使用 --newer-mtime=\"2015/06/17\" 选项。 tarfile, tarballtarfile | 纯打包、未压缩的 tar 文档 tarball | 压缩了的 tar 文档 2. zip格式（linux 一般也会自带，详细的请man） 压缩命令：zip 压缩目录：zip -r filename.zip directory r 表示递归压缩，压缩包会包含这个目录 解压命令：unzip 解压到某目录：unzip -d directory filename.zip (-d dir 表示将内容解压到dir目录内) -t 测试压缩档的完整性 -x filename 排除某文档 3. 7z格式（需要p7zip，deepin自带，更多的请man） 查看目录树：7z l file.7z (List contents of archive) 压缩：7z a file.7z file1 directory1 (a 为创建压缩档或向压缩档中添加文档/目录，一次可指定多个要压缩的文档或目录) 解压：7z x file.7z -o directory (解压到指定目录) 测试完整性： 7z t file.7z p7zip 安装好后，会提供 7z、7za、7zr 三个指令，一般来说直接用 7z 就行。 P.S. 7z 不会保存 Linux 文档的用户、用户组信息，因此不能直接用于 Linux 系统备份，此时建议用 tar.xz 或 tar.7z（也就是先用tar打包） 4. rar格式（还是那句话，更多的请man）rar 是非开源的格式，Linux 默认是不会包含 rar 压缩软件的，但是它的解压软件是开源的，deepin 自带 unrar，顺便 7zip 也可解压 rar 文档。若想用linux创建rar压缩档，需要从rarlab上下载 Linux 版，（deepin源自带）不过要注意的是该 linux 版是 40 天试用版，要长期使用的话，可能需要破解。（rar 的 key 网上一搜一大把） 压缩：rar a file.rar file （这个是试用的） 解压：unrar x file.rar （这个开源免费） 其实我挺中意 rar 的修复功能的，不知道为啥 7z xz 这样的新格式没有添加类似的recorvery records。上次下个 idea 的 tarball，下了四五次才下到一个完整的，要是用rar 的话，大概一键修复就好了，可 tar.gz 我不知道怎幺修复，只好一遍遍重复下载。。 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:1","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#4-rar格式还是那句话更多的请man"},{"categories":["tech"],"content":" 六、参考 档案与档案系统的压缩,打包与备份 维基百科 rar tar gz zip 7z 有什幺区别? - 知乎 为什幺linux的包都是.tar.gz？要解压两次 - 知乎 ","date":"2019-01-14","objectID":"/posts/compression-related-instructions-under-linux/:5:2","series":null,"tags":["Linux","Compression","压缩","tar"],"title":"常见压缩格式的区别，及 Linux 下的压缩相关指令","uri":"/posts/compression-related-instructions-under-linux/#六参考"},{"categories":["life"],"content":" 高中考虑志向的时候，我最开始想选择电子信息工程，因为小时候就喜欢摆弄各种电子器件，这个方向硬件软件都能玩，就感觉很有趣，只是担心自己高三太放浪形骸考不上。偶然想起在学校阅览室读杂志时，曾被科幻世界2013年12期里沖氏武彦的《在回声中重历》给打动——用耳朵“看见”世界实在是太奇妙了，我当时痴痴地幻想了好几天。 这样我开始考虑选择声学。 我从同桌推荐的《刀剑神域》开始接触日本的 ACG 文化，后来接触到初音未来和洛天依，就对歌声合成(singing synthesis)产生了很大的兴趣，仔细一想发现这也应该是声学的范畴，这使我坚定了我选择声学的想法。 从那时到现在的各种破事，实在不想多说，就略过不提了。总之我选择了声学，然后干得很失败。。。 为啥想写这篇文章呢？话还得从去年十一月份说起，当时要学 Matlab，就在 bilibili 上找了个教程：自制合成器演奏鸟之诗，讲用 Matlab 做吉他音合成，讲得特别棒，我跟着做出了guitar-synthesizer 这个小玩意儿。 然后今天发现那个教程的作者就是做歌声合成的，而且从 2011 年 13 岁开始，因为想让初音唱中文， 就开始写代码，一写就写到现在，从初中写到留学美国常春藤，从简单的时域拼接到现在的深度神经网络、隐马尔可夫模型。他在上个暑假到雅马哈（vocaloid 的制作公司）实习，并推出了自己第五次重制的歌声合成引擎 Synthesizer V。这一系列的事迹，七年五次重制，从初中生到现在双修计算机科学和数学，简直让人叹为观止。这位作者的名字叫华侃如（Kanru Hua），网络常用昵称 sleepwalking，他的个人网站Kanru Hua’s Website - About Me. 通过他的博客，我了解到歌声合成需要两个方向的知识：信号处理和机器学习。 于是我想起了我的初心。 我小的时候特别喜欢拆各种电子设备，曾经用手机电池和坏手电做过手电筒，再加上个手机振动器用来吓人，还喜欢在家做各种实验。可整个大学，我的各种手工课实验课弄得一塌糊涂，垫底的存在。这样说来，如果当初选了电子信息工程，可能会混得更差。 信号处理学得一团糟，一直想努力可在这方面总是半途而废。 说到底还是自控力太差，为何我就不能按部就班一回呢？到底是怎么搞的会养成这样的性格，这样的时候会很恨自己窝囊的性格。 如果我也能像室友一样按部就班的完成学业，只在空闲时间做自己的事…可惜没有如果。 经常会告诉自己做过的事已经不可逆转了，不需要后悔，所以也不会后悔。大概潜意识里还是会有怨念，所以才会在这样的时刻爆发出满满的恶意。 只是后悔却不做出改变是没有用的，但我完全不相信自己能改掉这样的性格，自暴自弃。纵欲一时爽， 一直纵欲一直爽hhh… 说虽这样说，还是想继续挣扎下去，信号处理就像一道槛，不跨过去我无法面对自己。 “希望明年年底的总结中，我不要再这么丧”，这样大声说出来的话，言就能“灵”吧？我记得我运气一直挺好的（又窝囊到要靠运气。。） 更新： 今天二书介绍给我一篇文章我的编程经历，作者和我一样“不能兼顾兴趣与学业”，大一结束就退学了，现在在阿里巴巴工作。他博客的观点很中肯： 我家人都是很传统的一代，不理解我学习编程最终能做出什么，他们主张先完成学业，再做这件事。但是我很清楚我自己的智商，不足以多线程处理不同的大领域，所以我顶着压力，努力地做出来他们能看出来的「成绩」，才能换来他们的理解。所以，如果你通过你的理性分析，坚持认为某件事情是对的，就努力的去做，不要放弃了以后看到另一个人做了你曾经想做的东西然后感慨当初应该怎么样怎么样。 –我的编程经历 只有在已经拥有解决问题的能力的时候，才有资格考虑退学这件事。–你根本用不着退学 我仔细想了想，我不需要太在意自己“不能兼顾”、“自控力太弱”。如果真考虑清楚了得失，我只需要“ 拥有解决问题的能力”，然后去做我想做的就行了。 我自控力很弱，而且这么多年了一直改不了，即使再延期一年，也有可能还是拿不到毕业证，折磨自己而已。而上班的话，就暑假实习的经验来看，工作都有进度条催着，反而活的更有朝气。 ","date":"2019-01-08","objectID":"/posts/relive-in-the-echo/:0:0","series":null,"tags":["闲言碎语","初心","歌声合成"],"title":"在回声中重历","uri":"/posts/relive-in-the-echo/#"},{"categories":["tech"],"content":" 本笔记整理自《SQL 基础教程》、《MySQL 必知必会》和网上资料。个人笔记不保证正确。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:0:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#"},{"categories":["tech"],"content":" 一、复杂查询","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:1:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#一复杂查询"},{"categories":["tech"],"content":" 视图将 SELECT 查询包装成一个虚拟表，该虚拟表就被称为视图。（因为只是一个包装，因此视图的数据也会随着原表的更新而更新） 用途： 简化复杂的SQL查询，用它替换子查询，能降低查询的嵌套深度。 SELECT 查询的重用，减少重复查询。 … 创建视图： sql CREATE VIEW \u003c视图名称\u003e (\u003c视图列名1\u003e, \u003c视图列名2\u003e... ) AS \u003cSELECT 语句\u003e; 其中 SELECT 的结果列和视图列名一一对应。3. 视图的限制 1. 视图的 SELECT 子句，不能包含 ORDER BY 子句。因为视图也是表，而表是集合，它没有顺序。（也有些DB支持该用法，但不通用）1. 视图的更新：只在很有限的条件下，才能在视图上使用 INSERT/DELETE/UPDATE 这样的变更数据的语句。（视图应该只用于检索，能不更新就不要更新它）4. 删除视图：DROP VIEW \u003c视图名称\u003e; ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:1:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#视图"},{"categories":["tech"],"content":" 子查询子查询，其实就是一次性的视图: sql SELECT ... FROM ( SELECT ... -- 这就是一个子查询：嵌套的 select 语句 ) AS \u003c别名\u003e ... 上面的查询的 FROM 子句中，给另一 SELECT 子句定义了一个别名，并将它作为了查询对象。这就是一个子查询。 子查询不仅能用于 FROM，还能用在 WHERE 子句等很多地方。 关联子查询即用到了外部数据的子查询语句： sql SELECT ... FROM product AS p1 WHERE ( SELECT ... FROM duck AS p2 WHERE p1.price \u003e p2.price -- 这里，内部子查询访问了外部查询的表p1，因此是一个关联子查询。 ); ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:1:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#子查询"},{"categories":["tech"],"content":" 子查询子查询，其实就是一次性的视图: sql SELECT ... FROM ( SELECT ... -- 这就是一个子查询：嵌套的 select 语句 ) AS \u003c别名\u003e ... 上面的查询的 FROM 子句中，给另一 SELECT 子句定义了一个别名，并将它作为了查询对象。这就是一个子查询。 子查询不仅能用于 FROM，还能用在 WHERE 子句等很多地方。 关联子查询即用到了外部数据的子查询语句： sql SELECT ... FROM product AS p1 WHERE ( SELECT ... FROM duck AS p2 WHERE p1.price \u003e p2.price -- 这里，内部子查询访问了外部查询的表p1，因此是一个关联子查询。 ); ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:1:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#关联子查询"},{"categories":["tech"],"content":" 二、函数、谓词、CASE 表达式","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#二函数谓词case-表达式"},{"categories":["tech"],"content":" 函数 给出的链接都是 MySQL 的 算术函数 加减乘除：+ - * / ABS 绝对值 MOD 求余 ROUND 四舍五入 字符串函数 CONCAT(str1,str2,…) 拼接 LENGTH(str) 字符串的 bytes 长度 CHAR_LENGTH(str) LOWER/UPPER 大小写转换 REPLACE(str,from_str,to_str) 替换 SUBSTRING(str FROM pos FOR len) 截取 时间和日期函数 CURRENT_DATE 当前日期 CURRENT_TIME 当前时间 CURRENT_TIMESTAMP 当前的日期和时间 EXTRACT(unit FROM date) 截取日期元素，unit 可为 YEAR MONTH HOUR 等等 转换函数 CAST(expr AS type) 将 expr 的结果转换成 type 类型 COALESCE(value,…) 从左往右扫描，返回第一个非 NULL 的值。常用于将 NULL 转换为其他值。eg. COALESCE(sth, 1) 如果 sth 为 NULL 就会返回1. 聚合函数：基本就五个，已经学过了。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#函数httpsdevmysqlcomdocrefman57enfunctionshtml"},{"categories":["tech"],"content":" 谓词即返回布尔值的表达式 LIKE谓词——简单字符串匹配（慢） 匹配整个列 %：任意字符出现任意次 _：匹配任意一个字符 举例： sql SELECT name FROM list WHERE name LIKE '%Ryan%'; -- 匹配任意包含 'Ryan' 的字符串 REGEXP谓词——正则字符串匹配 MySQL 只实现了通用正则的一个子集，而且是search模式。（非match） 其他 BETWEEN：范围匹配，eg. BETWEEN 1 AND 10 IS NULL、IS NOT NULL IN、NOT IN：是否在某集合内 EXISTS、NOT EXISTS（比较难的一个，入门阶段不要求）：该谓词比较特殊，只需要右侧一个参数，而且该参数绝大多数情况下，都是一个关联子查询。而且该子查询的SELECT子句的参数基本可以随意，通常使用SELECT *. 对于子查询有返回值的列，它返回True，否则返回False. 但要注意为 NULL 时返回 UNKNOW.（而 WHERE 只认 True） ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#谓词"},{"categories":["tech"],"content":" 谓词即返回布尔值的表达式 LIKE谓词——简单字符串匹配（慢） 匹配整个列 %：任意字符出现任意次 _：匹配任意一个字符 举例： sql SELECT name FROM list WHERE name LIKE '%Ryan%'; -- 匹配任意包含 'Ryan' 的字符串 REGEXP谓词——正则字符串匹配 MySQL 只实现了通用正则的一个子集，而且是search模式。（非match） 其他 BETWEEN：范围匹配，eg. BETWEEN 1 AND 10 IS NULL、IS NOT NULL IN、NOT IN：是否在某集合内 EXISTS、NOT EXISTS（比较难的一个，入门阶段不要求）：该谓词比较特殊，只需要右侧一个参数，而且该参数绝大多数情况下，都是一个关联子查询。而且该子查询的SELECT子句的参数基本可以随意，通常使用SELECT *. 对于子查询有返回值的列，它返回True，否则返回False. 但要注意为 NULL 时返回 UNKNOW.（而 WHERE 只认 True） ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#like谓词简单字符串匹配慢"},{"categories":["tech"],"content":" 谓词即返回布尔值的表达式 LIKE谓词——简单字符串匹配（慢） 匹配整个列 %：任意字符出现任意次 _：匹配任意一个字符 举例： sql SELECT name FROM list WHERE name LIKE '%Ryan%'; -- 匹配任意包含 'Ryan' 的字符串 REGEXP谓词——正则字符串匹配 MySQL 只实现了通用正则的一个子集，而且是search模式。（非match） 其他 BETWEEN：范围匹配，eg. BETWEEN 1 AND 10 IS NULL、IS NOT NULL IN、NOT IN：是否在某集合内 EXISTS、NOT EXISTS（比较难的一个，入门阶段不要求）：该谓词比较特殊，只需要右侧一个参数，而且该参数绝大多数情况下，都是一个关联子查询。而且该子查询的SELECT子句的参数基本可以随意，通常使用SELECT *. 对于子查询有返回值的列，它返回True，否则返回False. 但要注意为 NULL 时返回 UNKNOW.（而 WHERE 只认 True） ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#regexp谓词正则字符串匹配"},{"categories":["tech"],"content":" 谓词即返回布尔值的表达式 LIKE谓词——简单字符串匹配（慢） 匹配整个列 %：任意字符出现任意次 _：匹配任意一个字符 举例： sql SELECT name FROM list WHERE name LIKE '%Ryan%'; -- 匹配任意包含 'Ryan' 的字符串 REGEXP谓词——正则字符串匹配 MySQL 只实现了通用正则的一个子集，而且是search模式。（非match） 其他 BETWEEN：范围匹配，eg. BETWEEN 1 AND 10 IS NULL、IS NOT NULL IN、NOT IN：是否在某集合内 EXISTS、NOT EXISTS（比较难的一个，入门阶段不要求）：该谓词比较特殊，只需要右侧一个参数，而且该参数绝大多数情况下，都是一个关联子查询。而且该子查询的SELECT子句的参数基本可以随意，通常使用SELECT *. 对于子查询有返回值的列，它返回True，否则返回False. 但要注意为 NULL 时返回 UNKNOW.（而 WHERE 只认 True） ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#其他"},{"categories":["tech"],"content":" CASE 表达式 if - else if - else 形式： sql CASE WHEN \u003c求值表达式\u003e THEN \u003c表达式\u003e WHEN \u003c求值表达式\u003e THEN \u003c表达式\u003e WHEN \u003c求值表达式\u003e THEN \u003c表达式\u003e ... ELSE \u003c表达式\u003e END switch 模式（但不需要break） sql CASE \u003c表达式\u003e WHEN \u003c表达式\u003e THEN \u003c表达式\u003e WHEN \u003c表达式\u003e THEN \u003c表达式\u003e ... ELSE \u003c表达式\u003e END 这是对 CASE 后的元素做switch比较。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:2:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#case-表达式"},{"categories":["tech"],"content":" 三、集合运算","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#三集合运算"},{"categories":["tech"],"content":" 注意事项 作为运算对象的结果集，列字段必须一一对应，而且对应列的类型必须相同。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#注意事项"},{"categories":["tech"],"content":" 结果集的交并差 \u003c查询1\u003e UNION \u003c查询2\u003e：对两个结果集求并 UNION ALL：添加 ALL 能使结果集包含重复行。 \u003c查询1\u003e INTERSECT \u003c查询2\u003e：两结果集的交集 \u003c查询1\u003e EXCEPT \u003c查询2\u003e：两结果集的差集 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#结果集的交并差"},{"categories":["tech"],"content":" 以列为单位，对表进行联结(JOIN) 最强大的功能之一 交并差是以行为单位的操作，是竖向的运算。而联结是以列为单位的操作，是横向的拼接。 内联(INNER JOIN) 内联结果只包含两表的交集 语法： sql SELECT ... FROM (product INNER JOIN shop ON product.p_id = shop.p_id) WHERE filter_condition; 使用 跟在 INNER JOIN 子句后的 ON 子句指定联结条件。（这里我特意用了括号，表示 JOIN 和 ON 两个子句是配套的） 也有另一个很常用的语法（但是现在已经不推荐使用）： sql SELECT ... FROM product, shop WHERE product.p_id = shop.p_id AND filter_condition; 对于 shop 表中有多行对应同一个 product 的情况（有多人购买了同一款商品），结果中该 product 会被复制给 shop 中的多个购买记录。（也就是说该 product 会变成多行） INNER 可以省略，也就是说只写 JOIN，就默认是 INNER JOIN 外联(OUTER JOIN) 外联以某表为主表，将另一表的列联结到该表。另一表没有值的列，就用 NULL 代替。使用LEFT 或 RIGHT指定主表。（两个关键字都能实现同样的效果，不过用 LEFT 的多一些） 语法： sql SELECT ... FROM product LEFT OUTER JOIN shop ON product.p_id = shop.p_id; 这和内联很相似，差别只是联结关键词改成了LEFT OUTER JOIN。这表示以左边的表为主表，把右边的表的内容联结上去。因此左表的所有列都会出现在结果集中。 多表联查举例： sql -- 登录异常的账号及密码 select distinct batches.identity_number as '登录失败账号', accounts.password from ((batches left outer join tasks on batches.id = tasks.batch_id) -- 批次表联结具体的任务表 left outer join `status` on tasks.id = status.task_id) -- 再联结上状态表 left outer join accounts on batches.identity_number = accounts.identity_number -- 再联结上账号表 where `status`.step_type = 'check_login' -- 只提取 \"check_login\" 步骤的记录 and status.status != 'info' -- 状态不为 info，说明登录异常 此外还有 FULL OUTER JOIN 表示返回左右两表的所有行！所有没有匹配的行都给出 NULL P.S. 其中的关键字 OUTER 通常可省略。但是 LEFT、RIGHT、FULL 不可以省略。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#以列为单位对表进行联结join"},{"categories":["tech"],"content":" 以列为单位，对表进行联结(JOIN) 最强大的功能之一 交并差是以行为单位的操作，是竖向的运算。而联结是以列为单位的操作，是横向的拼接。 内联(INNER JOIN) 内联结果只包含两表的交集 语法： sql SELECT ... FROM (product INNER JOIN shop ON product.p_id = shop.p_id) WHERE filter_condition; 使用 跟在 INNER JOIN 子句后的 ON 子句指定联结条件。（这里我特意用了括号，表示 JOIN 和 ON 两个子句是配套的） 也有另一个很常用的语法（但是现在已经不推荐使用）： sql SELECT ... FROM product, shop WHERE product.p_id = shop.p_id AND filter_condition; 对于 shop 表中有多行对应同一个 product 的情况（有多人购买了同一款商品），结果中该 product 会被复制给 shop 中的多个购买记录。（也就是说该 product 会变成多行） INNER 可以省略，也就是说只写 JOIN，就默认是 INNER JOIN 外联(OUTER JOIN) 外联以某表为主表，将另一表的列联结到该表。另一表没有值的列，就用 NULL 代替。使用LEFT 或 RIGHT指定主表。（两个关键字都能实现同样的效果，不过用 LEFT 的多一些） 语法： sql SELECT ... FROM product LEFT OUTER JOIN shop ON product.p_id = shop.p_id; 这和内联很相似，差别只是联结关键词改成了LEFT OUTER JOIN。这表示以左边的表为主表，把右边的表的内容联结上去。因此左表的所有列都会出现在结果集中。 多表联查举例： sql -- 登录异常的账号及密码 select distinct batches.identity_number as '登录失败账号', accounts.password from ((batches left outer join tasks on batches.id = tasks.batch_id) -- 批次表联结具体的任务表 left outer join `status` on tasks.id = status.task_id) -- 再联结上状态表 left outer join accounts on batches.identity_number = accounts.identity_number -- 再联结上账号表 where `status`.step_type = 'check_login' -- 只提取 \"check_login\" 步骤的记录 and status.status != 'info' -- 状态不为 info，说明登录异常 此外还有 FULL OUTER JOIN 表示返回左右两表的所有行！所有没有匹配的行都给出 NULL P.S. 其中的关键字 OUTER 通常可省略。但是 LEFT、RIGHT、FULL 不可以省略。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#内联inner-join"},{"categories":["tech"],"content":" 以列为单位，对表进行联结(JOIN) 最强大的功能之一 交并差是以行为单位的操作，是竖向的运算。而联结是以列为单位的操作，是横向的拼接。 内联(INNER JOIN) 内联结果只包含两表的交集 语法： sql SELECT ... FROM (product INNER JOIN shop ON product.p_id = shop.p_id) WHERE filter_condition; 使用 跟在 INNER JOIN 子句后的 ON 子句指定联结条件。（这里我特意用了括号，表示 JOIN 和 ON 两个子句是配套的） 也有另一个很常用的语法（但是现在已经不推荐使用）： sql SELECT ... FROM product, shop WHERE product.p_id = shop.p_id AND filter_condition; 对于 shop 表中有多行对应同一个 product 的情况（有多人购买了同一款商品），结果中该 product 会被复制给 shop 中的多个购买记录。（也就是说该 product 会变成多行） INNER 可以省略，也就是说只写 JOIN，就默认是 INNER JOIN 外联(OUTER JOIN) 外联以某表为主表，将另一表的列联结到该表。另一表没有值的列，就用 NULL 代替。使用LEFT 或 RIGHT指定主表。（两个关键字都能实现同样的效果，不过用 LEFT 的多一些） 语法： sql SELECT ... FROM product LEFT OUTER JOIN shop ON product.p_id = shop.p_id; 这和内联很相似，差别只是联结关键词改成了LEFT OUTER JOIN。这表示以左边的表为主表，把右边的表的内容联结上去。因此左表的所有列都会出现在结果集中。 多表联查举例： sql -- 登录异常的账号及密码 select distinct batches.identity_number as '登录失败账号', accounts.password from ((batches left outer join tasks on batches.id = tasks.batch_id) -- 批次表联结具体的任务表 left outer join `status` on tasks.id = status.task_id) -- 再联结上状态表 left outer join accounts on batches.identity_number = accounts.identity_number -- 再联结上账号表 where `status`.step_type = 'check_login' -- 只提取 \"check_login\" 步骤的记录 and status.status != 'info' -- 状态不为 info，说明登录异常 此外还有 FULL OUTER JOIN 表示返回左右两表的所有行！所有没有匹配的行都给出 NULL P.S. 其中的关键字 OUTER 通常可省略。但是 LEFT、RIGHT、FULL 不可以省略。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#外联outer-join"},{"categories":["tech"],"content":" 画外：字段引用符号如果数据库的字段名/数据库名/表名可能和数据库关键字重复，就需要用引用符号将他们引用起来，消除歧义。 MySQL 中经常用反引号干这个事。而 SQL Server 则使用方括号。标准 SQL 使用双引号。在看到这些符号时要知道这些差别。 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:4:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#画外字段引用符号"},{"categories":["tech"],"content":" 查询语句分析 MySQL Explain详解 ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:5:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#查询语句分析"},{"categories":["tech"],"content":" 常见问题","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:6:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#常见问题"},{"categories":["tech"],"content":" 隐式类型转换在MySQL中，当操作符与不同类型的操作数一起使用时，会发生类型转换以使操作数兼容。则会发生隐式类型转换。 隐式类型转换会导致查询不会走索引！！！可能会严重拖累性能。另外还可能会导致各种奇怪的问题。 详见 MYSQL隐式类型转换 完。（接下来就是用 Python/Java 连接 MySQL 了） ","date":"2018-06-17","objectID":"/posts/sql-basics-2-queries/:6:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（二）进阶查询","uri":"/posts/sql-basics-2-queries/#隐式类型转换"},{"categories":["tech"],"content":" 本笔记整理自《SQL 基础教程》、《MySQL 必知必会》和网上资料。个人笔记不保证正确。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:0:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#"},{"categories":["tech"],"content":" 一、基础SQL，即结构化查询语言，是为访问与操作关系数据库中的数据而设计的语言。 关系数据库以行(row)为单位读写数据 SQL 根据功能的不同，可分为三类（其中DML用得最多，增删查改嘛） DDL(Data Definition Language, 数据定义语言): CREATE/DROP/ALTER DML(Data Manipulation Language, 数据操作语言): SELECT/INSERT/UPDATE/DELETE DCL(Data Control Language, 数据控制语言): COMMIT/ROLLBACK/GRANT/REVOKE SQL 语句要以分号结尾。换行在 SQL 中不表示结束，而等同于空格。 SQL 不区分**关键字(Keyword)**的大小写，但是描述符就不一定了。 这里有个坑：MySQL 中，数据库和表其实就是数据目录下的目录和文件，因而，操作系统的敏感性决定数据库名和表名 是否大小写敏感。这就意味着数据库名和表名在 Windows 中是大小写不敏感的，而在大多数类型的 Unix/Linux 系统中是大小写敏感的。（注意仅指数据库名和表名）可通过修改配置文件的lower_case_table_names属性来统一这一行为。 而字段名、字段内容都是内部数据，是操作系统无关的。它们的大小写敏感性，由 MySQL 的的校对（COLLATE）规则来控制。该规则体现在 MySQL 的 校对字符集（COLLATION）的后缀上：比如 utf8字符集，utf8_general_ci表示不区分大小写，这个是 utf8 字符集默认的校对规则；utf8_general_cs 表示区分大小写，utf8_bin 表示二进制比较，同样也区分大小写 。 SQL 中的字符串和日期需要用单引号引用起来，日期有特定格式年-月-日 修改字符集：set names \u003c字符集名\u003e 记住在 MySQL 中，utf-8mb4 才是完全的 utf-8字符集。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:1:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#一基础"},{"categories":["tech"],"content":" 二、DDL","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#二ddl"},{"categories":["tech"],"content":" 1. 数据库的创建和删除 创建数据库 sql CREATE DATABASE \u003c数据库名称\u003e; sql DROP DATABASE \u003c数据库名称\u003e; ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#1-数据库的创建和删除"},{"categories":["tech"],"content":" 2. 创建表： 关系表的设计关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： sql CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： sql CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL）有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句， 在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#2-创建表"},{"categories":["tech"],"content":" 2. 创建表： 关系表的设计关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： sql CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： sql CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL）有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句， 在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#关系表的设计"},{"categories":["tech"],"content":" 2. 创建表： 关系表的设计关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： sql CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： sql CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL）有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句， 在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#语句"},{"categories":["tech"],"content":" 2. 创建表： 关系表的设计关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： sql CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： sql CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL）有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句， 在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#字段类型mysql"},{"categories":["tech"],"content":" 2. 创建表： 关系表的设计关系表的设计，要确保把信息分解成多个表，一类信息一个表，各表通过某些常用的，基本不会改变的值（即关系表设计中的关系，也常称为外键）互相关联。尽量不要有冗余数据。 语句： sql CREATE TABLE \u003c表名\u003e ( \u003c列名1\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名2\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, \u003c列名3\u003e \u003c数据类型\u003e \u003c该列所需约束\u003e, ... \u003c该表的约束1\u003e, \u003c该表的约束2\u003e... ); 举例： sql CREATE TABLE `persons` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT, `name` CHAR(20) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 后面的是设置引擎和默认字符集。工作上，表的设计一定要深思熟虑，因为改起来很困难。 字段类型（MySQL）有四类数据类型：字符串、数字、日期、二进制。它们又根据数据长度的区别，下分为多个类型。 字符串： 数字 日期 二进制 约束SQL 约束是除了数据类型之外，对列中数据追加的限定条件。 类型约束：NOT NULL、AUTO_INCREMENT、UNSIGNED（这个只 MySQL 支持） 默认值：DEFAULT，举例 \u003c列名3\u003e VARCHAR(32) NOT NULL DEFAULT \"los angeles\" 表约束：PRIMARY KEY 主键约束（主键默认 UNIQUE 且 NOT NULL） 此外还有 FOREIGN KEY 和 CHECK 两个约束语句， 在进阶笔记中介绍。 P.S. 字段约束也可以写成表约束（比如主键约束），而反过来很可能不行。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#约束"},{"categories":["tech"],"content":" 3. 删除表和更新表定义 删除表（危险操作） 删除整个表： sql DROP TABLE \u003c表名\u003e; - 只清空表内数据，但留下表： sql TRUNCATE \u003c表名\u003e; -- 非标准SQL语句，但是大部分DB都支持。（可能不能ROLLBACK） 更新表定义（麻烦的操作）所以所创建表前要仔细想好格式了，更新表定义是不得已才能为之。添加列定义： sql ALTER TABLE \u003c表名\u003e ADD COLUMN \u003c列名\u003e \u003c数据类型\u003e \u003c该列的约束\u003e; 删除列定义： sql ALTER TABLE \u003c表名\u003e DROP COLUMN \u003c列名\u003e; ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:2:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#3-删除表和更新表定义"},{"categories":["tech"],"content":" 三、DML 万恶之源 NULL ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#三dml"},{"categories":["tech"],"content":" 1. 查询（重点） 基本语句： text SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件\u003e; 可用 DISTINCT 修饰列名，使查询结果无重。例：SELECT DISTINCT \u003c列名\u003e FROM \u003c表名\u003e 过滤条件可使用比较运算(\u003c\u003e、=等)和逻辑运算(AND OR NOT). 过滤条件中，比较运算会永远忽略 NULL 值，如果需要对 NULL 值做操作，需要使用 IS NULL 或 IS NOT NULL（说忽略也许不太准确，NULL 既不为真也不为假，反正少用 NULL。。） 包含NULL的四则运算，得到的结果总为NULL ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#1-查询重点"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)， 再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样）因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING 中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了）格式： sql SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#2-聚合与排序重点"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)， 再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样）因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING 中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了）格式： sql SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#聚合函数"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)， 再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样）因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING 中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了）格式： sql SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#分组group-by"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)， 再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样）因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING 中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了）格式： sql SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#聚合对select子句的限制"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)， 再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样）因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING 中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了）格式： sql SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#对聚合结果进行过滤having"},{"categories":["tech"],"content":" 2. 聚合与排序（重点） 聚合函数即对列进行统计分析的函数，主要有五个： COUNT：计算列的行数。（只有COUNT(*)会计算NULL行） SUM：求该列之和。 AVG：求该列的平均值。 MAX/MIN：求该列的 最大/最小 值 NOTE： 聚合函数计算时会排除所有NULL行。只有COUNT(*)例外，NULL行也会被它计数。 MAX/MIN 几乎适用于所有数据类型的列（对非数值型，以其二进制值来排序），而SUM/AVG只能用于数值类型的列。 聚合函数操作的列，也能用DISTINCT修饰。例：SELECT COUNT(DISTINCT \u003c列名\u003e) FROM \u003c表名\u003e 聚合函数只能用于SELECT子句和还没讲的HAVING子句（以及 ORDER BY 子句）中。 分组(GROUP BY)分组以聚合键为分类标准，将数据分为多个逻辑组，从而能分别对每个组进行聚合运算。（分组是为了分类聚合） 若出现了 GROUP BY 子句，查询结果一定是每一组一行。 GROUP BY 会将 NULL 作为一组特定数据，显示为空。 聚合对SELECT子句的限制首先要理解的是： 聚合函数的操作对象为某一列，而产生的结果只有一个值。 GROUP BY 的操作对象为一列或者多列，产生的结果呢，是每一组一个值。 因此为了避免歧义，只要使用了聚合函数或 GROUP BY 二者之一，SELECT 子句就只能包含： 常数 其他聚合函数（该聚合函数的操作对象可以为其他列） 如果使用了 GROUP BY 子句，还能包括该子句所指定的列名。（也就是聚合键）但是绝不能包含其他的列名，因为这会有歧义。 此外，还有一个问题是由 SQL 的执行顺序引起的。应该能很容易猜到，SELECT 语句的执行顺序和书写顺序是不一致的。查询应该是从表开始，所以 FROM 语句一定先执行。然后应该要过滤(WHERE)， 再是分组(GROUP BY)，最后才是 SELECT 语句。（就已经学到的子句而言，顺序是这样）因此按理说，SELECT 语句 定义的别名，是不能在 GROUP BY 里使用的。（也有些DB支持该用法，但不通用） 对聚合结果进行过滤(HAVING)从刚刚说过的SQL执行顺序可见，WHERE要比GROUP BY先执行，因此如果想过滤分组后的结果，不能用它。而应该使用 HAVING 子句。HAVING 子句和 WHERE 子句都是用来过滤的，但是执行顺序的不同也就决定了它们的用途不同。 NOTE： 有时候，会发现某个过滤条件，不论是先执行（就是写在WHERE子句中）还是后执行（写在HAVING 中）都没问题，这时候应该将它写在WHERE子句中，这样GROUP BY操作的数据会更少，处理更快。 HAVING 子句的元素，也存在和 SELECT 子句同样的限制。不能使用聚合键以外的列名。 排序(ORDER BY)ORDER BY 子句在 SELECT 子句之后执行，因此它能使用 SELECT 子句中定义的别名。（而 GROUP BY 之前已经说过不能用别名了）格式： sql SELECT \u003c字段1\u003e AS \u003c别名1\u003e, \u003c字段2\u003e AS \u003c别名2\u003e, ... FROM \u003c表名\u003e WHERE \u003c过滤条件1\u003e GROUP BY \u003c列名1\u003e, \u003c列名2\u003e... HAVING \u003c过滤条件2\u003e ORDER BY \u003c列名/别名1\u003e, \u003c列名/别名2\u003e... ; 多排序键/列：指定多排序键时的排序规则为：优先使用左侧的列，如果该列存在相同值，再接着参考右侧的键，依此类推。（如果左侧键值不同，右侧的键就不会被使用了） NULL 值的顺序：排序键中出现了 NULL 值时，这类值会在结果的开头或结尾汇总，究竟是排在开头还是结尾，并没有特殊规定。 ORDER BY 子句只影响结果的先后顺序，因此排序键可以是结果集以外的东西，比如其他的列，或者使用了 GROUP BY 时，还能用聚合函数。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:2","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#排序order-by"},{"categories":["tech"],"content":" 3. 数据的增、删、改 插入(INSERT INTO) 也算用的多了语法： sql INSERT INTO \u003c表名\u003e (列名1, 列名2...) VALUES (值1, 值2...); 或者也可以使用 SELECT 语句来替代 VALUES 子句，达到将 SELECT 到的结果集插入某表的效果。（但是不要用ORDER BY，因为结果是集，没有顺序，排序是徒劳无功的）插入时主键不能重复，否则会报错。（因此如果需要包含重复数据，一般都会定义一个自增的id字段） 删除(DELETE)清空表（危险操作，而且效率不如 TRUNCATE）： sql DELETE FROM \u003c表名\u003e; 条件删除： sql DELETE FROM \u003c表名\u003e WHERE \u003c条件\u003e; 因此使用DELETE时，一定要记得带WHERE，不然就好玩了。。 更新(UPDATE) sql UPDATE \u003c表名\u003e SET \u003c列名1\u003e = \u003c算术表达式1\u003e, \u003c列名2\u003e = \u003c表达式2\u003e, ... WHERE \u003c条件\u003e; 同 DELETE 一样，不带 WHERE 子句的 UPDATE 是很危险的。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#3-数据的增删改"},{"categories":["tech"],"content":" 3. 数据的增、删、改 插入(INSERT INTO) 也算用的多了语法： sql INSERT INTO \u003c表名\u003e (列名1, 列名2...) VALUES (值1, 值2...); 或者也可以使用 SELECT 语句来替代 VALUES 子句，达到将 SELECT 到的结果集插入某表的效果。（但是不要用ORDER BY，因为结果是集，没有顺序，排序是徒劳无功的）插入时主键不能重复，否则会报错。（因此如果需要包含重复数据，一般都会定义一个自增的id字段） 删除(DELETE)清空表（危险操作，而且效率不如 TRUNCATE）： sql DELETE FROM \u003c表名\u003e; 条件删除： sql DELETE FROM \u003c表名\u003e WHERE \u003c条件\u003e; 因此使用DELETE时，一定要记得带WHERE，不然就好玩了。。 更新(UPDATE) sql UPDATE \u003c表名\u003e SET \u003c列名1\u003e = \u003c算术表达式1\u003e, \u003c列名2\u003e = \u003c表达式2\u003e, ... WHERE \u003c条件\u003e; 同 DELETE 一样，不带 WHERE 子句的 UPDATE 是很危险的。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#插入insert-into-也算用的多了"},{"categories":["tech"],"content":" 3. 数据的增、删、改 插入(INSERT INTO) 也算用的多了语法： sql INSERT INTO \u003c表名\u003e (列名1, 列名2...) VALUES (值1, 值2...); 或者也可以使用 SELECT 语句来替代 VALUES 子句，达到将 SELECT 到的结果集插入某表的效果。（但是不要用ORDER BY，因为结果是集，没有顺序，排序是徒劳无功的）插入时主键不能重复，否则会报错。（因此如果需要包含重复数据，一般都会定义一个自增的id字段） 删除(DELETE)清空表（危险操作，而且效率不如 TRUNCATE）： sql DELETE FROM \u003c表名\u003e; 条件删除： sql DELETE FROM \u003c表名\u003e WHERE \u003c条件\u003e; 因此使用DELETE时，一定要记得带WHERE，不然就好玩了。。 更新(UPDATE) sql UPDATE \u003c表名\u003e SET \u003c列名1\u003e = \u003c算术表达式1\u003e, \u003c列名2\u003e = \u003c表达式2\u003e, ... WHERE \u003c条件\u003e; 同 DELETE 一样，不带 WHERE 子句的 UPDATE 是很危险的。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#删除delete"},{"categories":["tech"],"content":" 3. 数据的增、删、改 插入(INSERT INTO) 也算用的多了语法： sql INSERT INTO \u003c表名\u003e (列名1, 列名2...) VALUES (值1, 值2...); 或者也可以使用 SELECT 语句来替代 VALUES 子句，达到将 SELECT 到的结果集插入某表的效果。（但是不要用ORDER BY，因为结果是集，没有顺序，排序是徒劳无功的）插入时主键不能重复，否则会报错。（因此如果需要包含重复数据，一般都会定义一个自增的id字段） 删除(DELETE)清空表（危险操作，而且效率不如 TRUNCATE）： sql DELETE FROM \u003c表名\u003e; 条件删除： sql DELETE FROM \u003c表名\u003e WHERE \u003c条件\u003e; 因此使用DELETE时，一定要记得带WHERE，不然就好玩了。。 更新(UPDATE) sql UPDATE \u003c表名\u003e SET \u003c列名1\u003e = \u003c算术表达式1\u003e, \u003c列名2\u003e = \u003c表达式2\u003e, ... WHERE \u003c条件\u003e; 同 DELETE 一样，不带 WHERE 子句的 UPDATE 是很危险的。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:3:3","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#更新update"},{"categories":["tech"],"content":" 四、DCL - 事务处理(MySQL)事务是一系列不可分割的数据库操作，也就是说，这一系列操作要么全部执行，要么全部不执行。如果执行过程中发生了问题（检查执行状态），可以通过执行 ROLLBACK 回滚到该事务执行前的状态。 （注意并不会自动回滚） mysql START TRANSACTION; -- do something COMMIT; START TRANSACTION: 标识事务的开始 COMMIT：提交事务。一旦提交，所执行过的操作就已成定论，恢复不了了。 ROLLBACK：事务回滚，只能回滚未 COMMIT 的 DML 操作！也就是说只能用在START TRANSACTION 和 COMMIT 之间，并且只能回滚 INSERT/UPDATE/DELETE。（回滚 SELECT 没啥意义） SAVEPOINT \u003c保留点\u003e 和 ROLLBACK TO \u003c保留点\u003e：同样只能用在 START TRANSACTION 和COMMIT 之间，其优势在于，ROLLBACK TO 可以指定回滚到某特定保留点，更灵活，而ROLLBACK 只能回滚到事务开始前。 需要注意的有： COMMIT 和 ROLLBACK 语句也是事务的结束，因此如果执行了 ROLLBACK，那它与 COMMIT 之间的内容会被跳过。（在这一点上，它相当于大多数 PL 的 return） 如果事务执行出现问题，问题行后面的所有语句都不会被执行！包括 COMMIT 和ROLLBACK！ 如果想用纯 SQL 实现事务原子性，必须使用存储过程检查执行状态！举例如下： text CREATE PROCEDURE my_test() BEGIN DECLARE EXIT HANDLER FOR SQLEXCEPTION ROLLBACK -- 检测到 SQLEXCEPTION 则 rollback，然后 exit START TRANSACTION INSERT INTO table_test VALUES(1, 'A') INSERT INTO table_test VALUES(1, 'B') -- 这里主键冲突，会触发 SQLEXCEPTION COMMIT END CALL my_test() 或者在 PL 中通过异常处理执行 ROLLBACK。（事务虽然中止了，但并未结束！所以仍然可以ROLLBACK 或者 COMMIT） ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:4:0","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#四dcl---事务处理mysql"},{"categories":["tech"],"content":" 数据何时被提交到数据库 显式提交：在事务中使用 COMMIT 提交数据操作被称为显式提交 隐式提交：非 DML 操作会被立即提交，也就是说这些语句本身就隐含了提交语义 自动提交： 如果 AUTOCOMMIT 被设置为 ON，当前 session 中的 DML 语句会在执行后被自动提交（START TRANSACTION 内部的 DML 除外，在它内部必须显式 COMMIT） 所有的 DML 语句都是要显式提交的，MySQL session 的 AUTOCOMMIT 默认为 ON，所以 DML 会被自动提交。 P.S. 许多语言的数据库 API 会定义自己的事务操作，不一定与这里一致。 ","date":"2018-06-15","objectID":"/posts/sql-basics-1/:4:1","series":null,"tags":["SQL","数据库","Database"],"title":"SQL 基础笔记（一）","uri":"/posts/sql-basics-1/#数据何时被提交到数据库"},{"categories":["tech"],"content":" 本文最初于 2018-05-25 发表在博客园，2022-08-13 搬迁至https://thiscute.world ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:0","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#"},{"categories":["tech"],"content":" 0. 话说在前头最新版使用了画布方式实现，和本文相比改动非常大，如果对旧版本的实现没啥兴趣，可以直接移步video2chars，它的效果动画见 极乐净土。新版本的核心代码不算注释70行不到，功能更强大。 下面的效果动画是使用 html 实现的字符动画效果（上一篇的效果动画是 shell 版的）： There should have been a video here but your browser does not seem to support it. 本文的优化仍然是针对 shell 版本的，html 版由于缺陷太大就不写文章介绍了。 ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:1","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#0-话说在前头"},{"categories":["tech"],"content":" 1. 速度优化要是每次播放都要等个一分钟，也太痛苦了一点。所以可以用 pickle 模块把 video_chars 保存下来，下次播放时，如果发现当前目录下有这个保存下来的数据，就跳过转换，直接播放了。这样就快多了。只需要改一下测试代码，先在开头添加两个依赖 python import os import pickle 然后在文件结尾添加代码： python def dump(obj, file_name): \"\"\" 将指定对象，以file_name为名，保存到本地 \"\"\" with open(file_name, 'wb') as f: pickle.dump(obj, f) return def load(filename): \"\"\" 从当前文件夹的指定文件中load对象 \"\"\" with open(filename, 'rb') as f: return pickle.load(f) def get_file_name(file_path): \"\"\" 从文件路径中提取出不带拓展名的文件名 \"\"\" # 从文件路径获取文件名 _name path, file_name_with_extension = os.path.split(file_path) # 拿到文件名前缀 file_name, file_extension = os.path.splitext(file_name_with_extension) return file_name def has_file(path, file_name): \"\"\" 判断指定目录下，是否存在某文件 \"\"\" return file_name in os.listdir(path) def get_video_chars(video_path, size): \"\"\" 返回视频对应的字符视频 \"\"\" video_dump = get_file_name(video_path) + \".pickle\" # 如果 video_dump 已经存在于当前文件夹，就可以直接读取进来了 if has_file(\".\", video_dump): print(\"发现该视频的转换缓存，直接读取\") video_chars = load(video_dump) else: print(\"未发现缓存，开始字符视频转换\") print(\"开始逐帧读取\") # 视频转字符动画 imgs = video2imgs(video_path, size) print(\"视频已全部转换到图像， 开始逐帧转换为字符画\") video_chars = imgs2chars(imgs) print(\"转换完成，开始缓存结果\") # 把转换结果保存下来 dump(video_chars, video_dump) print(\"缓存完毕\") return video_chars if __name__ == \"__main__\": # 宽，高 size = (64, 48) # 视频路径，换成你自己的 video_path = \"BadApple.mp4\" video_chars = get_video_chars(video_path, size) play_video(video_chars) 另一个优化方法就是边转换边播放，就是同时执行上述三个步骤。学会了的话，可以自己实现一下试试。 ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:2","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#1-速度优化"},{"categories":["tech"],"content":" 2. 字符视频和音乐同时播放没有配乐的动画，虽然做出来了是很有成就感，但是你可能看上两遍就厌倦了。所以让我们来给它加上配乐。（不要担心，其实就只需要添加几行代码而已） 首先我们需要找个方法来播放视频的配乐，怎么做呢？先介绍一下一个跨平台视频播放器：mpv，它有很棒的命令行支持，请先安装好它。要让 mpv 只播放视频的音乐部分，只需要命令： shell mpv --no-video video_path 好了，现在有了音乐，可总不能还让人开俩shell，先放音乐，再放字符画吧。这时候，我们需要的功能是：使用 Python 调用外部应用. 但是 mpv 使用了类似 curses 的功能，标准库的 os.system 不能隐藏掉这个部分，播放效果不尽如人意。因此我使用了 pyinvoke 模块，只要给它指定参数hide=True，就可以完美隐藏掉被调用程序的输出（指 stdout，其实 subprocess 也可以的）。运行下面代码前，请先用pip安装好 invoke.（能够看到这里的，安装个模块还不是小菜一碟） 好了废话说这么多，上代码： python import invoke video_path = \"BadApple.mp4\" invoke.run(f\"mpv --no-video {video_path}\", hide=True, warn=True) 运行上面的测试代码，如果听到了音乐，而shell啥都没输出，但是能听到音乐的话，就正常了。我们继续。（这里使用了python3.6的f字符串） 音乐已经有了，那就好办了。添加一个播放音乐的函数 python import invoke def play_audio(video_path): invoke.run(f\"mpv --no-video {video_path}\", hide=True, warn=True) 然后修改main()方法： python def main(): # 宽，高 size = (64, 48) # 视频路径，换成你自己的 video_path = \"BadApple.mp4\" # 只转换三十秒，这个属性是才添加的，但是上一篇的代码没有更新。你可能需要先上github看看最新的代码。其实就稍微改了一点。 seconds = 30 # 这里的fps是帧率，也就是每秒钟播放的的字符画数。用于和音乐同步。这个更新也没写进上一篇，请上github看看新代码。 video_chars, fps = get_video_chars(video_path, size, seconds) # 播放音轨 play_audio(video_path) # 播放视频 play_video(video_chars, fps) if __name__ == \"__main__\": main() 然后运行。。并不是我坑你，你只听到了声音，却没看到字符画。。原因是： invoke.run()函数是阻塞的，音乐没放完，代码就到不了play_video(video_chars, fps)这一行。 所以 play_audio 还要改一下，改成这样： python import invoke from threading import Thread def play_audio(video_path): def call(): invoke.run(f\"mpv --no-video {video_path}\", hide=True, warn=True) # 这里创建子线程来执行音乐播放指令，因为 invoke.run() 是一个阻塞的方法，要同时播放字符画和音乐的话，就要用多线程/进程。 # P.S. 更新：现在发现可以用 subprocess.Popen 实现异步调用 mpv，不需要开新线程。有兴趣的同学可以自己试试。 p = Thread(target=call) p.setDaemon(True) p.start() 这里使用标准库的 threading.Thread 类来创建子线程，让音乐的播放在子线程里执行，然后字符动画还是主线程执行，Ok，这就可以看到最终效果了。实际上只添加了十多行代码而已。 ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:3","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#2-字符视频和音乐同时播放"},{"categories":["tech"],"content":" 3. 彩色字符动画 html+javascript 方式：核心都是一样的内容，只是需要点 html 和 javascript 的知识。代码见video2chars-html 画布方式：直接把画在图片上，然后自动合成为 mp4 文件。这种方式要优于 html 方式，而且有个很方便的库能用，核心代码就 70 行的样子。代码见video2chars ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:4","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#3-彩色字符动画"},{"categories":["tech"],"content":" 参考 Python将视频转换为全字符视频（含音频） ","date":"2018-05-25","objectID":"/posts/video2chars-2/:0:5","series":null,"tags":[],"title":"Python 视频转字符动画（二）进阶","uri":"/posts/video2chars-2/#参考"},{"categories":["life"],"content":"对我而言，学英语是一件挺痛苦的事。从初中开始学英文，可从来不觉得它有趣，主动性也就不强。 直到我开始学计算机，我开始认识到英文是不可避免的。于是尝试了很多方法。 最普遍的方式：背单词，可我从初中背到现在，背单词的计划从没哪次坚持超过一个月的。 后来听说看英文原版书有效，信心满满，结果也是看了一星期 Harry Potter，稍微看了点 Animal Farm，就没后续了。 又想练听力，开始听 ESLPod、EnglishPod，也就听了一个月的时间。。 之后又跟着练习英语口语的办法 - 恶魔奶爸的规划练口语，听完了《赖世雄美语音标》，又看了点美国的口语纠正视频。这件事干得倒还算可以，口语的确标准了不少。不过也就花了十多天，就没然后了。 我学计算机的过程，和我的英文学习过程也有不少重合的地方。 印象中第一次读英文资料，是想学计算机图形学，被知乎上的高手们推荐看一个英文教程。死嗑了三天，坚持不下去放弃了。。 之后听了季索清学长的推荐，又见知乎上也都说 Python 好，就开始学 Python。看了 A Byte of Python, 印象中花了一天看完的，但是没在脑子里留下啥印象。 后来慢慢的开始熟悉 Python，在图书馆借了 Head First Python 英文版，可能是被厚度吓到了，看了几页就不了了之了。。 再后来用 Github，Pycharm IDEA 也是英文的，Python Docs 和 Java Docs 也全是英文的，标准库里的注释是英文的，Error 信息是英文的…… 虽然学啥都半途而废，但英文水平的确是慢慢地提升着。 慢慢地，能够不怎么吃力地看懂 Python 标准库了，有问题也可以看英文博客解决了。 最近看一个动漫看完不过瘾，转去看这部动漫的轻小说。轻小说中文翻译的太呆板，发现居然有英文的，直接啃起了英文。 换了好几部小说，现在在看Grimgar of Fantasy and Ash、Tasogare-iro no Uta Tsukai(黄昏色の詠使い)， 还找了个英文动漫网站，颇有以后看动漫也只看英文字幕的打算。希望能持续下去吧。 不过也有点苦恼，因为暑假就要找工作，现在却沉迷看英文小说。。本来这个月该学算法的，可半个月都过了，我的进度大概才认真的时候的五天的样子。。真不知道未来会是啥样。 ","date":"2018-05-16","objectID":"/posts/learning-english/:0:0","series":null,"tags":["英语","语言学习"],"title":"学英语啊学英语","uri":"/posts/learning-english/#"},{"categories":["书藉","影视","life"],"content":"看完了动画，也看了点小说。最敬佩、最喜欢、最向往的人物是珠晶，也就是供王。能感觉得到她是所有角色中，最自信、方向最明确的，而且她思考一直比较理性。身为富商之女，年仅十二，却能拥有超出所有国民的觉悟，“既然大人们没有勇气，那就由我去当王！”，并最终称王，不得不敬佩。她有句让人难以忘却的台词，“我之所以能过着比别人更好的生活，是因为我担负了相比更沉重的责任。如果没能完成相应的使命，我就会像峰王一样被砍掉脑袋。而祥琼没有认识到这一点，她不想担负责任，却觉得自己应该享受荣华富贵。” 其次就是“专职心理治疗”的乐俊小老鼠了，我简直有点怀疑存不存在这样可敬的老鼠(废话)。乐俊成熟得不适合当主角，几乎无可挑剔，大概也因此而戏份不多。 而花了大篇幅描写的庆东国的景王，还有一路走来的祥琼和铃、更夜还有泰麒，他们一度迷失掉了自我，虽然作为结果的他们实现了自我救赎，但是这个过程我喜欢不起来。大概因为我也是个偏激的人吧…… 珠晶遇到了顽丘，景王和祥琼被乐俊救赎，铃也有自己的贵人，更夜在斡由被杀时终于承认了自己的错误，泰麒也是被麒麟们合力救回来的。 谁都不可能只活在自己的世界，就能得到救赎。(这样就又得到了一个和刺猬的优雅类似的结论…) ","date":"2018-04-27","objectID":"/posts/the-twelve-kingdoms/:0:0","series":null,"tags":["读后感","观后感","动漫"],"title":"《十二国记》","uri":"/posts/the-twelve-kingdoms/#"},{"categories":["tech"],"content":" 一、charAt 与 codePointAt我们知道 Java 内部使用的是 utf-16 作为它的 char、String 的字符编码方式，这里我们叫它内部字符集。而 utf-16 是变长编码，一个字符的编码被称为一个 code point，它可能是 16 位 —— 一个 code unit，也可能是 32 位 —— 两个 code unit。Java 的 char 类型长度为二字节，它对应的是 code unit。换句话说，一个字符的编码，可能需要用两个 char 来存储。 作为一个输入法爱好者，我偶尔会编程处理一些生僻字。其中有些生僻字大概是后来才加入 unicode 字符集里的，直接用 charAt 方法读取它们，会得到一堆问号。原因很清楚 —— 因为这些字符（eg.“𫖯”）是用两个 code unit，也就是两个 char 表示的。charAt 找不到对应的编码，就会将这些 char 输出成「?」。 java //示例 public class Test { public static void main(String[] args){ String s = \"𫖯\"; System.out.println(s.length()); //输出：2 System.out.println(s.charAt(0)); //输出：? System.out.println(s.charAt(1)); //输出：? } } 因此，涉及到中文，一定要使用 String 而不是 char，并且使用 codePoint 相关方法来处理它。否则的话，如果用户使用了生僻字，很可能就会得到不想要的结果。 下面是一个使用 codePoint 遍历一个字符串的示例，需要注意的是，codePoint 是 int 类型的（因为 char 不足以保存一个 codepoint），因此需要做些额外的转换： java public class Test { public static void main(String[] args){ String s = \"赵孟𫖯孟\"; for (int i = 0; i \u003c s.codePointCount(0,s.length()); i++) { System.out.println( new String(Character.toChars(s.codePointAt(i)))); // 这里的轨迹是：类型为 int 的 codepoint -\u003e char数组 -\u003e String } } } /* 结果： 赵 孟 𫖯 ? */ 问题来了，「𫖯」这个字是正常地输出了，可最后的「孟」却变成了黑人问号。。原因就在于 codepointAt(i) 是以 char 偏移量索引的。。所以只是这样输出也是不行的。。 正确的遍历姿势是这样的 java final int length = s.length(); for (int offset = 0; offset \u003c length; ) { final int codepoint = s.codePointAt(offset); System.out.println(new String(Character.toChars(codepoint))); offset += Character.charCount(codepoint); } 这个代码保持了一个变量offset, 来指示下一个 codepoint 的偏移量。最后那一句在处理完毕后，更新这个偏移量 而 Java 8 添加了CharSequence#codePoints， 该方法返回一个 IntStream，该流包含所有的 codepoint。可以直接通过 forEach 方法来遍历他。 java string.codePoints().forEach( c -\u003e System.out.println(new String(Character.toChars(c))); ); 或者用循环 java for(int c : string.codePoints().toArray()){ System.out.println(new String(Character.toChars(c))); } ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:1","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#一charat-与-codepointat"},{"categories":["tech"],"content":" 二、内部字符集与输出字符集（内码与外码）现在我们知道了中文字符在 java 内部可能会保存成两个 char，可还有个问题：如果我把一个字符输出到某个流，它还会是两个 char，也就是 4 字节么？回想一下，Java io 有字符流，字符流使用 jvm 默认的字符集输出，而若要指定字符集，可使用转换流。因此，一个中文字符，在内部是使用 utf-16 表示，可输出就不一定了。来看个示例： java import java.io.UnsupportedEncodingException; public class Test { public static void main(String[] args) throws UnsupportedEncodingException { String s = \"中\"; //𫖯 System.out.println(s + \": chars: \" + s.length()); System.out.println(s + \": utf-8 bytes:\" + s.getBytes(\"utf-8\").length); System.out.println(s + \": unicode bytes: \" + s.getBytes(\"unicode\").length); System.out.println(s + \": utf-16 bytes: \" + s.getBytes(\"utf-16\").length); } } 输出为： text 中: chars: 1 // 2 bytes 中: utf-8 bytes:3 中: unicode bytes: 4 中: utf-16 bytes: 4 𫖯: chars: 2 // 4 bytes 𫖯: utf-8 bytes:4 𫖯: unicode bytes: 6 𫖯: utf-16 bytes: 6 一个「中」字，内部存储只用了一个 char，也就是 2 个字节。可转换成 utf-8 编码后，却用了 3 个字节。怎么会不一样呢，是不是程序出了问题？当然不是程序的问题，这是内码(utf-16)转换成外码 (utf-8)，字符集发生了改变，所使用的字节数自然也可能会改变。（尤其这俩字符集还都是变长编码） ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:2","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#二内部字符集与输出字符集内码与外码"},{"categories":["tech"],"content":" 三、utf-16、utf-16le、utf-16be、bom不知道在刚刚的示例中，你有没有发现问题：同是 utf-16，为何「中」和「𫖯」的s.getBytes(\"utf-16\").length 比 s.length 要多个 2？开头就说了 String 也是 utf-16 编码的，这两个数应该相等才对不是吗？原因在于，utf-16 以 16 位为单位表示数据，而计算机是以字节为基本单位来存储/读取数据的。因此一个 utf-16 的 code unit 会被存储为两个字节，需要明确指明这两个字节的先后顺序，计算机才能正确地找出它对应的字符。而 utf-16 本身并没有指定这些，所以它会在字符串开头插入一个两字节的数据，来存储这些信息（大端还是小端）。这两个字节被称为 BOM（Byte Order Mark）。刚刚发现的多出的两字节就是这么来的。如果你指定编码为 utf-16le 或 utf-16be，就不会有这个 BOM 的存在了。这时就需要你自己记住该文件的大小端。。 ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:3","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#三utf-16utf-16leutf-16bebom"},{"categories":["tech"],"content":" 四、更多：utf-8 unicode 在 windows 中，utf-8 格式的文件也可能会带有 BOM，但 utf-8 的基本单位本来就是一个字节， 因此它不需要 BOM 来表示 所谓大小端。这个 BOM 一般是用来表示该文件是一个 utf-8 文件。不过 linux 系统则对这种带 BOM 的文件不太友好。不般不建议加。。（虽如此说，上面的测试中，utf-8 的数据应该是没加 bom 的结果） unicode字符集UCS（Unicode Character Set） 就是一张包含全世界所有文字的一个编码表，但是 UCS 太占内存了，所以实际使用基本都是使用它的其他变体。一般来说，指定字符集时使用的 unicode 基本等同于 utf-16.（所以你会发现第二节演示的小程序里，utf-16 和 unicode 得出的结果是一样的。） ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:4","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#四更多utf-8-unicode"},{"categories":["tech"],"content":" 四、与 Python3 对比python3 在字符串表示上，做了大刀阔斧的改革，python3 的 len(str) 得到的就是 unicode 字符数，因此程序员完全不需要去考虑字符的底层表示的问题。（实际上其内部表示也可能随着更新而变化）带 BOM 的 utf-8 也可通过指定字符集为 utf-8-sig 解决。若需要做字符集层面处理，需要 encode 为特定字符集的 byte 类型。 Encoding pertains mostly to files and transfers. Once loaded into a Python string, text in memory has no notion of an “encoding,” and is simply a sequence of Unicode characters (a.k.a. code points) stored generically. -- Learning Python 5th P.S. Python2 存在和 Java 相同的问题 ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:5","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#四与-python3-对比"},{"categories":["tech"],"content":" 参考 java 语言中的一个字符占几个字节？ - RednaxelaFX - 知乎 How can I iterate through the unicode codepoints of a Java String? 彻底搞懂字符编码(unicode,mbcs,utf-8,utf-16,utf-32,big endian,little endian…) Java_字符编码 本文允许转载，但要求附上源链接：Java 中文编码分析 ","date":"2018-03-11","objectID":"/posts/how-java-handles-chinese/:0:6","series":null,"tags":["Java","字符集","字符编码"],"title":"Java 中文编码分析","uri":"/posts/how-java-handles-chinese/#参考"},{"categories":["tech"],"content":" 个人笔记，不保证正确。 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:0","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#"},{"categories":["tech"],"content":" 一、进程 Process：（并行运算，分布式）每一个进程，都可以看作是一个完整的 Program，它有自己完全独立的内容。不与其他进程直接共享数据。（一个工作(job)可以由多个 process 完成，例如电脑上的qq/360就会有好几个进程，这种程序可能会有一个守护进程，当主进程挂掉，它会自动重启主进程。） 每个进程可以由多个线程组成。进程抽象由操作系统提供，Linux 是使用 fork 函数，Windows 是用 CreateProcess。 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:1","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#一进程-process并行运算分布式"},{"categories":["tech"],"content":" 二、线程 Thread：（并发执行）属于同一个进程的线程之间，是共享一套工作内容的。这使得线程的创建和移除开销很小，但同时也使编程变得复杂。 关于线程，分用户级线程和内核级线程。不同的语言中，这两种线程的对应关系也不尽相同。 多对一模型将多个用户级线程映射到一个内核级线程，线程管理在用户空间完成，这种模型下操作系统并不知道多线程的存在。Python 就是这种模型。 优点：线程管理是在用户空间进行的，切换上下文开销比较小，性能较高。 缺点：当一个线程在使用内核服务时被阻塞，那么整个进程都会被阻塞；多个线程不能并行地运行在多处理机上。 一对一模型将每个用户级线程映射到一个内核级线程。Java的线程就属于这种模型。 优点：当一个线程被阻塞后，允许另一个线程继续执行，所以并发能力较强；能很好的利用到 CPU的多核心。 缺点：每创建一个用户级线程都需要创建一个内核级线程与其对应，这样创建线程的开销比较大，会影响到应用程序的性能。并且切换线程要进出内核，代价比较大。 多对多模型将n个用户级线程映射到m个内核级线程上，要求 m \u003c= n。GO（1.5之后）的协程就属于这种线程模型。 特点：既克服了多对一模型的并发度不高的缺点，又克服了一对一模型的一个用户进程占用太多内核级线程，开销太大的缺点。又拥有多对一模型和一对一模型各自的优点。 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:2","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#二线程-thread并发执行"},{"categories":["tech"],"content":" 三、协程 Coroutine（并发执行）如果说线程是轻量级的进程，那么协程就是轻量级的线程。线程跑在进程里，协程就跑在线程里。 优点： 协程是跑在同一个线程里，并且是由程序本身来调度的。协程间的切换就是函数的调用，完全没有线程切换那么大的开销。 线程的数量越多，协程的优势越大 因为协程是程序调度的，它实际上是串行运行的，因此不需要复杂的锁机制来保证线程安全。 在协程中控制共享资源不加锁，只需要判断状态就好了。这免去了锁机制带来的开销。 因为协程跑在单个线程内，所占用的 CPU 资源有限，所以多协程并不能提升计算性能。不仅如此，因为多了程序本身的调度开销，计算密集型程序的性能反而会下降。 此外，协程代码中决不能出现阻塞，否则整个线程都会停下来等待该操作完成，这就麻烦了。 协程适合用于 IO 密集型任务，可用于简化异步 IO 的 callback hell。例如 Python 的 asyncio 就是用协程实现的。 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:3","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#三协程-coroutine并发执行"},{"categories":["tech"],"content":" 并发并行由此，又引出两个名词： 并发（Concurrent）：多个任务交替进行。 并行（Parallel）：多个任务同时进行。 一张图说明两者的差别 Note：进程 和 线程 都可能是 并发 或 并行 的。关键看你程序的运行状态。多核是并行的前提。并发则只要求交替执行，因此单核也没问题。 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:4","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#并发并行"},{"categories":["tech"],"content":" 同步异步 同步：不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以协调一致，称这些程序单元是同步执行的。 多线程编程中，所有修改共享变量的行为，都必须加锁，保证顺序执行，保证同步。或者加原子锁，保证该修改操作是原子的。 同步意味着有序 异步：为完成某个任务，不同程序单元之间过程中无需通信协调，也能完成任务的方式。 不相关的程序单元之间可以是异步的。比如爬虫下载网页 异步意味着无序 进程、线程和协程 ","date":"2018-01-23","objectID":"/posts/process-thread-coroutines-concurrency-parallelism/:0:5","series":null,"tags":["进程","线程","协程","并发","并行","Coroutines","Concurrency"],"title":"进程线程协程与并发并行","uri":"/posts/process-thread-coroutines-concurrency-parallelism/#同步异步"},{"categories":["书藉","life"],"content":"店小二杀了巡界使，然后离了客栈，入了道德宗，应了一错缘，又给道德宗干了一堆破事，活的浑浑噩噩。 受了一剑斩缘后，本以为终于能求得解脱，谁知造化弄人，自己竟又从地府回得人界。 本一心复仇，临到头还是没对吟风下得杀手。 欲与顾清再续缘，却被说一句看不穿。 再次醒来，本欲杀上天界，可看到青衣时，猛然醒悟。 什么王图霸业，什么诸界称雄，什么夙世情仇，在这一刻，皆化浮云。眼中便只有这一世尘缘了。 青石与巡界使，被命运捉弄，被贬下凡间受百世轮回，九十九世相濡以沫，最后一世却横生波折。 终于回得仙界，相对大笑三声，相忘于江湖。 ","date":"2017-11-18","objectID":"/posts/fate-of-mortals/:0:0","series":null,"tags":["读后感","小说"],"title":"《尘缘》","uri":"/posts/fate-of-mortals/#"},{"categories":["life"],"content":" 啊啊，还有十天就可以摆脱这个城市，回到那个令人安心的山林里了，一边期待着，一边焦躁着，想着为什么剩下的十天这么难熬这样的问题。 复习又是一塌糊涂，我也太懒了点。 这样懒散的我还做着码完几千行代码这样的春秋大梦，太不现实了。有点想认命了。 半夜一点多，寝室空调还是不习惯，过道阳台上的凉风倒是很舒服，这座城市此刻的静谧倒也有几分韵味。 不过不管怎么说，好想回家… ","date":"2017-06-27","objectID":"/posts/the-end-of-another-semester/:0:0","series":null,"tags":[],"title":"又一个期末","uri":"/posts/the-end-of-another-semester/#"},{"categories":["影视","life"],"content":"即使大人的世界也千疮百孔，但是要是让我们自己来做的话，只能做的更差？所以你看船里的小社会是什么样子的吧，嫉妒、欺骗、恐惧、自暴自弃、愤怒、自私……各种欲望。于是阶级分化、工作效率低下，有人开始求助于神，暴动也时不时的发生。 舰长害怕有人死去，害怕承担责任，不敢冒险，喜欢上强势的阿岚，心心念念那个人，把事情搞的一团糟。祐希心里有话不讲，一味的揍人泄愤。伊克米恐惧死亡，恐惧得不像样。梢有点自我中心，觉得伊克米是她一个人的，只为了她而努力。阿岚很有当舰长的能力，但是一心复仇，除了这个其他的都不怎么管。男主忍受着各种非议和殴打，但是坚守本心。不管局势如何变化，他一直在做着正确的事，虽然有时候正确的事未必是最好的…… 人在坏掉。 大家都在自欺欺人吗？ 真的可以抛弃过去活下去吗？ 如果不可以，那为了活着，背负过去是必要的？ 最后反转剧情，靠的还是一个「情」字。也对，科技发展的动力也是人的欲望。 欲望有好有坏，理智的把握它。无欲无求，就是死。 ","date":"2017-06-14","objectID":"/posts/infinite-ryvius/:0:0","series":null,"tags":["观后感","动漫"],"title":"《无限的未知》","uri":"/posts/infinite-ryvius/#"},{"categories":["数学","tech"],"content":"很早就学过欧几里得算法，但是一直不知道它的原理。几乎每本算法书都会提到它，但是貌似只有数学书上才会见到它的原理。。。 前段时间粗粗看了点数论（《什么是数学》），惊讶于这个原理的奇妙。现在把它通俗地写下来，以免自己忘记。 欧几里得算法是求两个数的最大公约数(Greatest Common Divisor (GCD))的算法，我们首先假设有两个数 $a$ 和 $b$，其中 $a$ 是不小于 $b$ 的数， 记 $a$ 被 $b$ 除的余数为 $r$，那么 $a$ 可以写成这样的形式： $$a = bq + r$$ 其中 $q$ 是整数（我们不需要去管 $q$ 到底是多少，这和我们的目标无关）。 现在假设 $a$ 和 $b$ 的一个约数为 $u$，那么 $a$ 和 $b$ 都能被 $u$ 整除，即 $$a = su$$ $$b = tu$$ $s$ 和 $t$ 都是整数（同样的，我们只需要知道存在这样的整数 $s$ 和 $t$ 就行）。 这样可以得出 $$r = a - bq = su - (tu)q = (s - tq)u$$ 所以 $r$ 也能被 $u$ 整除，一般规律如下 $a$ 和 $b$ 的约数也整除它们的余数 $r$，所以 $a$ 和 $b$ 的任一约数同时也是 $b$ 和 $r$ 的约数。 —— 条件一 反过来可以得出 $b$ 和 $r$ 的任一约数同时也是 $a$ 和 $b$ 的约数。 ——条件二 这是因为对 $b$ 和 $r$ 每一个约数 $v$，有 $$b = kv$$ $$r = cv$$ 于是有 $$a = bq + r = (kv)q + cv = (kq + c)v$$ 由条件一和条件二可知 $a$ 和 $b$ 的约数的集合，全等于 $b$ 和 $r$ 的约数的集合。 于是 $a$ 和 $b$ 的最大公约数，就是 $b$ 和 $r$ 的最大公约数。 接下来用递推法， $a \\div b$ 余 $r$，现在设 $b \\div r$ 余 $r_1$ $r \\div r_1$ 余 $r_2$ …… $r_{n-3} \\div r_{n-2}$ 余 $r_{n-1}$ $r_{n-2} \\div r_{n-1}$ 余 $r_n=0$ 因为 $a \\ge b$，可以看出余数 $r_n$ 会越来越小，最终变成 $0$. 当 $r_{n-1} \\neq 0$ 且 $r_n = 0$ 时，可知 $r_{n-2}$ 可被 $r_{n-1}$ 整除（余数为 $0$ 嘛） 此时 $r_{n-2}$ 和 $r_{n-1}$ 的约数就只有：$r_{n-1}$ 和 $r_{n-1}$ 的因数，所以他们的最大公约数就是 $r_{n-1}$！ 所以 $r_{n-1}$ 就是 $a$ 和 $b$ 的最大公约数。（若 $r = 0$，则 $b$ 为最大公约数） 这个递推法写成c语言函数是这样的（比推导更简洁…）: c unsigned int Gcd(unsigned int M,unsigned int N){ unsigned int Rem; while(N){ Rem = M % N; M = N; N = Rem; } return Rem; } 可以发现这里没有要求 M\u003e=N，这是因为如果那样，循环会自动交换它们的值。 P.S. 此外，还有最小公倍数(Least Common Multiple (LCM))算法，详见GCD and LCM calculator ","date":"2017-05-26","objectID":"/posts/mathematics-in-euclidean-gcd/:0:0","series":null,"tags":["算法"],"title":"欧几里得算法求最大公约数(GCD)的数学原理","uri":"/posts/mathematics-in-euclidean-gcd/#"},{"categories":["life"],"content":"生活总是在给你希望之时，再埋点伏笔。本来我以为进了大学，就是一个全新的世界了，我可以重新开始，只要我很努力很努力，一切困难都将不堪一击。 显然那个时候，我还不知道，现实不同于想象。 高三在高压下全线崩溃，因此对大学寄予了过多期望。但这期望同时也带来了更大的压力。 我患上了阅读焦虑症。 从进入大学的那一刻起，就开始疯狂地制定阅读计划，泡图书馆，看各种学习方法、读书方法、记忆方法、速读术之类的书籍，恨不得一目十行。 但是很快的，我就发现自己出了问题：我太想提升自己了，因此翻开书的第一页，就期盼着翻到最后一页，读书的愉悦，被对看完一本书的渴望冲淡了。更多的时候，感觉到的是还没把这本书看完的焦虑。 而且因为长时间全神贯注，一本书看不到一半，耐心也渐渐失去，于是翻页速度越来越快，这个时候所谓的“阅读”已经名存实亡了。 这样的阅读的结果，只是在读书量上徒然添加几个数字，于自我提升而言，却是收效甚微。我很明白这一点，但是明白和作出改变之间，隔着一道鸿沟，我怎么也跨不过去。明明知道松弛有度效率会更高， 但是心理上的焦虑让我无法说服自己放下书本哪怕一分钟，直到自己的耐心消耗殆尽…… 买了一大堆文学书放在柜头。可笑的是，大一整整一年，除了韩寒，我没看任何一本文学书超过半小时。“快速浏览”完十几本方法类书籍后，我开始阅读技术书籍。但是除了韩寒的书和几本技术书籍，阅读过程中的焦虑感从未远离我，这不仅降低了我的学习效率，更让我的倦怠期长了数倍(过度消耗精力)。其结果是，往往一本厚一点的书读上两三天，就有半个月会厌倦到不想碰它。 我能感觉到如果按着计划读书，我的成果绝不会差到现在这样。也想着有计划性一点，可是一看QQ，人家初三的小男孩已经学遍了高中数学、算法、初等数论、自然数学……网上认识的同龄人已经开始做神经网络了，知乎上一大群自学者也在努力攻克python/c/算法，我就停不下来，甚至平静下来做个计划都觉得浪费时间(实际上很明显这样带着焦虑阅读才是浪费时间)。 迫切的想要成为那个“自己想要成为的人”，因此连基本的理性都无法保持。 我想要的是从容、带着脑子的阅读，而不是这样走马观花，盲目追求量的阅读。 我又焦虑地打开知乎，不断搜索，然后写下这篇文章。 ","date":"2017-03-07","objectID":"/posts/reading-anxiety/:0:0","series":null,"tags":["阅读","焦虑"],"title":"我患上了阅读焦虑症","uri":"/posts/reading-anxiety/#"},{"categories":["life"],"content":" 2017年2月的18号，清晨6点。天还只是朦朦亮，当空挂着半边弯月，一颗不知名的星星(大约是大角星) 缀在月的旁边。还没开学，学校几乎看不到人。 南食堂的一楼已亮起了灯，鸟儿们开始鸣叫个不停，可以听出有好几种鸟叫声。 易海仍是风平浪静。 我背着书包，拖着皮箱，耳边最清晰的声音便是皮箱轮胎与地面的摩擦声。 手机随便放起一首歌，恰好是《遥远的歌》。这首歌真是应景呢，逝去的时光遥远得无法触及，自己也离家千里，未来更是难以捉摸。 我还会记得吗？记得这个我印象中，最宁静安详的，安徽建筑大学。 ","date":"2017-02-18","objectID":"/posts/quiet-and-peaceful-campus/:0:0","series":null,"tags":[],"title":"少有人迹的校园","uri":"/posts/quiet-and-peaceful-campus/#"},{"categories":["life"],"content":" 上个暑假，刚刚从低谷爬出来，那时候整个人散发着一股子向上的气息，豪情万丈，甚至感染了周围的亲朋好友。那个时候，满以为以后的挫折都不能阻挡我的脚步。 可是，到底为什么，现在又变成了这个样子了呢？人生这样的东西，总是出人意料，以至于怎么也猜不透。 十二月十四，我度过了我的十九岁生日，现在应该正是那所谓的青春将逝未逝之时。而我的青春应该最辉煌的时候，我在干什么呢？ 我在翻山越岭。 上山时一路坎坷，累的要死不活的。陡然间萌生退意，心就在不断挣扎。就在却意战胜壮志之时，忽然间天地开阔，才发觉自己已然站在了大山之巅，于是一切痛苦尽皆远去，心也变得如这天地一般开阔。这个时候自然豪情万丈，看山山美，看水水秀。想当然的就觉得后面的山岭有再多的阻碍，也不能阻挡这个见过如此美景的登山人了。 可是事与愿违，山岭就像时间一样看不到边，翻过了一座又是一座，这又是一种更大的痛苦。这个登山人身心俱疲，只好万事随缘，继续一脚深，一脚浅的往那无尽山岭行去。 最近看了是枝裕和的电影《比海更深》，“我的人生到底出了什么差错？” 这样一个问句，道出了多少辛酸苦辣……想起了以前写过一篇文章，标题是《对不起，我没有成为你想成为的那个人》。 理想与现实之间仿佛总隔着一道鸿沟。 现在没有了万丈豪情，不再敢说“未来将是一片坦途”；也没有绝望到要写“我的人生到底出了什么差错？”这样的句子，那还是用我最喜欢的那个模棱两可的四字词作结吧。 且行且寻 ","date":"2017-02-06","objectID":"/posts/the-holiday-is-coming-to-an-end/:0:0","series":null,"tags":[],"title":"忽而假末","uri":"/posts/the-holiday-is-coming-to-an-end/#"},{"categories":["tech"],"content":" 本文最初于 2016-10-18 发表在博客园，2022-08-13 搬迁至https://thiscute.world 昨晚我朋友 @三十六咲 跟我说在网上看到了别人做的视频转字符动画，觉得很厉害，我于是也打算玩玩。今天中午花时间实现了这样一个小玩意。顺便把过程记录在这里。 注：最新版使用了画布方式实现，和本文相比改动非常大，如果对旧版本的实现没啥兴趣，可以直接移步video2chars， 它的效果动画见 极乐净土。新版本的核心代码不算注释 70 行不到，功能更强大。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:0","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#"},{"categories":["tech"],"content":" 效果先上效果，来点动力。 源视频 【東方】Bad Apple!! ＰＶ【影絵】 转换后的效果如下： There should have been a video here but your browser does not seem to support it. ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:1","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#效果"},{"categories":["tech"],"content":" 步骤 将视频转化为一帧一帧的图片 把图片转化为字符画 按顺序播放字符画 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:2","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#步骤"},{"categories":["tech"],"content":" 一、准备 1. 模块这个程序需要用到这样几个模块: opencv-python # 用来读取视频和图片 numpy # opencv-python 依赖于它 准备阶段，首先安装依赖： shell pip3 install numpy opencv-python 然后新建 python 代码文档，在开头添加上下面的导入语句 python #-*- coding:utf-8 -*- # numpy 是一个矩阵运算库，图像处理需要用到。 import numpy as np 2. 材料材料就是需要转换的视频文件了，我这里用的是BadApple.mp4， 下载下来和代码放到同一目录下你也可以换成自己的，建议是学习时尽量选个短一点的视频，几十秒就行了，不然调试起来很痛苦。（或者自己稍微修改一下函数，只转换一定范围、一定数量的帧。）此外，要选择对比度高的视频。否则的话，就需要彩色字符才能有足够好的表现，有时间我试试。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:3","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#一准备"},{"categories":["tech"],"content":" 一、准备 1. 模块这个程序需要用到这样几个模块: opencv-python # 用来读取视频和图片 numpy # opencv-python 依赖于它 准备阶段，首先安装依赖： shell pip3 install numpy opencv-python 然后新建 python 代码文档，在开头添加上下面的导入语句 python #-*- coding:utf-8 -*- # numpy 是一个矩阵运算库，图像处理需要用到。 import numpy as np 2. 材料材料就是需要转换的视频文件了，我这里用的是BadApple.mp4， 下载下来和代码放到同一目录下你也可以换成自己的，建议是学习时尽量选个短一点的视频，几十秒就行了，不然调试起来很痛苦。（或者自己稍微修改一下函数，只转换一定范围、一定数量的帧。）此外，要选择对比度高的视频。否则的话，就需要彩色字符才能有足够好的表现，有时间我试试。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:3","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#1-模块"},{"categories":["tech"],"content":" 一、准备 1. 模块这个程序需要用到这样几个模块: opencv-python # 用来读取视频和图片 numpy # opencv-python 依赖于它 准备阶段，首先安装依赖： shell pip3 install numpy opencv-python 然后新建 python 代码文档，在开头添加上下面的导入语句 python #-*- coding:utf-8 -*- # numpy 是一个矩阵运算库，图像处理需要用到。 import numpy as np 2. 材料材料就是需要转换的视频文件了，我这里用的是BadApple.mp4， 下载下来和代码放到同一目录下你也可以换成自己的，建议是学习时尽量选个短一点的视频，几十秒就行了，不然调试起来很痛苦。（或者自己稍微修改一下函数，只转换一定范围、一定数量的帧。）此外，要选择对比度高的视频。否则的话，就需要彩色字符才能有足够好的表现，有时间我试试。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:3","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#2-材料"},{"categories":["tech"],"content":" 二、按帧读取视频现在继续添加代码，实现第一步：按帧读取视频。下面这个函数，接受视频路径和字符视频的尺寸信息，返回一个 img 列表，其中的 img 是尺寸都为指定大小的灰度图。 python #导入 opencv import cv2 def video2imgs(video_name, size): \"\"\" :param video_name: 字符串, 视频文件的路径 :param size: 二元组，(宽, 高)，用于指定生成的字符画的尺寸 :return: 一个 img 对象的列表，img对象实际上就是 numpy.ndarray 数组 \"\"\" img_list = [] # 从指定文件创建一个VideoCapture对象 cap = cv2.VideoCapture(video_name) # 如果cap对象已经初始化完成了，就返回true，换句话说这是一个 while true 循环 while cap.isOpened(): # cap.read() 返回值介绍： # ret 表示是否读取到图像 # frame 为图像矩阵，类型为 numpy.ndarry. ret, frame = cap.read() if ret: # 转换成灰度图，也可不做这一步，转换成彩色字符视频。 gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # resize 图片，保证图片转换成字符画后，能完整地在命令行中显示。 img = cv2.resize(gray, size, interpolation=cv2.INTER_AREA) # 分帧保存转换结果 img_list.append(img) else: break # 结束时要释放空间 cap.release() return img_list 写完后可以写个 main 方法测试一下，像这样： python if __name__ == \"__main__\": imgs = video2imgs(\"BadApple.mp4\", (64, 48)) assert len(imgs) \u003e 10 如果运行没报错，就没问题代码里的注释应该写得很清晰了，继续下一步。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:4","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#二按帧读取视频"},{"categories":["tech"],"content":" 三、图像转化为字符画视频转换成了图像，这一步便是把图像转换成字符画下面这个函数，接受一个 img 对象为参数，返回对应的字符画。 python # 用于生成字符画的像素，越往后视觉上越明显。。这是我自己按感觉排的，你可以随意调整。 pixels = \" .,-'`:!1+*abcdefghijklmnopqrstuvwxyz\u003c\u003e()\\/{}[]?234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ%\u0026@#$\" def img2chars(img): \"\"\" :param img: numpy.ndarray, 图像矩阵 :return: 字符串的列表：图像对应的字符画，其每一行对应图像的一行像素 \"\"\" res = [] # 灰度是用8位表示的，最大值为255。 # 这里将灰度转换到0-1之间 # 使用 numpy 的逐元素除法加速，这里 numpy 会直接对 img 中的所有元素都除以 255 percents = img / 255 # 将灰度值进一步转换到 0 到 (len(pixels) - 1) 之间，这样就和 pixels 里的字符对应起来了 # 同样使用 numpy 的逐元素算法，然后使用 astype 将元素全部转换成 int 值。 indexes = (percents * (len(pixels) - 1)).astype(np.int) # 要注意这里的顺序和 之前的 size 刚好相反（numpy 的 shape 返回 (行数、列数)） height, width = img.shape for row in range(height): line = \"\" for col in range(width): index = indexes[row][col] # 添加字符像素（最后面加一个空格，是因为命令行有行距却没几乎有字符间距，用空格当间距） line += pixels[index] + \" \" res.append(line) return res 上面的函数只接受一帧为参数，一次只转换一帧，可我们需要的是转换所有的帧，所以就再把它包装一下： python def imgs2chars(imgs): video_chars = [] for img in imgs: video_chars.append(img2chars(img)) return video_chars 好了，现在我们可以测试一下： python if __name__ == \"__main__\": imgs = video2imgs(\"BadApple.mp4\", (64, 48)) video_chars = imgs2chars(imgs) assert len(video_chars) \u003e 10 没报错的话，就可以下一步了。(这一步比较慢，测试阶段建议用短一点的视频，或者稍微改一下，只处理前 30 秒之类的) ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:5","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#三图像转化为字符画"},{"categories":["tech"],"content":" 四、播放字符视频写了这么多代码，现在终于要出成果了。现在就是最激动人心的一步：播放字符画了。同样的，我把它封装成了一个函数。下面这个函数接受一个字符画的列表并播放。 通用版（使用 shell 的 clear 命令清屏，但是因为效率不高，可能会有一闪一闪的问题）这个版本适用于 linux/windows python # 导入需要的模块 import time import subprocess def play_video(video_chars): \"\"\" 播放字符视频 :param video_chars: 字符画的列表，每个元素为一帧 :return: None \"\"\" # 获取字符画的尺寸 width, height = len(video_chars[0][0]), len(video_chars[0]) for pic_i in range(len(video_chars)): # 显示 pic_i，即第i帧字符画 for line_i in range(height): # 将pic_i的第i行写入第i列。 print(video_chars[pic_i][line_i]) time.sleep(1 / 24) # 粗略地控制播放速度。 # 调用 shell 命令清屏 subprocess.run(\"clear\", shell=True) # linux 版 # subrpocess.run(\"cls\", shell=True) # cmd 版，windows 系统请用这一行。 Unix 系版本（使用了只支援 unix 系 的 curses 库，比 clear 更流畅） python # 导入需要的模块 import time import curses def play_video(video_chars): \"\"\" 播放字符视频， :param video_chars: 字符画的列表，每个元素为一帧 :return: None \"\"\" # 获取字符画的尺寸 width, height = len(video_chars[0][0]), len(video_chars[0]) # 初始化curses，这个是必须的，直接抄就行 stdscr = curses.initscr() curses.start_color() try: # 调整窗口大小，宽度最好略大于字符画宽度。另外注意curses的height和width的顺序 stdscr.resize(height, width * 2) for pic_i in range(len(video_chars)): # 显示 pic_i，即第i帧字符画 for line_i in range(height): # 将pic_i的第i行写入第i列。(line_i, 0)表示从第i行的开头开始写入。最后一个参数设置字符为白色 stdscr.addstr(line_i, 0, video_chars[pic_i][line_i], curses.COLOR_WHITE) stdscr.refresh() # 写入后需要refresh才会立即更新界面 time.sleep(1 / 24) # 粗略地控制播放速度(24帧/秒)。更精确的方式是使用游戏编程里，精灵的概念 finally: # curses 使用前要初始化，用完后无论有没有异常，都要关闭 curses.endwin() return 好，接下来就是见证奇迹的时刻 不过开始前要注意，字符画的播放必须在 shell 窗口下运行，在 pycharm 里运行会看到一堆无意义字符。另外播放前要先最大化 shell 窗口 python if __name__ == \"__main__\": imgs = video2imgs(\"BadApple.mp4\", (64, 48)) video_chars = imgs2chars(imgs) input(\"`转换完成！按enter键开始播放\") play_video(video_chars) 写完后，开个 shell，最大化窗口，然后键入（文件名换成你的） shell python3 video2chars.py 可能要等很久。我使用示例视频大概需要 12 秒左右。看到提示的时候，按回车，开始播放！ **这样就完成了视频到字符动画的转换, 除去注释, 大概七十行代码的样子. 稍微超出了点预期, 不过效果真是挺棒的. ** ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:6","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#四播放字符视频"},{"categories":["tech"],"content":" 五、进一步优化到了这里，核心功能基本都完成了。不过仔细想想，其实还有很多可以做的： 能不能手动指定要转换的区间、帧率？ 每次转换都要很久的时间，能不能边转换边播放？或者转换后把数据保存起来，下次播放时，就直接读缓存。 为啥我的字符动画没有声音，看无声电影么？ 视频的播放速度能不能精确控制？ 能不能用彩色字符？ 这些东西，就不写这里了，再写下去，你们肯定要说我这标题是骗人了哈哈。所以如果有兴趣的，请移步这个系列的下一篇：视频转字符动画（二）进阶 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:7","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#五进一步优化"},{"categories":["tech"],"content":" 六、总结完整代码见video2chars.py，要注意的是代码库的代码，包含了第二篇文章的内容（音频、缓存、帧率控制等），而且相对这篇文章也有一些小改动（目的是方便使用，但是稍微增加了点代码量，所以改动没有写在这篇文章里了）想运行起来的话，还是建议跟着文章做。。 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:8","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#六总结"},{"categories":["tech"],"content":" 七、参考 Opencv-Python Tutorials - Video Playing Python 图片转字符画 允许转载, 但是要求附上来源链接:Python 视频转字符动画（一）60 行代码 ","date":"2016-10-18","objectID":"/posts/video2chars-1-basics/:0:9","series":null,"tags":[],"title":"Python 视频转字符动画（一）60 行代码","uri":"/posts/video2chars-1-basics/#七参考"},{"categories":null,"content":"\u003c!doctype html\u003e 小鹤拆字 隐私政策 小鹤拆字 隐私政策 欢迎您访问我们的产品。 小鹤拆字 （包括App等产品提供的服务，以下简称“产品和服务”）是由 This Cute World （以下简称“我们”）开发并运营的。 确保用户的数据安全和隐私保护是我们的首要任务， 本隐私政策载明了您访问和使用我们的产品和服务时所收集的数据及其处理方式。 请您在继续使用我们的产品前务必认真仔细阅读并确认充分理解本隐私政策全部规则和要点， 一旦您选择使用，即视为您同意本隐私政策的全部内容，同意我们按其收集和使用您的相关信息。 如您在在阅读过程中，对本政策有任何疑问，可联系我们的客服咨询， 请通过 xiaoyin_c@qq.com 或产品中的反馈方式与我们取得联系。 如您不同意相关协议或其中的任何条款的，您应停止使用我们的产品和服务。 本隐私政策帮助您了解以下内容： 一、我们如何收集和使用您的个人信息； 二、我们如何存储和保护您的个人信息； 三、我们如何共享、转让、公开披露您的个人信息； 一、我们如何收集和使用您的个人信息 个人信息是指以电子或者其他方式记录的能够单独或者与其他信息， 结合识别特定自然人身份或者反映特定自然人活动情况的各种信息。 我们根据《中华人民共和国网络安全法》和《信息安全技术个人信息安全规范》（GB/T 35273-2017） 以及其它相关法律法规的要求，并严格遵循正当、合法、必要的原则， 出于您使用我们提供的服务和/或产品等过程中而收集和使用您的个人信息。 二、我们如何存储和保护您的个人信息 作为一般规则，我们仅在实现信息收集目的所需的时间内保留您的个人信息。 我们会在对于管理与您之间的关系严格必要的时间内保留您的个人信息 （例如，当您开立帐户，从我们的产品获取服务时）。 出于遵守法律义务或为证明某项权利或合同满足适用的诉讼时效要求的目的， 我们可能需要在上述期限到期后保留您存档的个人信息，并且无法按您的要求删除。 当您的个人信息对于我们的法定义务或法定时效对应的目的或档案不再必要时， 我们确保将其完全删除或匿名化。 我们使用符合业界标准的安全防护措施保护您提供的个人信息，并加密其中的关键数据， 防止其遭到未经授权访问、公开披露、使用、修改、损坏或丢失。我们会采取一切合理可行的措施，保护您的个人信息。 我们会使用加密技术确保数据的保密性；我们会使用受信赖的保护机制防止数据遭到恶意攻击。 三、我们如何共享、转让、公开披露您的个人信息 在管理我们的日常业务活动所需要时，为追求合法利益以更好地服务客户， 我们将合规且恰当的使用您的个人信息。出于对业务和各个方面的综合考虑， 我们仅自身使用这些数据，不与任何第三方分享。 我们可能会根据法律法规规定，或按政府主管部门的强制性要求，对外共享您的个人信息。 在符合法律法规的前提下，当我们收到上述披露信息的请求时，我们会要求必须出具与之相应的法律文件，如传票或调查函。 我们坚信，对于要求我们提供的信息，应该在法律允许的范围内尽可能保持透明。 在以下情形中，共享、转让、公开披露您的个人信息无需事先征得您的授权同意： 1、与国家安全、国防安全直接相关的； 2、与犯罪侦查、起诉、审判和判决执行等直接相关的； 3、出于维护您或其他个人的生命、财产等重大合法权益但又很难得到本人同意的； 4、您自行向社会公众公开的个人信息； 5、从合法公开披露的信息中收集个人信息的，如合法的新闻报道、政府信息公开等渠道。 6、根据个人信息主体要求签订和履行合同所必需的； 7、用于维护所提供的产品或服务的安全稳定运行所必需的，例如发现、处置产品或服务的故障； 8、法律法规规定的其他情形。 ","date":"0001-01-01","objectID":"/flypyhelper/privacy_policy/:0:0","series":null,"tags":null,"title":"","uri":"/flypyhelper/privacy_policy/#"}]